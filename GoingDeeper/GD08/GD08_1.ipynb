{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74cf5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "import ray\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/node_data/GD08/mpii'\n",
    "MODEL_PATH = os.getenv('HOME') + '/aiffel/model_weight/GD08'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4ebdc",
   "metadata": {},
   "source": [
    "### Label 변환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d90b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368f4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e2e5bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91a7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7aebd1",
   "metadata": {},
   "source": [
    "### 모델 구축 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017028fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backborn(input_tensoro, input_shapeo, weighto):\n",
    "    backborn = ResNet50(include_top=False,\n",
    "                        weights=weighto,\n",
    "                        input_tensor=input_tensoro,\n",
    "                        input_shape=input_shapeo,\n",
    "                        pooling=None,)\n",
    "    return backborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7652cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder_Block(inputs, decording_num, decord_eliment):\n",
    "    for i in range(decording_num):\n",
    "        inputs = Conv2DTranspose(filters=decord_eliment[0],\n",
    "                                 kernel_size=decord_eliment[1],\n",
    "                                 strides=decord_eliment[2],\n",
    "                                 padding=\"same\",\n",
    "                                 kernel_initializer='he_normal')(inputs)\n",
    "        inputs = BatchNormalization()(inputs)\n",
    "        inputs = ReLU()(inputs)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49228790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleBaselineNetwork(input_shape=(256, 256, 3),\n",
    "                          weights='imagenet',\n",
    "                          decording_num=3,\n",
    "                          decord_eliment=[256,(4,4),(2,2)],\n",
    "                          num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    backborn = Backborn(inputs, input_shape, weights)\n",
    "    \n",
    "    output = Decoder_Block(backborn.output, decording_num, decord_eliment)\n",
    "    output = Conv2D(num_heatmap, (1, 1), (1, 1))(output)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8239fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "        loss = tf.math.reduce_mean(\n",
    "            tf.math.square(labels - outputs) * weights) * (1. / self.global_batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def compute_mAP(self, labels, outputs, thr=0.5):\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        outputs = tf.cast(outputs, dtype=tf.float32)\n",
    "        distances = tf.norm(labels - outputs, axis=-1)\n",
    "        correct_predictions = tf.cast(distances <= thr, tf.float32)\n",
    "        precision = tf.reduce_mean(correct_predictions, axis=0)\n",
    "        mAP = tf.reduce_mean(precision)\n",
    "        return mAP\n",
    "    \n",
    "    def compute_PCKh(self, labels, outputs, thr=0.5):\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        outputs = tf.cast(outputs, dtype=tf.float32)\n",
    "        \n",
    "        def get_coords(heatmap):\n",
    "            if len(heatmap.shape) <= 3:\n",
    "                heatmap = tf.expand_dims(heatmap, axis=-1)\n",
    "                \n",
    "            height = heatmap.shape[1]\n",
    "            widht = heatmap.shape[2]\n",
    "            classes = heatmap.shape[3]\n",
    "            \n",
    "            flattened_heatmap = tf.reshape(tensor=heatmap, shape=[8, -1, classes])\n",
    "            max_coords = tf.argmax(flattened_heatmap, axis=1)\n",
    "            \n",
    "            y = max_coords // height\n",
    "            x = max_coords % widht\n",
    "            \n",
    "            return tf.stack([x, y], axis=-2)\n",
    "        \n",
    "        label_0_coords = tf.cast(tf.squeeze(get_coords(labels[..., 0]), axis=-1), tf.float32)\n",
    "        label_1_coords = tf.cast(tf.squeeze(get_coords(labels[..., 1]), axis=-1), tf.float32)\n",
    "        head_size = tf.sqrt(tf.reduce_sum(tf.square(label_0_coords - label_1_coords)))\n",
    "        \n",
    "        predictions = []\n",
    "        for class_idx in range(labels.shape[-1]):\n",
    "            label_coords = tf.cast(tf.squeeze(get_coords(labels[..., class_idx])), tf.float32)\n",
    "            output_coords = tf.cast(tf.squeeze(get_coords(outputs[..., class_idx])), tf.float32)\n",
    "            \n",
    "            dist = tf.sqrt(tf.reduce_sum(tf.square(label_coords - output_coords)))\n",
    "            correct_predictions = tf.cast(dist <= thr * head_size, tf.float32)\n",
    "            predictions.append(correct_predictions)\n",
    "            \n",
    "        predictions = tf.stack(predictions, axis=-1)\n",
    "        \n",
    "        PCKh = tf.reduce_mean(predictions)\n",
    "        \n",
    "        return PCKh\n",
    "        \n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "            \n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        mAP = self.compute_mAP(labels, outputs)\n",
    "        PCKh = self.compute_PCKh(labels, outputs)\n",
    "        \n",
    "        return loss, mAP, PCKh\n",
    "    \n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        \n",
    "        mAP = self.compute_mAP(labels, outputs)\n",
    "        PCKh = self.compute_PCKh(labels, outputs)\n",
    "        \n",
    "        return loss, mAP, PCKh\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            total_map = 0.0\n",
    "            total_pckh = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss, per_replica_map, per_replica_pckh  = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                \n",
    "                num_train_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                batch_map = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_map, axis=None)\n",
    "                batch_pckh = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_pckh, axis=None)\n",
    "                tf.print('Trained batch', num_train_batches,\n",
    "                         'batch loss', batch_loss,\n",
    "                         'batch mAP', batch_map,\n",
    "                         'batch PCKh', batch_pckh)\n",
    "                \n",
    "                total_loss += batch_loss\n",
    "                total_map += batch_map\n",
    "                total_pckh += batch_pckh\n",
    "                \n",
    "            return total_loss, total_map, total_pckh, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            total_map = 0.0\n",
    "            total_pckh = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss, per_replica_map, per_replica_pckh = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                \n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                batch_map = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_map, axis=None)\n",
    "                batch_pckh = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_pckh, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches,\n",
    "                         'batch loss', batch_loss,\n",
    "                         'batch mAP', batch_map,\n",
    "                         'batch PCKh', batch_pckh)\n",
    "                \n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                    total_map += batch_map\n",
    "                    total_pckh += batch_pckh\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "                    \n",
    "            return total_loss, total_map, total_pckh, num_val_batches\n",
    "        \n",
    "        history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"train_map\": [],\n",
    "            \"val_map\": [],\n",
    "            \"train_pckh\": [],\n",
    "            \"val_pckh\": [],\n",
    "            \"epoch_time\": []}\n",
    "        \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            start_time = time.time()\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "            \n",
    "            train_total_loss, train_total_map, train_total_pckh, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            train_map = train_total_map / num_train_batches\n",
    "            train_pckh = train_total_pckh / num_train_batches\n",
    "            print('Epoch {} train loss {} train mAP {} train PCKh'.format(epoch, train_loss, train_map, train_pckh))\n",
    "            \n",
    "            val_total_loss, val_total_map, val_total_pckh, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            val_map = val_total_map / num_val_batches\n",
    "            val_pckh = val_total_pckh / num_val_batches\n",
    "            print('Epoch {} val loss {} val mAP {} val PCKh'.format(epoch, val_loss, val_map, val_pckh))\n",
    "            \n",
    "            end_time = time.time()\n",
    "            epoch_duration = end_time - start_time\n",
    "            print(f'Epoch {epoch} completed in {epoch_duration:.2f} seconds')\n",
    "            \n",
    "            history[\"train_loss\"].append(train_loss.numpy())\n",
    "            history[\"val_loss\"].append(val_loss.numpy())\n",
    "            history[\"train_map\"].append(train_map.numpy())\n",
    "            history[\"val_map\"].append(val_map.numpy())\n",
    "            history[\"train_pckh\"].append(train_pckh.numpy())\n",
    "            history[\"val_pckh\"].append(val_pckh.numpy())\n",
    "            history[\"epoch_time\"].append(epoch_duration)\n",
    "            \n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "            \n",
    "        return self.best_model, history\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/y_model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b56eb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f58aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = SimpleBaselineNetwork(input_shape=IMAGE_SHAPE, num_heatmap=num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ed59aea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 0s 0us/step\n",
      "94781440/94765736 [==============================] - 0s 0us/step\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.33758414 batch mAP 0 batch PCKh 0\n",
      "Trained batch 2 batch loss 1.20967841 batch mAP 0 batch PCKh 0\n",
      "Trained batch 3 batch loss 1.10014319 batch mAP 0 batch PCKh 0\n",
      "Trained batch 4 batch loss 1.00782752 batch mAP 0 batch PCKh 0\n",
      "Trained batch 5 batch loss 0.946699381 batch mAP 0 batch PCKh 0.125\n",
      "Trained batch 6 batch loss 0.957578123 batch mAP 0 batch PCKh 0.625\n",
      "Trained batch 7 batch loss 0.872984111 batch mAP 0 batch PCKh 0.25\n",
      "Trained batch 8 batch loss 0.910666823 batch mAP 0 batch PCKh 0.1875\n",
      "Trained batch 9 batch loss 0.822868109 batch mAP 0 batch PCKh 0\n",
      "Trained batch 10 batch loss 0.88401854 batch mAP 0 batch PCKh 0.25\n",
      "Trained batch 11 batch loss 0.86519134 batch mAP 0 batch PCKh 0\n",
      "Trained batch 12 batch loss 0.822047949 batch mAP 0 batch PCKh 0.25\n",
      "Trained batch 13 batch loss 0.854593098 batch mAP 0 batch PCKh 0.0625\n",
      "Trained batch 14 batch loss 0.83784771 batch mAP 0 batch PCKh 0.5\n",
      "Trained batch 15 batch loss 0.812401652 batch mAP 0 batch PCKh 0.625\n",
      "Trained batch 16 batch loss 0.787260056 batch mAP 0 batch PCKh 0.625\n",
      "Trained batch 17 batch loss 0.79837 batch mAP 0 batch PCKh 0.25\n",
      "Trained batch 18 batch loss 0.842974663 batch mAP 0 batch PCKh 0\n",
      "Trained batch 19 batch loss 0.89993161 batch mAP 0 batch PCKh 0\n",
      "Trained batch 20 batch loss 0.894107461 batch mAP 0 batch PCKh 0\n",
      "Trained batch 21 batch loss 0.852281332 batch mAP 0 batch PCKh 0\n",
      "Trained batch 22 batch loss 0.862106562 batch mAP 0 batch PCKh 0.0625\n",
      "Trained batch 23 batch loss 0.841242373 batch mAP 0 batch PCKh 0\n",
      "Trained batch 24 batch loss 0.855254591 batch mAP 0 batch PCKh 0.125\n",
      "Trained batch 25 batch loss 0.853471518 batch mAP 0 batch PCKh 0.125\n",
      "Trained batch 26 batch loss 0.855287611 batch mAP 0 batch PCKh 0.125\n",
      "Trained batch 27 batch loss 0.858453512 batch mAP 0 batch PCKh 0.0625\n",
      "Trained batch 28 batch loss 0.848545253 batch mAP 0 batch PCKh 0.375\n",
      "Trained batch 29 batch loss 0.844135 batch mAP 0 batch PCKh 0\n",
      "Trained batch 30 batch loss 0.864041 batch mAP 0 batch PCKh 0\n",
      "Trained batch 31 batch loss 0.844639301 batch mAP 3.05175781e-05 batch PCKh 0.0625\n",
      "Trained batch 32 batch loss 0.800080836 batch mAP 0 batch PCKh 0\n",
      "Trained batch 33 batch loss 0.807369 batch mAP 0 batch PCKh 0.0625\n",
      "Trained batch 34 batch loss 0.845507145 batch mAP 3.05175781e-05 batch PCKh 0.75\n",
      "Trained batch 35 batch loss 0.837584913 batch mAP 6.10351562e-05 batch PCKh 0\n",
      "Trained batch 36 batch loss 0.844082475 batch mAP 0.000640869141 batch PCKh 0\n",
      "Trained batch 37 batch loss 0.816518605 batch mAP 0.00259399414 batch PCKh 0.375\n",
      "Trained batch 38 batch loss 0.843044937 batch mAP 0.00253295898 batch PCKh 0.1875\n",
      "Trained batch 39 batch loss 0.844866812 batch mAP 0.00524902344 batch PCKh 0.5\n",
      "Trained batch 40 batch loss 0.814365864 batch mAP 0.0094909668 batch PCKh 0.375\n",
      "Trained batch 41 batch loss 0.839009225 batch mAP 0.0168457031 batch PCKh 0\n",
      "Trained batch 42 batch loss 0.821884751 batch mAP 0.0297851562 batch PCKh 0\n",
      "Trained batch 43 batch loss 0.81479156 batch mAP 0.0389099121 batch PCKh 0.5\n",
      "Trained batch 44 batch loss 0.79077667 batch mAP 0.0526123047 batch PCKh 0.875\n",
      "Trained batch 45 batch loss 0.825952768 batch mAP 0.059387207 batch PCKh 0.375\n",
      "Trained batch 46 batch loss 0.82413888 batch mAP 0.0701293945 batch PCKh 0.4375\n",
      "Trained batch 47 batch loss 0.834328413 batch mAP 0.0784912109 batch PCKh 0.0625\n",
      "Trained batch 48 batch loss 0.825697541 batch mAP 0.0983276367 batch PCKh 0.125\n",
      "Trained batch 49 batch loss 0.828602 batch mAP 0.125488281 batch PCKh 0.1875\n",
      "Trained batch 50 batch loss 0.8070153 batch mAP 0.156188965 batch PCKh 0.625\n",
      "Trained batch 51 batch loss 0.812542915 batch mAP 0.153839111 batch PCKh 0.1875\n",
      "Trained batch 52 batch loss 0.78461051 batch mAP 0.163726807 batch PCKh 0.5625\n",
      "Trained batch 53 batch loss 0.755162895 batch mAP 0.1953125 batch PCKh 0.125\n",
      "Trained batch 54 batch loss 0.792620182 batch mAP 0.194610596 batch PCKh 0.375\n",
      "Trained batch 55 batch loss 0.750320137 batch mAP 0.169494629 batch PCKh 0.1875\n",
      "Trained batch 56 batch loss 0.755754709 batch mAP 0.169036865 batch PCKh 0.125\n",
      "Trained batch 57 batch loss 0.817930102 batch mAP 0.142883301 batch PCKh 0.125\n",
      "Trained batch 58 batch loss 0.814735472 batch mAP 0.165557861 batch PCKh 0.5\n",
      "Trained batch 59 batch loss 0.815451503 batch mAP 0.169006348 batch PCKh 0.125\n",
      "Trained batch 60 batch loss 0.850148916 batch mAP 0.181091309 batch PCKh 0.0625\n",
      "Trained batch 61 batch loss 0.822860599 batch mAP 0.168609619 batch PCKh 0.5625\n",
      "Trained batch 62 batch loss 0.85292083 batch mAP 0.176086426 batch PCKh 0\n",
      "Trained batch 63 batch loss 0.878056288 batch mAP 0.167633057 batch PCKh 0.125\n",
      "Trained batch 64 batch loss 0.848265707 batch mAP 0.206665039 batch PCKh 0\n",
      "Trained batch 65 batch loss 0.830487609 batch mAP 0.104705811 batch PCKh 0.125\n",
      "Trained batch 66 batch loss 0.83023876 batch mAP 0.0869140625 batch PCKh 0\n",
      "Trained batch 67 batch loss 0.792226076 batch mAP 0.0481872559 batch PCKh 0.3125\n",
      "Trained batch 68 batch loss 0.805059433 batch mAP 0.0375366211 batch PCKh 0.125\n",
      "Trained batch 69 batch loss 0.819326282 batch mAP 0.0469665527 batch PCKh 0.125\n",
      "Trained batch 70 batch loss 0.797191858 batch mAP 0.0601806641 batch PCKh 0.125\n",
      "Trained batch 71 batch loss 0.794014215 batch mAP 0.0692749 batch PCKh 0.4375\n",
      "Trained batch 72 batch loss 0.735425472 batch mAP 0.109405518 batch PCKh 0.625\n",
      "Trained batch 73 batch loss 0.827288508 batch mAP 0.103881836 batch PCKh 0.1875\n",
      "Trained batch 74 batch loss 0.728336453 batch mAP 0.12020874 batch PCKh 0.25\n",
      "Trained batch 75 batch loss 0.787008047 batch mAP 0.131286621 batch PCKh 0\n",
      "Trained batch 76 batch loss 0.810714841 batch mAP 0.183563232 batch PCKh 0.4375\n",
      "Trained batch 77 batch loss 0.750062346 batch mAP 0.163238525 batch PCKh 0.25\n",
      "Trained batch 78 batch loss 0.832052827 batch mAP 0.175750732 batch PCKh 0.6875\n",
      "Trained batch 79 batch loss 0.836213291 batch mAP 0.216796875 batch PCKh 0.125\n",
      "Trained batch 80 batch loss 0.794190526 batch mAP 0.233642578 batch PCKh 0.375\n",
      "Trained batch 81 batch loss 0.822204232 batch mAP 0.220214844 batch PCKh 0.0625\n",
      "Trained batch 82 batch loss 0.773064494 batch mAP 0.21875 batch PCKh 0.875\n",
      "Trained batch 83 batch loss 0.790843189 batch mAP 0.21875 batch PCKh 0.3125\n",
      "Trained batch 84 batch loss 0.785324335 batch mAP 0.240448 batch PCKh 0.0625\n",
      "Trained batch 85 batch loss 0.781968176 batch mAP 0.2215271 batch PCKh 0.875\n",
      "Trained batch 86 batch loss 0.759058595 batch mAP 0.225921631 batch PCKh 0.375\n",
      "Trained batch 87 batch loss 0.767700255 batch mAP 0.163879395 batch PCKh 0.5625\n",
      "Trained batch 88 batch loss 0.775393844 batch mAP 0.188232422 batch PCKh 0.0625\n",
      "Trained batch 89 batch loss 0.821983457 batch mAP 0.136962891 batch PCKh 0.0625\n",
      "Trained batch 90 batch loss 0.689636827 batch mAP 0.212219238 batch PCKh 0.3125\n",
      "Trained batch 91 batch loss 0.725765884 batch mAP 0.235443115 batch PCKh 0.375\n",
      "Trained batch 92 batch loss 0.755682111 batch mAP 0.236114502 batch PCKh 0.4375\n",
      "Trained batch 93 batch loss 0.743557811 batch mAP 0.21081543 batch PCKh 0.625\n",
      "Trained batch 94 batch loss 0.713028073 batch mAP 0.203125 batch PCKh 0.5\n",
      "Trained batch 95 batch loss 0.665817857 batch mAP 0.183074951 batch PCKh 0.4375\n",
      "Trained batch 96 batch loss 0.670673966 batch mAP 0.18951416 batch PCKh 0.5\n",
      "Trained batch 97 batch loss 0.66294 batch mAP 0.17086792 batch PCKh 0.625\n",
      "Trained batch 98 batch loss 0.670074105 batch mAP 0.172393799 batch PCKh 0.375\n",
      "Trained batch 99 batch loss 0.730185211 batch mAP 0.174255371 batch PCKh 0.3125\n",
      "Trained batch 100 batch loss 0.675094485 batch mAP 0.20199585 batch PCKh 0.25\n",
      "Trained batch 101 batch loss 0.712218881 batch mAP 0.203094482 batch PCKh 0.4375\n",
      "Trained batch 102 batch loss 0.775497437 batch mAP 0.246856689 batch PCKh 0.6875\n",
      "Trained batch 103 batch loss 0.751042783 batch mAP 0.269775391 batch PCKh 0.5\n",
      "Trained batch 104 batch loss 0.81684 batch mAP 0.238952637 batch PCKh 0.1875\n",
      "Trained batch 105 batch loss 0.831752419 batch mAP 0.253173828 batch PCKh 0.125\n",
      "Trained batch 106 batch loss 0.802688122 batch mAP 0.240936279 batch PCKh 0.125\n",
      "Trained batch 107 batch loss 0.809721053 batch mAP 0.255981445 batch PCKh 0.1875\n",
      "Trained batch 108 batch loss 0.806161284 batch mAP 0.21105957 batch PCKh 0.25\n",
      "Trained batch 109 batch loss 0.767434895 batch mAP 0.220031738 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 110 batch loss 0.763802886 batch mAP 0.201385498 batch PCKh 0.6875\n",
      "Trained batch 111 batch loss 0.772072077 batch mAP 0.182647705 batch PCKh 0.4375\n",
      "Trained batch 112 batch loss 0.760504782 batch mAP 0.172149658 batch PCKh 0.3125\n",
      "Trained batch 113 batch loss 0.759239793 batch mAP 0.175140381 batch PCKh 0.375\n",
      "Trained batch 114 batch loss 0.753146589 batch mAP 0.197052 batch PCKh 0.5\n",
      "Trained batch 115 batch loss 0.76298213 batch mAP 0.196716309 batch PCKh 0.375\n",
      "Trained batch 116 batch loss 0.745808125 batch mAP 0.247589111 batch PCKh 0.75\n",
      "Trained batch 117 batch loss 0.694065034 batch mAP 0.232025146 batch PCKh 0.6875\n",
      "Trained batch 118 batch loss 0.647380888 batch mAP 0.249389648 batch PCKh 0.625\n",
      "Trained batch 119 batch loss 0.725767076 batch mAP 0.216125488 batch PCKh 0.6875\n",
      "Trained batch 120 batch loss 0.797588 batch mAP 0.240905762 batch PCKh 0.8125\n",
      "Trained batch 121 batch loss 0.803072 batch mAP 0.217071533 batch PCKh 0.5625\n",
      "Trained batch 122 batch loss 0.770208538 batch mAP 0.174438477 batch PCKh 0.125\n",
      "Trained batch 123 batch loss 0.750305057 batch mAP 0.17010498 batch PCKh 0.5625\n",
      "Trained batch 124 batch loss 0.769810796 batch mAP 0.165039062 batch PCKh 0.125\n",
      "Trained batch 125 batch loss 0.789933443 batch mAP 0.204589844 batch PCKh 0.5625\n",
      "Trained batch 126 batch loss 0.774929762 batch mAP 0.219207764 batch PCKh 0.1875\n",
      "Trained batch 127 batch loss 0.775213599 batch mAP 0.232208252 batch PCKh 0.1875\n",
      "Trained batch 128 batch loss 0.775973082 batch mAP 0.245880127 batch PCKh 0.125\n",
      "Trained batch 129 batch loss 0.747348309 batch mAP 0.250213623 batch PCKh 0.25\n",
      "Trained batch 130 batch loss 0.750813842 batch mAP 0.249694824 batch PCKh 0.1875\n",
      "Trained batch 131 batch loss 0.750520706 batch mAP 0.248626709 batch PCKh 0.1875\n",
      "Trained batch 132 batch loss 0.774040759 batch mAP 0.258575439 batch PCKh 0.5625\n",
      "Trained batch 133 batch loss 0.756641626 batch mAP 0.246459961 batch PCKh 0.125\n",
      "Trained batch 134 batch loss 0.77526933 batch mAP 0.257049561 batch PCKh 0\n",
      "Trained batch 135 batch loss 0.768413424 batch mAP 0.287963867 batch PCKh 0.0625\n",
      "Trained batch 136 batch loss 0.815553069 batch mAP 0.308776855 batch PCKh 0.125\n",
      "Trained batch 137 batch loss 0.751143694 batch mAP 0.312072754 batch PCKh 0.125\n",
      "Trained batch 138 batch loss 0.769807696 batch mAP 0.298095703 batch PCKh 0.25\n",
      "Trained batch 139 batch loss 0.745595 batch mAP 0.32724 batch PCKh 0.375\n",
      "Trained batch 140 batch loss 0.705187261 batch mAP 0.328033447 batch PCKh 0.3125\n",
      "Trained batch 141 batch loss 0.744228423 batch mAP 0.314086914 batch PCKh 0.4375\n",
      "Trained batch 142 batch loss 0.731681526 batch mAP 0.361694336 batch PCKh 0.375\n",
      "Trained batch 143 batch loss 0.740750253 batch mAP 0.348022461 batch PCKh 0.375\n",
      "Trained batch 144 batch loss 0.744532108 batch mAP 0.2996521 batch PCKh 0.5\n",
      "Trained batch 145 batch loss 0.694030344 batch mAP 0.316802979 batch PCKh 0.3125\n",
      "Trained batch 146 batch loss 0.757983506 batch mAP 0.21975708 batch PCKh 0.25\n",
      "Trained batch 147 batch loss 0.776545167 batch mAP 0.264373779 batch PCKh 0.25\n",
      "Trained batch 148 batch loss 0.771498561 batch mAP 0.287811279 batch PCKh 0.5\n",
      "Trained batch 149 batch loss 0.70971638 batch mAP 0.282562256 batch PCKh 0.6875\n",
      "Trained batch 150 batch loss 0.68813777 batch mAP 0.285308838 batch PCKh 0.75\n",
      "Trained batch 151 batch loss 0.706270814 batch mAP 0.297668457 batch PCKh 0.125\n",
      "Trained batch 152 batch loss 0.728984594 batch mAP 0.310760498 batch PCKh 0.625\n",
      "Trained batch 153 batch loss 0.795141816 batch mAP 0.268829346 batch PCKh 0.3125\n",
      "Trained batch 154 batch loss 0.74637866 batch mAP 0.263092041 batch PCKh 0.6875\n",
      "Trained batch 155 batch loss 0.677342951 batch mAP 0.238708496 batch PCKh 0.1875\n",
      "Trained batch 156 batch loss 0.728096247 batch mAP 0.261871338 batch PCKh 0.8125\n",
      "Trained batch 157 batch loss 0.746086717 batch mAP 0.240753174 batch PCKh 0.4375\n",
      "Trained batch 158 batch loss 0.769902408 batch mAP 0.229156494 batch PCKh 0.125\n",
      "Trained batch 159 batch loss 0.689720392 batch mAP 0.258453369 batch PCKh 0.75\n",
      "Trained batch 160 batch loss 0.658740401 batch mAP 0.249969482 batch PCKh 0\n",
      "Trained batch 161 batch loss 0.750220299 batch mAP 0.251983643 batch PCKh 0.125\n",
      "Trained batch 162 batch loss 0.739116073 batch mAP 0.249572754 batch PCKh 0.4375\n",
      "Trained batch 163 batch loss 0.774504662 batch mAP 0.263061523 batch PCKh 0.4375\n",
      "Trained batch 164 batch loss 0.667452097 batch mAP 0.273712158 batch PCKh 0.4375\n",
      "Trained batch 165 batch loss 0.707029879 batch mAP 0.30355835 batch PCKh 0.1875\n",
      "Trained batch 166 batch loss 0.73518151 batch mAP 0.314117432 batch PCKh 0.5625\n",
      "Trained batch 167 batch loss 0.71554935 batch mAP 0.315460205 batch PCKh 0\n",
      "Trained batch 168 batch loss 0.706160665 batch mAP 0.324646 batch PCKh 0.125\n",
      "Trained batch 169 batch loss 0.642656267 batch mAP 0.317749023 batch PCKh 0.0625\n",
      "Trained batch 170 batch loss 0.777589619 batch mAP 0.328613281 batch PCKh 0.375\n",
      "Trained batch 171 batch loss 0.767310739 batch mAP 0.28729248 batch PCKh 0\n",
      "Trained batch 172 batch loss 0.746819079 batch mAP 0.302886963 batch PCKh 0.625\n",
      "Trained batch 173 batch loss 0.779337764 batch mAP 0.309783936 batch PCKh 0.6875\n",
      "Trained batch 174 batch loss 0.763777733 batch mAP 0.352539062 batch PCKh 0.6875\n",
      "Trained batch 175 batch loss 0.735675395 batch mAP 0.33480835 batch PCKh 0.125\n",
      "Trained batch 176 batch loss 0.732487381 batch mAP 0.353057861 batch PCKh 0.5625\n",
      "Trained batch 177 batch loss 0.721013486 batch mAP 0.372406 batch PCKh 0.3125\n",
      "Trained batch 178 batch loss 0.709111691 batch mAP 0.38079834 batch PCKh 0.375\n",
      "Trained batch 179 batch loss 0.654139817 batch mAP 0.373748779 batch PCKh 0.0625\n",
      "Trained batch 180 batch loss 0.577046275 batch mAP 0.364227295 batch PCKh 0.25\n",
      "Trained batch 181 batch loss 0.610934615 batch mAP 0.359222412 batch PCKh 0\n",
      "Trained batch 182 batch loss 0.676751375 batch mAP 0.385528564 batch PCKh 0.3125\n",
      "Trained batch 183 batch loss 0.672947943 batch mAP 0.392730713 batch PCKh 0\n",
      "Trained batch 184 batch loss 0.756830573 batch mAP 0.396179199 batch PCKh 0.3125\n",
      "Trained batch 185 batch loss 0.704072177 batch mAP 0.398681641 batch PCKh 0.4375\n",
      "Trained batch 186 batch loss 0.674450874 batch mAP 0.39440918 batch PCKh 0.4375\n",
      "Trained batch 187 batch loss 0.70560956 batch mAP 0.418640137 batch PCKh 0.1875\n",
      "Trained batch 188 batch loss 0.742239237 batch mAP 0.436553955 batch PCKh 0.4375\n",
      "Trained batch 189 batch loss 0.693540335 batch mAP 0.402191162 batch PCKh 0.1875\n",
      "Trained batch 190 batch loss 0.663818359 batch mAP 0.387329102 batch PCKh 0.0625\n",
      "Trained batch 191 batch loss 0.752766371 batch mAP 0.365997314 batch PCKh 0.25\n",
      "Trained batch 192 batch loss 0.752617657 batch mAP 0.38394165 batch PCKh 0\n",
      "Trained batch 193 batch loss 0.72956121 batch mAP 0.359375 batch PCKh 0.625\n",
      "Trained batch 194 batch loss 0.712234914 batch mAP 0.350463867 batch PCKh 0\n",
      "Trained batch 195 batch loss 0.737649083 batch mAP 0.411743164 batch PCKh 0.375\n",
      "Trained batch 196 batch loss 0.716417074 batch mAP 0.372253418 batch PCKh 0.0625\n",
      "Trained batch 197 batch loss 0.729393959 batch mAP 0.352539062 batch PCKh 0.3125\n",
      "Trained batch 198 batch loss 0.726953506 batch mAP 0.389648438 batch PCKh 0.25\n",
      "Trained batch 199 batch loss 0.738683224 batch mAP 0.341156 batch PCKh 0.375\n",
      "Trained batch 200 batch loss 0.748840094 batch mAP 0.360076904 batch PCKh 0.375\n",
      "Trained batch 201 batch loss 0.750661 batch mAP 0.33972168 batch PCKh 0.4375\n",
      "Trained batch 202 batch loss 0.783942699 batch mAP 0.355377197 batch PCKh 0.0625\n",
      "Trained batch 203 batch loss 0.656451046 batch mAP 0.365478516 batch PCKh 0\n",
      "Trained batch 204 batch loss 0.629079223 batch mAP 0.389129639 batch PCKh 0\n",
      "Trained batch 205 batch loss 0.572257519 batch mAP 0.397460938 batch PCKh 0\n",
      "Trained batch 206 batch loss 0.494006366 batch mAP 0.391052246 batch PCKh 0\n",
      "Trained batch 207 batch loss 0.531744659 batch mAP 0.383667 batch PCKh 0\n",
      "Trained batch 208 batch loss 0.669892311 batch mAP 0.394165039 batch PCKh 0.5625\n",
      "Trained batch 209 batch loss 0.755096912 batch mAP 0.391387939 batch PCKh 0.25\n",
      "Trained batch 210 batch loss 0.825719655 batch mAP 0.379211426 batch PCKh 0\n",
      "Trained batch 211 batch loss 0.836998701 batch mAP 0.425292969 batch PCKh 0\n",
      "Trained batch 212 batch loss 0.861352861 batch mAP 0.413024902 batch PCKh 0\n",
      "Trained batch 213 batch loss 0.867399 batch mAP 0.356628418 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 214 batch loss 0.823729813 batch mAP 0.332550049 batch PCKh 0\n",
      "Trained batch 215 batch loss 0.831636727 batch mAP 0.332611084 batch PCKh 0\n",
      "Trained batch 216 batch loss 0.822733402 batch mAP 0.329437256 batch PCKh 0.125\n",
      "Trained batch 217 batch loss 0.763721824 batch mAP 0.319396973 batch PCKh 0.3125\n",
      "Trained batch 218 batch loss 0.783037663 batch mAP 0.308441162 batch PCKh 0.1875\n",
      "Trained batch 219 batch loss 0.77417922 batch mAP 0.292724609 batch PCKh 0.0625\n",
      "Trained batch 220 batch loss 0.790216506 batch mAP 0.276733398 batch PCKh 0.4375\n",
      "Trained batch 221 batch loss 0.797048569 batch mAP 0.266967773 batch PCKh 0.8125\n",
      "Trained batch 222 batch loss 0.80104655 batch mAP 0.249908447 batch PCKh 0.1875\n",
      "Trained batch 223 batch loss 0.781533 batch mAP 0.260894775 batch PCKh 0.5\n",
      "Trained batch 224 batch loss 0.777770162 batch mAP 0.252105713 batch PCKh 0.0625\n",
      "Trained batch 225 batch loss 0.754687071 batch mAP 0.2394104 batch PCKh 0\n",
      "Trained batch 226 batch loss 0.770722151 batch mAP 0.216552734 batch PCKh 0.75\n",
      "Trained batch 227 batch loss 0.662043393 batch mAP 0.211730957 batch PCKh 0\n",
      "Trained batch 228 batch loss 0.653650522 batch mAP 0.216125488 batch PCKh 0.25\n",
      "Trained batch 229 batch loss 0.627414584 batch mAP 0.234771729 batch PCKh 0.25\n",
      "Trained batch 230 batch loss 0.672157466 batch mAP 0.201385498 batch PCKh 0.4375\n",
      "Trained batch 231 batch loss 0.749733 batch mAP 0.201660156 batch PCKh 0.625\n",
      "Trained batch 232 batch loss 0.841389 batch mAP 0.194671631 batch PCKh 0.0625\n",
      "Trained batch 233 batch loss 0.840913892 batch mAP 0.208648682 batch PCKh 0.0625\n",
      "Trained batch 234 batch loss 0.795925736 batch mAP 0.190338135 batch PCKh 0.0625\n",
      "Trained batch 235 batch loss 0.779403687 batch mAP 0.198028564 batch PCKh 0.0625\n",
      "Trained batch 236 batch loss 0.81757468 batch mAP 0.177307129 batch PCKh 0.3125\n",
      "Trained batch 237 batch loss 0.835096717 batch mAP 0.208648682 batch PCKh 0\n",
      "Trained batch 238 batch loss 0.816316783 batch mAP 0.225250244 batch PCKh 0\n",
      "Trained batch 239 batch loss 0.811721265 batch mAP 0.240936279 batch PCKh 0.375\n",
      "Trained batch 240 batch loss 0.803057313 batch mAP 0.242980957 batch PCKh 0.125\n",
      "Trained batch 241 batch loss 0.764794111 batch mAP 0.269073486 batch PCKh 0.375\n",
      "Trained batch 242 batch loss 0.804528475 batch mAP 0.302520752 batch PCKh 0.1875\n",
      "Trained batch 243 batch loss 0.757912159 batch mAP 0.311248779 batch PCKh 0.4375\n",
      "Trained batch 244 batch loss 0.787381 batch mAP 0.321655273 batch PCKh 0\n",
      "Trained batch 245 batch loss 0.783670664 batch mAP 0.301727295 batch PCKh 0.125\n",
      "Trained batch 246 batch loss 0.782774329 batch mAP 0.292358398 batch PCKh 0.625\n",
      "Trained batch 247 batch loss 0.733490527 batch mAP 0.339324951 batch PCKh 0.875\n",
      "Trained batch 248 batch loss 0.758517504 batch mAP 0.311950684 batch PCKh 0.6875\n",
      "Trained batch 249 batch loss 0.715680897 batch mAP 0.30380249 batch PCKh 0.5625\n",
      "Trained batch 250 batch loss 0.791441321 batch mAP 0.305633545 batch PCKh 0\n",
      "Trained batch 251 batch loss 0.793857813 batch mAP 0.311309814 batch PCKh 0.25\n",
      "Trained batch 252 batch loss 0.736256 batch mAP 0.283721924 batch PCKh 0.375\n",
      "Trained batch 253 batch loss 0.744044 batch mAP 0.283966064 batch PCKh 0.6875\n",
      "Trained batch 254 batch loss 0.713608742 batch mAP 0.29208374 batch PCKh 0.375\n",
      "Trained batch 255 batch loss 0.775980055 batch mAP 0.288269043 batch PCKh 0.0625\n",
      "Trained batch 256 batch loss 0.790244222 batch mAP 0.271759033 batch PCKh 0.75\n",
      "Trained batch 257 batch loss 0.756860852 batch mAP 0.25378418 batch PCKh 0\n",
      "Trained batch 258 batch loss 0.709315419 batch mAP 0.26852417 batch PCKh 0.625\n",
      "Trained batch 259 batch loss 0.747926712 batch mAP 0.28125 batch PCKh 0\n",
      "Trained batch 260 batch loss 0.729268909 batch mAP 0.299835205 batch PCKh 0.3125\n",
      "Trained batch 261 batch loss 0.705064535 batch mAP 0.297546387 batch PCKh 0.6875\n",
      "Trained batch 262 batch loss 0.718826175 batch mAP 0.294067383 batch PCKh 0\n",
      "Trained batch 263 batch loss 0.72198391 batch mAP 0.26159668 batch PCKh 0\n",
      "Trained batch 264 batch loss 0.716998219 batch mAP 0.278198242 batch PCKh 0.375\n",
      "Trained batch 265 batch loss 0.704082251 batch mAP 0.329315186 batch PCKh 0.1875\n",
      "Trained batch 266 batch loss 0.718291402 batch mAP 0.313262939 batch PCKh 0.3125\n",
      "Trained batch 267 batch loss 0.698062479 batch mAP 0.35559082 batch PCKh 0.6875\n",
      "Trained batch 268 batch loss 0.746994793 batch mAP 0.321228027 batch PCKh 0.6875\n",
      "Trained batch 269 batch loss 0.7178092 batch mAP 0.335723877 batch PCKh 0.75\n",
      "Trained batch 270 batch loss 0.770360112 batch mAP 0.295562744 batch PCKh 0.125\n",
      "Trained batch 271 batch loss 0.745767951 batch mAP 0.302825928 batch PCKh 0\n",
      "Trained batch 272 batch loss 0.79227978 batch mAP 0.309967041 batch PCKh 0\n",
      "Trained batch 273 batch loss 0.784120679 batch mAP 0.339141846 batch PCKh 0.8125\n",
      "Trained batch 274 batch loss 0.700268149 batch mAP 0.336517334 batch PCKh 0.5625\n",
      "Trained batch 275 batch loss 0.72750783 batch mAP 0.341644287 batch PCKh 0.5625\n",
      "Trained batch 276 batch loss 0.704636276 batch mAP 0.35849 batch PCKh 0.1875\n",
      "Trained batch 277 batch loss 0.598720372 batch mAP 0.331665039 batch PCKh 0\n",
      "Trained batch 278 batch loss 0.626083374 batch mAP 0.370483398 batch PCKh 0.25\n",
      "Trained batch 279 batch loss 0.694073558 batch mAP 0.374145508 batch PCKh 0.4375\n",
      "Trained batch 280 batch loss 0.6237275 batch mAP 0.387634277 batch PCKh 0.0625\n",
      "Trained batch 281 batch loss 0.685837746 batch mAP 0.363586426 batch PCKh 0.3125\n",
      "Trained batch 282 batch loss 0.73262316 batch mAP 0.384185791 batch PCKh 0.75\n",
      "Trained batch 283 batch loss 0.733017504 batch mAP 0.406768799 batch PCKh 0.625\n",
      "Trained batch 284 batch loss 0.748548269 batch mAP 0.378265381 batch PCKh 0.375\n",
      "Trained batch 285 batch loss 0.734539807 batch mAP 0.366546631 batch PCKh 0\n",
      "Trained batch 286 batch loss 0.730660379 batch mAP 0.387054443 batch PCKh 0.25\n",
      "Trained batch 287 batch loss 0.800881624 batch mAP 0.378173828 batch PCKh 0.1875\n",
      "Trained batch 288 batch loss 0.744060278 batch mAP 0.373046875 batch PCKh 0.5625\n",
      "Trained batch 289 batch loss 0.726804137 batch mAP 0.384735107 batch PCKh 0.125\n",
      "Trained batch 290 batch loss 0.729662061 batch mAP 0.354370117 batch PCKh 0.5\n",
      "Trained batch 291 batch loss 0.769468606 batch mAP 0.350189209 batch PCKh 0.5625\n",
      "Trained batch 292 batch loss 0.744015396 batch mAP 0.405151367 batch PCKh 0.875\n",
      "Trained batch 293 batch loss 0.726850271 batch mAP 0.401550293 batch PCKh 0.3125\n",
      "Trained batch 294 batch loss 0.687769532 batch mAP 0.369812 batch PCKh 0\n",
      "Trained batch 295 batch loss 0.731536508 batch mAP 0.414520264 batch PCKh 0.5\n",
      "Trained batch 296 batch loss 0.699137866 batch mAP 0.393188477 batch PCKh 0.5625\n",
      "Trained batch 297 batch loss 0.709455252 batch mAP 0.381774902 batch PCKh 0\n",
      "Trained batch 298 batch loss 0.731459856 batch mAP 0.422515869 batch PCKh 0.25\n",
      "Trained batch 299 batch loss 0.757181644 batch mAP 0.37701416 batch PCKh 0.375\n",
      "Trained batch 300 batch loss 0.685552359 batch mAP 0.354034424 batch PCKh 0\n",
      "Trained batch 301 batch loss 0.748441339 batch mAP 0.38949585 batch PCKh 0.625\n",
      "Trained batch 302 batch loss 0.73643744 batch mAP 0.354217529 batch PCKh 0.4375\n",
      "Trained batch 303 batch loss 0.837383151 batch mAP 0.337585449 batch PCKh 0\n",
      "Trained batch 304 batch loss 0.723071456 batch mAP 0.316558838 batch PCKh 0.25\n",
      "Trained batch 305 batch loss 0.724930406 batch mAP 0.304077148 batch PCKh 0.4375\n",
      "Trained batch 306 batch loss 0.730569184 batch mAP 0.301544189 batch PCKh 0.125\n",
      "Trained batch 307 batch loss 0.7154 batch mAP 0.315216064 batch PCKh 0.625\n",
      "Trained batch 308 batch loss 0.688483357 batch mAP 0.33392334 batch PCKh 0.1875\n",
      "Trained batch 309 batch loss 0.711045921 batch mAP 0.359741211 batch PCKh 0.6875\n",
      "Trained batch 310 batch loss 0.729224741 batch mAP 0.346130371 batch PCKh 0.1875\n",
      "Trained batch 311 batch loss 0.649513 batch mAP 0.349487305 batch PCKh 0.1875\n",
      "Trained batch 312 batch loss 0.696347475 batch mAP 0.39553833 batch PCKh 0.1875\n",
      "Trained batch 313 batch loss 0.728600264 batch mAP 0.398590088 batch PCKh 0.25\n",
      "Trained batch 314 batch loss 0.735199928 batch mAP 0.412689209 batch PCKh 0.375\n",
      "Trained batch 315 batch loss 0.701247871 batch mAP 0.399475098 batch PCKh 0.375\n",
      "Trained batch 316 batch loss 0.694794774 batch mAP 0.408905029 batch PCKh 0.375\n",
      "Trained batch 317 batch loss 0.68666172 batch mAP 0.397888184 batch PCKh 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 318 batch loss 0.729861498 batch mAP 0.349945068 batch PCKh 0.3125\n",
      "Trained batch 319 batch loss 0.698553 batch mAP 0.361450195 batch PCKh 0.25\n",
      "Trained batch 320 batch loss 0.732288718 batch mAP 0.37991333 batch PCKh 0.875\n",
      "Trained batch 321 batch loss 0.722887218 batch mAP 0.370910645 batch PCKh 0.8125\n",
      "Trained batch 322 batch loss 0.721034169 batch mAP 0.368286133 batch PCKh 0.125\n",
      "Trained batch 323 batch loss 0.689430654 batch mAP 0.367706299 batch PCKh 0.4375\n",
      "Trained batch 324 batch loss 0.707820773 batch mAP 0.326843262 batch PCKh 0.3125\n",
      "Trained batch 325 batch loss 0.705363035 batch mAP 0.340148926 batch PCKh 0.1875\n",
      "Trained batch 326 batch loss 0.670104861 batch mAP 0.299804688 batch PCKh 0\n",
      "Trained batch 327 batch loss 0.71503818 batch mAP 0.366424561 batch PCKh 0.625\n",
      "Trained batch 328 batch loss 0.685009956 batch mAP 0.331726074 batch PCKh 0.0625\n",
      "Trained batch 329 batch loss 0.60736084 batch mAP 0.33392334 batch PCKh 0\n",
      "Trained batch 330 batch loss 0.788749039 batch mAP 0.335449219 batch PCKh 0.625\n",
      "Trained batch 331 batch loss 0.797993064 batch mAP 0.336547852 batch PCKh 0.375\n",
      "Trained batch 332 batch loss 0.825876951 batch mAP 0.34564209 batch PCKh 0.125\n",
      "Trained batch 333 batch loss 0.826054335 batch mAP 0.385772705 batch PCKh 0.25\n",
      "Trained batch 334 batch loss 0.804398894 batch mAP 0.455535889 batch PCKh 0.4375\n",
      "Trained batch 335 batch loss 0.72793752 batch mAP 0.432983398 batch PCKh 0.625\n",
      "Trained batch 336 batch loss 0.715472817 batch mAP 0.424469 batch PCKh 0.0625\n",
      "Trained batch 337 batch loss 0.799806 batch mAP 0.389953613 batch PCKh 0\n",
      "Trained batch 338 batch loss 0.822679222 batch mAP 0.384521484 batch PCKh 0\n",
      "Trained batch 339 batch loss 0.847914338 batch mAP 0.375915527 batch PCKh 0\n",
      "Trained batch 340 batch loss 0.804833114 batch mAP 0.338684082 batch PCKh 0\n",
      "Trained batch 341 batch loss 0.783649266 batch mAP 0.317962646 batch PCKh 0.3125\n",
      "Trained batch 342 batch loss 0.749652565 batch mAP 0.231445312 batch PCKh 0.0625\n",
      "Trained batch 343 batch loss 0.765081465 batch mAP 0.19140625 batch PCKh 0\n",
      "Trained batch 344 batch loss 0.791519046 batch mAP 0.173156738 batch PCKh 0\n",
      "Trained batch 345 batch loss 0.769376278 batch mAP 0.175384521 batch PCKh 0.75\n",
      "Trained batch 346 batch loss 0.706257164 batch mAP 0.206695557 batch PCKh 0.75\n",
      "Trained batch 347 batch loss 0.745234311 batch mAP 0.215637207 batch PCKh 0.4375\n",
      "Trained batch 348 batch loss 0.747666359 batch mAP 0.240722656 batch PCKh 0.25\n",
      "Trained batch 349 batch loss 0.757039249 batch mAP 0.263763428 batch PCKh 0.5\n",
      "Trained batch 350 batch loss 0.769988954 batch mAP 0.287078857 batch PCKh 0.5625\n",
      "Trained batch 351 batch loss 0.749522924 batch mAP 0.307342529 batch PCKh 0.25\n",
      "Trained batch 352 batch loss 0.711504519 batch mAP 0.33190918 batch PCKh 0.125\n",
      "Trained batch 353 batch loss 0.678062081 batch mAP 0.313903809 batch PCKh 0.4375\n",
      "Trained batch 354 batch loss 0.686145067 batch mAP 0.32144165 batch PCKh 0.75\n",
      "Trained batch 355 batch loss 0.746783614 batch mAP 0.342956543 batch PCKh 0.4375\n",
      "Trained batch 356 batch loss 0.723987639 batch mAP 0.349243164 batch PCKh 0.75\n",
      "Trained batch 357 batch loss 0.788682103 batch mAP 0.347961426 batch PCKh 0.6875\n",
      "Trained batch 358 batch loss 0.747820139 batch mAP 0.335662842 batch PCKh 0.375\n",
      "Trained batch 359 batch loss 0.690576315 batch mAP 0.348083496 batch PCKh 0.375\n",
      "Trained batch 360 batch loss 0.696978092 batch mAP 0.396362305 batch PCKh 0.625\n",
      "Trained batch 361 batch loss 0.715607166 batch mAP 0.352813721 batch PCKh 0.375\n",
      "Trained batch 362 batch loss 0.697039962 batch mAP 0.363372803 batch PCKh 0.1875\n",
      "Trained batch 363 batch loss 0.712199569 batch mAP 0.368408203 batch PCKh 0.3125\n",
      "Trained batch 364 batch loss 0.603671432 batch mAP 0.35446167 batch PCKh 0.3125\n",
      "Trained batch 365 batch loss 0.690956295 batch mAP 0.347351074 batch PCKh 0.6875\n",
      "Trained batch 366 batch loss 0.704789 batch mAP 0.347930908 batch PCKh 0.4375\n",
      "Trained batch 367 batch loss 0.70430249 batch mAP 0.354522705 batch PCKh 0\n",
      "Trained batch 368 batch loss 0.707441 batch mAP 0.344970703 batch PCKh 0.0625\n",
      "Trained batch 369 batch loss 0.698212087 batch mAP 0.365844727 batch PCKh 0.6875\n",
      "Trained batch 370 batch loss 0.73065722 batch mAP 0.358337402 batch PCKh 0.3125\n",
      "Trained batch 371 batch loss 0.699944258 batch mAP 0.348938 batch PCKh 0.6875\n",
      "Trained batch 372 batch loss 0.774268746 batch mAP 0.310028076 batch PCKh 0\n",
      "Trained batch 373 batch loss 0.755379319 batch mAP 0.308288574 batch PCKh 0.75\n",
      "Trained batch 374 batch loss 0.776825845 batch mAP 0.310760498 batch PCKh 0.625\n",
      "Trained batch 375 batch loss 0.813250422 batch mAP 0.309631348 batch PCKh 0.125\n",
      "Trained batch 376 batch loss 0.813318908 batch mAP 0.317138672 batch PCKh 0.25\n",
      "Trained batch 377 batch loss 0.825614095 batch mAP 0.328369141 batch PCKh 0.1875\n",
      "Trained batch 378 batch loss 0.777274132 batch mAP 0.314208984 batch PCKh 0.5\n",
      "Trained batch 379 batch loss 0.78461653 batch mAP 0.325195312 batch PCKh 0.25\n",
      "Trained batch 380 batch loss 0.758397341 batch mAP 0.310394287 batch PCKh 0\n",
      "Trained batch 381 batch loss 0.718328476 batch mAP 0.26751709 batch PCKh 0.375\n",
      "Trained batch 382 batch loss 0.693758965 batch mAP 0.254516602 batch PCKh 0.6875\n",
      "Trained batch 383 batch loss 0.653828204 batch mAP 0.222106934 batch PCKh 0.5625\n",
      "Trained batch 384 batch loss 0.595003366 batch mAP 0.240539551 batch PCKh 0\n",
      "Trained batch 385 batch loss 0.639787197 batch mAP 0.24710083 batch PCKh 0.3125\n",
      "Trained batch 386 batch loss 0.612648368 batch mAP 0.249359131 batch PCKh 0.125\n",
      "Trained batch 387 batch loss 0.585916281 batch mAP 0.256774902 batch PCKh 0.4375\n",
      "Trained batch 388 batch loss 0.573593736 batch mAP 0.265655518 batch PCKh 0.6875\n",
      "Trained batch 389 batch loss 0.614371955 batch mAP 0.273742676 batch PCKh 0.625\n",
      "Trained batch 390 batch loss 0.592415452 batch mAP 0.332214355 batch PCKh 0.375\n",
      "Trained batch 391 batch loss 0.599690735 batch mAP 0.331878662 batch PCKh 0.4375\n",
      "Trained batch 392 batch loss 0.485613853 batch mAP 0.321685791 batch PCKh 0\n",
      "Trained batch 393 batch loss 0.595614195 batch mAP 0.315704346 batch PCKh 0.375\n",
      "Trained batch 394 batch loss 0.570815146 batch mAP 0.351501465 batch PCKh 0.25\n",
      "Trained batch 395 batch loss 0.691158175 batch mAP 0.321838379 batch PCKh 0.25\n",
      "Trained batch 396 batch loss 0.758532524 batch mAP 0.321472168 batch PCKh 0.625\n",
      "Trained batch 397 batch loss 0.72473675 batch mAP 0.327941895 batch PCKh 0.25\n",
      "Trained batch 398 batch loss 0.657137752 batch mAP 0.350463867 batch PCKh 0.75\n",
      "Trained batch 399 batch loss 0.707083821 batch mAP 0.30859375 batch PCKh 0.25\n",
      "Trained batch 400 batch loss 0.673134 batch mAP 0.295196533 batch PCKh 0.1875\n",
      "Trained batch 401 batch loss 0.748159885 batch mAP 0.344177246 batch PCKh 0.3125\n",
      "Trained batch 402 batch loss 0.69691962 batch mAP 0.304260254 batch PCKh 0.375\n",
      "Trained batch 403 batch loss 0.700615644 batch mAP 0.324157715 batch PCKh 0.375\n",
      "Trained batch 404 batch loss 0.69436729 batch mAP 0.356903076 batch PCKh 0.3125\n",
      "Trained batch 405 batch loss 0.698663831 batch mAP 0.289825439 batch PCKh 0.1875\n",
      "Trained batch 406 batch loss 0.687522 batch mAP 0.322967529 batch PCKh 0.5625\n",
      "Trained batch 407 batch loss 0.760201395 batch mAP 0.296417236 batch PCKh 0.1875\n",
      "Trained batch 408 batch loss 0.761284649 batch mAP 0.299224854 batch PCKh 0.4375\n",
      "Trained batch 409 batch loss 0.697630882 batch mAP 0.37713623 batch PCKh 0.4375\n",
      "Trained batch 410 batch loss 0.706439555 batch mAP 0.338378906 batch PCKh 0.375\n",
      "Trained batch 411 batch loss 0.826895475 batch mAP 0.328063965 batch PCKh 0.25\n",
      "Trained batch 412 batch loss 0.706870914 batch mAP 0.342712402 batch PCKh 0.1875\n",
      "Trained batch 413 batch loss 0.66301024 batch mAP 0.314361572 batch PCKh 0.1875\n",
      "Trained batch 414 batch loss 0.627954066 batch mAP 0.311187744 batch PCKh 0.625\n",
      "Trained batch 415 batch loss 0.706594229 batch mAP 0.264587402 batch PCKh 0.4375\n",
      "Trained batch 416 batch loss 0.691606045 batch mAP 0.259033203 batch PCKh 0\n",
      "Trained batch 417 batch loss 0.683223248 batch mAP 0.229614258 batch PCKh 0.5625\n",
      "Trained batch 418 batch loss 0.692330599 batch mAP 0.228912354 batch PCKh 0.8125\n",
      "Trained batch 419 batch loss 0.659584 batch mAP 0.236938477 batch PCKh 0.4375\n",
      "Trained batch 420 batch loss 0.691781938 batch mAP 0.242370605 batch PCKh 0.125\n",
      "Trained batch 421 batch loss 0.76252985 batch mAP 0.260467529 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 422 batch loss 0.708992839 batch mAP 0.280517578 batch PCKh 0.8125\n",
      "Trained batch 423 batch loss 0.718271315 batch mAP 0.290679932 batch PCKh 0.3125\n",
      "Trained batch 424 batch loss 0.780736923 batch mAP 0.300170898 batch PCKh 0.4375\n",
      "Trained batch 425 batch loss 0.78914535 batch mAP 0.272888184 batch PCKh 0.25\n",
      "Trained batch 426 batch loss 0.83372587 batch mAP 0.268737793 batch PCKh 0.25\n",
      "Trained batch 427 batch loss 0.755536675 batch mAP 0.234588623 batch PCKh 0.3125\n",
      "Trained batch 428 batch loss 0.623285472 batch mAP 0.202514648 batch PCKh 0.5\n",
      "Trained batch 429 batch loss 0.682589293 batch mAP 0.161529541 batch PCKh 0.5625\n",
      "Trained batch 430 batch loss 0.706116676 batch mAP 0.143371582 batch PCKh 0.75\n",
      "Trained batch 431 batch loss 0.672939301 batch mAP 0.159118652 batch PCKh 0.5625\n",
      "Trained batch 432 batch loss 0.623710513 batch mAP 0.163818359 batch PCKh 0.5625\n",
      "Trained batch 433 batch loss 0.700950086 batch mAP 0.150817871 batch PCKh 0.25\n",
      "Trained batch 434 batch loss 0.697441936 batch mAP 0.200439453 batch PCKh 0.3125\n",
      "Trained batch 435 batch loss 0.622653842 batch mAP 0.200744629 batch PCKh 0.5625\n",
      "Trained batch 436 batch loss 0.751404166 batch mAP 0.206451416 batch PCKh 0.75\n",
      "Trained batch 437 batch loss 0.763138592 batch mAP 0.253051758 batch PCKh 0.4375\n",
      "Trained batch 438 batch loss 0.72655344 batch mAP 0.229827881 batch PCKh 0.25\n",
      "Trained batch 439 batch loss 0.712973237 batch mAP 0.224853516 batch PCKh 0.375\n",
      "Trained batch 440 batch loss 0.692999601 batch mAP 0.222198486 batch PCKh 0.3125\n",
      "Trained batch 441 batch loss 0.72895509 batch mAP 0.260162354 batch PCKh 0.0625\n",
      "Trained batch 442 batch loss 0.785240412 batch mAP 0.267608643 batch PCKh 0.3125\n",
      "Trained batch 443 batch loss 0.757857919 batch mAP 0.300018311 batch PCKh 0.25\n",
      "Trained batch 444 batch loss 0.76877445 batch mAP 0.299041748 batch PCKh 0.125\n",
      "Trained batch 445 batch loss 0.766289592 batch mAP 0.309631348 batch PCKh 0.1875\n",
      "Trained batch 446 batch loss 0.761848211 batch mAP 0.294555664 batch PCKh 0.0625\n",
      "Trained batch 447 batch loss 0.735465348 batch mAP 0.329101562 batch PCKh 0.1875\n",
      "Trained batch 448 batch loss 0.748692036 batch mAP 0.314544678 batch PCKh 0.3125\n",
      "Trained batch 449 batch loss 0.700276196 batch mAP 0.33380127 batch PCKh 0.875\n",
      "Trained batch 450 batch loss 0.698296428 batch mAP 0.332183838 batch PCKh 0.6875\n",
      "Trained batch 451 batch loss 0.720956326 batch mAP 0.327423096 batch PCKh 0.6875\n",
      "Trained batch 452 batch loss 0.721919656 batch mAP 0.314941406 batch PCKh 0.625\n",
      "Trained batch 453 batch loss 0.734284282 batch mAP 0.271972656 batch PCKh 0\n",
      "Trained batch 454 batch loss 0.775465608 batch mAP 0.274353027 batch PCKh 0.3125\n",
      "Trained batch 455 batch loss 0.785345435 batch mAP 0.299377441 batch PCKh 0\n",
      "Trained batch 456 batch loss 0.735573709 batch mAP 0.350219727 batch PCKh 0.6875\n",
      "Trained batch 457 batch loss 0.672356844 batch mAP 0.357543945 batch PCKh 0.1875\n",
      "Trained batch 458 batch loss 0.727116644 batch mAP 0.295623779 batch PCKh 0.5\n",
      "Trained batch 459 batch loss 0.700471461 batch mAP 0.337982178 batch PCKh 0.25\n",
      "Trained batch 460 batch loss 0.686759591 batch mAP 0.33605957 batch PCKh 0.1875\n",
      "Trained batch 461 batch loss 0.732154727 batch mAP 0.309417725 batch PCKh 0\n",
      "Trained batch 462 batch loss 0.709577203 batch mAP 0.273040771 batch PCKh 0.625\n",
      "Trained batch 463 batch loss 0.71842289 batch mAP 0.288360596 batch PCKh 0.5\n",
      "Trained batch 464 batch loss 0.623465061 batch mAP 0.28805542 batch PCKh 0\n",
      "Trained batch 465 batch loss 0.686418056 batch mAP 0.298614502 batch PCKh 0.125\n",
      "Trained batch 466 batch loss 0.604088426 batch mAP 0.295257568 batch PCKh 0\n",
      "Trained batch 467 batch loss 0.710218251 batch mAP 0.303039551 batch PCKh 0.125\n",
      "Trained batch 468 batch loss 0.790142417 batch mAP 0.276000977 batch PCKh 0.4375\n",
      "Trained batch 469 batch loss 0.763154149 batch mAP 0.301116943 batch PCKh 0.0625\n",
      "Trained batch 470 batch loss 0.710882783 batch mAP 0.291412354 batch PCKh 0.3125\n",
      "Trained batch 471 batch loss 0.753559828 batch mAP 0.294311523 batch PCKh 0.5\n",
      "Trained batch 472 batch loss 0.735664308 batch mAP 0.27444458 batch PCKh 0.1875\n",
      "Trained batch 473 batch loss 0.684493423 batch mAP 0.278839111 batch PCKh 0.5\n",
      "Trained batch 474 batch loss 0.651292801 batch mAP 0.29284668 batch PCKh 0.75\n",
      "Trained batch 475 batch loss 0.628822565 batch mAP 0.261566162 batch PCKh 0.5\n",
      "Trained batch 476 batch loss 0.634706497 batch mAP 0.318939209 batch PCKh 0.5\n",
      "Trained batch 477 batch loss 0.737388372 batch mAP 0.317321777 batch PCKh 0.625\n",
      "Trained batch 478 batch loss 0.764460683 batch mAP 0.305938721 batch PCKh 0.4375\n",
      "Trained batch 479 batch loss 0.69039464 batch mAP 0.330810547 batch PCKh 0.375\n",
      "Trained batch 480 batch loss 0.733366907 batch mAP 0.288604736 batch PCKh 0.5\n",
      "Trained batch 481 batch loss 0.730651498 batch mAP 0.319885254 batch PCKh 0.125\n",
      "Trained batch 482 batch loss 0.706920326 batch mAP 0.284790039 batch PCKh 0.5\n",
      "Trained batch 483 batch loss 0.738022447 batch mAP 0.307067871 batch PCKh 0.0625\n",
      "Trained batch 484 batch loss 0.724950731 batch mAP 0.34576416 batch PCKh 0.125\n",
      "Trained batch 485 batch loss 0.701434791 batch mAP 0.333618164 batch PCKh 0.125\n",
      "Trained batch 486 batch loss 0.679613888 batch mAP 0.339599609 batch PCKh 0.125\n",
      "Trained batch 487 batch loss 0.805672586 batch mAP 0.364501953 batch PCKh 0\n",
      "Trained batch 488 batch loss 0.806368589 batch mAP 0.364563 batch PCKh 0\n",
      "Trained batch 489 batch loss 0.78805 batch mAP 0.374145508 batch PCKh 0.125\n",
      "Trained batch 490 batch loss 0.710341811 batch mAP 0.429718018 batch PCKh 0.5625\n",
      "Trained batch 491 batch loss 0.599142194 batch mAP 0.419219971 batch PCKh 0.375\n",
      "Trained batch 492 batch loss 0.793356419 batch mAP 0.407226562 batch PCKh 0.0625\n",
      "Trained batch 493 batch loss 0.657001436 batch mAP 0.414550781 batch PCKh 0.3125\n",
      "Trained batch 494 batch loss 0.743593 batch mAP 0.413391113 batch PCKh 0.125\n",
      "Trained batch 495 batch loss 0.808930635 batch mAP 0.412750244 batch PCKh 0.375\n",
      "Trained batch 496 batch loss 0.782623053 batch mAP 0.416290283 batch PCKh 0.375\n",
      "Trained batch 497 batch loss 0.721904278 batch mAP 0.446380615 batch PCKh 0.4375\n",
      "Trained batch 498 batch loss 0.73397243 batch mAP 0.447052 batch PCKh 0.4375\n",
      "Trained batch 499 batch loss 0.646619558 batch mAP 0.443847656 batch PCKh 0.375\n",
      "Trained batch 500 batch loss 0.733290553 batch mAP 0.433746338 batch PCKh 0\n",
      "Trained batch 501 batch loss 0.673268437 batch mAP 0.403778076 batch PCKh 0.375\n",
      "Trained batch 502 batch loss 0.687351286 batch mAP 0.41116333 batch PCKh 0.5625\n",
      "Trained batch 503 batch loss 0.684383 batch mAP 0.405181885 batch PCKh 0.5625\n",
      "Trained batch 504 batch loss 0.644067764 batch mAP 0.379547119 batch PCKh 0.1875\n",
      "Trained batch 505 batch loss 0.659349501 batch mAP 0.383667 batch PCKh 0.1875\n",
      "Trained batch 506 batch loss 0.736920536 batch mAP 0.379974365 batch PCKh 0.6875\n",
      "Trained batch 507 batch loss 0.708419561 batch mAP 0.379211426 batch PCKh 0.5625\n",
      "Trained batch 508 batch loss 0.708162 batch mAP 0.397277832 batch PCKh 0.625\n",
      "Trained batch 509 batch loss 0.686434865 batch mAP 0.414672852 batch PCKh 0.25\n",
      "Trained batch 510 batch loss 0.747449517 batch mAP 0.412139893 batch PCKh 0.1875\n",
      "Trained batch 511 batch loss 0.702620447 batch mAP 0.402679443 batch PCKh 0.1875\n",
      "Trained batch 512 batch loss 0.655734599 batch mAP 0.405151367 batch PCKh 0.0625\n",
      "Trained batch 513 batch loss 0.669919074 batch mAP 0.411743164 batch PCKh 0.1875\n",
      "Trained batch 514 batch loss 0.640689552 batch mAP 0.441467285 batch PCKh 0.5625\n",
      "Trained batch 515 batch loss 0.677866 batch mAP 0.444641113 batch PCKh 0.0625\n",
      "Trained batch 516 batch loss 0.707442045 batch mAP 0.429077148 batch PCKh 0.1875\n",
      "Trained batch 517 batch loss 0.66431886 batch mAP 0.446380615 batch PCKh 0.6875\n",
      "Trained batch 518 batch loss 0.692707539 batch mAP 0.427764893 batch PCKh 0.1875\n",
      "Trained batch 519 batch loss 0.659005523 batch mAP 0.384735107 batch PCKh 0.3125\n",
      "Trained batch 520 batch loss 0.644102514 batch mAP 0.402893066 batch PCKh 0.625\n",
      "Trained batch 521 batch loss 0.683205426 batch mAP 0.372406 batch PCKh 0.5\n",
      "Trained batch 522 batch loss 0.717429757 batch mAP 0.402618408 batch PCKh 0.75\n",
      "Trained batch 523 batch loss 0.709950566 batch mAP 0.420074463 batch PCKh 0.5625\n",
      "Trained batch 524 batch loss 0.7372365 batch mAP 0.439147949 batch PCKh 0.1875\n",
      "Trained batch 525 batch loss 0.685838103 batch mAP 0.441253662 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 526 batch loss 0.689066887 batch mAP 0.458892822 batch PCKh 0.1875\n",
      "Trained batch 527 batch loss 0.665576696 batch mAP 0.44644165 batch PCKh 0.25\n",
      "Trained batch 528 batch loss 0.700771868 batch mAP 0.449371338 batch PCKh 0.25\n",
      "Trained batch 529 batch loss 0.747736454 batch mAP 0.43572998 batch PCKh 0.25\n",
      "Trained batch 530 batch loss 0.802576184 batch mAP 0.413085938 batch PCKh 0.1875\n",
      "Trained batch 531 batch loss 0.758305073 batch mAP 0.409942627 batch PCKh 0.25\n",
      "Trained batch 532 batch loss 0.72784847 batch mAP 0.425750732 batch PCKh 0.8125\n",
      "Trained batch 533 batch loss 0.740644 batch mAP 0.414093018 batch PCKh 0.3125\n",
      "Trained batch 534 batch loss 0.697132 batch mAP 0.398345947 batch PCKh 0.5625\n",
      "Trained batch 535 batch loss 0.698680401 batch mAP 0.387207031 batch PCKh 0.875\n",
      "Trained batch 536 batch loss 0.685012579 batch mAP 0.408660889 batch PCKh 0.875\n",
      "Trained batch 537 batch loss 0.688814759 batch mAP 0.438537598 batch PCKh 0.75\n",
      "Trained batch 538 batch loss 0.708237112 batch mAP 0.379821777 batch PCKh 0.8125\n",
      "Trained batch 539 batch loss 0.712230682 batch mAP 0.362731934 batch PCKh 0.8125\n",
      "Trained batch 540 batch loss 0.695592701 batch mAP 0.356964111 batch PCKh 0.1875\n",
      "Trained batch 541 batch loss 0.675844073 batch mAP 0.395935059 batch PCKh 0.5625\n",
      "Trained batch 542 batch loss 0.6993258 batch mAP 0.419281 batch PCKh 0.375\n",
      "Trained batch 543 batch loss 0.773165464 batch mAP 0.412414551 batch PCKh 0\n",
      "Trained batch 544 batch loss 0.730094552 batch mAP 0.414733887 batch PCKh 0.0625\n",
      "Trained batch 545 batch loss 0.745115757 batch mAP 0.412475586 batch PCKh 0.5625\n",
      "Trained batch 546 batch loss 0.79869771 batch mAP 0.378082275 batch PCKh 0.3125\n",
      "Trained batch 547 batch loss 0.810733378 batch mAP 0.359741211 batch PCKh 0.1875\n",
      "Trained batch 548 batch loss 0.784902215 batch mAP 0.368133545 batch PCKh 0.3125\n",
      "Trained batch 549 batch loss 0.812988222 batch mAP 0.402282715 batch PCKh 0.25\n",
      "Trained batch 550 batch loss 0.752366364 batch mAP 0.396453857 batch PCKh 0.5\n",
      "Trained batch 551 batch loss 0.748539209 batch mAP 0.371643066 batch PCKh 0.125\n",
      "Trained batch 552 batch loss 0.772003174 batch mAP 0.313049316 batch PCKh 0.0625\n",
      "Trained batch 553 batch loss 0.788534701 batch mAP 0.281311035 batch PCKh 0\n",
      "Trained batch 554 batch loss 0.770562768 batch mAP 0.221984863 batch PCKh 0\n",
      "Trained batch 555 batch loss 0.707714915 batch mAP 0.117218018 batch PCKh 0.125\n",
      "Trained batch 556 batch loss 0.764463127 batch mAP 0.149871826 batch PCKh 0.125\n",
      "Trained batch 557 batch loss 0.806475163 batch mAP 0.144805908 batch PCKh 0\n",
      "Trained batch 558 batch loss 0.788603663 batch mAP 0.130340576 batch PCKh 0.0625\n",
      "Trained batch 559 batch loss 0.806017756 batch mAP 0.160949707 batch PCKh 0.125\n",
      "Trained batch 560 batch loss 0.802315831 batch mAP 0.137756348 batch PCKh 0.0625\n",
      "Trained batch 561 batch loss 0.698865056 batch mAP 0.111846924 batch PCKh 0.1875\n",
      "Trained batch 562 batch loss 0.73858 batch mAP 0.0985717773 batch PCKh 0.375\n",
      "Trained batch 563 batch loss 0.662475646 batch mAP 0.183288574 batch PCKh 0.25\n",
      "Trained batch 564 batch loss 0.681582 batch mAP 0.288513184 batch PCKh 0.5625\n",
      "Trained batch 565 batch loss 0.742166877 batch mAP 0.308837891 batch PCKh 0.4375\n",
      "Trained batch 566 batch loss 0.724203408 batch mAP 0.330474854 batch PCKh 0.5625\n",
      "Trained batch 567 batch loss 0.686588287 batch mAP 0.355865479 batch PCKh 0.1875\n",
      "Trained batch 568 batch loss 0.711674631 batch mAP 0.398193359 batch PCKh 0.25\n",
      "Trained batch 569 batch loss 0.758757949 batch mAP 0.407775879 batch PCKh 0.4375\n",
      "Trained batch 570 batch loss 0.639329672 batch mAP 0.410308838 batch PCKh 0.375\n",
      "Trained batch 571 batch loss 0.689358234 batch mAP 0.396362305 batch PCKh 0.3125\n",
      "Trained batch 572 batch loss 0.678011 batch mAP 0.402587891 batch PCKh 0.4375\n",
      "Trained batch 573 batch loss 0.697100639 batch mAP 0.387878418 batch PCKh 0.1875\n",
      "Trained batch 574 batch loss 0.657468557 batch mAP 0.428436279 batch PCKh 0.4375\n",
      "Trained batch 575 batch loss 0.69563961 batch mAP 0.414520264 batch PCKh 0.5\n",
      "Trained batch 576 batch loss 0.724292397 batch mAP 0.339294434 batch PCKh 0\n",
      "Trained batch 577 batch loss 0.688213348 batch mAP 0.38092041 batch PCKh 0.4375\n",
      "Trained batch 578 batch loss 0.676018596 batch mAP 0.373260498 batch PCKh 0.1875\n",
      "Trained batch 579 batch loss 0.616937399 batch mAP 0.394073486 batch PCKh 0.125\n",
      "Trained batch 580 batch loss 0.645071208 batch mAP 0.315368652 batch PCKh 0\n",
      "Trained batch 581 batch loss 0.666109562 batch mAP 0.311218262 batch PCKh 0\n",
      "Trained batch 582 batch loss 0.559604764 batch mAP 0.319946289 batch PCKh 0\n",
      "Trained batch 583 batch loss 0.553813696 batch mAP 0.311340332 batch PCKh 0.125\n",
      "Trained batch 584 batch loss 0.55935657 batch mAP 0.297088623 batch PCKh 0.1875\n",
      "Trained batch 585 batch loss 0.593038082 batch mAP 0.179321289 batch PCKh 0.0625\n",
      "Trained batch 586 batch loss 0.613002717 batch mAP 0.0775756836 batch PCKh 0\n",
      "Trained batch 587 batch loss 0.755502 batch mAP 0.0881347656 batch PCKh 0.0625\n",
      "Trained batch 588 batch loss 0.850741148 batch mAP 0.0902099609 batch PCKh 0\n",
      "Trained batch 589 batch loss 0.777130604 batch mAP 0.0998535156 batch PCKh 0.0625\n",
      "Trained batch 590 batch loss 0.722123146 batch mAP 0.0575866699 batch PCKh 0.625\n",
      "Trained batch 591 batch loss 0.752003372 batch mAP 0.079284668 batch PCKh 0.4375\n",
      "Trained batch 592 batch loss 0.699358225 batch mAP 0.0877990723 batch PCKh 0\n",
      "Trained batch 593 batch loss 0.681588948 batch mAP 0.102111816 batch PCKh 0\n",
      "Trained batch 594 batch loss 0.677225113 batch mAP 0.10760498 batch PCKh 0.625\n",
      "Trained batch 595 batch loss 0.724263608 batch mAP 0.171173096 batch PCKh 0.75\n",
      "Trained batch 596 batch loss 0.711218417 batch mAP 0.142181396 batch PCKh 0.25\n",
      "Trained batch 597 batch loss 0.67906487 batch mAP 0.102661133 batch PCKh 0.5\n",
      "Trained batch 598 batch loss 0.610315323 batch mAP 0.20690918 batch PCKh 0.25\n",
      "Trained batch 599 batch loss 0.606181502 batch mAP 0.281860352 batch PCKh 0.25\n",
      "Trained batch 600 batch loss 0.612633467 batch mAP 0.277191162 batch PCKh 0\n",
      "Trained batch 601 batch loss 0.687349558 batch mAP 0.239227295 batch PCKh 0.6875\n",
      "Trained batch 602 batch loss 0.731025577 batch mAP 0.248138428 batch PCKh 0.4375\n",
      "Trained batch 603 batch loss 0.632108569 batch mAP 0.283538818 batch PCKh 0\n",
      "Trained batch 604 batch loss 0.737070739 batch mAP 0.265686035 batch PCKh 0.625\n",
      "Trained batch 605 batch loss 0.65678519 batch mAP 0.302642822 batch PCKh 0\n",
      "Trained batch 606 batch loss 0.724843204 batch mAP 0.299926758 batch PCKh 0.5\n",
      "Trained batch 607 batch loss 0.701982 batch mAP 0.320251465 batch PCKh 0\n",
      "Trained batch 608 batch loss 0.61073035 batch mAP 0.358062744 batch PCKh 0\n",
      "Trained batch 609 batch loss 0.639163 batch mAP 0.341552734 batch PCKh 0\n",
      "Trained batch 610 batch loss 0.643513739 batch mAP 0.323394775 batch PCKh 0\n",
      "Trained batch 611 batch loss 0.709966898 batch mAP 0.314483643 batch PCKh 0\n",
      "Trained batch 612 batch loss 0.718554318 batch mAP 0.302429199 batch PCKh 0.6875\n",
      "Trained batch 613 batch loss 0.855231822 batch mAP 0.28805542 batch PCKh 0\n",
      "Trained batch 614 batch loss 0.811696291 batch mAP 0.266815186 batch PCKh 0\n",
      "Trained batch 615 batch loss 0.695400119 batch mAP 0.22869873 batch PCKh 0.1875\n",
      "Trained batch 616 batch loss 0.698837757 batch mAP 0.209777832 batch PCKh 0.3125\n",
      "Trained batch 617 batch loss 0.668737352 batch mAP 0.176849365 batch PCKh 0.1875\n",
      "Trained batch 618 batch loss 0.631947398 batch mAP 0.206665039 batch PCKh 0.1875\n",
      "Trained batch 619 batch loss 0.681857347 batch mAP 0.203582764 batch PCKh 0.0625\n",
      "Trained batch 620 batch loss 0.680676281 batch mAP 0.246795654 batch PCKh 0.5625\n",
      "Trained batch 621 batch loss 0.697292447 batch mAP 0.282104492 batch PCKh 0.0625\n",
      "Trained batch 622 batch loss 0.711219728 batch mAP 0.319244385 batch PCKh 0.125\n",
      "Trained batch 623 batch loss 0.70168674 batch mAP 0.310821533 batch PCKh 0.125\n",
      "Trained batch 624 batch loss 0.80671525 batch mAP 0.323883057 batch PCKh 0.25\n",
      "Trained batch 625 batch loss 0.805008531 batch mAP 0.318908691 batch PCKh 0.25\n",
      "Trained batch 626 batch loss 0.699489772 batch mAP 0.371154785 batch PCKh 0.1875\n",
      "Trained batch 627 batch loss 0.757327914 batch mAP 0.316680908 batch PCKh 0\n",
      "Trained batch 628 batch loss 0.723320544 batch mAP 0.413879395 batch PCKh 0.0625\n",
      "Trained batch 629 batch loss 0.713519096 batch mAP 0.421630859 batch PCKh 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 630 batch loss 0.69924885 batch mAP 0.424987793 batch PCKh 0.1875\n",
      "Trained batch 631 batch loss 0.718289852 batch mAP 0.457397461 batch PCKh 0.5\n",
      "Trained batch 632 batch loss 0.705125332 batch mAP 0.408325195 batch PCKh 0\n",
      "Trained batch 633 batch loss 0.727114081 batch mAP 0.429443359 batch PCKh 0.3125\n",
      "Trained batch 634 batch loss 0.664737642 batch mAP 0.392974854 batch PCKh 0\n",
      "Trained batch 635 batch loss 0.676112056 batch mAP 0.384887695 batch PCKh 0.375\n",
      "Trained batch 636 batch loss 0.684486032 batch mAP 0.363311768 batch PCKh 0.3125\n",
      "Trained batch 637 batch loss 0.671099484 batch mAP 0.45401 batch PCKh 0.3125\n",
      "Trained batch 638 batch loss 0.769379675 batch mAP 0.393859863 batch PCKh 0.625\n",
      "Trained batch 639 batch loss 0.800963938 batch mAP 0.380767822 batch PCKh 0\n",
      "Trained batch 640 batch loss 0.786679685 batch mAP 0.421905518 batch PCKh 0\n",
      "Trained batch 641 batch loss 0.752190053 batch mAP 0.393585205 batch PCKh 0.5\n",
      "Trained batch 642 batch loss 0.724221051 batch mAP 0.430755615 batch PCKh 0.875\n",
      "Trained batch 643 batch loss 0.725488424 batch mAP 0.422515869 batch PCKh 0.875\n",
      "Trained batch 644 batch loss 0.643679917 batch mAP 0.344085693 batch PCKh 0.3125\n",
      "Trained batch 645 batch loss 0.733659387 batch mAP 0.366943359 batch PCKh 0.625\n",
      "Trained batch 646 batch loss 0.604170203 batch mAP 0.348327637 batch PCKh 0.1875\n",
      "Trained batch 647 batch loss 0.620161951 batch mAP 0.302001953 batch PCKh 0.3125\n",
      "Trained batch 648 batch loss 0.676538 batch mAP 0.313385 batch PCKh 0.75\n",
      "Trained batch 649 batch loss 0.635457695 batch mAP 0.313385 batch PCKh 0.4375\n",
      "Trained batch 650 batch loss 0.789515436 batch mAP 0.305786133 batch PCKh 0.125\n",
      "Trained batch 651 batch loss 0.785684168 batch mAP 0.301513672 batch PCKh 0\n",
      "Trained batch 652 batch loss 0.751505375 batch mAP 0.332794189 batch PCKh 0.1875\n",
      "Trained batch 653 batch loss 0.774787426 batch mAP 0.282409668 batch PCKh 0\n",
      "Trained batch 654 batch loss 0.802537441 batch mAP 0.294525146 batch PCKh 0\n",
      "Trained batch 655 batch loss 0.776056409 batch mAP 0.295654297 batch PCKh 0.125\n",
      "Trained batch 656 batch loss 0.721216083 batch mAP 0.286804199 batch PCKh 0.75\n",
      "Trained batch 657 batch loss 0.756536186 batch mAP 0.30279541 batch PCKh 0.1875\n",
      "Trained batch 658 batch loss 0.733836234 batch mAP 0.295227051 batch PCKh 0.1875\n",
      "Trained batch 659 batch loss 0.636600494 batch mAP 0.324493408 batch PCKh 0\n",
      "Trained batch 660 batch loss 0.631888688 batch mAP 0.258331299 batch PCKh 0\n",
      "Trained batch 661 batch loss 0.708696306 batch mAP 0.323486328 batch PCKh 0.5\n",
      "Trained batch 662 batch loss 0.689978242 batch mAP 0.353942871 batch PCKh 0.5625\n",
      "Trained batch 663 batch loss 0.707022429 batch mAP 0.359008789 batch PCKh 0.75\n",
      "Trained batch 664 batch loss 0.661443114 batch mAP 0.401092529 batch PCKh 0.5\n",
      "Trained batch 665 batch loss 0.742296517 batch mAP 0.412445068 batch PCKh 0.375\n",
      "Trained batch 666 batch loss 0.707890809 batch mAP 0.412750244 batch PCKh 0\n",
      "Trained batch 667 batch loss 0.641364157 batch mAP 0.426147461 batch PCKh 0.6875\n",
      "Trained batch 668 batch loss 0.667212844 batch mAP 0.398620605 batch PCKh 0.5\n",
      "Trained batch 669 batch loss 0.769674778 batch mAP 0.348388672 batch PCKh 0.3125\n",
      "Trained batch 670 batch loss 0.7601946 batch mAP 0.280700684 batch PCKh 0\n",
      "Trained batch 671 batch loss 0.758028746 batch mAP 0.24911499 batch PCKh 0.375\n",
      "Trained batch 672 batch loss 0.675463498 batch mAP 0.322479248 batch PCKh 0.125\n",
      "Trained batch 673 batch loss 0.636952698 batch mAP 0.365905762 batch PCKh 0.5\n",
      "Trained batch 674 batch loss 0.691673696 batch mAP 0.385528564 batch PCKh 0.0625\n",
      "Trained batch 675 batch loss 0.698451519 batch mAP 0.420684814 batch PCKh 0.4375\n",
      "Trained batch 676 batch loss 0.768597722 batch mAP 0.380767822 batch PCKh 0.3125\n",
      "Trained batch 677 batch loss 0.712358892 batch mAP 0.417266846 batch PCKh 0.0625\n",
      "Trained batch 678 batch loss 0.71176368 batch mAP 0.410858154 batch PCKh 0.125\n",
      "Trained batch 679 batch loss 0.667875051 batch mAP 0.417785645 batch PCKh 0.1875\n",
      "Trained batch 680 batch loss 0.672174692 batch mAP 0.419555664 batch PCKh 0.5625\n",
      "Trained batch 681 batch loss 0.701316118 batch mAP 0.445831299 batch PCKh 0.1875\n",
      "Trained batch 682 batch loss 0.710844636 batch mAP 0.414398193 batch PCKh 0.3125\n",
      "Trained batch 683 batch loss 0.721052766 batch mAP 0.427948 batch PCKh 0.25\n",
      "Trained batch 684 batch loss 0.664749861 batch mAP 0.388885498 batch PCKh 0.5625\n",
      "Trained batch 685 batch loss 0.679541767 batch mAP 0.321502686 batch PCKh 0.5\n",
      "Trained batch 686 batch loss 0.68471688 batch mAP 0.405273438 batch PCKh 0.3125\n",
      "Trained batch 687 batch loss 0.613483846 batch mAP 0.368774414 batch PCKh 0.375\n",
      "Trained batch 688 batch loss 0.68587327 batch mAP 0.412811279 batch PCKh 0.25\n",
      "Trained batch 689 batch loss 0.652140498 batch mAP 0.363586426 batch PCKh 0.5\n",
      "Trained batch 690 batch loss 0.659322917 batch mAP 0.367279053 batch PCKh 0.75\n",
      "Trained batch 691 batch loss 0.680249035 batch mAP 0.343658447 batch PCKh 0.375\n",
      "Trained batch 692 batch loss 0.694354892 batch mAP 0.363067627 batch PCKh 0.625\n",
      "Trained batch 693 batch loss 0.722873449 batch mAP 0.344543457 batch PCKh 0.4375\n",
      "Trained batch 694 batch loss 0.733010173 batch mAP 0.316314697 batch PCKh 0.4375\n",
      "Trained batch 695 batch loss 0.688314378 batch mAP 0.384918213 batch PCKh 0.75\n",
      "Trained batch 696 batch loss 0.76358676 batch mAP 0.388305664 batch PCKh 0.4375\n",
      "Trained batch 697 batch loss 0.745797873 batch mAP 0.395141602 batch PCKh 0.5625\n",
      "Trained batch 698 batch loss 0.673200786 batch mAP 0.458679199 batch PCKh 0.375\n",
      "Trained batch 699 batch loss 0.643108368 batch mAP 0.439331055 batch PCKh 0.6875\n",
      "Trained batch 700 batch loss 0.778070569 batch mAP 0.383300781 batch PCKh 0.8125\n",
      "Trained batch 701 batch loss 0.731592059 batch mAP 0.370300293 batch PCKh 0.75\n",
      "Trained batch 702 batch loss 0.682406783 batch mAP 0.398223877 batch PCKh 0.25\n",
      "Trained batch 703 batch loss 0.712525249 batch mAP 0.41003418 batch PCKh 0.625\n",
      "Trained batch 704 batch loss 0.67825067 batch mAP 0.396240234 batch PCKh 0.875\n",
      "Trained batch 705 batch loss 0.74995327 batch mAP 0.38760376 batch PCKh 0.1875\n",
      "Trained batch 706 batch loss 0.74036938 batch mAP 0.379455566 batch PCKh 0.75\n",
      "Trained batch 707 batch loss 0.711269736 batch mAP 0.364013672 batch PCKh 0.8125\n",
      "Trained batch 708 batch loss 0.691466451 batch mAP 0.355102539 batch PCKh 0.6875\n",
      "Trained batch 709 batch loss 0.68070668 batch mAP 0.337921143 batch PCKh 0.125\n",
      "Trained batch 710 batch loss 0.727431476 batch mAP 0.347473145 batch PCKh 0.125\n",
      "Trained batch 711 batch loss 0.624925 batch mAP 0.369689941 batch PCKh 0.375\n",
      "Trained batch 712 batch loss 0.664459229 batch mAP 0.375549316 batch PCKh 0.5625\n",
      "Trained batch 713 batch loss 0.624703765 batch mAP 0.40246582 batch PCKh 0.4375\n",
      "Trained batch 714 batch loss 0.718632281 batch mAP 0.374938965 batch PCKh 0.6875\n",
      "Trained batch 715 batch loss 0.701628208 batch mAP 0.389709473 batch PCKh 0.75\n",
      "Trained batch 716 batch loss 0.70731324 batch mAP 0.364990234 batch PCKh 0.5625\n",
      "Trained batch 717 batch loss 0.731248558 batch mAP 0.411743164 batch PCKh 0.3125\n",
      "Trained batch 718 batch loss 0.766411126 batch mAP 0.411468506 batch PCKh 0.5625\n",
      "Trained batch 719 batch loss 0.704025805 batch mAP 0.432891846 batch PCKh 0.5625\n",
      "Trained batch 720 batch loss 0.641847491 batch mAP 0.391052246 batch PCKh 0.0625\n",
      "Trained batch 721 batch loss 0.680068612 batch mAP 0.407897949 batch PCKh 0.3125\n",
      "Trained batch 722 batch loss 0.733480155 batch mAP 0.401947021 batch PCKh 0.1875\n",
      "Trained batch 723 batch loss 0.70429337 batch mAP 0.393951416 batch PCKh 0.1875\n",
      "Trained batch 724 batch loss 0.72121346 batch mAP 0.390472412 batch PCKh 0.625\n",
      "Trained batch 725 batch loss 0.667720318 batch mAP 0.38659668 batch PCKh 0.5\n",
      "Trained batch 726 batch loss 0.704060793 batch mAP 0.367095947 batch PCKh 0.25\n",
      "Trained batch 727 batch loss 0.739503 batch mAP 0.398071289 batch PCKh 0.375\n",
      "Trained batch 728 batch loss 0.680855572 batch mAP 0.386291504 batch PCKh 0.25\n",
      "Trained batch 729 batch loss 0.554282188 batch mAP 0.453186035 batch PCKh 0.625\n",
      "Trained batch 730 batch loss 0.590805173 batch mAP 0.450378418 batch PCKh 0.5\n",
      "Trained batch 731 batch loss 0.717345476 batch mAP 0.41204834 batch PCKh 0.6875\n",
      "Trained batch 732 batch loss 0.713164 batch mAP 0.445007324 batch PCKh 0.0625\n",
      "Trained batch 733 batch loss 0.734393418 batch mAP 0.368499756 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 734 batch loss 0.764537156 batch mAP 0.364440918 batch PCKh 0.125\n",
      "Trained batch 735 batch loss 0.736923695 batch mAP 0.406158447 batch PCKh 0.0625\n",
      "Trained batch 736 batch loss 0.703648567 batch mAP 0.386657715 batch PCKh 0\n",
      "Trained batch 737 batch loss 0.663318753 batch mAP 0.397003174 batch PCKh 0.1875\n",
      "Trained batch 738 batch loss 0.702713966 batch mAP 0.39831543 batch PCKh 0.125\n",
      "Trained batch 739 batch loss 0.699151516 batch mAP 0.369995117 batch PCKh 0.375\n",
      "Trained batch 740 batch loss 0.660327673 batch mAP 0.390960693 batch PCKh 0.125\n",
      "Trained batch 741 batch loss 0.676808596 batch mAP 0.362976074 batch PCKh 0.5\n",
      "Trained batch 742 batch loss 0.675115108 batch mAP 0.399017334 batch PCKh 0.875\n",
      "Trained batch 743 batch loss 0.750828743 batch mAP 0.391052246 batch PCKh 0\n",
      "Trained batch 744 batch loss 0.765569448 batch mAP 0.361572266 batch PCKh 0.6875\n",
      "Trained batch 745 batch loss 0.722941339 batch mAP 0.368896484 batch PCKh 0.4375\n",
      "Trained batch 746 batch loss 0.663167357 batch mAP 0.404205322 batch PCKh 0.6875\n",
      "Trained batch 747 batch loss 0.648542881 batch mAP 0.396881104 batch PCKh 0.6875\n",
      "Trained batch 748 batch loss 0.634842098 batch mAP 0.376800537 batch PCKh 0.25\n",
      "Trained batch 749 batch loss 0.712869763 batch mAP 0.391815186 batch PCKh 0.4375\n",
      "Trained batch 750 batch loss 0.748328924 batch mAP 0.398651123 batch PCKh 0.75\n",
      "Trained batch 751 batch loss 0.623012125 batch mAP 0.420623779 batch PCKh 0.3125\n",
      "Trained batch 752 batch loss 0.690198064 batch mAP 0.410644531 batch PCKh 0.6875\n",
      "Trained batch 753 batch loss 0.691994548 batch mAP 0.436157227 batch PCKh 0.75\n",
      "Trained batch 754 batch loss 0.673501253 batch mAP 0.4402771 batch PCKh 0.6875\n",
      "Trained batch 755 batch loss 0.680684149 batch mAP 0.429718018 batch PCKh 0.6875\n",
      "Trained batch 756 batch loss 0.676689446 batch mAP 0.43762207 batch PCKh 0.375\n",
      "Trained batch 757 batch loss 0.643331766 batch mAP 0.471435547 batch PCKh 0.375\n",
      "Trained batch 758 batch loss 0.702088 batch mAP 0.438415527 batch PCKh 0.0625\n",
      "Trained batch 759 batch loss 0.657014549 batch mAP 0.448913574 batch PCKh 0.3125\n",
      "Trained batch 760 batch loss 0.700510144 batch mAP 0.431518555 batch PCKh 0.4375\n",
      "Trained batch 761 batch loss 0.720279574 batch mAP 0.456573486 batch PCKh 0.75\n",
      "Trained batch 762 batch loss 0.825836241 batch mAP 0.406616211 batch PCKh 0\n",
      "Trained batch 763 batch loss 0.760706246 batch mAP 0.392150879 batch PCKh 0.0625\n",
      "Trained batch 764 batch loss 0.694245338 batch mAP 0.425109863 batch PCKh 0.5\n",
      "Trained batch 765 batch loss 0.621729314 batch mAP 0.376556396 batch PCKh 0.5625\n",
      "Trained batch 766 batch loss 0.70076555 batch mAP 0.37991333 batch PCKh 0.8125\n",
      "Trained batch 767 batch loss 0.729380786 batch mAP 0.354064941 batch PCKh 0.6875\n",
      "Trained batch 768 batch loss 0.677314937 batch mAP 0.37020874 batch PCKh 0.1875\n",
      "Trained batch 769 batch loss 0.61062932 batch mAP 0.3515625 batch PCKh 0.1875\n",
      "Trained batch 770 batch loss 0.565796196 batch mAP 0.375427246 batch PCKh 0.25\n",
      "Trained batch 771 batch loss 0.662140787 batch mAP 0.421203613 batch PCKh 0.1875\n",
      "Trained batch 772 batch loss 0.597467721 batch mAP 0.388702393 batch PCKh 0.1875\n",
      "Trained batch 773 batch loss 0.701870203 batch mAP 0.409118652 batch PCKh 0.25\n",
      "Trained batch 774 batch loss 0.723085523 batch mAP 0.466674805 batch PCKh 0.1875\n",
      "Trained batch 775 batch loss 0.606570184 batch mAP 0.437774658 batch PCKh 0.125\n",
      "Trained batch 776 batch loss 0.655235 batch mAP 0.415649414 batch PCKh 0.25\n",
      "Trained batch 777 batch loss 0.734703541 batch mAP 0.334289551 batch PCKh 0.0625\n",
      "Trained batch 778 batch loss 0.751925051 batch mAP 0.407165527 batch PCKh 0.1875\n",
      "Trained batch 779 batch loss 0.71881628 batch mAP 0.349212646 batch PCKh 0.125\n",
      "Trained batch 780 batch loss 0.71140486 batch mAP 0.398010254 batch PCKh 0.125\n",
      "Trained batch 781 batch loss 0.765427828 batch mAP 0.413970947 batch PCKh 0.25\n",
      "Trained batch 782 batch loss 0.705333352 batch mAP 0.379821777 batch PCKh 0.5\n",
      "Trained batch 783 batch loss 0.715551674 batch mAP 0.369384766 batch PCKh 0\n",
      "Trained batch 784 batch loss 0.679645598 batch mAP 0.275756836 batch PCKh 0.1875\n",
      "Trained batch 785 batch loss 0.700647831 batch mAP 0.306182861 batch PCKh 0.8125\n",
      "Trained batch 786 batch loss 0.663886666 batch mAP 0.272766113 batch PCKh 0\n",
      "Trained batch 787 batch loss 0.663681865 batch mAP 0.176269531 batch PCKh 0.1875\n",
      "Trained batch 788 batch loss 0.711431265 batch mAP 0.243927 batch PCKh 0\n",
      "Trained batch 789 batch loss 0.742519438 batch mAP 0.277740479 batch PCKh 0.5625\n",
      "Trained batch 790 batch loss 0.739825726 batch mAP 0.323852539 batch PCKh 0.25\n",
      "Trained batch 791 batch loss 0.686752 batch mAP 0.301483154 batch PCKh 0.625\n",
      "Trained batch 792 batch loss 0.658580065 batch mAP 0.278930664 batch PCKh 0.375\n",
      "Trained batch 793 batch loss 0.698784173 batch mAP 0.304412842 batch PCKh 0.8125\n",
      "Trained batch 794 batch loss 0.719456255 batch mAP 0.31552124 batch PCKh 0.25\n",
      "Trained batch 795 batch loss 0.65136683 batch mAP 0.373718262 batch PCKh 0.75\n",
      "Trained batch 796 batch loss 0.707059145 batch mAP 0.375396729 batch PCKh 0.5625\n",
      "Trained batch 797 batch loss 0.710876346 batch mAP 0.432922363 batch PCKh 0.625\n",
      "Trained batch 798 batch loss 0.648246527 batch mAP 0.376373291 batch PCKh 0.25\n",
      "Trained batch 799 batch loss 0.649671793 batch mAP 0.446746826 batch PCKh 0.25\n",
      "Trained batch 800 batch loss 0.665131 batch mAP 0.478942871 batch PCKh 0.1875\n",
      "Trained batch 801 batch loss 0.700678587 batch mAP 0.474761963 batch PCKh 0.3125\n",
      "Trained batch 802 batch loss 0.742785752 batch mAP 0.470733643 batch PCKh 0.25\n",
      "Trained batch 803 batch loss 0.75726 batch mAP 0.490478516 batch PCKh 0.1875\n",
      "Trained batch 804 batch loss 0.734151542 batch mAP 0.437927246 batch PCKh 0.625\n",
      "Trained batch 805 batch loss 0.773827493 batch mAP 0.438446045 batch PCKh 0.1875\n",
      "Trained batch 806 batch loss 0.782981515 batch mAP 0.472015381 batch PCKh 0.125\n",
      "Trained batch 807 batch loss 0.707922101 batch mAP 0.428741455 batch PCKh 0.3125\n",
      "Trained batch 808 batch loss 0.656687 batch mAP 0.45803833 batch PCKh 0.1875\n",
      "Trained batch 809 batch loss 0.769625306 batch mAP 0.446105957 batch PCKh 0.0625\n",
      "Trained batch 810 batch loss 0.720544338 batch mAP 0.432769775 batch PCKh 0.3125\n",
      "Trained batch 811 batch loss 0.727674723 batch mAP 0.442840576 batch PCKh 0.625\n",
      "Trained batch 812 batch loss 0.713656545 batch mAP 0.373596191 batch PCKh 0.625\n",
      "Trained batch 813 batch loss 0.728862524 batch mAP 0.431793213 batch PCKh 0.0625\n",
      "Trained batch 814 batch loss 0.693893909 batch mAP 0.410797119 batch PCKh 0.75\n",
      "Trained batch 815 batch loss 0.65674144 batch mAP 0.449005127 batch PCKh 0.1875\n",
      "Trained batch 816 batch loss 0.698395729 batch mAP 0.437469482 batch PCKh 0.25\n",
      "Trained batch 817 batch loss 0.708482921 batch mAP 0.418701172 batch PCKh 0.375\n",
      "Trained batch 818 batch loss 0.684237 batch mAP 0.394897461 batch PCKh 0.25\n",
      "Trained batch 819 batch loss 0.739231944 batch mAP 0.425384521 batch PCKh 0\n",
      "Trained batch 820 batch loss 0.728049517 batch mAP 0.443084717 batch PCKh 0.625\n",
      "Trained batch 821 batch loss 0.663251281 batch mAP 0.39440918 batch PCKh 0.6875\n",
      "Trained batch 822 batch loss 0.635993 batch mAP 0.403106689 batch PCKh 0.5625\n",
      "Trained batch 823 batch loss 0.637620151 batch mAP 0.414337158 batch PCKh 0.75\n",
      "Trained batch 824 batch loss 0.727368116 batch mAP 0.399017334 batch PCKh 0.8125\n",
      "Trained batch 825 batch loss 0.752109647 batch mAP 0.405426025 batch PCKh 0\n",
      "Trained batch 826 batch loss 0.713471711 batch mAP 0.397705078 batch PCKh 0.25\n",
      "Trained batch 827 batch loss 0.72961396 batch mAP 0.435302734 batch PCKh 0.25\n",
      "Trained batch 828 batch loss 0.699341536 batch mAP 0.387023926 batch PCKh 0.75\n",
      "Trained batch 829 batch loss 0.605199218 batch mAP 0.402618408 batch PCKh 0.0625\n",
      "Trained batch 830 batch loss 0.560722232 batch mAP 0.402984619 batch PCKh 0.125\n",
      "Trained batch 831 batch loss 0.570392251 batch mAP 0.398834229 batch PCKh 0.375\n",
      "Trained batch 832 batch loss 0.554245234 batch mAP 0.405548096 batch PCKh 0.25\n",
      "Trained batch 833 batch loss 0.622638166 batch mAP 0.420776367 batch PCKh 0\n",
      "Trained batch 834 batch loss 0.709194899 batch mAP 0.375701904 batch PCKh 0\n",
      "Trained batch 835 batch loss 0.793893158 batch mAP 0.346160889 batch PCKh 0.125\n",
      "Trained batch 836 batch loss 0.764874339 batch mAP 0.376739502 batch PCKh 0.375\n",
      "Trained batch 837 batch loss 0.739716589 batch mAP 0.371154785 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 838 batch loss 0.661446512 batch mAP 0.391082764 batch PCKh 0\n",
      "Trained batch 839 batch loss 0.75211823 batch mAP 0.351928711 batch PCKh 0.125\n",
      "Trained batch 840 batch loss 0.700460911 batch mAP 0.271575928 batch PCKh 0.25\n",
      "Trained batch 841 batch loss 0.693825364 batch mAP 0.293914795 batch PCKh 0.75\n",
      "Trained batch 842 batch loss 0.653913856 batch mAP 0.280548096 batch PCKh 0\n",
      "Trained batch 843 batch loss 0.680893123 batch mAP 0.220184326 batch PCKh 0.0625\n",
      "Trained batch 844 batch loss 0.698594868 batch mAP 0.274505615 batch PCKh 0.1875\n",
      "Trained batch 845 batch loss 0.716851711 batch mAP 0.355163574 batch PCKh 0.1875\n",
      "Trained batch 846 batch loss 0.64910388 batch mAP 0.342559814 batch PCKh 0.5\n",
      "Trained batch 847 batch loss 0.716247201 batch mAP 0.357025146 batch PCKh 0\n",
      "Trained batch 848 batch loss 0.727421641 batch mAP 0.393005371 batch PCKh 0.3125\n",
      "Trained batch 849 batch loss 0.609810114 batch mAP 0.386261 batch PCKh 0\n",
      "Trained batch 850 batch loss 0.724396765 batch mAP 0.362213135 batch PCKh 0.3125\n",
      "Trained batch 851 batch loss 0.662418246 batch mAP 0.387512207 batch PCKh 0.125\n",
      "Trained batch 852 batch loss 0.645180762 batch mAP 0.388183594 batch PCKh 0.375\n",
      "Trained batch 853 batch loss 0.672712326 batch mAP 0.371643066 batch PCKh 0.1875\n",
      "Trained batch 854 batch loss 0.655114651 batch mAP 0.380462646 batch PCKh 0.8125\n",
      "Trained batch 855 batch loss 0.650714517 batch mAP 0.418243408 batch PCKh 0.75\n",
      "Trained batch 856 batch loss 0.664092302 batch mAP 0.408081055 batch PCKh 0.8125\n",
      "Trained batch 857 batch loss 0.698294163 batch mAP 0.417694092 batch PCKh 0.5\n",
      "Trained batch 858 batch loss 0.652806818 batch mAP 0.424591064 batch PCKh 0.625\n",
      "Trained batch 859 batch loss 0.659181416 batch mAP 0.4090271 batch PCKh 0.4375\n",
      "Trained batch 860 batch loss 0.711550295 batch mAP 0.400268555 batch PCKh 0.625\n",
      "Trained batch 861 batch loss 0.660425 batch mAP 0.42767334 batch PCKh 0.1875\n",
      "Trained batch 862 batch loss 0.698124409 batch mAP 0.370239258 batch PCKh 0.1875\n",
      "Trained batch 863 batch loss 0.69719708 batch mAP 0.374450684 batch PCKh 0\n",
      "Trained batch 864 batch loss 0.69306761 batch mAP 0.382080078 batch PCKh 0.5\n",
      "Trained batch 865 batch loss 0.696283042 batch mAP 0.392730713 batch PCKh 0.0625\n",
      "Trained batch 866 batch loss 0.694399178 batch mAP 0.406707764 batch PCKh 0.6875\n",
      "Trained batch 867 batch loss 0.643350601 batch mAP 0.433685303 batch PCKh 0.0625\n",
      "Trained batch 868 batch loss 0.641733885 batch mAP 0.407196045 batch PCKh 0.75\n",
      "Trained batch 869 batch loss 0.69326216 batch mAP 0.377502441 batch PCKh 0.75\n",
      "Trained batch 870 batch loss 0.590655088 batch mAP 0.438049316 batch PCKh 0.625\n",
      "Trained batch 871 batch loss 0.62349993 batch mAP 0.434967041 batch PCKh 0.375\n",
      "Trained batch 872 batch loss 0.664741874 batch mAP 0.399017334 batch PCKh 0.5\n",
      "Trained batch 873 batch loss 0.697396398 batch mAP 0.414825439 batch PCKh 0.5625\n",
      "Trained batch 874 batch loss 0.717622757 batch mAP 0.394500732 batch PCKh 0.125\n",
      "Trained batch 875 batch loss 0.732637584 batch mAP 0.375610352 batch PCKh 0.5625\n",
      "Trained batch 876 batch loss 0.689830542 batch mAP 0.357116699 batch PCKh 0.125\n",
      "Trained batch 877 batch loss 0.708971 batch mAP 0.41003418 batch PCKh 0.125\n",
      "Trained batch 878 batch loss 0.713225245 batch mAP 0.340484619 batch PCKh 0\n",
      "Trained batch 879 batch loss 0.673672438 batch mAP 0.373199463 batch PCKh 0.5625\n",
      "Trained batch 880 batch loss 0.651491046 batch mAP 0.355072021 batch PCKh 0.625\n",
      "Trained batch 881 batch loss 0.731024086 batch mAP 0.36138916 batch PCKh 0.5625\n",
      "Trained batch 882 batch loss 0.659199 batch mAP 0.358154297 batch PCKh 0.625\n",
      "Trained batch 883 batch loss 0.62977314 batch mAP 0.341369629 batch PCKh 0.5625\n",
      "Trained batch 884 batch loss 0.656888127 batch mAP 0.33190918 batch PCKh 0.0625\n",
      "Trained batch 885 batch loss 0.683750629 batch mAP 0.332580566 batch PCKh 0.5625\n",
      "Trained batch 886 batch loss 0.723878 batch mAP 0.310272217 batch PCKh 0.5625\n",
      "Trained batch 887 batch loss 0.716021359 batch mAP 0.325042725 batch PCKh 0.0625\n",
      "Trained batch 888 batch loss 0.724113 batch mAP 0.306915283 batch PCKh 0.625\n",
      "Trained batch 889 batch loss 0.669598699 batch mAP 0.246307373 batch PCKh 0.5625\n",
      "Trained batch 890 batch loss 0.641497612 batch mAP 0.244140625 batch PCKh 0.25\n",
      "Trained batch 891 batch loss 0.665795326 batch mAP 0.286193848 batch PCKh 0.625\n",
      "Trained batch 892 batch loss 0.630498707 batch mAP 0.375732422 batch PCKh 0.4375\n",
      "Trained batch 893 batch loss 0.6764974 batch mAP 0.37677002 batch PCKh 0.25\n",
      "Trained batch 894 batch loss 0.651885211 batch mAP 0.398620605 batch PCKh 0.3125\n",
      "Trained batch 895 batch loss 0.677524209 batch mAP 0.461395264 batch PCKh 0.3125\n",
      "Trained batch 896 batch loss 0.714417934 batch mAP 0.408782959 batch PCKh 0.375\n",
      "Trained batch 897 batch loss 0.755564868 batch mAP 0.417175293 batch PCKh 0.4375\n",
      "Trained batch 898 batch loss 0.71644485 batch mAP 0.467529297 batch PCKh 0\n",
      "Trained batch 899 batch loss 0.752983809 batch mAP 0.428833 batch PCKh 0.3125\n",
      "Trained batch 900 batch loss 0.770537615 batch mAP 0.442871094 batch PCKh 0.875\n",
      "Trained batch 901 batch loss 0.714030325 batch mAP 0.405822754 batch PCKh 0.5\n",
      "Trained batch 902 batch loss 0.684033871 batch mAP 0.427307129 batch PCKh 0.6875\n",
      "Trained batch 903 batch loss 0.614736915 batch mAP 0.3828125 batch PCKh 0.625\n",
      "Trained batch 904 batch loss 0.623360276 batch mAP 0.350341797 batch PCKh 0.375\n",
      "Trained batch 905 batch loss 0.626197338 batch mAP 0.341186523 batch PCKh 0.75\n",
      "Trained batch 906 batch loss 0.587421119 batch mAP 0.315673828 batch PCKh 0.875\n",
      "Trained batch 907 batch loss 0.634434104 batch mAP 0.279785156 batch PCKh 0.625\n",
      "Trained batch 908 batch loss 0.611637712 batch mAP 0.314147949 batch PCKh 0.6875\n",
      "Trained batch 909 batch loss 0.627596 batch mAP 0.37322998 batch PCKh 0.5625\n",
      "Trained batch 910 batch loss 0.714268208 batch mAP 0.305511475 batch PCKh 0\n",
      "Trained batch 911 batch loss 0.606859624 batch mAP 0.430664062 batch PCKh 0.25\n",
      "Trained batch 912 batch loss 0.500162721 batch mAP 0.447113037 batch PCKh 0.1875\n",
      "Trained batch 913 batch loss 0.586213529 batch mAP 0.410522461 batch PCKh 0.625\n",
      "Trained batch 914 batch loss 0.658706427 batch mAP 0.396575928 batch PCKh 0.1875\n",
      "Trained batch 915 batch loss 0.72567457 batch mAP 0.437103271 batch PCKh 0\n",
      "Trained batch 916 batch loss 0.786930561 batch mAP 0.37197876 batch PCKh 0.0625\n",
      "Trained batch 917 batch loss 0.736092746 batch mAP 0.418457031 batch PCKh 0.4375\n",
      "Trained batch 918 batch loss 0.714359522 batch mAP 0.42880249 batch PCKh 0.5\n",
      "Trained batch 919 batch loss 0.7390517 batch mAP 0.421569824 batch PCKh 0.4375\n",
      "Trained batch 920 batch loss 0.739367127 batch mAP 0.441925049 batch PCKh 0.25\n",
      "Trained batch 921 batch loss 0.710467458 batch mAP 0.452270508 batch PCKh 0.125\n",
      "Trained batch 922 batch loss 0.756044507 batch mAP 0.374725342 batch PCKh 0.375\n",
      "Trained batch 923 batch loss 0.755407035 batch mAP 0.387939453 batch PCKh 0.125\n",
      "Trained batch 924 batch loss 0.708183527 batch mAP 0.410003662 batch PCKh 0.1875\n",
      "Trained batch 925 batch loss 0.710285068 batch mAP 0.35369873 batch PCKh 0.25\n",
      "Trained batch 926 batch loss 0.703518748 batch mAP 0.347473145 batch PCKh 0.5625\n",
      "Trained batch 927 batch loss 0.657182097 batch mAP 0.364196777 batch PCKh 0.75\n",
      "Trained batch 928 batch loss 0.614739 batch mAP 0.387237549 batch PCKh 0.75\n",
      "Trained batch 929 batch loss 0.639474928 batch mAP 0.4090271 batch PCKh 0.75\n",
      "Trained batch 930 batch loss 0.700147867 batch mAP 0.390838623 batch PCKh 0.875\n",
      "Trained batch 931 batch loss 0.694376588 batch mAP 0.425842285 batch PCKh 0.625\n",
      "Trained batch 932 batch loss 0.72066462 batch mAP 0.450622559 batch PCKh 0.625\n",
      "Trained batch 933 batch loss 0.737865686 batch mAP 0.445343018 batch PCKh 0.5625\n",
      "Trained batch 934 batch loss 0.699849486 batch mAP 0.451385498 batch PCKh 0.25\n",
      "Trained batch 935 batch loss 0.6874578 batch mAP 0.471862793 batch PCKh 0.625\n",
      "Trained batch 936 batch loss 0.739299893 batch mAP 0.399414062 batch PCKh 0\n",
      "Trained batch 937 batch loss 0.686692357 batch mAP 0.374053955 batch PCKh 0.375\n",
      "Trained batch 938 batch loss 0.775957823 batch mAP 0.420440674 batch PCKh 0.25\n",
      "Trained batch 939 batch loss 0.710994661 batch mAP 0.425201416 batch PCKh 0.3125\n",
      "Trained batch 940 batch loss 0.674624085 batch mAP 0.389526367 batch PCKh 0.6875\n",
      "Trained batch 941 batch loss 0.697366178 batch mAP 0.390258789 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 942 batch loss 0.723789215 batch mAP 0.390625 batch PCKh 0.4375\n",
      "Trained batch 943 batch loss 0.672836065 batch mAP 0.370239258 batch PCKh 0.6875\n",
      "Trained batch 944 batch loss 0.657650471 batch mAP 0.329803467 batch PCKh 0.625\n",
      "Trained batch 945 batch loss 0.755364895 batch mAP 0.286438 batch PCKh 0.75\n",
      "Trained batch 946 batch loss 0.773262262 batch mAP 0.314666748 batch PCKh 0.0625\n",
      "Trained batch 947 batch loss 0.803972602 batch mAP 0.303009033 batch PCKh 0.1875\n",
      "Trained batch 948 batch loss 0.717891216 batch mAP 0.265960693 batch PCKh 0.875\n",
      "Trained batch 949 batch loss 0.709952056 batch mAP 0.219665527 batch PCKh 0.875\n",
      "Trained batch 950 batch loss 0.720742345 batch mAP 0.222869873 batch PCKh 0.3125\n",
      "Trained batch 951 batch loss 0.789174259 batch mAP 0.227508545 batch PCKh 0.125\n",
      "Trained batch 952 batch loss 0.743642569 batch mAP 0.252685547 batch PCKh 0.3125\n",
      "Trained batch 953 batch loss 0.704727173 batch mAP 0.271179199 batch PCKh 0.25\n",
      "Trained batch 954 batch loss 0.733678639 batch mAP 0.276245117 batch PCKh 0.0625\n",
      "Trained batch 955 batch loss 0.734202325 batch mAP 0.33190918 batch PCKh 0.25\n",
      "Trained batch 956 batch loss 0.631271243 batch mAP 0.34564209 batch PCKh 0.25\n",
      "Trained batch 957 batch loss 0.708334804 batch mAP 0.335266113 batch PCKh 0.0625\n",
      "Trained batch 958 batch loss 0.647257924 batch mAP 0.364349365 batch PCKh 0.625\n",
      "Trained batch 959 batch loss 0.627883434 batch mAP 0.3828125 batch PCKh 0.125\n",
      "Trained batch 960 batch loss 0.671054 batch mAP 0.373077393 batch PCKh 0.3125\n",
      "Trained batch 961 batch loss 0.643905044 batch mAP 0.385742188 batch PCKh 0.375\n",
      "Trained batch 962 batch loss 0.60203445 batch mAP 0.38659668 batch PCKh 0.25\n",
      "Trained batch 963 batch loss 0.659279227 batch mAP 0.414001465 batch PCKh 0.375\n",
      "Trained batch 964 batch loss 0.726050735 batch mAP 0.356903076 batch PCKh 0\n",
      "Trained batch 965 batch loss 0.804671764 batch mAP 0.341064453 batch PCKh 0\n",
      "Trained batch 966 batch loss 0.661848485 batch mAP 0.385620117 batch PCKh 0.25\n",
      "Trained batch 967 batch loss 0.854985297 batch mAP 0.320465088 batch PCKh 0\n",
      "Trained batch 968 batch loss 0.82025516 batch mAP 0.371246338 batch PCKh 0\n",
      "Trained batch 969 batch loss 0.752844 batch mAP 0.36605835 batch PCKh 0.4375\n",
      "Trained batch 970 batch loss 0.750324249 batch mAP 0.390136719 batch PCKh 0.25\n",
      "Trained batch 971 batch loss 0.795489252 batch mAP 0.387420654 batch PCKh 0.5\n",
      "Trained batch 972 batch loss 0.763998091 batch mAP 0.383483887 batch PCKh 0.125\n",
      "Trained batch 973 batch loss 0.71936053 batch mAP 0.334625244 batch PCKh 0.75\n",
      "Trained batch 974 batch loss 0.740782142 batch mAP 0.306945801 batch PCKh 0.6875\n",
      "Trained batch 975 batch loss 0.72112453 batch mAP 0.172088623 batch PCKh 0.5625\n",
      "Trained batch 976 batch loss 0.750549376 batch mAP 0.119598389 batch PCKh 0.875\n",
      "Trained batch 977 batch loss 0.688990712 batch mAP 0.0914001465 batch PCKh 0.625\n",
      "Trained batch 978 batch loss 0.708156228 batch mAP 0.0435180664 batch PCKh 0.5625\n",
      "Trained batch 979 batch loss 0.789267778 batch mAP 0.0459899902 batch PCKh 0.5\n",
      "Trained batch 980 batch loss 0.667588651 batch mAP 0.0341491699 batch PCKh 0.75\n",
      "Trained batch 981 batch loss 0.685543895 batch mAP 0.0790710449 batch PCKh 0.375\n",
      "Trained batch 982 batch loss 0.673527479 batch mAP 0.149139404 batch PCKh 0.5\n",
      "Trained batch 983 batch loss 0.663439 batch mAP 0.068573 batch PCKh 0.4375\n",
      "Trained batch 984 batch loss 0.703793168 batch mAP 0.193511963 batch PCKh 0.0625\n",
      "Trained batch 985 batch loss 0.688715816 batch mAP 0.183624268 batch PCKh 0.3125\n",
      "Trained batch 986 batch loss 0.698603392 batch mAP 0.279998779 batch PCKh 0.1875\n",
      "Trained batch 987 batch loss 0.682401597 batch mAP 0.31439209 batch PCKh 0.125\n",
      "Trained batch 988 batch loss 0.773015499 batch mAP 0.373687744 batch PCKh 0.125\n",
      "Trained batch 989 batch loss 0.738211572 batch mAP 0.379547119 batch PCKh 0.375\n",
      "Trained batch 990 batch loss 0.687048316 batch mAP 0.375274658 batch PCKh 0.375\n",
      "Trained batch 991 batch loss 0.702714682 batch mAP 0.375671387 batch PCKh 0.75\n",
      "Trained batch 992 batch loss 0.661778748 batch mAP 0.34677124 batch PCKh 0\n",
      "Trained batch 993 batch loss 0.651974738 batch mAP 0.281768799 batch PCKh 0.625\n",
      "Trained batch 994 batch loss 0.664504349 batch mAP 0.208618164 batch PCKh 0.625\n",
      "Trained batch 995 batch loss 0.695977926 batch mAP 0.299377441 batch PCKh 0.625\n",
      "Trained batch 996 batch loss 0.691205144 batch mAP 0.289093018 batch PCKh 0.5\n",
      "Trained batch 997 batch loss 0.652520895 batch mAP 0.305480957 batch PCKh 0.75\n",
      "Trained batch 998 batch loss 0.6885131 batch mAP 0.325195312 batch PCKh 0.5625\n",
      "Trained batch 999 batch loss 0.696533 batch mAP 0.243988037 batch PCKh 0.6875\n",
      "Trained batch 1000 batch loss 0.620142758 batch mAP 0.316772461 batch PCKh 0.0625\n",
      "Trained batch 1001 batch loss 0.620549 batch mAP 0.391052246 batch PCKh 0.5625\n",
      "Trained batch 1002 batch loss 0.598636746 batch mAP 0.34185791 batch PCKh 0.1875\n",
      "Trained batch 1003 batch loss 0.740638614 batch mAP 0.265838623 batch PCKh 0.375\n",
      "Trained batch 1004 batch loss 0.754716933 batch mAP 0.279205322 batch PCKh 0\n",
      "Trained batch 1005 batch loss 0.680279374 batch mAP 0.339050293 batch PCKh 0.5625\n",
      "Trained batch 1006 batch loss 0.630791068 batch mAP 0.324127197 batch PCKh 0.75\n",
      "Trained batch 1007 batch loss 0.627464533 batch mAP 0.284240723 batch PCKh 0.25\n",
      "Trained batch 1008 batch loss 0.687411666 batch mAP 0.290710449 batch PCKh 0.5625\n",
      "Trained batch 1009 batch loss 0.690872848 batch mAP 0.296356201 batch PCKh 0.5\n",
      "Trained batch 1010 batch loss 0.716743827 batch mAP 0.305511475 batch PCKh 0\n",
      "Trained batch 1011 batch loss 0.690674305 batch mAP 0.3387146 batch PCKh 0.6875\n",
      "Trained batch 1012 batch loss 0.603681 batch mAP 0.355438232 batch PCKh 0.0625\n",
      "Trained batch 1013 batch loss 0.777884245 batch mAP 0.328063965 batch PCKh 0.5625\n",
      "Trained batch 1014 batch loss 0.703449309 batch mAP 0.367614746 batch PCKh 0.1875\n",
      "Trained batch 1015 batch loss 0.750475645 batch mAP 0.35458374 batch PCKh 0.1875\n",
      "Trained batch 1016 batch loss 0.628214717 batch mAP 0.349151611 batch PCKh 0.4375\n",
      "Trained batch 1017 batch loss 0.542411804 batch mAP 0.373382568 batch PCKh 0.75\n",
      "Trained batch 1018 batch loss 0.556827188 batch mAP 0.353973389 batch PCKh 0.3125\n",
      "Trained batch 1019 batch loss 0.542806268 batch mAP 0.359405518 batch PCKh 0.3125\n",
      "Trained batch 1020 batch loss 0.559359908 batch mAP 0.405761719 batch PCKh 0\n",
      "Trained batch 1021 batch loss 0.671594381 batch mAP 0.392944336 batch PCKh 0.375\n",
      "Trained batch 1022 batch loss 0.594721198 batch mAP 0.431030273 batch PCKh 0.125\n",
      "Trained batch 1023 batch loss 0.705325425 batch mAP 0.383575439 batch PCKh 0.3125\n",
      "Trained batch 1024 batch loss 0.662970304 batch mAP 0.401000977 batch PCKh 0.375\n",
      "Trained batch 1025 batch loss 0.611591 batch mAP 0.409301758 batch PCKh 0.5\n",
      "Trained batch 1026 batch loss 0.619544089 batch mAP 0.407928467 batch PCKh 0.4375\n",
      "Trained batch 1027 batch loss 0.688295543 batch mAP 0.371246338 batch PCKh 0\n",
      "Trained batch 1028 batch loss 0.653807104 batch mAP 0.359893799 batch PCKh 0.5625\n",
      "Trained batch 1029 batch loss 0.729351699 batch mAP 0.338409424 batch PCKh 0.5\n",
      "Trained batch 1030 batch loss 0.677901626 batch mAP 0.37789917 batch PCKh 0.6875\n",
      "Trained batch 1031 batch loss 0.736437 batch mAP 0.384521484 batch PCKh 0.6875\n",
      "Trained batch 1032 batch loss 0.676482439 batch mAP 0.399078369 batch PCKh 0.5625\n",
      "Trained batch 1033 batch loss 0.737293363 batch mAP 0.415618896 batch PCKh 0.1875\n",
      "Trained batch 1034 batch loss 0.668542385 batch mAP 0.411071777 batch PCKh 0.125\n",
      "Trained batch 1035 batch loss 0.684942722 batch mAP 0.393859863 batch PCKh 0.5625\n",
      "Trained batch 1036 batch loss 0.672175586 batch mAP 0.389801025 batch PCKh 0.75\n",
      "Trained batch 1037 batch loss 0.731763184 batch mAP 0.380157471 batch PCKh 0.25\n",
      "Trained batch 1038 batch loss 0.824665964 batch mAP 0.384887695 batch PCKh 0.0625\n",
      "Trained batch 1039 batch loss 0.824439526 batch mAP 0.388183594 batch PCKh 0\n",
      "Trained batch 1040 batch loss 0.77260989 batch mAP 0.3465271 batch PCKh 0\n",
      "Trained batch 1041 batch loss 0.76685524 batch mAP 0.304290771 batch PCKh 0.25\n",
      "Trained batch 1042 batch loss 0.717554 batch mAP 0.186065674 batch PCKh 0.25\n",
      "Trained batch 1043 batch loss 0.767250299 batch mAP 0.292449951 batch PCKh 0.125\n",
      "Trained batch 1044 batch loss 0.68123585 batch mAP 0.277496338 batch PCKh 0\n",
      "Trained batch 1045 batch loss 0.675771594 batch mAP 0.319152832 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1046 batch loss 0.711436391 batch mAP 0.321716309 batch PCKh 0.5\n",
      "Trained batch 1047 batch loss 0.728369772 batch mAP 0.321807861 batch PCKh 0.1875\n",
      "Trained batch 1048 batch loss 0.638693213 batch mAP 0.350646973 batch PCKh 0.75\n",
      "Trained batch 1049 batch loss 0.652275801 batch mAP 0.368530273 batch PCKh 0.75\n",
      "Trained batch 1050 batch loss 0.614363313 batch mAP 0.374786377 batch PCKh 0.625\n",
      "Trained batch 1051 batch loss 0.739804745 batch mAP 0.389129639 batch PCKh 0.1875\n",
      "Trained batch 1052 batch loss 0.710137546 batch mAP 0.416015625 batch PCKh 0.6875\n",
      "Trained batch 1053 batch loss 0.695443332 batch mAP 0.430542 batch PCKh 0.25\n",
      "Trained batch 1054 batch loss 0.64748621 batch mAP 0.380126953 batch PCKh 0.3125\n",
      "Trained batch 1055 batch loss 0.605926871 batch mAP 0.413726807 batch PCKh 0.0625\n",
      "Trained batch 1056 batch loss 0.647311449 batch mAP 0.364837646 batch PCKh 0.4375\n",
      "Trained batch 1057 batch loss 0.669443488 batch mAP 0.410247803 batch PCKh 0.75\n",
      "Trained batch 1058 batch loss 0.677280188 batch mAP 0.404296875 batch PCKh 0.5\n",
      "Trained batch 1059 batch loss 0.710074961 batch mAP 0.355621338 batch PCKh 0.5\n",
      "Trained batch 1060 batch loss 0.730392456 batch mAP 0.3046875 batch PCKh 0.625\n",
      "Trained batch 1061 batch loss 0.634601116 batch mAP 0.380188 batch PCKh 0.25\n",
      "Trained batch 1062 batch loss 0.730835557 batch mAP 0.308258057 batch PCKh 0.75\n",
      "Trained batch 1063 batch loss 0.735174775 batch mAP 0.383209229 batch PCKh 0.625\n",
      "Trained batch 1064 batch loss 0.688655 batch mAP 0.351501465 batch PCKh 0.375\n",
      "Trained batch 1065 batch loss 0.723355532 batch mAP 0.417266846 batch PCKh 0.5625\n",
      "Trained batch 1066 batch loss 0.681107163 batch mAP 0.386535645 batch PCKh 0.875\n",
      "Trained batch 1067 batch loss 0.679794073 batch mAP 0.394989 batch PCKh 0.5\n",
      "Trained batch 1068 batch loss 0.742861271 batch mAP 0.355529785 batch PCKh 0.5\n",
      "Trained batch 1069 batch loss 0.711471319 batch mAP 0.306030273 batch PCKh 0.6875\n",
      "Trained batch 1070 batch loss 0.712960541 batch mAP 0.305450439 batch PCKh 0.5\n",
      "Trained batch 1071 batch loss 0.74145782 batch mAP 0.25378418 batch PCKh 0.25\n",
      "Trained batch 1072 batch loss 0.738740325 batch mAP 0.31930542 batch PCKh 0\n",
      "Trained batch 1073 batch loss 0.694506049 batch mAP 0.300415039 batch PCKh 0.5625\n",
      "Trained batch 1074 batch loss 0.686231613 batch mAP 0.336639404 batch PCKh 0.8125\n",
      "Trained batch 1075 batch loss 0.737558365 batch mAP 0.342498779 batch PCKh 0\n",
      "Trained batch 1076 batch loss 0.696328044 batch mAP 0.362518311 batch PCKh 0.75\n",
      "Trained batch 1077 batch loss 0.651003361 batch mAP 0.344665527 batch PCKh 0.1875\n",
      "Trained batch 1078 batch loss 0.663782418 batch mAP 0.313232422 batch PCKh 0\n",
      "Trained batch 1079 batch loss 0.70249474 batch mAP 0.36340332 batch PCKh 0.0625\n",
      "Trained batch 1080 batch loss 0.667580485 batch mAP 0.354766846 batch PCKh 0.4375\n",
      "Trained batch 1081 batch loss 0.661406755 batch mAP 0.345184326 batch PCKh 0.3125\n",
      "Trained batch 1082 batch loss 0.539432347 batch mAP 0.373046875 batch PCKh 0.4375\n",
      "Trained batch 1083 batch loss 0.601783812 batch mAP 0.392608643 batch PCKh 0.6875\n",
      "Trained batch 1084 batch loss 0.677693248 batch mAP 0.407592773 batch PCKh 0.875\n",
      "Trained batch 1085 batch loss 0.744970798 batch mAP 0.374389648 batch PCKh 0.0625\n",
      "Trained batch 1086 batch loss 0.861106873 batch mAP 0.284576416 batch PCKh 0.25\n",
      "Trained batch 1087 batch loss 0.828540444 batch mAP 0.371490479 batch PCKh 0\n",
      "Trained batch 1088 batch loss 0.703884602 batch mAP 0.395965576 batch PCKh 0.1875\n",
      "Trained batch 1089 batch loss 0.69553411 batch mAP 0.41784668 batch PCKh 0.875\n",
      "Trained batch 1090 batch loss 0.6819911 batch mAP 0.424377441 batch PCKh 0.75\n",
      "Trained batch 1091 batch loss 0.680496931 batch mAP 0.421020508 batch PCKh 0.5625\n",
      "Trained batch 1092 batch loss 0.725279331 batch mAP 0.381256104 batch PCKh 0\n",
      "Trained batch 1093 batch loss 0.7375772 batch mAP 0.359771729 batch PCKh 0.25\n",
      "Trained batch 1094 batch loss 0.730369866 batch mAP 0.393585205 batch PCKh 0.125\n",
      "Trained batch 1095 batch loss 0.667790592 batch mAP 0.352233887 batch PCKh 0.5\n",
      "Trained batch 1096 batch loss 0.775554 batch mAP 0.3621521 batch PCKh 0\n",
      "Trained batch 1097 batch loss 0.689422667 batch mAP 0.308105469 batch PCKh 0\n",
      "Trained batch 1098 batch loss 0.650485337 batch mAP 0.273162842 batch PCKh 0.5625\n",
      "Trained batch 1099 batch loss 0.674274743 batch mAP 0.232391357 batch PCKh 0.0625\n",
      "Trained batch 1100 batch loss 0.658596873 batch mAP 0.240997314 batch PCKh 0.25\n",
      "Trained batch 1101 batch loss 0.651435733 batch mAP 0.304748535 batch PCKh 0\n",
      "Trained batch 1102 batch loss 0.663172483 batch mAP 0.362487793 batch PCKh 0.6875\n",
      "Trained batch 1103 batch loss 0.662271 batch mAP 0.365386963 batch PCKh 0.5625\n",
      "Trained batch 1104 batch loss 0.637329042 batch mAP 0.324523926 batch PCKh 0\n",
      "Trained batch 1105 batch loss 0.693597198 batch mAP 0.36505127 batch PCKh 0.5\n",
      "Trained batch 1106 batch loss 0.753019392 batch mAP 0.342437744 batch PCKh 0.1875\n",
      "Trained batch 1107 batch loss 0.714995742 batch mAP 0.419708252 batch PCKh 0.8125\n",
      "Trained batch 1108 batch loss 0.717560887 batch mAP 0.430206299 batch PCKh 0.5625\n",
      "Trained batch 1109 batch loss 0.730995774 batch mAP 0.394439697 batch PCKh 0.4375\n",
      "Trained batch 1110 batch loss 0.736484885 batch mAP 0.339202881 batch PCKh 0\n",
      "Trained batch 1111 batch loss 0.725974381 batch mAP 0.383239746 batch PCKh 0.4375\n",
      "Trained batch 1112 batch loss 0.705166698 batch mAP 0.377410889 batch PCKh 0.4375\n",
      "Trained batch 1113 batch loss 0.670982957 batch mAP 0.330627441 batch PCKh 0.5625\n",
      "Trained batch 1114 batch loss 0.714823186 batch mAP 0.353027344 batch PCKh 0.1875\n",
      "Trained batch 1115 batch loss 0.750680327 batch mAP 0.348846436 batch PCKh 0.1875\n",
      "Trained batch 1116 batch loss 0.698049307 batch mAP 0.314605713 batch PCKh 0.625\n",
      "Trained batch 1117 batch loss 0.598641753 batch mAP 0.324005127 batch PCKh 0.1875\n",
      "Trained batch 1118 batch loss 0.696144581 batch mAP 0.363067627 batch PCKh 0.125\n",
      "Trained batch 1119 batch loss 0.638618946 batch mAP 0.38104248 batch PCKh 0.25\n",
      "Trained batch 1120 batch loss 0.65713197 batch mAP 0.420684814 batch PCKh 0.5\n",
      "Trained batch 1121 batch loss 0.620753646 batch mAP 0.433990479 batch PCKh 0.1875\n",
      "Trained batch 1122 batch loss 0.689141273 batch mAP 0.42010498 batch PCKh 0.125\n",
      "Trained batch 1123 batch loss 0.697275043 batch mAP 0.406982422 batch PCKh 0.375\n",
      "Trained batch 1124 batch loss 0.703701079 batch mAP 0.452331543 batch PCKh 0.5625\n",
      "Trained batch 1125 batch loss 0.665000558 batch mAP 0.424682617 batch PCKh 0.125\n",
      "Trained batch 1126 batch loss 0.697326779 batch mAP 0.410339355 batch PCKh 0.25\n",
      "Trained batch 1127 batch loss 0.622783422 batch mAP 0.346038818 batch PCKh 0.1875\n",
      "Trained batch 1128 batch loss 0.702201486 batch mAP 0.332244873 batch PCKh 0.125\n",
      "Trained batch 1129 batch loss 0.710107386 batch mAP 0.339324951 batch PCKh 0.375\n",
      "Trained batch 1130 batch loss 0.668527186 batch mAP 0.290527344 batch PCKh 0.1875\n",
      "Trained batch 1131 batch loss 0.681379616 batch mAP 0.286224365 batch PCKh 0.125\n",
      "Trained batch 1132 batch loss 0.664906144 batch mAP 0.298217773 batch PCKh 0.5\n",
      "Trained batch 1133 batch loss 0.664049745 batch mAP 0.288208 batch PCKh 0.0625\n",
      "Trained batch 1134 batch loss 0.659017801 batch mAP 0.260314941 batch PCKh 0.5\n",
      "Trained batch 1135 batch loss 0.747587323 batch mAP 0.303192139 batch PCKh 0.3125\n",
      "Trained batch 1136 batch loss 0.666953802 batch mAP 0.245727539 batch PCKh 0\n",
      "Trained batch 1137 batch loss 0.622290313 batch mAP 0.280212402 batch PCKh 0.1875\n",
      "Trained batch 1138 batch loss 0.588353753 batch mAP 0.313476562 batch PCKh 0.5\n",
      "Trained batch 1139 batch loss 0.661480427 batch mAP 0.272735596 batch PCKh 0.375\n",
      "Trained batch 1140 batch loss 0.66361469 batch mAP 0.292938232 batch PCKh 0.1875\n",
      "Trained batch 1141 batch loss 0.642831683 batch mAP 0.185638428 batch PCKh 0.625\n",
      "Trained batch 1142 batch loss 0.629663706 batch mAP 0.240081787 batch PCKh 0.5\n",
      "Trained batch 1143 batch loss 0.643042 batch mAP 0.328063965 batch PCKh 0.3125\n",
      "Trained batch 1144 batch loss 0.67933476 batch mAP 0.29498291 batch PCKh 0.1875\n",
      "Trained batch 1145 batch loss 0.634156942 batch mAP 0.28225708 batch PCKh 0.375\n",
      "Trained batch 1146 batch loss 0.605822086 batch mAP 0.267486572 batch PCKh 0.25\n",
      "Trained batch 1147 batch loss 0.621113658 batch mAP 0.286956787 batch PCKh 0.1875\n",
      "Trained batch 1148 batch loss 0.571512938 batch mAP 0.277008057 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1149 batch loss 0.639356315 batch mAP 0.289581299 batch PCKh 0.4375\n",
      "Trained batch 1150 batch loss 0.619592667 batch mAP 0.290527344 batch PCKh 0.625\n",
      "Trained batch 1151 batch loss 0.567402363 batch mAP 0.292022705 batch PCKh 0.375\n",
      "Trained batch 1152 batch loss 0.68408674 batch mAP 0.261474609 batch PCKh 0.8125\n",
      "Trained batch 1153 batch loss 0.634736836 batch mAP 0.279663086 batch PCKh 0.5\n",
      "Trained batch 1154 batch loss 0.592990458 batch mAP 0.172241211 batch PCKh 0.3125\n",
      "Trained batch 1155 batch loss 0.637742221 batch mAP 0.315490723 batch PCKh 0.5\n",
      "Trained batch 1156 batch loss 0.59713912 batch mAP 0.33782959 batch PCKh 0.1875\n",
      "Trained batch 1157 batch loss 0.572819412 batch mAP 0.313354492 batch PCKh 0.625\n",
      "Trained batch 1158 batch loss 0.663565874 batch mAP 0.272277832 batch PCKh 0.75\n",
      "Trained batch 1159 batch loss 0.68585515 batch mAP 0.294647217 batch PCKh 0.125\n",
      "Trained batch 1160 batch loss 0.6493361 batch mAP 0.307098389 batch PCKh 0.5\n",
      "Trained batch 1161 batch loss 0.664054453 batch mAP 0.303955078 batch PCKh 0.125\n",
      "Trained batch 1162 batch loss 0.692340612 batch mAP 0.330169678 batch PCKh 0\n",
      "Trained batch 1163 batch loss 0.805960536 batch mAP 0.25604248 batch PCKh 0\n",
      "Trained batch 1164 batch loss 0.697958 batch mAP 0.367797852 batch PCKh 0.375\n",
      "Trained batch 1165 batch loss 0.749239326 batch mAP 0.376617432 batch PCKh 0.5\n",
      "Trained batch 1166 batch loss 0.755543351 batch mAP 0.415344238 batch PCKh 0.1875\n",
      "Trained batch 1167 batch loss 0.70253408 batch mAP 0.429992676 batch PCKh 0.1875\n",
      "Trained batch 1168 batch loss 0.703802824 batch mAP 0.370239258 batch PCKh 0.3125\n",
      "Trained batch 1169 batch loss 0.670694947 batch mAP 0.361053467 batch PCKh 0.125\n",
      "Trained batch 1170 batch loss 0.671144545 batch mAP 0.376403809 batch PCKh 0.1875\n",
      "Trained batch 1171 batch loss 0.6095016 batch mAP 0.348754883 batch PCKh 0.4375\n",
      "Trained batch 1172 batch loss 0.63629508 batch mAP 0.302398682 batch PCKh 0.125\n",
      "Trained batch 1173 batch loss 0.631749809 batch mAP 0.332061768 batch PCKh 0.4375\n",
      "Trained batch 1174 batch loss 0.653871477 batch mAP 0.364501953 batch PCKh 0.5\n",
      "Trained batch 1175 batch loss 0.631816387 batch mAP 0.369232178 batch PCKh 0.5625\n",
      "Trained batch 1176 batch loss 0.685187578 batch mAP 0.418182373 batch PCKh 0.25\n",
      "Trained batch 1177 batch loss 0.711412 batch mAP 0.412567139 batch PCKh 0.1875\n",
      "Trained batch 1178 batch loss 0.691913128 batch mAP 0.450744629 batch PCKh 0.3125\n",
      "Trained batch 1179 batch loss 0.669404387 batch mAP 0.481048584 batch PCKh 0.1875\n",
      "Trained batch 1180 batch loss 0.677932858 batch mAP 0.478668213 batch PCKh 0.375\n",
      "Trained batch 1181 batch loss 0.663069844 batch mAP 0.492553711 batch PCKh 0.3125\n",
      "Trained batch 1182 batch loss 0.668244302 batch mAP 0.486053467 batch PCKh 0.3125\n",
      "Trained batch 1183 batch loss 0.660556197 batch mAP 0.498718262 batch PCKh 0.1875\n",
      "Trained batch 1184 batch loss 0.652439654 batch mAP 0.504303 batch PCKh 0.6875\n",
      "Trained batch 1185 batch loss 0.645550847 batch mAP 0.527801514 batch PCKh 0.5\n",
      "Trained batch 1186 batch loss 0.649346232 batch mAP 0.510620117 batch PCKh 0.3125\n",
      "Trained batch 1187 batch loss 0.684906363 batch mAP 0.532928467 batch PCKh 0.1875\n",
      "Trained batch 1188 batch loss 0.653235257 batch mAP 0.513366699 batch PCKh 0.25\n",
      "Trained batch 1189 batch loss 0.672481179 batch mAP 0.482635498 batch PCKh 0.8125\n",
      "Trained batch 1190 batch loss 0.713851154 batch mAP 0.450561523 batch PCKh 0.4375\n",
      "Trained batch 1191 batch loss 0.683126 batch mAP 0.479492188 batch PCKh 0.5625\n",
      "Trained batch 1192 batch loss 0.669691443 batch mAP 0.462249756 batch PCKh 0.375\n",
      "Trained batch 1193 batch loss 0.706184328 batch mAP 0.424316406 batch PCKh 0.6875\n",
      "Trained batch 1194 batch loss 0.619431138 batch mAP 0.497772217 batch PCKh 0\n",
      "Trained batch 1195 batch loss 0.610806882 batch mAP 0.448394775 batch PCKh 0.5625\n",
      "Trained batch 1196 batch loss 0.703014612 batch mAP 0.421203613 batch PCKh 0.5625\n",
      "Trained batch 1197 batch loss 0.657640278 batch mAP 0.421661377 batch PCKh 0.375\n",
      "Trained batch 1198 batch loss 0.611886 batch mAP 0.422027588 batch PCKh 0.3125\n",
      "Trained batch 1199 batch loss 0.718804598 batch mAP 0.399047852 batch PCKh 0.25\n",
      "Trained batch 1200 batch loss 0.703407645 batch mAP 0.368774414 batch PCKh 0.3125\n",
      "Trained batch 1201 batch loss 0.692107081 batch mAP 0.408538818 batch PCKh 0.4375\n",
      "Trained batch 1202 batch loss 0.754188776 batch mAP 0.352172852 batch PCKh 0.375\n",
      "Trained batch 1203 batch loss 0.662525237 batch mAP 0.348938 batch PCKh 0.4375\n",
      "Trained batch 1204 batch loss 0.618976 batch mAP 0.361083984 batch PCKh 0.0625\n",
      "Trained batch 1205 batch loss 0.787522674 batch mAP 0.391998291 batch PCKh 0.5625\n",
      "Trained batch 1206 batch loss 0.714167953 batch mAP 0.393798828 batch PCKh 0.3125\n",
      "Trained batch 1207 batch loss 0.734335542 batch mAP 0.3487854 batch PCKh 0.6875\n",
      "Trained batch 1208 batch loss 0.713702202 batch mAP 0.344573975 batch PCKh 0.3125\n",
      "Trained batch 1209 batch loss 0.711011291 batch mAP 0.316467285 batch PCKh 0.5625\n",
      "Trained batch 1210 batch loss 0.718091965 batch mAP 0.310943604 batch PCKh 0.5\n",
      "Trained batch 1211 batch loss 0.746649444 batch mAP 0.372436523 batch PCKh 0.375\n",
      "Trained batch 1212 batch loss 0.656335473 batch mAP 0.347747803 batch PCKh 0.5\n",
      "Trained batch 1213 batch loss 0.64025569 batch mAP 0.313232422 batch PCKh 0.375\n",
      "Trained batch 1214 batch loss 0.634633899 batch mAP 0.326385498 batch PCKh 0.4375\n",
      "Trained batch 1215 batch loss 0.698166788 batch mAP 0.352081299 batch PCKh 0.6875\n",
      "Trained batch 1216 batch loss 0.682363391 batch mAP 0.401428223 batch PCKh 0.625\n",
      "Trained batch 1217 batch loss 0.67006278 batch mAP 0.371307373 batch PCKh 0.6875\n",
      "Trained batch 1218 batch loss 0.691495478 batch mAP 0.368438721 batch PCKh 0.875\n",
      "Trained batch 1219 batch loss 0.684856534 batch mAP 0.384246826 batch PCKh 0.8125\n",
      "Trained batch 1220 batch loss 0.78978312 batch mAP 0.345611572 batch PCKh 0.3125\n",
      "Trained batch 1221 batch loss 0.763143 batch mAP 0.368286133 batch PCKh 0.5625\n",
      "Trained batch 1222 batch loss 0.627762735 batch mAP 0.402587891 batch PCKh 0.0625\n",
      "Trained batch 1223 batch loss 0.709805727 batch mAP 0.41027832 batch PCKh 0.1875\n",
      "Trained batch 1224 batch loss 0.716889501 batch mAP 0.363128662 batch PCKh 0.5\n",
      "Trained batch 1225 batch loss 0.709348381 batch mAP 0.436340332 batch PCKh 0\n",
      "Trained batch 1226 batch loss 0.733320951 batch mAP 0.453918457 batch PCKh 0.125\n",
      "Trained batch 1227 batch loss 0.753419816 batch mAP 0.386138916 batch PCKh 0\n",
      "Trained batch 1228 batch loss 0.693894267 batch mAP 0.427368164 batch PCKh 0.8125\n",
      "Trained batch 1229 batch loss 0.676478148 batch mAP 0.414489746 batch PCKh 0.625\n",
      "Trained batch 1230 batch loss 0.649110675 batch mAP 0.427307129 batch PCKh 0.5625\n",
      "Trained batch 1231 batch loss 0.71308738 batch mAP 0.440917969 batch PCKh 0.625\n",
      "Trained batch 1232 batch loss 0.635349691 batch mAP 0.454315186 batch PCKh 0.25\n",
      "Trained batch 1233 batch loss 0.703888535 batch mAP 0.485229492 batch PCKh 0.25\n",
      "Trained batch 1234 batch loss 0.789424062 batch mAP 0.450164795 batch PCKh 0.0625\n",
      "Trained batch 1235 batch loss 0.774210393 batch mAP 0.432495117 batch PCKh 0.0625\n",
      "Trained batch 1236 batch loss 0.746244967 batch mAP 0.474884033 batch PCKh 0\n",
      "Trained batch 1237 batch loss 0.785417914 batch mAP 0.458282471 batch PCKh 0\n",
      "Trained batch 1238 batch loss 0.67770946 batch mAP 0.495147705 batch PCKh 0.25\n",
      "Trained batch 1239 batch loss 0.658868194 batch mAP 0.494598389 batch PCKh 0.3125\n",
      "Trained batch 1240 batch loss 0.636114 batch mAP 0.500305176 batch PCKh 0.375\n",
      "Trained batch 1241 batch loss 0.688152194 batch mAP 0.461212158 batch PCKh 0.4375\n",
      "Trained batch 1242 batch loss 0.683803618 batch mAP 0.453460693 batch PCKh 0.25\n",
      "Trained batch 1243 batch loss 0.732919455 batch mAP 0.435028076 batch PCKh 0.1875\n",
      "Trained batch 1244 batch loss 0.730196893 batch mAP 0.426025391 batch PCKh 0.25\n",
      "Trained batch 1245 batch loss 0.661025167 batch mAP 0.399261475 batch PCKh 0.375\n",
      "Trained batch 1246 batch loss 0.618190229 batch mAP 0.362365723 batch PCKh 0.5625\n",
      "Trained batch 1247 batch loss 0.709838152 batch mAP 0.3465271 batch PCKh 0.75\n",
      "Trained batch 1248 batch loss 0.800738931 batch mAP 0.370941162 batch PCKh 0.375\n",
      "Trained batch 1249 batch loss 0.739834905 batch mAP 0.365325928 batch PCKh 0.6875\n",
      "Trained batch 1250 batch loss 0.769426823 batch mAP 0.371154785 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1251 batch loss 0.756514251 batch mAP 0.352813721 batch PCKh 0.4375\n",
      "Trained batch 1252 batch loss 0.697898 batch mAP 0.335540771 batch PCKh 0.375\n",
      "Trained batch 1253 batch loss 0.759765863 batch mAP 0.325195312 batch PCKh 0.0625\n",
      "Trained batch 1254 batch loss 0.653728843 batch mAP 0.296020508 batch PCKh 0\n",
      "Trained batch 1255 batch loss 0.561440229 batch mAP 0.268432617 batch PCKh 0.375\n",
      "Trained batch 1256 batch loss 0.65467608 batch mAP 0.204071045 batch PCKh 0.0625\n",
      "Trained batch 1257 batch loss 0.602822065 batch mAP 0.20123291 batch PCKh 0\n",
      "Trained batch 1258 batch loss 0.578931451 batch mAP 0.211395264 batch PCKh 0\n",
      "Trained batch 1259 batch loss 0.634113967 batch mAP 0.213897705 batch PCKh 0.25\n",
      "Trained batch 1260 batch loss 0.672547877 batch mAP 0.279449463 batch PCKh 0.5\n",
      "Trained batch 1261 batch loss 0.675862134 batch mAP 0.309234619 batch PCKh 0.6875\n",
      "Trained batch 1262 batch loss 0.600072861 batch mAP 0.291503906 batch PCKh 0.625\n",
      "Trained batch 1263 batch loss 0.680400312 batch mAP 0.272735596 batch PCKh 0.1875\n",
      "Trained batch 1264 batch loss 0.708035767 batch mAP 0.251495361 batch PCKh 0.4375\n",
      "Trained batch 1265 batch loss 0.675656915 batch mAP 0.299133301 batch PCKh 0\n",
      "Trained batch 1266 batch loss 0.696772 batch mAP 0.278930664 batch PCKh 0.1875\n",
      "Trained batch 1267 batch loss 0.732842743 batch mAP 0.284820557 batch PCKh 0.125\n",
      "Trained batch 1268 batch loss 0.688627481 batch mAP 0.308990479 batch PCKh 0.1875\n",
      "Trained batch 1269 batch loss 0.715988874 batch mAP 0.340240479 batch PCKh 0.4375\n",
      "Trained batch 1270 batch loss 0.679504931 batch mAP 0.306518555 batch PCKh 0.5625\n",
      "Trained batch 1271 batch loss 0.613516152 batch mAP 0.373199463 batch PCKh 0.5\n",
      "Trained batch 1272 batch loss 0.604035854 batch mAP 0.355438232 batch PCKh 0.3125\n",
      "Trained batch 1273 batch loss 0.704691768 batch mAP 0.34942627 batch PCKh 0.625\n",
      "Trained batch 1274 batch loss 0.665137589 batch mAP 0.351104736 batch PCKh 0.375\n",
      "Trained batch 1275 batch loss 0.6086725 batch mAP 0.394287109 batch PCKh 0.5\n",
      "Trained batch 1276 batch loss 0.612950504 batch mAP 0.377868652 batch PCKh 0.5\n",
      "Trained batch 1277 batch loss 0.630454898 batch mAP 0.413757324 batch PCKh 0.875\n",
      "Trained batch 1278 batch loss 0.612979293 batch mAP 0.42855835 batch PCKh 0.8125\n",
      "Trained batch 1279 batch loss 0.626659095 batch mAP 0.442382812 batch PCKh 0.8125\n",
      "Trained batch 1280 batch loss 0.659391 batch mAP 0.383972168 batch PCKh 0.6875\n",
      "Trained batch 1281 batch loss 0.599881172 batch mAP 0.363006592 batch PCKh 0.75\n",
      "Trained batch 1282 batch loss 0.664905429 batch mAP 0.347961426 batch PCKh 0.75\n",
      "Trained batch 1283 batch loss 0.654533088 batch mAP 0.340393066 batch PCKh 0.75\n",
      "Trained batch 1284 batch loss 0.684475899 batch mAP 0.306274414 batch PCKh 0.4375\n",
      "Trained batch 1285 batch loss 0.697358131 batch mAP 0.350952148 batch PCKh 0.875\n",
      "Trained batch 1286 batch loss 0.520172656 batch mAP 0.373840332 batch PCKh 0.6875\n",
      "Trained batch 1287 batch loss 0.653317213 batch mAP 0.356781 batch PCKh 0.5\n",
      "Trained batch 1288 batch loss 0.607004225 batch mAP 0.348114 batch PCKh 0.5625\n",
      "Trained batch 1289 batch loss 0.677406549 batch mAP 0.362518311 batch PCKh 0.125\n",
      "Trained batch 1290 batch loss 0.664651036 batch mAP 0.352050781 batch PCKh 0.25\n",
      "Trained batch 1291 batch loss 0.547054648 batch mAP 0.42376709 batch PCKh 0.75\n",
      "Trained batch 1292 batch loss 0.669668674 batch mAP 0.391296387 batch PCKh 0.625\n",
      "Trained batch 1293 batch loss 0.565538526 batch mAP 0.411346436 batch PCKh 0.3125\n",
      "Trained batch 1294 batch loss 0.81282568 batch mAP 0.40335083 batch PCKh 0\n",
      "Trained batch 1295 batch loss 0.729149 batch mAP 0.392913818 batch PCKh 0.125\n",
      "Trained batch 1296 batch loss 0.682146192 batch mAP 0.365020752 batch PCKh 0.5625\n",
      "Trained batch 1297 batch loss 0.687408626 batch mAP 0.399780273 batch PCKh 0.625\n",
      "Trained batch 1298 batch loss 0.714333117 batch mAP 0.347625732 batch PCKh 0.3125\n",
      "Trained batch 1299 batch loss 0.696452856 batch mAP 0.37701416 batch PCKh 0.3125\n",
      "Trained batch 1300 batch loss 0.663603783 batch mAP 0.400878906 batch PCKh 0.625\n",
      "Trained batch 1301 batch loss 0.66547972 batch mAP 0.373626709 batch PCKh 0.125\n",
      "Trained batch 1302 batch loss 0.654007196 batch mAP 0.407958984 batch PCKh 0.625\n",
      "Trained batch 1303 batch loss 0.69422543 batch mAP 0.391571045 batch PCKh 0.0625\n",
      "Trained batch 1304 batch loss 0.709716678 batch mAP 0.38671875 batch PCKh 0.1875\n",
      "Trained batch 1305 batch loss 0.622492 batch mAP 0.432067871 batch PCKh 0.0625\n",
      "Trained batch 1306 batch loss 0.596724689 batch mAP 0.433929443 batch PCKh 0.25\n",
      "Trained batch 1307 batch loss 0.696720362 batch mAP 0.424163818 batch PCKh 0.5\n",
      "Trained batch 1308 batch loss 0.684277654 batch mAP 0.427398682 batch PCKh 0.3125\n",
      "Trained batch 1309 batch loss 0.633955479 batch mAP 0.415039062 batch PCKh 0.25\n",
      "Trained batch 1310 batch loss 0.768227816 batch mAP 0.384399414 batch PCKh 0.1875\n",
      "Trained batch 1311 batch loss 0.664102912 batch mAP 0.376220703 batch PCKh 0.0625\n",
      "Trained batch 1312 batch loss 0.532075942 batch mAP 0.327148438 batch PCKh 0.0625\n",
      "Trained batch 1313 batch loss 0.535506129 batch mAP 0.322296143 batch PCKh 0.125\n",
      "Trained batch 1314 batch loss 0.690753281 batch mAP 0.34564209 batch PCKh 0.0625\n",
      "Trained batch 1315 batch loss 0.745536447 batch mAP 0.373474121 batch PCKh 0\n",
      "Trained batch 1316 batch loss 0.689695358 batch mAP 0.420166016 batch PCKh 0.4375\n",
      "Trained batch 1317 batch loss 0.743152916 batch mAP 0.400665283 batch PCKh 0.75\n",
      "Trained batch 1318 batch loss 0.741053164 batch mAP 0.408569336 batch PCKh 0.0625\n",
      "Trained batch 1319 batch loss 0.711019754 batch mAP 0.386566162 batch PCKh 0\n",
      "Trained batch 1320 batch loss 0.664350152 batch mAP 0.344818115 batch PCKh 0.375\n",
      "Trained batch 1321 batch loss 0.653113842 batch mAP 0.352264404 batch PCKh 0.25\n",
      "Trained batch 1322 batch loss 0.614267409 batch mAP 0.286804199 batch PCKh 0.5625\n",
      "Trained batch 1323 batch loss 0.60910213 batch mAP 0.320098877 batch PCKh 0.625\n",
      "Trained batch 1324 batch loss 0.608431935 batch mAP 0.30581665 batch PCKh 0.4375\n",
      "Trained batch 1325 batch loss 0.673116803 batch mAP 0.344635 batch PCKh 0.25\n",
      "Trained batch 1326 batch loss 0.627207041 batch mAP 0.351074219 batch PCKh 0.4375\n",
      "Trained batch 1327 batch loss 0.592327 batch mAP 0.344390869 batch PCKh 0.4375\n",
      "Trained batch 1328 batch loss 0.593405306 batch mAP 0.408844 batch PCKh 0.3125\n",
      "Trained batch 1329 batch loss 0.598746836 batch mAP 0.414581299 batch PCKh 0.4375\n",
      "Trained batch 1330 batch loss 0.595180511 batch mAP 0.418304443 batch PCKh 0.0625\n",
      "Trained batch 1331 batch loss 0.62814492 batch mAP 0.378112793 batch PCKh 0.5\n",
      "Trained batch 1332 batch loss 0.55394274 batch mAP 0.43057251 batch PCKh 0.125\n",
      "Trained batch 1333 batch loss 0.650688529 batch mAP 0.382720947 batch PCKh 0.5\n",
      "Trained batch 1334 batch loss 0.636501908 batch mAP 0.288330078 batch PCKh 0.875\n",
      "Trained batch 1335 batch loss 0.659566045 batch mAP 0.287963867 batch PCKh 0.8125\n",
      "Trained batch 1336 batch loss 0.644099951 batch mAP 0.349517822 batch PCKh 0.75\n",
      "Trained batch 1337 batch loss 0.637441099 batch mAP 0.347412109 batch PCKh 0.875\n",
      "Trained batch 1338 batch loss 0.662442625 batch mAP 0.403594971 batch PCKh 0.75\n",
      "Trained batch 1339 batch loss 0.7285465 batch mAP 0.325897217 batch PCKh 0.875\n",
      "Trained batch 1340 batch loss 0.680437624 batch mAP 0.350006104 batch PCKh 0.8125\n",
      "Trained batch 1341 batch loss 0.672211885 batch mAP 0.344818115 batch PCKh 0.75\n",
      "Trained batch 1342 batch loss 0.676593363 batch mAP 0.325866699 batch PCKh 0.625\n",
      "Trained batch 1343 batch loss 0.663463712 batch mAP 0.330932617 batch PCKh 0.25\n",
      "Trained batch 1344 batch loss 0.672741354 batch mAP 0.240875244 batch PCKh 0\n",
      "Trained batch 1345 batch loss 0.501346052 batch mAP 0.219970703 batch PCKh 0.1875\n",
      "Trained batch 1346 batch loss 0.55070734 batch mAP 0.168945312 batch PCKh 0.4375\n",
      "Trained batch 1347 batch loss 0.496189594 batch mAP 0.202911377 batch PCKh 0\n",
      "Trained batch 1348 batch loss 0.52542305 batch mAP 0.187469482 batch PCKh 0\n",
      "Trained batch 1349 batch loss 0.495957792 batch mAP 0.20892334 batch PCKh 0\n",
      "Trained batch 1350 batch loss 0.611818433 batch mAP 0.203369141 batch PCKh 0.5\n",
      "Trained batch 1351 batch loss 0.67155993 batch mAP 0.268920898 batch PCKh 0.5625\n",
      "Trained batch 1352 batch loss 0.616948903 batch mAP 0.326385498 batch PCKh 0.5\n",
      "Trained batch 1353 batch loss 0.777273357 batch mAP 0.349273682 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1354 batch loss 0.76278168 batch mAP 0.351348877 batch PCKh 0.125\n",
      "Trained batch 1355 batch loss 0.895500302 batch mAP 0.323120117 batch PCKh 0\n",
      "Trained batch 1356 batch loss 0.8812989 batch mAP 0.339141846 batch PCKh 0\n",
      "Trained batch 1357 batch loss 0.776776075 batch mAP 0.237182617 batch PCKh 0.0625\n",
      "Trained batch 1358 batch loss 0.662192404 batch mAP 0.103210449 batch PCKh 0.1875\n",
      "Trained batch 1359 batch loss 0.616601 batch mAP 0.0401611328 batch PCKh 0.375\n",
      "Trained batch 1360 batch loss 0.616913319 batch mAP 0.013458252 batch PCKh 0.6875\n",
      "Trained batch 1361 batch loss 0.574211299 batch mAP 0.00164794922 batch PCKh 0.25\n",
      "Trained batch 1362 batch loss 0.713852406 batch mAP 0.00662231445 batch PCKh 0.4375\n",
      "Trained batch 1363 batch loss 0.685333073 batch mAP 0.000946044922 batch PCKh 0.3125\n",
      "Trained batch 1364 batch loss 0.696866393 batch mAP 0.00775146484 batch PCKh 0.5\n",
      "Trained batch 1365 batch loss 0.695684254 batch mAP 0.0190734863 batch PCKh 0.3125\n",
      "Trained batch 1366 batch loss 0.699861407 batch mAP 0.0794372559 batch PCKh 0.25\n",
      "Trained batch 1367 batch loss 0.673689783 batch mAP 0.120056152 batch PCKh 0.3125\n",
      "Trained batch 1368 batch loss 0.689603925 batch mAP 0.216827393 batch PCKh 0.5625\n",
      "Trained batch 1369 batch loss 0.699106932 batch mAP 0.249572754 batch PCKh 0.125\n",
      "Trained batch 1370 batch loss 0.679496884 batch mAP 0.309906 batch PCKh 0.5625\n",
      "Trained batch 1371 batch loss 0.691404104 batch mAP 0.279083252 batch PCKh 0.875\n",
      "Trained batch 1372 batch loss 0.732791 batch mAP 0.132781982 batch PCKh 0.25\n",
      "Trained batch 1373 batch loss 0.727544427 batch mAP 0.105133057 batch PCKh 0.3125\n",
      "Trained batch 1374 batch loss 0.618121386 batch mAP 0.146820068 batch PCKh 0.5\n",
      "Trained batch 1375 batch loss 0.66323781 batch mAP 0.14743042 batch PCKh 0.3125\n",
      "Trained batch 1376 batch loss 0.67561233 batch mAP 0.128692627 batch PCKh 0.25\n",
      "Trained batch 1377 batch loss 0.637367249 batch mAP 0.220062256 batch PCKh 0.375\n",
      "Trained batch 1378 batch loss 0.630509734 batch mAP 0.174713135 batch PCKh 0.1875\n",
      "Trained batch 1379 batch loss 0.617717147 batch mAP 0.113342285 batch PCKh 0.4375\n",
      "Trained batch 1380 batch loss 0.649164 batch mAP 0.13885498 batch PCKh 0\n",
      "Trained batch 1381 batch loss 0.698758721 batch mAP 0.285247803 batch PCKh 0.3125\n",
      "Trained batch 1382 batch loss 0.696918607 batch mAP 0.395690918 batch PCKh 0.5625\n",
      "Trained batch 1383 batch loss 0.643471599 batch mAP 0.460357666 batch PCKh 0.625\n",
      "Trained batch 1384 batch loss 0.67482233 batch mAP 0.429290771 batch PCKh 0.125\n",
      "Trained batch 1385 batch loss 0.648641706 batch mAP 0.431060791 batch PCKh 0.75\n",
      "Trained batch 1386 batch loss 0.653288603 batch mAP 0.494506836 batch PCKh 0.375\n",
      "Trained batch 1387 batch loss 0.56179595 batch mAP 0.417022705 batch PCKh 0.0625\n",
      "Trained batch 1388 batch loss 0.585789621 batch mAP 0.409545898 batch PCKh 0.0625\n",
      "Trained batch 1389 batch loss 0.614075601 batch mAP 0.513275146 batch PCKh 0.4375\n",
      "Trained batch 1390 batch loss 0.612702131 batch mAP 0.484588623 batch PCKh 0.875\n",
      "Trained batch 1391 batch loss 0.646835864 batch mAP 0.450958252 batch PCKh 0.75\n",
      "Trained batch 1392 batch loss 0.714946151 batch mAP 0.434173584 batch PCKh 0.375\n",
      "Trained batch 1393 batch loss 0.667073965 batch mAP 0.438171387 batch PCKh 0.5625\n",
      "Trained batch 1394 batch loss 0.672954917 batch mAP 0.411071777 batch PCKh 0.75\n",
      "Trained batch 1395 batch loss 0.652384937 batch mAP 0.420654297 batch PCKh 0.5625\n",
      "Trained batch 1396 batch loss 0.674409151 batch mAP 0.343688965 batch PCKh 0.1875\n",
      "Trained batch 1397 batch loss 0.765990078 batch mAP 0.382751465 batch PCKh 0\n",
      "Trained batch 1398 batch loss 0.760065615 batch mAP 0.364929199 batch PCKh 0.3125\n",
      "Trained batch 1399 batch loss 0.756249845 batch mAP 0.325469971 batch PCKh 0.1875\n",
      "Trained batch 1400 batch loss 0.725598 batch mAP 0.265258789 batch PCKh 0.25\n",
      "Trained batch 1401 batch loss 0.763736367 batch mAP 0.332275391 batch PCKh 0.375\n",
      "Trained batch 1402 batch loss 0.793037355 batch mAP 0.276824951 batch PCKh 0\n",
      "Trained batch 1403 batch loss 0.742579341 batch mAP 0.324401855 batch PCKh 0.375\n",
      "Trained batch 1404 batch loss 0.575159 batch mAP 0.30178833 batch PCKh 0\n",
      "Trained batch 1405 batch loss 0.575514913 batch mAP 0.300811768 batch PCKh 0.125\n",
      "Trained batch 1406 batch loss 0.607911468 batch mAP 0.25668335 batch PCKh 0.3125\n",
      "Trained batch 1407 batch loss 0.603019536 batch mAP 0.143463135 batch PCKh 0.1875\n",
      "Trained batch 1408 batch loss 0.741619885 batch mAP 0.338287354 batch PCKh 0.125\n",
      "Trained batch 1409 batch loss 0.652633309 batch mAP 0.317382812 batch PCKh 0.4375\n",
      "Trained batch 1410 batch loss 0.64422977 batch mAP 0.377349854 batch PCKh 0.3125\n",
      "Trained batch 1411 batch loss 0.634049952 batch mAP 0.409210205 batch PCKh 0\n",
      "Trained batch 1412 batch loss 0.599541306 batch mAP 0.459381104 batch PCKh 0.5625\n",
      "Trained batch 1413 batch loss 0.62507987 batch mAP 0.497192383 batch PCKh 0.1875\n",
      "Trained batch 1414 batch loss 0.63552916 batch mAP 0.496643066 batch PCKh 0.5625\n",
      "Trained batch 1415 batch loss 0.626980305 batch mAP 0.508544922 batch PCKh 0.5625\n",
      "Trained batch 1416 batch loss 0.623589039 batch mAP 0.518676758 batch PCKh 0.25\n",
      "Trained batch 1417 batch loss 0.602603376 batch mAP 0.47454834 batch PCKh 0.375\n",
      "Trained batch 1418 batch loss 0.634216785 batch mAP 0.41708374 batch PCKh 0.0625\n",
      "Trained batch 1419 batch loss 0.570270896 batch mAP 0.437011719 batch PCKh 0.25\n",
      "Trained batch 1420 batch loss 0.604455471 batch mAP 0.388519287 batch PCKh 0.1875\n",
      "Trained batch 1421 batch loss 0.583582163 batch mAP 0.382537842 batch PCKh 0.5\n",
      "Trained batch 1422 batch loss 0.575129449 batch mAP 0.319580078 batch PCKh 0.3125\n",
      "Trained batch 1423 batch loss 0.548605204 batch mAP 0.389709473 batch PCKh 0.125\n",
      "Trained batch 1424 batch loss 0.672172546 batch mAP 0.0665283203 batch PCKh 0.3125\n",
      "Trained batch 1425 batch loss 0.63387 batch mAP 0.268585205 batch PCKh 0.375\n",
      "Trained batch 1426 batch loss 0.71632874 batch mAP 0.114105225 batch PCKh 0.3125\n",
      "Trained batch 1427 batch loss 0.612150311 batch mAP 0.321533203 batch PCKh 0.25\n",
      "Trained batch 1428 batch loss 0.60291183 batch mAP 0.292144775 batch PCKh 0.3125\n",
      "Trained batch 1429 batch loss 0.607395887 batch mAP 0.380950928 batch PCKh 0.125\n",
      "Trained batch 1430 batch loss 0.633604646 batch mAP 0.376251221 batch PCKh 0.1875\n",
      "Trained batch 1431 batch loss 0.634601772 batch mAP 0.385284424 batch PCKh 0\n",
      "Trained batch 1432 batch loss 0.644852042 batch mAP 0.42590332 batch PCKh 0.375\n",
      "Trained batch 1433 batch loss 0.696709514 batch mAP 0.437530518 batch PCKh 0.0625\n",
      "Trained batch 1434 batch loss 0.613710165 batch mAP 0.461364746 batch PCKh 0.1875\n",
      "Trained batch 1435 batch loss 0.770984709 batch mAP 0.415771484 batch PCKh 0\n",
      "Trained batch 1436 batch loss 0.677080154 batch mAP 0.4168396 batch PCKh 0.0625\n",
      "Trained batch 1437 batch loss 0.652664125 batch mAP 0.42868042 batch PCKh 0.25\n",
      "Trained batch 1438 batch loss 0.772545934 batch mAP 0.41506958 batch PCKh 0\n",
      "Trained batch 1439 batch loss 0.730342388 batch mAP 0.405365 batch PCKh 0.0625\n",
      "Trained batch 1440 batch loss 0.711306632 batch mAP 0.388885498 batch PCKh 0.875\n",
      "Trained batch 1441 batch loss 0.688411713 batch mAP 0.363189697 batch PCKh 0\n",
      "Trained batch 1442 batch loss 0.675456882 batch mAP 0.30960083 batch PCKh 0.0625\n",
      "Trained batch 1443 batch loss 0.666812897 batch mAP 0.322235107 batch PCKh 0.6875\n",
      "Trained batch 1444 batch loss 0.735004961 batch mAP 0.30255127 batch PCKh 0.5625\n",
      "Trained batch 1445 batch loss 0.688252 batch mAP 0.160369873 batch PCKh 0.3125\n",
      "Trained batch 1446 batch loss 0.678802133 batch mAP 0.280456543 batch PCKh 0.75\n",
      "Trained batch 1447 batch loss 0.74043715 batch mAP 0.328430176 batch PCKh 0.125\n",
      "Trained batch 1448 batch loss 0.720248938 batch mAP 0.337493896 batch PCKh 0.4375\n",
      "Trained batch 1449 batch loss 0.609647155 batch mAP 0.327331543 batch PCKh 0.4375\n",
      "Trained batch 1450 batch loss 0.698874533 batch mAP 0.350097656 batch PCKh 0.5\n",
      "Trained batch 1451 batch loss 0.743254662 batch mAP 0.325408936 batch PCKh 0.125\n",
      "Trained batch 1452 batch loss 0.701060355 batch mAP 0.341430664 batch PCKh 0.625\n",
      "Trained batch 1453 batch loss 0.703929305 batch mAP 0.345397949 batch PCKh 0.1875\n",
      "Trained batch 1454 batch loss 0.714648664 batch mAP 0.372528076 batch PCKh 0\n",
      "Trained batch 1455 batch loss 0.621763587 batch mAP 0.398681641 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1456 batch loss 0.67496115 batch mAP 0.442504883 batch PCKh 0.1875\n",
      "Trained batch 1457 batch loss 0.724815845 batch mAP 0.472137451 batch PCKh 0.375\n",
      "Trained batch 1458 batch loss 0.706965804 batch mAP 0.464355469 batch PCKh 0.5625\n",
      "Trained batch 1459 batch loss 0.729082882 batch mAP 0.450927734 batch PCKh 0.8125\n",
      "Trained batch 1460 batch loss 0.676852703 batch mAP 0.459869385 batch PCKh 0.375\n",
      "Trained batch 1461 batch loss 0.678915739 batch mAP 0.491973877 batch PCKh 0.875\n",
      "Trained batch 1462 batch loss 0.648288846 batch mAP 0.470733643 batch PCKh 0.625\n",
      "Trained batch 1463 batch loss 0.667128265 batch mAP 0.431335449 batch PCKh 0.6875\n",
      "Trained batch 1464 batch loss 0.702840507 batch mAP 0.445526123 batch PCKh 0.4375\n",
      "Trained batch 1465 batch loss 0.667103171 batch mAP 0.416168213 batch PCKh 0\n",
      "Trained batch 1466 batch loss 0.683759451 batch mAP 0.433044434 batch PCKh 0.5\n",
      "Trained batch 1467 batch loss 0.676914096 batch mAP 0.403869629 batch PCKh 0.3125\n",
      "Trained batch 1468 batch loss 0.696758807 batch mAP 0.456207275 batch PCKh 0.875\n",
      "Trained batch 1469 batch loss 0.637 batch mAP 0.408233643 batch PCKh 0.5\n",
      "Trained batch 1470 batch loss 0.65417248 batch mAP 0.391723633 batch PCKh 0.5\n",
      "Trained batch 1471 batch loss 0.636483252 batch mAP 0.390991211 batch PCKh 0.4375\n",
      "Trained batch 1472 batch loss 0.601611614 batch mAP 0.379272461 batch PCKh 0.6875\n",
      "Trained batch 1473 batch loss 0.627778828 batch mAP 0.406829834 batch PCKh 0.3125\n",
      "Trained batch 1474 batch loss 0.596906424 batch mAP 0.407531738 batch PCKh 0.0625\n",
      "Trained batch 1475 batch loss 0.62938118 batch mAP 0.386047363 batch PCKh 0\n",
      "Trained batch 1476 batch loss 0.653416693 batch mAP 0.344360352 batch PCKh 0.6875\n",
      "Trained batch 1477 batch loss 0.668171406 batch mAP 0.365631104 batch PCKh 0.6875\n",
      "Trained batch 1478 batch loss 0.69075346 batch mAP 0.394317627 batch PCKh 0.6875\n",
      "Trained batch 1479 batch loss 0.67109555 batch mAP 0.354644775 batch PCKh 0.4375\n",
      "Trained batch 1480 batch loss 0.654130578 batch mAP 0.332946777 batch PCKh 0.25\n",
      "Trained batch 1481 batch loss 0.615079105 batch mAP 0.313385 batch PCKh 0\n",
      "Trained batch 1482 batch loss 0.709900737 batch mAP 0.37600708 batch PCKh 0.5\n",
      "Trained batch 1483 batch loss 0.574890375 batch mAP 0.40737915 batch PCKh 0.1875\n",
      "Trained batch 1484 batch loss 0.642646313 batch mAP 0.411346436 batch PCKh 0.375\n",
      "Trained batch 1485 batch loss 0.63974309 batch mAP 0.451751709 batch PCKh 0.125\n",
      "Trained batch 1486 batch loss 0.677280962 batch mAP 0.45123291 batch PCKh 0.75\n",
      "Trained batch 1487 batch loss 0.707359135 batch mAP 0.482299805 batch PCKh 0.5\n",
      "Trained batch 1488 batch loss 0.640570521 batch mAP 0.468505859 batch PCKh 0.375\n",
      "Trained batch 1489 batch loss 0.667789638 batch mAP 0.435058594 batch PCKh 0.5625\n",
      "Trained batch 1490 batch loss 0.630097806 batch mAP 0.457122803 batch PCKh 0.5625\n",
      "Trained batch 1491 batch loss 0.707154274 batch mAP 0.416412354 batch PCKh 0.625\n",
      "Trained batch 1492 batch loss 0.611275315 batch mAP 0.422973633 batch PCKh 0.3125\n",
      "Trained batch 1493 batch loss 0.643895149 batch mAP 0.398803711 batch PCKh 0.625\n",
      "Trained batch 1494 batch loss 0.675724268 batch mAP 0.43927002 batch PCKh 0.625\n",
      "Trained batch 1495 batch loss 0.715181947 batch mAP 0.327850342 batch PCKh 0.75\n",
      "Trained batch 1496 batch loss 0.684681177 batch mAP 0.432098389 batch PCKh 0.4375\n",
      "Trained batch 1497 batch loss 0.738972306 batch mAP 0.370819092 batch PCKh 0.125\n",
      "Trained batch 1498 batch loss 0.687841177 batch mAP 0.500915527 batch PCKh 0.125\n",
      "Trained batch 1499 batch loss 0.702571332 batch mAP 0.450836182 batch PCKh 0.0625\n",
      "Trained batch 1500 batch loss 0.729444683 batch mAP 0.411773682 batch PCKh 0.625\n",
      "Trained batch 1501 batch loss 0.749563 batch mAP 0.42980957 batch PCKh 0.375\n",
      "Trained batch 1502 batch loss 0.67119354 batch mAP 0.400634766 batch PCKh 0.6875\n",
      "Trained batch 1503 batch loss 0.761852145 batch mAP 0.392547607 batch PCKh 0.125\n",
      "Trained batch 1504 batch loss 0.675108492 batch mAP 0.41015625 batch PCKh 0.125\n",
      "Trained batch 1505 batch loss 0.668978631 batch mAP 0.339508057 batch PCKh 0.125\n",
      "Trained batch 1506 batch loss 0.623531163 batch mAP 0.335632324 batch PCKh 0\n",
      "Trained batch 1507 batch loss 0.658782244 batch mAP 0.299591064 batch PCKh 0.375\n",
      "Trained batch 1508 batch loss 0.646006346 batch mAP 0.273925781 batch PCKh 0.375\n",
      "Trained batch 1509 batch loss 0.679619133 batch mAP 0.254760742 batch PCKh 0.25\n",
      "Trained batch 1510 batch loss 0.647373796 batch mAP 0.260467529 batch PCKh 0.25\n",
      "Trained batch 1511 batch loss 0.637999475 batch mAP 0.270568848 batch PCKh 0.1875\n",
      "Trained batch 1512 batch loss 0.644759774 batch mAP 0.252441406 batch PCKh 0.25\n",
      "Trained batch 1513 batch loss 0.614522934 batch mAP 0.278594971 batch PCKh 0.125\n",
      "Trained batch 1514 batch loss 0.578051567 batch mAP 0.349334717 batch PCKh 0.625\n",
      "Trained batch 1515 batch loss 0.585586965 batch mAP 0.374420166 batch PCKh 0.125\n",
      "Trained batch 1516 batch loss 0.698406935 batch mAP 0.401245117 batch PCKh 0.875\n",
      "Trained batch 1517 batch loss 0.555663 batch mAP 0.424621582 batch PCKh 0.25\n",
      "Trained batch 1518 batch loss 0.713042617 batch mAP 0.327545166 batch PCKh 0.5\n",
      "Trained batch 1519 batch loss 0.690460086 batch mAP 0.37979126 batch PCKh 0.1875\n",
      "Trained batch 1520 batch loss 0.631695032 batch mAP 0.405548096 batch PCKh 0.125\n",
      "Trained batch 1521 batch loss 0.642239749 batch mAP 0.408996582 batch PCKh 0.375\n",
      "Trained batch 1522 batch loss 0.682392955 batch mAP 0.406066895 batch PCKh 0.375\n",
      "Trained batch 1523 batch loss 0.614883602 batch mAP 0.410858154 batch PCKh 0.5\n",
      "Trained batch 1524 batch loss 0.615993381 batch mAP 0.431152344 batch PCKh 0.125\n",
      "Trained batch 1525 batch loss 0.589218318 batch mAP 0.484588623 batch PCKh 0.25\n",
      "Trained batch 1526 batch loss 0.643318355 batch mAP 0.469512939 batch PCKh 0.75\n",
      "Trained batch 1527 batch loss 0.57805717 batch mAP 0.474365234 batch PCKh 0.375\n",
      "Trained batch 1528 batch loss 0.647078693 batch mAP 0.447113037 batch PCKh 0.1875\n",
      "Trained batch 1529 batch loss 0.589437723 batch mAP 0.271575928 batch PCKh 0.3125\n",
      "Trained batch 1530 batch loss 0.599383652 batch mAP 0.428955078 batch PCKh 0.125\n",
      "Trained batch 1531 batch loss 0.617379427 batch mAP 0.471862793 batch PCKh 0.625\n",
      "Trained batch 1532 batch loss 0.600531 batch mAP 0.434875488 batch PCKh 0.5625\n",
      "Trained batch 1533 batch loss 0.563482404 batch mAP 0.363708496 batch PCKh 0.625\n",
      "Trained batch 1534 batch loss 0.608187675 batch mAP 0.402709961 batch PCKh 0.25\n",
      "Trained batch 1535 batch loss 0.636425138 batch mAP 0.373077393 batch PCKh 0.75\n",
      "Trained batch 1536 batch loss 0.675750136 batch mAP 0.42855835 batch PCKh 0\n",
      "Trained batch 1537 batch loss 0.618809283 batch mAP 0.390655518 batch PCKh 0.4375\n",
      "Trained batch 1538 batch loss 0.7120561 batch mAP 0.442169189 batch PCKh 0.0625\n",
      "Trained batch 1539 batch loss 0.581876814 batch mAP 0.360778809 batch PCKh 0.6875\n",
      "Trained batch 1540 batch loss 0.609283149 batch mAP 0.437805176 batch PCKh 0.25\n",
      "Trained batch 1541 batch loss 0.663995206 batch mAP 0.453704834 batch PCKh 0\n",
      "Trained batch 1542 batch loss 0.542676032 batch mAP 0.434173584 batch PCKh 0.1875\n",
      "Trained batch 1543 batch loss 0.663493395 batch mAP 0.402893066 batch PCKh 0.25\n",
      "Trained batch 1544 batch loss 0.673137486 batch mAP 0.449249268 batch PCKh 0.625\n",
      "Trained batch 1545 batch loss 0.645253718 batch mAP 0.413269043 batch PCKh 0.4375\n",
      "Trained batch 1546 batch loss 0.656130373 batch mAP 0.435516357 batch PCKh 0.5\n",
      "Trained batch 1547 batch loss 0.707173586 batch mAP 0.429718018 batch PCKh 0\n",
      "Trained batch 1548 batch loss 0.697005033 batch mAP 0.436126709 batch PCKh 0.4375\n",
      "Trained batch 1549 batch loss 0.717094421 batch mAP 0.433044434 batch PCKh 0.125\n",
      "Trained batch 1550 batch loss 0.686623 batch mAP 0.445526123 batch PCKh 0.4375\n",
      "Trained batch 1551 batch loss 0.714443803 batch mAP 0.417053223 batch PCKh 0\n",
      "Trained batch 1552 batch loss 0.722297907 batch mAP 0.40133667 batch PCKh 0.0625\n",
      "Trained batch 1553 batch loss 0.613431513 batch mAP 0.341400146 batch PCKh 0.25\n",
      "Trained batch 1554 batch loss 0.633285642 batch mAP 0.355529785 batch PCKh 0.0625\n",
      "Trained batch 1555 batch loss 0.68332839 batch mAP 0.314208984 batch PCKh 0.25\n",
      "Trained batch 1556 batch loss 0.709516048 batch mAP 0.355194092 batch PCKh 0\n",
      "Trained batch 1557 batch loss 0.623094141 batch mAP 0.382171631 batch PCKh 0.75\n",
      "Trained batch 1558 batch loss 0.685954213 batch mAP 0.345977783 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1559 batch loss 0.675558 batch mAP 0.352966309 batch PCKh 0.625\n",
      "Trained batch 1560 batch loss 0.681000948 batch mAP 0.353820801 batch PCKh 0.25\n",
      "Trained batch 1561 batch loss 0.591365039 batch mAP 0.368865967 batch PCKh 0.5625\n",
      "Trained batch 1562 batch loss 0.677188098 batch mAP 0.37902832 batch PCKh 0.4375\n",
      "Trained batch 1563 batch loss 0.738969207 batch mAP 0.396087646 batch PCKh 0.1875\n",
      "Trained batch 1564 batch loss 0.717727065 batch mAP 0.429473877 batch PCKh 0\n",
      "Trained batch 1565 batch loss 0.681311131 batch mAP 0.40447998 batch PCKh 0.25\n",
      "Trained batch 1566 batch loss 0.676010609 batch mAP 0.37512207 batch PCKh 0.0625\n",
      "Trained batch 1567 batch loss 0.625807226 batch mAP 0.379486084 batch PCKh 0\n",
      "Trained batch 1568 batch loss 0.568722785 batch mAP 0.364227295 batch PCKh 0.1875\n",
      "Trained batch 1569 batch loss 0.544005752 batch mAP 0.245391846 batch PCKh 0.625\n",
      "Trained batch 1570 batch loss 0.554439664 batch mAP 0.330200195 batch PCKh 0.0625\n",
      "Trained batch 1571 batch loss 0.684972167 batch mAP 0.38583374 batch PCKh 0.8125\n",
      "Trained batch 1572 batch loss 0.624873161 batch mAP 0.373840332 batch PCKh 0.375\n",
      "Trained batch 1573 batch loss 0.565750957 batch mAP 0.375854492 batch PCKh 0.5625\n",
      "Trained batch 1574 batch loss 0.617660344 batch mAP 0.390075684 batch PCKh 0.625\n",
      "Trained batch 1575 batch loss 0.591658592 batch mAP 0.405670166 batch PCKh 0\n",
      "Trained batch 1576 batch loss 0.706153154 batch mAP 0.37387085 batch PCKh 0.625\n",
      "Trained batch 1577 batch loss 0.601335883 batch mAP 0.373413086 batch PCKh 0.1875\n",
      "Trained batch 1578 batch loss 0.654824 batch mAP 0.354705811 batch PCKh 0.5625\n",
      "Trained batch 1579 batch loss 0.785258353 batch mAP 0.198150635 batch PCKh 0\n",
      "Trained batch 1580 batch loss 0.691377282 batch mAP 0.354278564 batch PCKh 0.0625\n",
      "Trained batch 1581 batch loss 0.555183887 batch mAP 0.413726807 batch PCKh 0\n",
      "Trained batch 1582 batch loss 0.66865927 batch mAP 0.475219727 batch PCKh 0.5625\n",
      "Trained batch 1583 batch loss 0.734112859 batch mAP 0.450042725 batch PCKh 0.1875\n",
      "Trained batch 1584 batch loss 0.662198722 batch mAP 0.447265625 batch PCKh 0.25\n",
      "Trained batch 1585 batch loss 0.675195456 batch mAP 0.408844 batch PCKh 0.5\n",
      "Trained batch 1586 batch loss 0.687332213 batch mAP 0.442596436 batch PCKh 0.125\n",
      "Trained batch 1587 batch loss 0.737769604 batch mAP 0.400604248 batch PCKh 0.125\n",
      "Trained batch 1588 batch loss 0.75575006 batch mAP 0.412963867 batch PCKh 0.0625\n",
      "Trained batch 1589 batch loss 0.768112659 batch mAP 0.404052734 batch PCKh 0.4375\n",
      "Trained batch 1590 batch loss 0.786824822 batch mAP 0.382415771 batch PCKh 0.125\n",
      "Trained batch 1591 batch loss 0.799856067 batch mAP 0.276580811 batch PCKh 0.0625\n",
      "Trained batch 1592 batch loss 0.764526844 batch mAP 0.164428711 batch PCKh 0\n",
      "Trained batch 1593 batch loss 0.720465958 batch mAP 0.109344482 batch PCKh 0.5625\n",
      "Trained batch 1594 batch loss 0.718756914 batch mAP 0.0908508301 batch PCKh 0.1875\n",
      "Trained batch 1595 batch loss 0.630569756 batch mAP 0.0758972168 batch PCKh 0\n",
      "Trained batch 1596 batch loss 0.583163857 batch mAP 0.0310058594 batch PCKh 0.25\n",
      "Trained batch 1597 batch loss 0.562765 batch mAP 0.0628356934 batch PCKh 0.0625\n",
      "Trained batch 1598 batch loss 0.712661266 batch mAP 0.0937194824 batch PCKh 0.1875\n",
      "Trained batch 1599 batch loss 0.721258879 batch mAP 0.146240234 batch PCKh 0.375\n",
      "Trained batch 1600 batch loss 0.714303911 batch mAP 0.204040527 batch PCKh 0.125\n",
      "Trained batch 1601 batch loss 0.786754489 batch mAP 0.218597412 batch PCKh 0.125\n",
      "Trained batch 1602 batch loss 0.777994692 batch mAP 0.244445801 batch PCKh 0\n",
      "Trained batch 1603 batch loss 0.734651864 batch mAP 0.253692627 batch PCKh 0\n",
      "Trained batch 1604 batch loss 0.722412825 batch mAP 0.144714355 batch PCKh 0.5\n",
      "Trained batch 1605 batch loss 0.672610939 batch mAP 0.146759033 batch PCKh 0.4375\n",
      "Trained batch 1606 batch loss 0.716560721 batch mAP 0.180419922 batch PCKh 0.5625\n",
      "Trained batch 1607 batch loss 0.6578632 batch mAP 0.200469971 batch PCKh 0.0625\n",
      "Trained batch 1608 batch loss 0.734350622 batch mAP 0.214538574 batch PCKh 0.1875\n",
      "Trained batch 1609 batch loss 0.66601944 batch mAP 0.231750488 batch PCKh 0.625\n",
      "Trained batch 1610 batch loss 0.671725273 batch mAP 0.284637451 batch PCKh 0.5625\n",
      "Trained batch 1611 batch loss 0.711688817 batch mAP 0.304199219 batch PCKh 0.1875\n",
      "Trained batch 1612 batch loss 0.722534537 batch mAP 0.262115479 batch PCKh 0.4375\n",
      "Trained batch 1613 batch loss 0.689465821 batch mAP 0.280700684 batch PCKh 0.5625\n",
      "Trained batch 1614 batch loss 0.678000212 batch mAP 0.318939209 batch PCKh 0.25\n",
      "Trained batch 1615 batch loss 0.650389254 batch mAP 0.319885254 batch PCKh 0.6875\n",
      "Trained batch 1616 batch loss 0.68755883 batch mAP 0.335296631 batch PCKh 0.3125\n",
      "Trained batch 1617 batch loss 0.678187728 batch mAP 0.359069824 batch PCKh 0.25\n",
      "Trained batch 1618 batch loss 0.672164083 batch mAP 0.242370605 batch PCKh 0.1875\n",
      "Trained batch 1619 batch loss 0.667542458 batch mAP 0.379669189 batch PCKh 0.1875\n",
      "Trained batch 1620 batch loss 0.694400072 batch mAP 0.320465088 batch PCKh 0.25\n",
      "Trained batch 1621 batch loss 0.66486156 batch mAP 0.257141113 batch PCKh 0.5625\n",
      "Trained batch 1622 batch loss 0.669820786 batch mAP 0.236846924 batch PCKh 0.625\n",
      "Trained batch 1623 batch loss 0.671621203 batch mAP 0.191680908 batch PCKh 0.1875\n",
      "Trained batch 1624 batch loss 0.695708036 batch mAP 0.19128418 batch PCKh 0.75\n",
      "Trained batch 1625 batch loss 0.668372333 batch mAP 0.298736572 batch PCKh 0.375\n",
      "Trained batch 1626 batch loss 0.631575346 batch mAP 0.38168335 batch PCKh 0.4375\n",
      "Trained batch 1627 batch loss 0.662078679 batch mAP 0.413909912 batch PCKh 0.625\n",
      "Trained batch 1628 batch loss 0.646988869 batch mAP 0.404205322 batch PCKh 0.75\n",
      "Trained batch 1629 batch loss 0.631959677 batch mAP 0.450012207 batch PCKh 0.1875\n",
      "Trained batch 1630 batch loss 0.641435146 batch mAP 0.430542 batch PCKh 0.1875\n",
      "Trained batch 1631 batch loss 0.618208349 batch mAP 0.433868408 batch PCKh 0.4375\n",
      "Trained batch 1632 batch loss 0.656758368 batch mAP 0.402099609 batch PCKh 0.375\n",
      "Trained batch 1633 batch loss 0.619612396 batch mAP 0.396026611 batch PCKh 0.4375\n",
      "Trained batch 1634 batch loss 0.658638239 batch mAP 0.398010254 batch PCKh 0.5\n",
      "Trained batch 1635 batch loss 0.630870879 batch mAP 0.463897705 batch PCKh 0.25\n",
      "Trained batch 1636 batch loss 0.565907955 batch mAP 0.43560791 batch PCKh 0.3125\n",
      "Trained batch 1637 batch loss 0.59533006 batch mAP 0.402984619 batch PCKh 0\n",
      "Trained batch 1638 batch loss 0.637222052 batch mAP 0.410308838 batch PCKh 0.6875\n",
      "Trained batch 1639 batch loss 0.669323504 batch mAP 0.396575928 batch PCKh 0.1875\n",
      "Trained batch 1640 batch loss 0.65828 batch mAP 0.381256104 batch PCKh 0.4375\n",
      "Trained batch 1641 batch loss 0.673662901 batch mAP 0.383300781 batch PCKh 0\n",
      "Trained batch 1642 batch loss 0.690150142 batch mAP 0.389984131 batch PCKh 0.6875\n",
      "Trained batch 1643 batch loss 0.665171146 batch mAP 0.355499268 batch PCKh 0.1875\n",
      "Trained batch 1644 batch loss 0.604799032 batch mAP 0.35546875 batch PCKh 0.375\n",
      "Trained batch 1645 batch loss 0.652026236 batch mAP 0.331878662 batch PCKh 0.375\n",
      "Trained batch 1646 batch loss 0.726757228 batch mAP 0.309936523 batch PCKh 0.6875\n",
      "Trained batch 1647 batch loss 0.726452112 batch mAP 0.271606445 batch PCKh 0.625\n",
      "Trained batch 1648 batch loss 0.672971785 batch mAP 0.200439453 batch PCKh 0.1875\n",
      "Trained batch 1649 batch loss 0.70139569 batch mAP 0.224975586 batch PCKh 0.8125\n",
      "Trained batch 1650 batch loss 0.684655249 batch mAP 0.208953857 batch PCKh 0.75\n",
      "Trained batch 1651 batch loss 0.671348453 batch mAP 0.285247803 batch PCKh 0.75\n",
      "Trained batch 1652 batch loss 0.691822171 batch mAP 0.297607422 batch PCKh 0.625\n",
      "Trained batch 1653 batch loss 0.712356806 batch mAP 0.241790771 batch PCKh 0.25\n",
      "Trained batch 1654 batch loss 0.571051061 batch mAP 0.0303039551 batch PCKh 0.4375\n",
      "Trained batch 1655 batch loss 0.628989697 batch mAP 0.0360412598 batch PCKh 0.375\n",
      "Trained batch 1656 batch loss 0.555422664 batch mAP 0.0463562 batch PCKh 0.375\n",
      "Trained batch 1657 batch loss 0.611784577 batch mAP 0.143096924 batch PCKh 0.3125\n",
      "Trained batch 1658 batch loss 0.622045636 batch mAP 0.116577148 batch PCKh 0.3125\n",
      "Trained batch 1659 batch loss 0.638102889 batch mAP 0.182739258 batch PCKh 0.25\n",
      "Trained batch 1660 batch loss 0.637066 batch mAP 0.278076172 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1661 batch loss 0.673649311 batch mAP 0.34185791 batch PCKh 0.1875\n",
      "Trained batch 1662 batch loss 0.636537552 batch mAP 0.36895752 batch PCKh 0.3125\n",
      "Trained batch 1663 batch loss 0.657236 batch mAP 0.410827637 batch PCKh 0.625\n",
      "Trained batch 1664 batch loss 0.614291787 batch mAP 0.388000488 batch PCKh 0.3125\n",
      "Trained batch 1665 batch loss 0.642758429 batch mAP 0.34967041 batch PCKh 0.0625\n",
      "Trained batch 1666 batch loss 0.566033 batch mAP 0.424133301 batch PCKh 0.375\n",
      "Trained batch 1667 batch loss 0.653137803 batch mAP 0.415466309 batch PCKh 0.1875\n",
      "Trained batch 1668 batch loss 0.648075521 batch mAP 0.427581787 batch PCKh 0.5625\n",
      "Trained batch 1669 batch loss 0.680298507 batch mAP 0.400390625 batch PCKh 0.375\n",
      "Trained batch 1670 batch loss 0.691418588 batch mAP 0.381866455 batch PCKh 0.125\n",
      "Trained batch 1671 batch loss 0.665701509 batch mAP 0.292388916 batch PCKh 0.5\n",
      "Trained batch 1672 batch loss 0.703590691 batch mAP 0.0999450684 batch PCKh 0.1875\n",
      "Trained batch 1673 batch loss 0.682155371 batch mAP 0.20199585 batch PCKh 0.125\n",
      "Trained batch 1674 batch loss 0.73919791 batch mAP 0.171447754 batch PCKh 0.625\n",
      "Trained batch 1675 batch loss 0.597946644 batch mAP 0.271881104 batch PCKh 0.25\n",
      "Trained batch 1676 batch loss 0.583884954 batch mAP 0.241912842 batch PCKh 0.125\n",
      "Trained batch 1677 batch loss 0.600254536 batch mAP 0.410888672 batch PCKh 0.0625\n",
      "Trained batch 1678 batch loss 0.673638642 batch mAP 0.382965088 batch PCKh 0.25\n",
      "Trained batch 1679 batch loss 0.624686718 batch mAP 0.432281494 batch PCKh 0.875\n",
      "Trained batch 1680 batch loss 0.61576438 batch mAP 0.444732666 batch PCKh 0.6875\n",
      "Trained batch 1681 batch loss 0.716376 batch mAP 0.452972412 batch PCKh 0.25\n",
      "Trained batch 1682 batch loss 0.714156747 batch mAP 0.425811768 batch PCKh 0.0625\n",
      "Trained batch 1683 batch loss 0.672128439 batch mAP 0.424346924 batch PCKh 0.5625\n",
      "Trained batch 1684 batch loss 0.752444386 batch mAP 0.438385 batch PCKh 0.0625\n",
      "Trained batch 1685 batch loss 0.681420147 batch mAP 0.430206299 batch PCKh 0.125\n",
      "Trained batch 1686 batch loss 0.692436278 batch mAP 0.40725708 batch PCKh 0.3125\n",
      "Trained batch 1687 batch loss 0.662896872 batch mAP 0.401306152 batch PCKh 0.4375\n",
      "Trained batch 1688 batch loss 0.681325734 batch mAP 0.404693604 batch PCKh 0.0625\n",
      "Trained batch 1689 batch loss 0.648420811 batch mAP 0.404144287 batch PCKh 0.625\n",
      "Trained batch 1690 batch loss 0.631335855 batch mAP 0.38760376 batch PCKh 0.125\n",
      "Trained batch 1691 batch loss 0.609381676 batch mAP 0.3621521 batch PCKh 0.5\n",
      "Trained batch 1692 batch loss 0.681102157 batch mAP 0.401275635 batch PCKh 0.1875\n",
      "Trained batch 1693 batch loss 0.681601346 batch mAP 0.375335693 batch PCKh 0.0625\n",
      "Trained batch 1694 batch loss 0.731103182 batch mAP 0.412994385 batch PCKh 0.125\n",
      "Trained batch 1695 batch loss 0.677021503 batch mAP 0.43460083 batch PCKh 0.4375\n",
      "Trained batch 1696 batch loss 0.629386723 batch mAP 0.441558838 batch PCKh 0.4375\n",
      "Trained batch 1697 batch loss 0.700187564 batch mAP 0.497436523 batch PCKh 0.0625\n",
      "Trained batch 1698 batch loss 0.639629722 batch mAP 0.471710205 batch PCKh 0.375\n",
      "Trained batch 1699 batch loss 0.664248705 batch mAP 0.504516602 batch PCKh 0.5625\n",
      "Trained batch 1700 batch loss 0.771338701 batch mAP 0.458404541 batch PCKh 0.375\n",
      "Trained batch 1701 batch loss 0.73000741 batch mAP 0.472930908 batch PCKh 0.5625\n",
      "Trained batch 1702 batch loss 0.660120785 batch mAP 0.475067139 batch PCKh 0.6875\n",
      "Trained batch 1703 batch loss 0.675043583 batch mAP 0.502868652 batch PCKh 0.625\n",
      "Trained batch 1704 batch loss 0.696229219 batch mAP 0.411376953 batch PCKh 0.6875\n",
      "Trained batch 1705 batch loss 0.625365257 batch mAP 0.478973389 batch PCKh 0.25\n",
      "Trained batch 1706 batch loss 0.713529825 batch mAP 0.440429688 batch PCKh 0.75\n",
      "Trained batch 1707 batch loss 0.643738031 batch mAP 0.452056885 batch PCKh 0.5\n",
      "Trained batch 1708 batch loss 0.74728632 batch mAP 0.441253662 batch PCKh 0.5625\n",
      "Trained batch 1709 batch loss 0.688583672 batch mAP 0.450744629 batch PCKh 0.625\n",
      "Trained batch 1710 batch loss 0.629772484 batch mAP 0.42880249 batch PCKh 0.25\n",
      "Trained batch 1711 batch loss 0.609680772 batch mAP 0.427978516 batch PCKh 0.625\n",
      "Trained batch 1712 batch loss 0.676192939 batch mAP 0.399780273 batch PCKh 0.4375\n",
      "Trained batch 1713 batch loss 0.693220139 batch mAP 0.408660889 batch PCKh 0.75\n",
      "Trained batch 1714 batch loss 0.683137119 batch mAP 0.433807373 batch PCKh 0.5\n",
      "Trained batch 1715 batch loss 0.616522193 batch mAP 0.444946289 batch PCKh 0.75\n",
      "Trained batch 1716 batch loss 0.610989749 batch mAP 0.417480469 batch PCKh 0.5\n",
      "Trained batch 1717 batch loss 0.712364912 batch mAP 0.42477417 batch PCKh 0.625\n",
      "Trained batch 1718 batch loss 0.680686712 batch mAP 0.415863037 batch PCKh 0.4375\n",
      "Trained batch 1719 batch loss 0.655695319 batch mAP 0.467773438 batch PCKh 0.5625\n",
      "Trained batch 1720 batch loss 0.602718 batch mAP 0.4659729 batch PCKh 0.6875\n",
      "Trained batch 1721 batch loss 0.60851866 batch mAP 0.458282471 batch PCKh 0.4375\n",
      "Trained batch 1722 batch loss 0.628592849 batch mAP 0.452819824 batch PCKh 0.5\n",
      "Trained batch 1723 batch loss 0.587282181 batch mAP 0.475860596 batch PCKh 0.5625\n",
      "Trained batch 1724 batch loss 0.584286 batch mAP 0.491546631 batch PCKh 0.375\n",
      "Trained batch 1725 batch loss 0.570984125 batch mAP 0.431671143 batch PCKh 0.625\n",
      "Trained batch 1726 batch loss 0.632564902 batch mAP 0.391693115 batch PCKh 0.875\n",
      "Trained batch 1727 batch loss 0.629325747 batch mAP 0.45803833 batch PCKh 0.625\n",
      "Trained batch 1728 batch loss 0.588977 batch mAP 0.452758789 batch PCKh 0.5\n",
      "Trained batch 1729 batch loss 0.64298439 batch mAP 0.299102783 batch PCKh 0.5\n",
      "Trained batch 1730 batch loss 0.594939113 batch mAP 0.433502197 batch PCKh 0.75\n",
      "Trained batch 1731 batch loss 0.608335257 batch mAP 0.379089355 batch PCKh 0.5\n",
      "Trained batch 1732 batch loss 0.559063613 batch mAP 0.440856934 batch PCKh 0.1875\n",
      "Trained batch 1733 batch loss 0.536740661 batch mAP 0.418365479 batch PCKh 0.3125\n",
      "Trained batch 1734 batch loss 0.635744154 batch mAP 0.433380127 batch PCKh 0.5625\n",
      "Trained batch 1735 batch loss 0.622604668 batch mAP 0.457000732 batch PCKh 0.6875\n",
      "Trained batch 1736 batch loss 0.596080661 batch mAP 0.439208984 batch PCKh 0.375\n",
      "Trained batch 1737 batch loss 0.650975347 batch mAP 0.380096436 batch PCKh 0.6875\n",
      "Trained batch 1738 batch loss 0.68181 batch mAP 0.413635254 batch PCKh 0.3125\n",
      "Trained batch 1739 batch loss 0.683515429 batch mAP 0.401092529 batch PCKh 0.625\n",
      "Trained batch 1740 batch loss 0.754222274 batch mAP 0.441345215 batch PCKh 0\n",
      "Trained batch 1741 batch loss 0.650550961 batch mAP 0.399993896 batch PCKh 0\n",
      "Trained batch 1742 batch loss 0.771098 batch mAP 0.402679443 batch PCKh 0.5625\n",
      "Trained batch 1743 batch loss 0.79814738 batch mAP 0.332275391 batch PCKh 0\n",
      "Trained batch 1744 batch loss 0.80424881 batch mAP 0.356018066 batch PCKh 0.0625\n",
      "Trained batch 1745 batch loss 0.801109195 batch mAP 0.334442139 batch PCKh 0\n",
      "Trained batch 1746 batch loss 0.710181236 batch mAP 0.216125488 batch PCKh 0\n",
      "Trained batch 1747 batch loss 0.718333066 batch mAP 0.120941162 batch PCKh 0.25\n",
      "Trained batch 1748 batch loss 0.749757528 batch mAP 0.252838135 batch PCKh 0\n",
      "Trained batch 1749 batch loss 0.706600904 batch mAP 0.203582764 batch PCKh 0.375\n",
      "Trained batch 1750 batch loss 0.697405517 batch mAP 0.137756348 batch PCKh 0.3125\n",
      "Trained batch 1751 batch loss 0.705533385 batch mAP 0.157897949 batch PCKh 0.375\n",
      "Trained batch 1752 batch loss 0.698841751 batch mAP 0.112213135 batch PCKh 0.625\n",
      "Trained batch 1753 batch loss 0.62979275 batch mAP 0.228973389 batch PCKh 0.3125\n",
      "Trained batch 1754 batch loss 0.570119083 batch mAP 0.287445068 batch PCKh 0.5\n",
      "Trained batch 1755 batch loss 0.586676598 batch mAP 0.324554443 batch PCKh 0.4375\n",
      "Trained batch 1756 batch loss 0.679705143 batch mAP 0.365905762 batch PCKh 0.125\n",
      "Trained batch 1757 batch loss 0.687318444 batch mAP 0.406097412 batch PCKh 0.25\n",
      "Trained batch 1758 batch loss 0.6978755 batch mAP 0.366973877 batch PCKh 0\n",
      "Trained batch 1759 batch loss 0.734905541 batch mAP 0.341949463 batch PCKh 0.4375\n",
      "Trained batch 1760 batch loss 0.75316 batch mAP 0.286010742 batch PCKh 0.3125\n",
      "Trained batch 1761 batch loss 0.727550447 batch mAP 0.284729 batch PCKh 0\n",
      "Trained batch 1762 batch loss 0.624046326 batch mAP 0.216644287 batch PCKh 0.6875\n",
      "Trained batch 1763 batch loss 0.678466618 batch mAP 0.425415039 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1764 batch loss 0.639052749 batch mAP 0.384521484 batch PCKh 0.5\n",
      "Trained batch 1765 batch loss 0.58820653 batch mAP 0.299194336 batch PCKh 0\n",
      "Trained batch 1766 batch loss 0.59711647 batch mAP 0.378814697 batch PCKh 0.625\n",
      "Trained batch 1767 batch loss 0.628904 batch mAP 0.404388428 batch PCKh 0.5625\n",
      "Trained batch 1768 batch loss 0.58637315 batch mAP 0.368255615 batch PCKh 0.1875\n",
      "Trained batch 1769 batch loss 0.595203757 batch mAP 0.413513184 batch PCKh 0.6875\n",
      "Trained batch 1770 batch loss 0.600179374 batch mAP 0.483001709 batch PCKh 0.25\n",
      "Trained batch 1771 batch loss 0.629896104 batch mAP 0.44140625 batch PCKh 0.5625\n",
      "Trained batch 1772 batch loss 0.628215075 batch mAP 0.475982666 batch PCKh 0.5625\n",
      "Trained batch 1773 batch loss 0.569837391 batch mAP 0.468902588 batch PCKh 0.625\n",
      "Trained batch 1774 batch loss 0.590005398 batch mAP 0.489379883 batch PCKh 0.5\n",
      "Trained batch 1775 batch loss 0.567710042 batch mAP 0.484832764 batch PCKh 0.5\n",
      "Trained batch 1776 batch loss 0.588270903 batch mAP 0.497619629 batch PCKh 0.25\n",
      "Trained batch 1777 batch loss 0.678314328 batch mAP 0.447296143 batch PCKh 0.0625\n",
      "Trained batch 1778 batch loss 0.687141895 batch mAP 0.482574463 batch PCKh 0.25\n",
      "Trained batch 1779 batch loss 0.651141763 batch mAP 0.513702393 batch PCKh 0.3125\n",
      "Trained batch 1780 batch loss 0.5954777 batch mAP 0.485992432 batch PCKh 0.1875\n",
      "Trained batch 1781 batch loss 0.663146615 batch mAP 0.487670898 batch PCKh 0.375\n",
      "Trained batch 1782 batch loss 0.595620513 batch mAP 0.480560303 batch PCKh 0.25\n",
      "Trained batch 1783 batch loss 0.656743526 batch mAP 0.479278564 batch PCKh 0.4375\n",
      "Trained batch 1784 batch loss 0.677157402 batch mAP 0.468109131 batch PCKh 0.1875\n",
      "Trained batch 1785 batch loss 0.649373829 batch mAP 0.425872803 batch PCKh 0.3125\n",
      "Trained batch 1786 batch loss 0.589854658 batch mAP 0.443237305 batch PCKh 0.1875\n",
      "Trained batch 1787 batch loss 0.593547285 batch mAP 0.437286377 batch PCKh 0.1875\n",
      "Trained batch 1788 batch loss 0.662838 batch mAP 0.418884277 batch PCKh 0.625\n",
      "Trained batch 1789 batch loss 0.577462435 batch mAP 0.462799072 batch PCKh 0.4375\n",
      "Trained batch 1790 batch loss 0.677259564 batch mAP 0.412780762 batch PCKh 0.3125\n",
      "Trained batch 1791 batch loss 0.71158433 batch mAP 0.37121582 batch PCKh 0\n",
      "Trained batch 1792 batch loss 0.630291224 batch mAP 0.438842773 batch PCKh 0.5\n",
      "Trained batch 1793 batch loss 0.615908 batch mAP 0.443084717 batch PCKh 0.9375\n",
      "Trained batch 1794 batch loss 0.542873442 batch mAP 0.416687 batch PCKh 0.375\n",
      "Trained batch 1795 batch loss 0.648148835 batch mAP 0.421539307 batch PCKh 0.3125\n",
      "Trained batch 1796 batch loss 0.617186666 batch mAP 0.371948242 batch PCKh 0\n",
      "Trained batch 1797 batch loss 0.644135 batch mAP 0.389526367 batch PCKh 0.3125\n",
      "Trained batch 1798 batch loss 0.592262685 batch mAP 0.345794678 batch PCKh 0\n",
      "Trained batch 1799 batch loss 0.584820628 batch mAP 0.365753174 batch PCKh 0.375\n",
      "Trained batch 1800 batch loss 0.58205843 batch mAP 0.30090332 batch PCKh 0.4375\n",
      "Trained batch 1801 batch loss 0.605359674 batch mAP 0.356018066 batch PCKh 0.0625\n",
      "Trained batch 1802 batch loss 0.580868125 batch mAP 0.353302 batch PCKh 0.1875\n",
      "Trained batch 1803 batch loss 0.571632504 batch mAP 0.359771729 batch PCKh 0.3125\n",
      "Trained batch 1804 batch loss 0.552416563 batch mAP 0.379333496 batch PCKh 0\n",
      "Trained batch 1805 batch loss 0.582850039 batch mAP 0.377227783 batch PCKh 0.5625\n",
      "Trained batch 1806 batch loss 0.651014 batch mAP 0.404632568 batch PCKh 0.125\n",
      "Trained batch 1807 batch loss 0.690164328 batch mAP 0.41204834 batch PCKh 0.1875\n",
      "Trained batch 1808 batch loss 0.663939893 batch mAP 0.434143066 batch PCKh 0.625\n",
      "Trained batch 1809 batch loss 0.580834687 batch mAP 0.441162109 batch PCKh 0.1875\n",
      "Trained batch 1810 batch loss 0.649689138 batch mAP 0.449676514 batch PCKh 0.25\n",
      "Trained batch 1811 batch loss 0.635075271 batch mAP 0.477783203 batch PCKh 0.0625\n",
      "Trained batch 1812 batch loss 0.631240487 batch mAP 0.50402832 batch PCKh 0.4375\n",
      "Trained batch 1813 batch loss 0.657461643 batch mAP 0.508331299 batch PCKh 0.25\n",
      "Trained batch 1814 batch loss 0.679500818 batch mAP 0.473632812 batch PCKh 0.25\n",
      "Trained batch 1815 batch loss 0.675435066 batch mAP 0.505889893 batch PCKh 0.1875\n",
      "Trained batch 1816 batch loss 0.664627373 batch mAP 0.466827393 batch PCKh 0\n",
      "Trained batch 1817 batch loss 0.735814393 batch mAP 0.446655273 batch PCKh 0.375\n",
      "Trained batch 1818 batch loss 0.672426 batch mAP 0.499115 batch PCKh 0.1875\n",
      "Trained batch 1819 batch loss 0.691682458 batch mAP 0.491760254 batch PCKh 0.1875\n",
      "Trained batch 1820 batch loss 0.672407746 batch mAP 0.517181396 batch PCKh 0.1875\n",
      "Trained batch 1821 batch loss 0.623924255 batch mAP 0.497467041 batch PCKh 0.5\n",
      "Trained batch 1822 batch loss 0.69846642 batch mAP 0.454559326 batch PCKh 0.1875\n",
      "Trained batch 1823 batch loss 0.743466735 batch mAP 0.457458496 batch PCKh 0.1875\n",
      "Trained batch 1824 batch loss 0.743520737 batch mAP 0.435058594 batch PCKh 0.5\n",
      "Trained batch 1825 batch loss 0.65748775 batch mAP 0.455413818 batch PCKh 0\n",
      "Trained batch 1826 batch loss 0.672064126 batch mAP 0.481536865 batch PCKh 0.5\n",
      "Trained batch 1827 batch loss 0.728174448 batch mAP 0.408172607 batch PCKh 0.875\n",
      "Trained batch 1828 batch loss 0.621779263 batch mAP 0.449737549 batch PCKh 0.625\n",
      "Trained batch 1829 batch loss 0.677583098 batch mAP 0.41192627 batch PCKh 0.875\n",
      "Trained batch 1830 batch loss 0.681595206 batch mAP 0.435394287 batch PCKh 0.25\n",
      "Trained batch 1831 batch loss 0.62878406 batch mAP 0.425048828 batch PCKh 0.5\n",
      "Trained batch 1832 batch loss 0.694319665 batch mAP 0.444244385 batch PCKh 0.625\n",
      "Trained batch 1833 batch loss 0.722718418 batch mAP 0.420501709 batch PCKh 0.5625\n",
      "Trained batch 1834 batch loss 0.628631711 batch mAP 0.377990723 batch PCKh 0.5625\n",
      "Trained batch 1835 batch loss 0.620357454 batch mAP 0.410705566 batch PCKh 0.125\n",
      "Trained batch 1836 batch loss 0.608948 batch mAP 0.383728027 batch PCKh 0.3125\n",
      "Trained batch 1837 batch loss 0.665485859 batch mAP 0.385894775 batch PCKh 0.5625\n",
      "Trained batch 1838 batch loss 0.648513198 batch mAP 0.392456055 batch PCKh 0.25\n",
      "Trained batch 1839 batch loss 0.543939 batch mAP 0.387115479 batch PCKh 0.375\n",
      "Trained batch 1840 batch loss 0.631129 batch mAP 0.427429199 batch PCKh 0.0625\n",
      "Trained batch 1841 batch loss 0.513586223 batch mAP 0.390869141 batch PCKh 0.1875\n",
      "Trained batch 1842 batch loss 0.581588447 batch mAP 0.408905029 batch PCKh 0.5625\n",
      "Trained batch 1843 batch loss 0.602935791 batch mAP 0.403015137 batch PCKh 0.625\n",
      "Trained batch 1844 batch loss 0.53412652 batch mAP 0.426147461 batch PCKh 0.4375\n",
      "Trained batch 1845 batch loss 0.651173949 batch mAP 0.396179199 batch PCKh 0.8125\n",
      "Trained batch 1846 batch loss 0.56927979 batch mAP 0.44732666 batch PCKh 0.75\n",
      "Trained batch 1847 batch loss 0.566019058 batch mAP 0.443695068 batch PCKh 0.625\n",
      "Trained batch 1848 batch loss 0.546225727 batch mAP 0.455444336 batch PCKh 0\n",
      "Trained batch 1849 batch loss 0.475121945 batch mAP 0.492156982 batch PCKh 0.6875\n",
      "Trained batch 1850 batch loss 0.460688114 batch mAP 0.520721436 batch PCKh 0\n",
      "Trained batch 1851 batch loss 0.526454687 batch mAP 0.458404541 batch PCKh 0\n",
      "Trained batch 1852 batch loss 0.536902547 batch mAP 0.447143555 batch PCKh 0.4375\n",
      "Trained batch 1853 batch loss 0.466476083 batch mAP 0.492034912 batch PCKh 0\n",
      "Trained batch 1854 batch loss 0.460079789 batch mAP 0.484588623 batch PCKh 0\n",
      "Trained batch 1855 batch loss 0.612483144 batch mAP 0.436615 batch PCKh 0.5\n",
      "Trained batch 1856 batch loss 0.688539267 batch mAP 0.462188721 batch PCKh 0.5625\n",
      "Trained batch 1857 batch loss 0.738240361 batch mAP 0.414337158 batch PCKh 0.125\n",
      "Trained batch 1858 batch loss 0.68319428 batch mAP 0.451934814 batch PCKh 0.1875\n",
      "Trained batch 1859 batch loss 0.635985196 batch mAP 0.433654785 batch PCKh 0.4375\n",
      "Trained batch 1860 batch loss 0.654723 batch mAP 0.430725098 batch PCKh 0.3125\n",
      "Trained batch 1861 batch loss 0.646494031 batch mAP 0.412902832 batch PCKh 0.1875\n",
      "Trained batch 1862 batch loss 0.661747456 batch mAP 0.377197266 batch PCKh 0.1875\n",
      "Trained batch 1863 batch loss 0.606619596 batch mAP 0.321960449 batch PCKh 0.5\n",
      "Trained batch 1864 batch loss 0.671384931 batch mAP 0.380096436 batch PCKh 0.125\n",
      "Trained batch 1865 batch loss 0.70206821 batch mAP 0.382202148 batch PCKh 0.3125\n",
      "Trained batch 1866 batch loss 0.715480089 batch mAP 0.38760376 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1867 batch loss 0.582992435 batch mAP 0.421569824 batch PCKh 0.125\n",
      "Trained batch 1868 batch loss 0.583143353 batch mAP 0.454620361 batch PCKh 0.3125\n",
      "Trained batch 1869 batch loss 0.710696936 batch mAP 0.377349854 batch PCKh 0.0625\n",
      "Trained batch 1870 batch loss 0.704589903 batch mAP 0.350952148 batch PCKh 0\n",
      "Trained batch 1871 batch loss 0.740893304 batch mAP 0.329528809 batch PCKh 0\n",
      "Trained batch 1872 batch loss 0.627100706 batch mAP 0.348022461 batch PCKh 0.25\n",
      "Trained batch 1873 batch loss 0.701454282 batch mAP 0.358276367 batch PCKh 0.5625\n",
      "Trained batch 1874 batch loss 0.713158488 batch mAP 0.348754883 batch PCKh 0.25\n",
      "Trained batch 1875 batch loss 0.718821883 batch mAP 0.333679199 batch PCKh 0.75\n",
      "Trained batch 1876 batch loss 0.731325865 batch mAP 0.382629395 batch PCKh 0.6875\n",
      "Trained batch 1877 batch loss 0.792420506 batch mAP 0.333862305 batch PCKh 0\n",
      "Trained batch 1878 batch loss 0.727703869 batch mAP 0.305358887 batch PCKh 0.0625\n",
      "Trained batch 1879 batch loss 0.664308429 batch mAP 0.248901367 batch PCKh 0.75\n",
      "Trained batch 1880 batch loss 0.656487226 batch mAP 0.279174805 batch PCKh 0.625\n",
      "Trained batch 1881 batch loss 0.661506772 batch mAP 0.302093506 batch PCKh 0.5\n",
      "Trained batch 1882 batch loss 0.704393804 batch mAP 0.33807373 batch PCKh 0.6875\n",
      "Trained batch 1883 batch loss 0.649099827 batch mAP 0.356048584 batch PCKh 0.6875\n",
      "Trained batch 1884 batch loss 0.65821588 batch mAP 0.375946045 batch PCKh 0.75\n",
      "Trained batch 1885 batch loss 0.64561832 batch mAP 0.39263916 batch PCKh 0.5625\n",
      "Trained batch 1886 batch loss 0.668767095 batch mAP 0.392791748 batch PCKh 0.4375\n",
      "Trained batch 1887 batch loss 0.69575 batch mAP 0.379882812 batch PCKh 0.0625\n",
      "Trained batch 1888 batch loss 0.654150188 batch mAP 0.400665283 batch PCKh 0.1875\n",
      "Trained batch 1889 batch loss 0.719245374 batch mAP 0.419952393 batch PCKh 0.1875\n",
      "Trained batch 1890 batch loss 0.707942724 batch mAP 0.400543213 batch PCKh 0.5\n",
      "Trained batch 1891 batch loss 0.689173281 batch mAP 0.368530273 batch PCKh 0.5625\n",
      "Trained batch 1892 batch loss 0.705690205 batch mAP 0.399749756 batch PCKh 0.5\n",
      "Trained batch 1893 batch loss 0.80997324 batch mAP 0.356689453 batch PCKh 0\n",
      "Trained batch 1894 batch loss 0.718244135 batch mAP 0.428161621 batch PCKh 0.125\n",
      "Trained batch 1895 batch loss 0.746125519 batch mAP 0.418518066 batch PCKh 0\n",
      "Trained batch 1896 batch loss 0.743042111 batch mAP 0.436035156 batch PCKh 0\n",
      "Trained batch 1897 batch loss 0.755110323 batch mAP 0.407836914 batch PCKh 0.25\n",
      "Trained batch 1898 batch loss 0.731054246 batch mAP 0.423278809 batch PCKh 0.5625\n",
      "Trained batch 1899 batch loss 0.745438933 batch mAP 0.431488037 batch PCKh 0.1875\n",
      "Trained batch 1900 batch loss 0.706608176 batch mAP 0.425140381 batch PCKh 0.625\n",
      "Trained batch 1901 batch loss 0.72341 batch mAP 0.435577393 batch PCKh 0.625\n",
      "Trained batch 1902 batch loss 0.729281425 batch mAP 0.413360596 batch PCKh 0\n",
      "Trained batch 1903 batch loss 0.732424676 batch mAP 0.42300415 batch PCKh 0.125\n",
      "Trained batch 1904 batch loss 0.695255578 batch mAP 0.437591553 batch PCKh 0.625\n",
      "Trained batch 1905 batch loss 0.726560056 batch mAP 0.445800781 batch PCKh 0.5625\n",
      "Trained batch 1906 batch loss 0.699785888 batch mAP 0.441680908 batch PCKh 0.1875\n",
      "Trained batch 1907 batch loss 0.676273942 batch mAP 0.422607422 batch PCKh 0.625\n",
      "Trained batch 1908 batch loss 0.721330643 batch mAP 0.406829834 batch PCKh 0.5\n",
      "Trained batch 1909 batch loss 0.699724436 batch mAP 0.408996582 batch PCKh 0.0625\n",
      "Trained batch 1910 batch loss 0.711174071 batch mAP 0.397125244 batch PCKh 0.25\n",
      "Trained batch 1911 batch loss 0.709710836 batch mAP 0.39465332 batch PCKh 0.1875\n",
      "Trained batch 1912 batch loss 0.647208571 batch mAP 0.430969238 batch PCKh 0\n",
      "Trained batch 1913 batch loss 0.645853639 batch mAP 0.46774292 batch PCKh 0.375\n",
      "Trained batch 1914 batch loss 0.646404445 batch mAP 0.472503662 batch PCKh 0.5625\n",
      "Trained batch 1915 batch loss 0.608936429 batch mAP 0.44732666 batch PCKh 0.3125\n",
      "Trained batch 1916 batch loss 0.674842656 batch mAP 0.457122803 batch PCKh 0.625\n",
      "Trained batch 1917 batch loss 0.640796304 batch mAP 0.457244873 batch PCKh 0.125\n",
      "Trained batch 1918 batch loss 0.64596951 batch mAP 0.375061035 batch PCKh 0.4375\n",
      "Trained batch 1919 batch loss 0.690446615 batch mAP 0.350616455 batch PCKh 0.25\n",
      "Trained batch 1920 batch loss 0.626030326 batch mAP 0.284729 batch PCKh 0.25\n",
      "Trained batch 1921 batch loss 0.69538939 batch mAP 0.293945312 batch PCKh 0\n",
      "Trained batch 1922 batch loss 0.704497218 batch mAP 0.341278076 batch PCKh 0.375\n",
      "Trained batch 1923 batch loss 0.585391045 batch mAP 0.406219482 batch PCKh 0.4375\n",
      "Trained batch 1924 batch loss 0.646108866 batch mAP 0.447113037 batch PCKh 0.75\n",
      "Trained batch 1925 batch loss 0.743093789 batch mAP 0.460327148 batch PCKh 0.3125\n",
      "Trained batch 1926 batch loss 0.715457618 batch mAP 0.473510742 batch PCKh 0.4375\n",
      "Trained batch 1927 batch loss 0.749759674 batch mAP 0.440979 batch PCKh 0.5\n",
      "Trained batch 1928 batch loss 0.680743814 batch mAP 0.472412109 batch PCKh 0.1875\n",
      "Trained batch 1929 batch loss 0.739309788 batch mAP 0.460876465 batch PCKh 0.5\n",
      "Trained batch 1930 batch loss 0.685378969 batch mAP 0.449920654 batch PCKh 0.625\n",
      "Trained batch 1931 batch loss 0.67748636 batch mAP 0.452728271 batch PCKh 0.6875\n",
      "Trained batch 1932 batch loss 0.650983751 batch mAP 0.415252686 batch PCKh 0.5625\n",
      "Trained batch 1933 batch loss 0.714722216 batch mAP 0.421875 batch PCKh 0.5625\n",
      "Trained batch 1934 batch loss 0.719656825 batch mAP 0.389587402 batch PCKh 0.875\n",
      "Trained batch 1935 batch loss 0.608032167 batch mAP 0.385192871 batch PCKh 0.5625\n",
      "Trained batch 1936 batch loss 0.577204168 batch mAP 0.414611816 batch PCKh 0.75\n",
      "Trained batch 1937 batch loss 0.623577237 batch mAP 0.378875732 batch PCKh 0.625\n",
      "Trained batch 1938 batch loss 0.6659109 batch mAP 0.388214111 batch PCKh 0.75\n",
      "Trained batch 1939 batch loss 0.592327178 batch mAP 0.401000977 batch PCKh 0.5625\n",
      "Trained batch 1940 batch loss 0.690648615 batch mAP 0.392700195 batch PCKh 0.125\n",
      "Trained batch 1941 batch loss 0.646100521 batch mAP 0.412414551 batch PCKh 0.5\n",
      "Trained batch 1942 batch loss 0.648081183 batch mAP 0.442779541 batch PCKh 0.6875\n",
      "Trained batch 1943 batch loss 0.61831373 batch mAP 0.4737854 batch PCKh 0\n",
      "Trained batch 1944 batch loss 0.64312613 batch mAP 0.422821045 batch PCKh 0.75\n",
      "Trained batch 1945 batch loss 0.658717453 batch mAP 0.435089111 batch PCKh 0.375\n",
      "Trained batch 1946 batch loss 0.69601059 batch mAP 0.422851562 batch PCKh 0.25\n",
      "Trained batch 1947 batch loss 0.62731123 batch mAP 0.430267334 batch PCKh 0.375\n",
      "Trained batch 1948 batch loss 0.646226883 batch mAP 0.440856934 batch PCKh 0.4375\n",
      "Trained batch 1949 batch loss 0.652355492 batch mAP 0.438934326 batch PCKh 0.125\n",
      "Trained batch 1950 batch loss 0.623823 batch mAP 0.464324951 batch PCKh 0.1875\n",
      "Trained batch 1951 batch loss 0.586941957 batch mAP 0.477508545 batch PCKh 0.5625\n",
      "Trained batch 1952 batch loss 0.677232146 batch mAP 0.484100342 batch PCKh 0.125\n",
      "Trained batch 1953 batch loss 0.686766803 batch mAP 0.456787109 batch PCKh 0.375\n",
      "Trained batch 1954 batch loss 0.646894753 batch mAP 0.478515625 batch PCKh 0.5\n",
      "Trained batch 1955 batch loss 0.662655354 batch mAP 0.462280273 batch PCKh 0.25\n",
      "Trained batch 1956 batch loss 0.747123122 batch mAP 0.325714111 batch PCKh 0.625\n",
      "Trained batch 1957 batch loss 0.831163645 batch mAP 0.331939697 batch PCKh 0\n",
      "Trained batch 1958 batch loss 0.805016279 batch mAP 0.396209717 batch PCKh 0\n",
      "Trained batch 1959 batch loss 0.721323073 batch mAP 0.407653809 batch PCKh 0\n",
      "Trained batch 1960 batch loss 0.687404811 batch mAP 0.440216064 batch PCKh 0.1875\n",
      "Trained batch 1961 batch loss 0.724642873 batch mAP 0.416717529 batch PCKh 0.3125\n",
      "Trained batch 1962 batch loss 0.701693177 batch mAP 0.36138916 batch PCKh 0.25\n",
      "Trained batch 1963 batch loss 0.720682502 batch mAP 0.36138916 batch PCKh 0.25\n",
      "Trained batch 1964 batch loss 0.643051 batch mAP 0.337738037 batch PCKh 0.4375\n",
      "Trained batch 1965 batch loss 0.665326774 batch mAP 0.358917236 batch PCKh 0.625\n",
      "Trained batch 1966 batch loss 0.662943304 batch mAP 0.353424072 batch PCKh 0.375\n",
      "Trained batch 1967 batch loss 0.657464683 batch mAP 0.361572266 batch PCKh 0.4375\n",
      "Trained batch 1968 batch loss 0.573445559 batch mAP 0.398010254 batch PCKh 0.0625\n",
      "Trained batch 1969 batch loss 0.626150966 batch mAP 0.419158936 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1970 batch loss 0.692872763 batch mAP 0.459869385 batch PCKh 0.625\n",
      "Trained batch 1971 batch loss 0.74284339 batch mAP 0.444519043 batch PCKh 0.1875\n",
      "Trained batch 1972 batch loss 0.628265262 batch mAP 0.460723877 batch PCKh 0.6875\n",
      "Trained batch 1973 batch loss 0.707524836 batch mAP 0.483062744 batch PCKh 0.5625\n",
      "Trained batch 1974 batch loss 0.697585 batch mAP 0.424743652 batch PCKh 0.125\n",
      "Trained batch 1975 batch loss 0.603091955 batch mAP 0.438720703 batch PCKh 0.125\n",
      "Trained batch 1976 batch loss 0.62725395 batch mAP 0.44619751 batch PCKh 0.25\n",
      "Trained batch 1977 batch loss 0.549403846 batch mAP 0.367584229 batch PCKh 0.25\n",
      "Trained batch 1978 batch loss 0.538846 batch mAP 0.474456787 batch PCKh 0.3125\n",
      "Trained batch 1979 batch loss 0.584071279 batch mAP 0.48223877 batch PCKh 0.3125\n",
      "Trained batch 1980 batch loss 0.580458283 batch mAP 0.485778809 batch PCKh 0.375\n",
      "Trained batch 1981 batch loss 0.529387951 batch mAP 0.470855713 batch PCKh 0.3125\n",
      "Trained batch 1982 batch loss 0.652648211 batch mAP 0.436523438 batch PCKh 0.3125\n",
      "Trained batch 1983 batch loss 0.682117462 batch mAP 0.253051758 batch PCKh 0.5625\n",
      "Trained batch 1984 batch loss 0.710812449 batch mAP 0.26083374 batch PCKh 0.4375\n",
      "Trained batch 1985 batch loss 0.791913152 batch mAP 0.0896911621 batch PCKh 0\n",
      "Trained batch 1986 batch loss 0.867258787 batch mAP 0.0883789062 batch PCKh 0.5\n",
      "Trained batch 1987 batch loss 0.753939152 batch mAP 0.316040039 batch PCKh 0.5625\n",
      "Trained batch 1988 batch loss 0.752869189 batch mAP 0.400238037 batch PCKh 0.375\n",
      "Trained batch 1989 batch loss 0.716401339 batch mAP 0.429290771 batch PCKh 0.5\n",
      "Trained batch 1990 batch loss 0.722393155 batch mAP 0.352874756 batch PCKh 0.75\n",
      "Trained batch 1991 batch loss 0.65049243 batch mAP 0.308074951 batch PCKh 0.625\n",
      "Trained batch 1992 batch loss 0.738271773 batch mAP 0.268890381 batch PCKh 0.6875\n",
      "Trained batch 1993 batch loss 0.763652146 batch mAP 0.207183838 batch PCKh 0.25\n",
      "Trained batch 1994 batch loss 0.787465215 batch mAP 0.150421143 batch PCKh 0.1875\n",
      "Trained batch 1995 batch loss 0.697802842 batch mAP 0.0269470215 batch PCKh 0.125\n",
      "Trained batch 1996 batch loss 0.721934557 batch mAP 0.0174865723 batch PCKh 0.4375\n",
      "Trained batch 1997 batch loss 0.713261187 batch mAP 0.00933837891 batch PCKh 0.6875\n",
      "Trained batch 1998 batch loss 0.705739617 batch mAP 0.0138549805 batch PCKh 0\n",
      "Trained batch 1999 batch loss 0.684708834 batch mAP 0.0127258301 batch PCKh 0.6875\n",
      "Trained batch 2000 batch loss 0.536395431 batch mAP 0.021270752 batch PCKh 0\n",
      "Trained batch 2001 batch loss 0.574748218 batch mAP 0.082244873 batch PCKh 0.1875\n",
      "Trained batch 2002 batch loss 0.566304743 batch mAP 0.148895264 batch PCKh 0.625\n",
      "Trained batch 2003 batch loss 0.598971 batch mAP 0.270324707 batch PCKh 0.6875\n",
      "Trained batch 2004 batch loss 0.523311317 batch mAP 0.31451416 batch PCKh 0.375\n",
      "Trained batch 2005 batch loss 0.553532302 batch mAP 0.341278076 batch PCKh 0.6875\n",
      "Trained batch 2006 batch loss 0.579082966 batch mAP 0.377594 batch PCKh 0.75\n",
      "Trained batch 2007 batch loss 0.733166218 batch mAP 0.316223145 batch PCKh 0.5\n",
      "Trained batch 2008 batch loss 0.698248744 batch mAP 0.392303467 batch PCKh 0.625\n",
      "Trained batch 2009 batch loss 0.654016376 batch mAP 0.285461426 batch PCKh 0.25\n",
      "Trained batch 2010 batch loss 0.715995908 batch mAP 0.32043457 batch PCKh 0.625\n",
      "Trained batch 2011 batch loss 0.68161732 batch mAP 0.390075684 batch PCKh 0\n",
      "Trained batch 2012 batch loss 0.717101693 batch mAP 0.369659424 batch PCKh 0.4375\n",
      "Trained batch 2013 batch loss 0.769181 batch mAP 0.386871338 batch PCKh 0.3125\n",
      "Trained batch 2014 batch loss 0.674232125 batch mAP 0.39956665 batch PCKh 0.125\n",
      "Trained batch 2015 batch loss 0.562825441 batch mAP 0.311248779 batch PCKh 0.375\n",
      "Trained batch 2016 batch loss 0.605750442 batch mAP 0.304901123 batch PCKh 0.375\n",
      "Trained batch 2017 batch loss 0.642585635 batch mAP 0.355529785 batch PCKh 0\n",
      "Trained batch 2018 batch loss 0.546582878 batch mAP 0.287963867 batch PCKh 0.6875\n",
      "Trained batch 2019 batch loss 0.740456939 batch mAP 0.369995117 batch PCKh 0\n",
      "Trained batch 2020 batch loss 0.704597652 batch mAP 0.395477295 batch PCKh 0.5\n",
      "Trained batch 2021 batch loss 0.685961485 batch mAP 0.372314453 batch PCKh 0.75\n",
      "Trained batch 2022 batch loss 0.608878911 batch mAP 0.381866455 batch PCKh 0.75\n",
      "Trained batch 2023 batch loss 0.736614406 batch mAP 0.435943604 batch PCKh 0.8125\n",
      "Trained batch 2024 batch loss 0.687418222 batch mAP 0.389373779 batch PCKh 0\n",
      "Trained batch 2025 batch loss 0.658241928 batch mAP 0.405181885 batch PCKh 0.5\n",
      "Trained batch 2026 batch loss 0.653125167 batch mAP 0.41217041 batch PCKh 0.1875\n",
      "Trained batch 2027 batch loss 0.659080327 batch mAP 0.452392578 batch PCKh 0.5625\n",
      "Trained batch 2028 batch loss 0.739165 batch mAP 0.380126953 batch PCKh 0.1875\n",
      "Trained batch 2029 batch loss 0.777964115 batch mAP 0.321899414 batch PCKh 0.75\n",
      "Trained batch 2030 batch loss 0.727911949 batch mAP 0.336669922 batch PCKh 0.875\n",
      "Trained batch 2031 batch loss 0.623908401 batch mAP 0.401306152 batch PCKh 0.6875\n",
      "Trained batch 2032 batch loss 0.709875703 batch mAP 0.442932129 batch PCKh 0.6875\n",
      "Trained batch 2033 batch loss 0.694822907 batch mAP 0.409210205 batch PCKh 0.75\n",
      "Trained batch 2034 batch loss 0.708505034 batch mAP 0.428070068 batch PCKh 0.8125\n",
      "Trained batch 2035 batch loss 0.594116807 batch mAP 0.402740479 batch PCKh 0.6875\n",
      "Trained batch 2036 batch loss 0.581829786 batch mAP 0.406158447 batch PCKh 0.4375\n",
      "Trained batch 2037 batch loss 0.583519697 batch mAP 0.394134521 batch PCKh 0.6875\n",
      "Trained batch 2038 batch loss 0.553058863 batch mAP 0.391845703 batch PCKh 0.375\n",
      "Trained batch 2039 batch loss 0.557038844 batch mAP 0.409393311 batch PCKh 0.3125\n",
      "Trained batch 2040 batch loss 0.535448551 batch mAP 0.371154785 batch PCKh 0.375\n",
      "Trained batch 2041 batch loss 0.553758204 batch mAP 0.392974854 batch PCKh 0.5\n",
      "Trained batch 2042 batch loss 0.545133233 batch mAP 0.39553833 batch PCKh 0.5625\n",
      "Trained batch 2043 batch loss 0.605452836 batch mAP 0.398498535 batch PCKh 0.625\n",
      "Trained batch 2044 batch loss 0.693521321 batch mAP 0.381347656 batch PCKh 0.3125\n",
      "Trained batch 2045 batch loss 0.695473075 batch mAP 0.382141113 batch PCKh 0.3125\n",
      "Trained batch 2046 batch loss 0.740430057 batch mAP 0.280090332 batch PCKh 0.375\n",
      "Trained batch 2047 batch loss 0.665546238 batch mAP 0.386627197 batch PCKh 0.75\n",
      "Trained batch 2048 batch loss 0.689480722 batch mAP 0.40838623 batch PCKh 0.6875\n",
      "Trained batch 2049 batch loss 0.721137822 batch mAP 0.369812 batch PCKh 0\n",
      "Trained batch 2050 batch loss 0.657840371 batch mAP 0.429138184 batch PCKh 0.8125\n",
      "Trained batch 2051 batch loss 0.681906 batch mAP 0.426422119 batch PCKh 0.6875\n",
      "Trained batch 2052 batch loss 0.778445542 batch mAP 0.413085938 batch PCKh 0\n",
      "Trained batch 2053 batch loss 0.732573569 batch mAP 0.410430908 batch PCKh 0\n",
      "Trained batch 2054 batch loss 0.679608524 batch mAP 0.368896484 batch PCKh 0.3125\n",
      "Trained batch 2055 batch loss 0.715441763 batch mAP 0.402282715 batch PCKh 0.5\n",
      "Trained batch 2056 batch loss 0.608098567 batch mAP 0.319152832 batch PCKh 0.5625\n",
      "Trained batch 2057 batch loss 0.654089928 batch mAP 0.347076416 batch PCKh 0.75\n",
      "Trained batch 2058 batch loss 0.714405656 batch mAP 0.368560791 batch PCKh 0\n",
      "Trained batch 2059 batch loss 0.721598327 batch mAP 0.363861084 batch PCKh 0.5625\n",
      "Trained batch 2060 batch loss 0.745080352 batch mAP 0.384796143 batch PCKh 0.125\n",
      "Trained batch 2061 batch loss 0.720809102 batch mAP 0.377380371 batch PCKh 0.1875\n",
      "Trained batch 2062 batch loss 0.625054657 batch mAP 0.359771729 batch PCKh 0.25\n",
      "Trained batch 2063 batch loss 0.653766274 batch mAP 0.436859131 batch PCKh 0.1875\n",
      "Trained batch 2064 batch loss 0.640395403 batch mAP 0.441345215 batch PCKh 0\n",
      "Trained batch 2065 batch loss 0.589097619 batch mAP 0.461700439 batch PCKh 0.125\n",
      "Trained batch 2066 batch loss 0.652545154 batch mAP 0.433166504 batch PCKh 0.3125\n",
      "Trained batch 2067 batch loss 0.540694296 batch mAP 0.473754883 batch PCKh 0.1875\n",
      "Trained batch 2068 batch loss 0.5895046 batch mAP 0.418121338 batch PCKh 0.125\n",
      "Trained batch 2069 batch loss 0.501294851 batch mAP 0.371734619 batch PCKh 0.125\n",
      "Trained batch 2070 batch loss 0.607184649 batch mAP 0.400817871 batch PCKh 0.0625\n",
      "Trained batch 2071 batch loss 0.562180281 batch mAP 0.374511719 batch PCKh 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2072 batch loss 0.697025895 batch mAP 0.390380859 batch PCKh 0.1875\n",
      "Trained batch 2073 batch loss 0.698890805 batch mAP 0.351165771 batch PCKh 0.0625\n",
      "Trained batch 2074 batch loss 0.656663716 batch mAP 0.401123047 batch PCKh 0.1875\n",
      "Trained batch 2075 batch loss 0.687557817 batch mAP 0.438446045 batch PCKh 0\n",
      "Trained batch 2076 batch loss 0.704822361 batch mAP 0.360961914 batch PCKh 0.875\n",
      "Trained batch 2077 batch loss 0.633651137 batch mAP 0.363922119 batch PCKh 0.5\n",
      "Trained batch 2078 batch loss 0.651289344 batch mAP 0.411560059 batch PCKh 0.6875\n",
      "Trained batch 2079 batch loss 0.739019811 batch mAP 0.378143311 batch PCKh 0.1875\n",
      "Trained batch 2080 batch loss 0.71343565 batch mAP 0.367095947 batch PCKh 0.5\n",
      "Trained batch 2081 batch loss 0.639658332 batch mAP 0.402862549 batch PCKh 0.8125\n",
      "Trained batch 2082 batch loss 0.599760771 batch mAP 0.355194092 batch PCKh 0.5\n",
      "Trained batch 2083 batch loss 0.551451266 batch mAP 0.395446777 batch PCKh 0.0625\n",
      "Trained batch 2084 batch loss 0.678048968 batch mAP 0.37020874 batch PCKh 0.3125\n",
      "Trained batch 2085 batch loss 0.667750537 batch mAP 0.373321533 batch PCKh 0\n",
      "Trained batch 2086 batch loss 0.791119218 batch mAP 0.351501465 batch PCKh 0.1875\n",
      "Trained batch 2087 batch loss 0.681037545 batch mAP 0.421081543 batch PCKh 0.125\n",
      "Trained batch 2088 batch loss 0.676956058 batch mAP 0.28112793 batch PCKh 0.375\n",
      "Trained batch 2089 batch loss 0.710780621 batch mAP 0.181091309 batch PCKh 0.375\n",
      "Trained batch 2090 batch loss 0.721200585 batch mAP 0.382171631 batch PCKh 0.3125\n",
      "Trained batch 2091 batch loss 0.726799 batch mAP 0.379333496 batch PCKh 0.625\n",
      "Trained batch 2092 batch loss 0.711132765 batch mAP 0.373901367 batch PCKh 0.5\n",
      "Trained batch 2093 batch loss 0.646286666 batch mAP 0.371307373 batch PCKh 0.5625\n",
      "Trained batch 2094 batch loss 0.66898191 batch mAP 0.382263184 batch PCKh 0.6875\n",
      "Trained batch 2095 batch loss 0.664202392 batch mAP 0.396148682 batch PCKh 0.625\n",
      "Trained batch 2096 batch loss 0.624924898 batch mAP 0.367279053 batch PCKh 0.375\n",
      "Trained batch 2097 batch loss 0.640980601 batch mAP 0.395324707 batch PCKh 0.25\n",
      "Trained batch 2098 batch loss 0.677512407 batch mAP 0.395111084 batch PCKh 0.0625\n",
      "Trained batch 2099 batch loss 0.68999213 batch mAP 0.406707764 batch PCKh 0.4375\n",
      "Trained batch 2100 batch loss 0.66052407 batch mAP 0.418212891 batch PCKh 0.4375\n",
      "Trained batch 2101 batch loss 0.661980271 batch mAP 0.432556152 batch PCKh 0.1875\n",
      "Trained batch 2102 batch loss 0.649981856 batch mAP 0.422821045 batch PCKh 0.5625\n",
      "Trained batch 2103 batch loss 0.643667221 batch mAP 0.403747559 batch PCKh 0.25\n",
      "Trained batch 2104 batch loss 0.647077 batch mAP 0.446655273 batch PCKh 0.6875\n",
      "Trained batch 2105 batch loss 0.621712387 batch mAP 0.415893555 batch PCKh 0.125\n",
      "Trained batch 2106 batch loss 0.634010553 batch mAP 0.421905518 batch PCKh 0.4375\n",
      "Trained batch 2107 batch loss 0.661198616 batch mAP 0.432281494 batch PCKh 0.5625\n",
      "Trained batch 2108 batch loss 0.662165165 batch mAP 0.410552979 batch PCKh 0.8125\n",
      "Trained batch 2109 batch loss 0.622749805 batch mAP 0.386291504 batch PCKh 0.5\n",
      "Trained batch 2110 batch loss 0.606327653 batch mAP 0.403381348 batch PCKh 0.125\n",
      "Trained batch 2111 batch loss 0.604492605 batch mAP 0.464080811 batch PCKh 0.25\n",
      "Trained batch 2112 batch loss 0.548529088 batch mAP 0.474395752 batch PCKh 0.375\n",
      "Trained batch 2113 batch loss 0.580928 batch mAP 0.476379395 batch PCKh 0.6875\n",
      "Trained batch 2114 batch loss 0.532657921 batch mAP 0.42755127 batch PCKh 0.75\n",
      "Trained batch 2115 batch loss 0.569603384 batch mAP 0.430480957 batch PCKh 0.75\n",
      "Trained batch 2116 batch loss 0.547644675 batch mAP 0.472808838 batch PCKh 0.75\n",
      "Trained batch 2117 batch loss 0.5836339 batch mAP 0.432312 batch PCKh 0.875\n",
      "Trained batch 2118 batch loss 0.683234572 batch mAP 0.288513184 batch PCKh 0.5625\n",
      "Trained batch 2119 batch loss 0.80186528 batch mAP 0.288208 batch PCKh 0\n",
      "Trained batch 2120 batch loss 0.672515512 batch mAP 0.253112793 batch PCKh 0.3125\n",
      "Trained batch 2121 batch loss 0.645760536 batch mAP 0.33795166 batch PCKh 0.875\n",
      "Trained batch 2122 batch loss 0.759637773 batch mAP 0.340179443 batch PCKh 0.25\n",
      "Trained batch 2123 batch loss 0.78312546 batch mAP 0.381958 batch PCKh 0\n",
      "Trained batch 2124 batch loss 0.713862419 batch mAP 0.42364502 batch PCKh 0.25\n",
      "Trained batch 2125 batch loss 0.694732606 batch mAP 0.344024658 batch PCKh 0.1875\n",
      "Trained batch 2126 batch loss 0.656221092 batch mAP 0.240936279 batch PCKh 0.1875\n",
      "Trained batch 2127 batch loss 0.652599454 batch mAP 0.119110107 batch PCKh 0.5\n",
      "Trained batch 2128 batch loss 0.647281766 batch mAP 0.0964050293 batch PCKh 0.5625\n",
      "Trained batch 2129 batch loss 0.642744362 batch mAP 0.0933532715 batch PCKh 0.25\n",
      "Trained batch 2130 batch loss 0.599807739 batch mAP 0.0944213867 batch PCKh 0.75\n",
      "Trained batch 2131 batch loss 0.640849769 batch mAP 0.196105957 batch PCKh 0.1875\n",
      "Trained batch 2132 batch loss 0.647924483 batch mAP 0.212585449 batch PCKh 0.4375\n",
      "Trained batch 2133 batch loss 0.616249084 batch mAP 0.197570801 batch PCKh 0.125\n",
      "Trained batch 2134 batch loss 0.622723103 batch mAP 0.279449463 batch PCKh 0.3125\n",
      "Trained batch 2135 batch loss 0.619658947 batch mAP 0.322174072 batch PCKh 0.375\n",
      "Trained batch 2136 batch loss 0.597967923 batch mAP 0.359954834 batch PCKh 0.4375\n",
      "Trained batch 2137 batch loss 0.662621379 batch mAP 0.259124756 batch PCKh 0.75\n",
      "Trained batch 2138 batch loss 0.718809247 batch mAP 0.294921875 batch PCKh 0.375\n",
      "Trained batch 2139 batch loss 0.707719326 batch mAP 0.168212891 batch PCKh 0\n",
      "Trained batch 2140 batch loss 0.737284064 batch mAP 0.27532959 batch PCKh 0\n",
      "Trained batch 2141 batch loss 0.65802747 batch mAP 0.399200439 batch PCKh 0.6875\n",
      "Trained batch 2142 batch loss 0.719725549 batch mAP 0.22265625 batch PCKh 0.3125\n",
      "Trained batch 2143 batch loss 0.675092578 batch mAP 0.346862793 batch PCKh 0.5625\n",
      "Trained batch 2144 batch loss 0.690523148 batch mAP 0.42401123 batch PCKh 0.25\n",
      "Trained batch 2145 batch loss 0.662468076 batch mAP 0.420196533 batch PCKh 0.5\n",
      "Trained batch 2146 batch loss 0.643278956 batch mAP 0.421661377 batch PCKh 0.25\n",
      "Trained batch 2147 batch loss 0.616421044 batch mAP 0.435211182 batch PCKh 0.375\n",
      "Trained batch 2148 batch loss 0.639186561 batch mAP 0.354309082 batch PCKh 0.125\n",
      "Trained batch 2149 batch loss 0.575514555 batch mAP 0.42666626 batch PCKh 0.5625\n",
      "Trained batch 2150 batch loss 0.533619523 batch mAP 0.469177246 batch PCKh 0.4375\n",
      "Trained batch 2151 batch loss 0.622867465 batch mAP 0.442626953 batch PCKh 0.75\n",
      "Trained batch 2152 batch loss 0.604899526 batch mAP 0.491363525 batch PCKh 0.625\n",
      "Trained batch 2153 batch loss 0.610105038 batch mAP 0.457763672 batch PCKh 0.3125\n",
      "Trained batch 2154 batch loss 0.672681212 batch mAP 0.405059814 batch PCKh 0.625\n",
      "Trained batch 2155 batch loss 0.62922895 batch mAP 0.389312744 batch PCKh 0.125\n",
      "Trained batch 2156 batch loss 0.69107306 batch mAP 0.423126221 batch PCKh 0.75\n",
      "Trained batch 2157 batch loss 0.675765753 batch mAP 0.417663574 batch PCKh 0.8125\n",
      "Trained batch 2158 batch loss 0.730664909 batch mAP 0.396636963 batch PCKh 0.875\n",
      "Trained batch 2159 batch loss 0.640375674 batch mAP 0.318939209 batch PCKh 0.3125\n",
      "Trained batch 2160 batch loss 0.648176491 batch mAP 0.156341553 batch PCKh 0.6875\n",
      "Trained batch 2161 batch loss 0.643820345 batch mAP 0.153106689 batch PCKh 0.5\n",
      "Trained batch 2162 batch loss 0.600968301 batch mAP 0.318237305 batch PCKh 0\n",
      "Trained batch 2163 batch loss 0.610516429 batch mAP 0.288024902 batch PCKh 0.5625\n",
      "Trained batch 2164 batch loss 0.632553279 batch mAP 0.281524658 batch PCKh 0.1875\n",
      "Trained batch 2165 batch loss 0.668534935 batch mAP 0.254608154 batch PCKh 0.5\n",
      "Trained batch 2166 batch loss 0.677085698 batch mAP 0.288024902 batch PCKh 0.8125\n",
      "Trained batch 2167 batch loss 0.532263279 batch mAP 0.285797119 batch PCKh 0.25\n",
      "Trained batch 2168 batch loss 0.564911842 batch mAP 0.301177979 batch PCKh 0.375\n",
      "Trained batch 2169 batch loss 0.61128509 batch mAP 0.380279541 batch PCKh 0.5\n",
      "Trained batch 2170 batch loss 0.727979779 batch mAP 0.390380859 batch PCKh 0.0625\n",
      "Trained batch 2171 batch loss 0.697949827 batch mAP 0.413116455 batch PCKh 0.8125\n",
      "Trained batch 2172 batch loss 0.616365671 batch mAP 0.411773682 batch PCKh 0.625\n",
      "Trained batch 2173 batch loss 0.581782639 batch mAP 0.403442383 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2174 batch loss 0.702402651 batch mAP 0.385314941 batch PCKh 0.3125\n",
      "Trained batch 2175 batch loss 0.704650402 batch mAP 0.440002441 batch PCKh 0.3125\n",
      "Trained batch 2176 batch loss 0.758336544 batch mAP 0.406921387 batch PCKh 0\n",
      "Trained batch 2177 batch loss 0.604121 batch mAP 0.411804199 batch PCKh 0.5\n",
      "Trained batch 2178 batch loss 0.634459615 batch mAP 0.390441895 batch PCKh 0.75\n",
      "Trained batch 2179 batch loss 0.586397648 batch mAP 0.422424316 batch PCKh 0.5\n",
      "Trained batch 2180 batch loss 0.576981843 batch mAP 0.396209717 batch PCKh 0.4375\n",
      "Trained batch 2181 batch loss 0.563165307 batch mAP 0.409332275 batch PCKh 0.5\n",
      "Trained batch 2182 batch loss 0.63003242 batch mAP 0.433044434 batch PCKh 0\n",
      "Trained batch 2183 batch loss 0.629255414 batch mAP 0.442108154 batch PCKh 0.75\n",
      "Trained batch 2184 batch loss 0.615232766 batch mAP 0.448364258 batch PCKh 0.1875\n",
      "Trained batch 2185 batch loss 0.668010414 batch mAP 0.431884766 batch PCKh 0.3125\n",
      "Trained batch 2186 batch loss 0.716796279 batch mAP 0.402771 batch PCKh 0.125\n",
      "Trained batch 2187 batch loss 0.645980299 batch mAP 0.447601318 batch PCKh 0.3125\n",
      "Trained batch 2188 batch loss 0.608840644 batch mAP 0.424072266 batch PCKh 0\n",
      "Trained batch 2189 batch loss 0.610986173 batch mAP 0.460693359 batch PCKh 0.5625\n",
      "Trained batch 2190 batch loss 0.634509385 batch mAP 0.486450195 batch PCKh 0.875\n",
      "Trained batch 2191 batch loss 0.662585616 batch mAP 0.452636719 batch PCKh 0.3125\n",
      "Trained batch 2192 batch loss 0.667159319 batch mAP 0.467865 batch PCKh 0.5625\n",
      "Trained batch 2193 batch loss 0.646307349 batch mAP 0.422210693 batch PCKh 0.0625\n",
      "Trained batch 2194 batch loss 0.657410443 batch mAP 0.376281738 batch PCKh 0.5\n",
      "Trained batch 2195 batch loss 0.55655694 batch mAP 0.376434326 batch PCKh 0.75\n",
      "Trained batch 2196 batch loss 0.631140947 batch mAP 0.444366455 batch PCKh 0.625\n",
      "Trained batch 2197 batch loss 0.620380104 batch mAP 0.402374268 batch PCKh 0.375\n",
      "Trained batch 2198 batch loss 0.648115635 batch mAP 0.388519287 batch PCKh 0.4375\n",
      "Trained batch 2199 batch loss 0.692859292 batch mAP 0.381866455 batch PCKh 0.6875\n",
      "Trained batch 2200 batch loss 0.691230416 batch mAP 0.33682251 batch PCKh 0\n",
      "Trained batch 2201 batch loss 0.593429863 batch mAP 0.425415039 batch PCKh 0.375\n",
      "Trained batch 2202 batch loss 0.620466 batch mAP 0.389251709 batch PCKh 0.1875\n",
      "Trained batch 2203 batch loss 0.641596794 batch mAP 0.387420654 batch PCKh 0.3125\n",
      "Trained batch 2204 batch loss 0.667031944 batch mAP 0.376281738 batch PCKh 0.5625\n",
      "Trained batch 2205 batch loss 0.655514956 batch mAP 0.40045166 batch PCKh 0.25\n",
      "Trained batch 2206 batch loss 0.646457314 batch mAP 0.414306641 batch PCKh 0.6875\n",
      "Trained batch 2207 batch loss 0.571108341 batch mAP 0.368011475 batch PCKh 0.75\n",
      "Trained batch 2208 batch loss 0.55566442 batch mAP 0.406768799 batch PCKh 0.5625\n",
      "Trained batch 2209 batch loss 0.653475463 batch mAP 0.420715332 batch PCKh 0\n",
      "Trained batch 2210 batch loss 0.665832877 batch mAP 0.40411377 batch PCKh 0.125\n",
      "Trained batch 2211 batch loss 0.761188149 batch mAP 0.419555664 batch PCKh 0\n",
      "Trained batch 2212 batch loss 0.626744211 batch mAP 0.447265625 batch PCKh 0.5\n",
      "Trained batch 2213 batch loss 0.681397438 batch mAP 0.419708252 batch PCKh 0.625\n",
      "Trained batch 2214 batch loss 0.653981805 batch mAP 0.43560791 batch PCKh 0.125\n",
      "Trained batch 2215 batch loss 0.580490053 batch mAP 0.406066895 batch PCKh 0.125\n",
      "Trained batch 2216 batch loss 0.659154356 batch mAP 0.38873291 batch PCKh 0.1875\n",
      "Trained batch 2217 batch loss 0.668988407 batch mAP 0.403564453 batch PCKh 0\n",
      "Trained batch 2218 batch loss 0.63297534 batch mAP 0.429351807 batch PCKh 0\n",
      "Trained batch 2219 batch loss 0.746627569 batch mAP 0.362854 batch PCKh 0\n",
      "Trained batch 2220 batch loss 0.659098506 batch mAP 0.38961792 batch PCKh 0.75\n",
      "Trained batch 2221 batch loss 0.630160093 batch mAP 0.177764893 batch PCKh 0.5625\n",
      "Trained batch 2222 batch loss 0.680936694 batch mAP 0.336730957 batch PCKh 0\n",
      "Trained batch 2223 batch loss 0.596103847 batch mAP 0.318573 batch PCKh 0\n",
      "Trained batch 2224 batch loss 0.618734539 batch mAP 0.174804688 batch PCKh 0.625\n",
      "Trained batch 2225 batch loss 0.588442683 batch mAP 0.20111084 batch PCKh 0.8125\n",
      "Trained batch 2226 batch loss 0.559136808 batch mAP 0.243865967 batch PCKh 0.6875\n",
      "Trained batch 2227 batch loss 0.630144596 batch mAP 0.278289795 batch PCKh 0.75\n",
      "Trained batch 2228 batch loss 0.586529911 batch mAP 0.40826416 batch PCKh 0.4375\n",
      "Trained batch 2229 batch loss 0.575240254 batch mAP 0.437042236 batch PCKh 0.5625\n",
      "Trained batch 2230 batch loss 0.63385129 batch mAP 0.419067383 batch PCKh 0.8125\n",
      "Trained batch 2231 batch loss 0.62381649 batch mAP 0.416931152 batch PCKh 0.6875\n",
      "Trained batch 2232 batch loss 0.657519698 batch mAP 0.452819824 batch PCKh 0.3125\n",
      "Trained batch 2233 batch loss 0.638687968 batch mAP 0.47833252 batch PCKh 0.6875\n",
      "Trained batch 2234 batch loss 0.696825266 batch mAP 0.451416016 batch PCKh 0.375\n",
      "Trained batch 2235 batch loss 0.664070606 batch mAP 0.488189697 batch PCKh 0.1875\n",
      "Trained batch 2236 batch loss 0.590885937 batch mAP 0.499847412 batch PCKh 0.25\n",
      "Trained batch 2237 batch loss 0.560445964 batch mAP 0.444488525 batch PCKh 0.75\n",
      "Trained batch 2238 batch loss 0.620755792 batch mAP 0.464233398 batch PCKh 0.5\n",
      "Trained batch 2239 batch loss 0.553940237 batch mAP 0.394714355 batch PCKh 0.1875\n",
      "Trained batch 2240 batch loss 0.583195 batch mAP 0.498840332 batch PCKh 0.6875\n",
      "Trained batch 2241 batch loss 0.646720529 batch mAP 0.496368408 batch PCKh 0.75\n",
      "Trained batch 2242 batch loss 0.719015 batch mAP 0.444824219 batch PCKh 0.3125\n",
      "Trained batch 2243 batch loss 0.765175939 batch mAP 0.321105957 batch PCKh 0.3125\n",
      "Trained batch 2244 batch loss 0.724797308 batch mAP 0.448791504 batch PCKh 0.5625\n",
      "Trained batch 2245 batch loss 0.642930746 batch mAP 0.459411621 batch PCKh 0.6875\n",
      "Trained batch 2246 batch loss 0.625602543 batch mAP 0.436828613 batch PCKh 0.875\n",
      "Trained batch 2247 batch loss 0.644298553 batch mAP 0.452728271 batch PCKh 0.1875\n",
      "Trained batch 2248 batch loss 0.652514815 batch mAP 0.390930176 batch PCKh 0.375\n",
      "Trained batch 2249 batch loss 0.551810086 batch mAP 0.432647705 batch PCKh 0.4375\n",
      "Trained batch 2250 batch loss 0.555931568 batch mAP 0.401519775 batch PCKh 0.1875\n",
      "Trained batch 2251 batch loss 0.523301244 batch mAP 0.214630127 batch PCKh 0.75\n",
      "Trained batch 2252 batch loss 0.524997294 batch mAP 0.277771 batch PCKh 0.625\n",
      "Trained batch 2253 batch loss 0.477921188 batch mAP 0.208435059 batch PCKh 0.5\n",
      "Trained batch 2254 batch loss 0.583945096 batch mAP 0.330657959 batch PCKh 0.6875\n",
      "Trained batch 2255 batch loss 0.566188753 batch mAP 0.405059814 batch PCKh 0.3125\n",
      "Trained batch 2256 batch loss 0.538113177 batch mAP 0.440979 batch PCKh 0.1875\n",
      "Trained batch 2257 batch loss 0.609626472 batch mAP 0.365020752 batch PCKh 0.1875\n",
      "Trained batch 2258 batch loss 0.629257798 batch mAP 0.213500977 batch PCKh 0.75\n",
      "Trained batch 2259 batch loss 0.689570546 batch mAP 0.236175537 batch PCKh 0.6875\n",
      "Trained batch 2260 batch loss 0.713063955 batch mAP 0.341796875 batch PCKh 0.1875\n",
      "Trained batch 2261 batch loss 0.718925059 batch mAP 0.395385742 batch PCKh 0.25\n",
      "Trained batch 2262 batch loss 0.641290903 batch mAP 0.476928711 batch PCKh 0.3125\n",
      "Trained batch 2263 batch loss 0.656438887 batch mAP 0.477935791 batch PCKh 0.75\n",
      "Trained batch 2264 batch loss 0.631874323 batch mAP 0.52645874 batch PCKh 0.8125\n",
      "Trained batch 2265 batch loss 0.69301331 batch mAP 0.458740234 batch PCKh 0.4375\n",
      "Trained batch 2266 batch loss 0.667430401 batch mAP 0.424591064 batch PCKh 0.5625\n",
      "Trained batch 2267 batch loss 0.710409462 batch mAP 0.382049561 batch PCKh 0.4375\n",
      "Trained batch 2268 batch loss 0.662383258 batch mAP 0.30090332 batch PCKh 0.3125\n",
      "Trained batch 2269 batch loss 0.655544162 batch mAP 0.245361328 batch PCKh 0.4375\n",
      "Trained batch 2270 batch loss 0.683320284 batch mAP 0.239715576 batch PCKh 0.1875\n",
      "Trained batch 2271 batch loss 0.629066229 batch mAP 0.239532471 batch PCKh 0.5625\n",
      "Trained batch 2272 batch loss 0.727078736 batch mAP 0.27053833 batch PCKh 0\n",
      "Trained batch 2273 batch loss 0.727565289 batch mAP 0.337768555 batch PCKh 0.25\n",
      "Trained batch 2274 batch loss 0.715379953 batch mAP 0.379425049 batch PCKh 0.1875\n",
      "Trained batch 2275 batch loss 0.727152407 batch mAP 0.414794922 batch PCKh 0.1875\n",
      "Trained batch 2276 batch loss 0.686294436 batch mAP 0.394592285 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2277 batch loss 0.556163549 batch mAP 0.399383545 batch PCKh 0.4375\n",
      "Trained batch 2278 batch loss 0.623107374 batch mAP 0.416503906 batch PCKh 0.5\n",
      "Trained batch 2279 batch loss 0.67737323 batch mAP 0.358795166 batch PCKh 0.375\n",
      "Trained batch 2280 batch loss 0.712564468 batch mAP 0.383453369 batch PCKh 0.625\n",
      "Trained batch 2281 batch loss 0.672905326 batch mAP 0.414215088 batch PCKh 0.4375\n",
      "Trained batch 2282 batch loss 0.684788704 batch mAP 0.41583252 batch PCKh 0.3125\n",
      "Trained batch 2283 batch loss 0.631423771 batch mAP 0.376312256 batch PCKh 0.25\n",
      "Trained batch 2284 batch loss 0.7130481 batch mAP 0.381591797 batch PCKh 0.125\n",
      "Trained batch 2285 batch loss 0.673486173 batch mAP 0.392913818 batch PCKh 0.0625\n",
      "Trained batch 2286 batch loss 0.631151915 batch mAP 0.342193604 batch PCKh 0.625\n",
      "Trained batch 2287 batch loss 0.705468953 batch mAP 0.408996582 batch PCKh 0.1875\n",
      "Trained batch 2288 batch loss 0.645373225 batch mAP 0.378509521 batch PCKh 0.375\n",
      "Trained batch 2289 batch loss 0.635701776 batch mAP 0.396911621 batch PCKh 0.5\n",
      "Trained batch 2290 batch loss 0.653292656 batch mAP 0.364715576 batch PCKh 0.5625\n",
      "Trained batch 2291 batch loss 0.697612643 batch mAP 0.357727051 batch PCKh 0.8125\n",
      "Trained batch 2292 batch loss 0.615601718 batch mAP 0.382873535 batch PCKh 0.8125\n",
      "Trained batch 2293 batch loss 0.591855407 batch mAP 0.398010254 batch PCKh 0.125\n",
      "Trained batch 2294 batch loss 0.759789705 batch mAP 0.346160889 batch PCKh 0\n",
      "Trained batch 2295 batch loss 0.683900237 batch mAP 0.39453125 batch PCKh 0.125\n",
      "Trained batch 2296 batch loss 0.696056247 batch mAP 0.427093506 batch PCKh 0.1875\n",
      "Trained batch 2297 batch loss 0.656544089 batch mAP 0.397155762 batch PCKh 0.5625\n",
      "Trained batch 2298 batch loss 0.602496505 batch mAP 0.4296875 batch PCKh 0.125\n",
      "Trained batch 2299 batch loss 0.603095114 batch mAP 0.443389893 batch PCKh 0.1875\n",
      "Trained batch 2300 batch loss 0.608854651 batch mAP 0.427154541 batch PCKh 0.25\n",
      "Trained batch 2301 batch loss 0.706716716 batch mAP 0.420562744 batch PCKh 0.1875\n",
      "Trained batch 2302 batch loss 0.622528195 batch mAP 0.466918945 batch PCKh 0.5625\n",
      "Trained batch 2303 batch loss 0.653339744 batch mAP 0.463806152 batch PCKh 0.5625\n",
      "Trained batch 2304 batch loss 0.631857634 batch mAP 0.405731201 batch PCKh 0.5625\n",
      "Trained batch 2305 batch loss 0.633588 batch mAP 0.429840088 batch PCKh 0.5\n",
      "Trained batch 2306 batch loss 0.555689156 batch mAP 0.445648193 batch PCKh 0.125\n",
      "Trained batch 2307 batch loss 0.57342118 batch mAP 0.407806396 batch PCKh 0.125\n",
      "Trained batch 2308 batch loss 0.54808104 batch mAP 0.467590332 batch PCKh 0.375\n",
      "Trained batch 2309 batch loss 0.593811691 batch mAP 0.465637207 batch PCKh 0.5\n",
      "Trained batch 2310 batch loss 0.639796734 batch mAP 0.476959229 batch PCKh 0.75\n",
      "Trained batch 2311 batch loss 0.658284426 batch mAP 0.484039307 batch PCKh 0.6875\n",
      "Trained batch 2312 batch loss 0.554752946 batch mAP 0.486236572 batch PCKh 0.5\n",
      "Trained batch 2313 batch loss 0.65190649 batch mAP 0.492156982 batch PCKh 0.5\n",
      "Trained batch 2314 batch loss 0.613543808 batch mAP 0.492767334 batch PCKh 0\n",
      "Trained batch 2315 batch loss 0.705351412 batch mAP 0.478637695 batch PCKh 0.5625\n",
      "Trained batch 2316 batch loss 0.656784773 batch mAP 0.491912842 batch PCKh 0.4375\n",
      "Trained batch 2317 batch loss 0.602034867 batch mAP 0.479370117 batch PCKh 0.625\n",
      "Trained batch 2318 batch loss 0.655579746 batch mAP 0.451049805 batch PCKh 0.125\n",
      "Trained batch 2319 batch loss 0.578532755 batch mAP 0.381317139 batch PCKh 0.625\n",
      "Trained batch 2320 batch loss 0.661743402 batch mAP 0.383911133 batch PCKh 0.3125\n",
      "Trained batch 2321 batch loss 0.603669822 batch mAP 0.396148682 batch PCKh 0.6875\n",
      "Trained batch 2322 batch loss 0.732525051 batch mAP 0.396911621 batch PCKh 0.25\n",
      "Trained batch 2323 batch loss 0.654163 batch mAP 0.402191162 batch PCKh 0.1875\n",
      "Trained batch 2324 batch loss 0.688568473 batch mAP 0.437103271 batch PCKh 0.5625\n",
      "Trained batch 2325 batch loss 0.663286567 batch mAP 0.411804199 batch PCKh 0.75\n",
      "Trained batch 2326 batch loss 0.646199167 batch mAP 0.448791504 batch PCKh 0.5\n",
      "Trained batch 2327 batch loss 0.668530822 batch mAP 0.463897705 batch PCKh 0.1875\n",
      "Trained batch 2328 batch loss 0.612060428 batch mAP 0.465087891 batch PCKh 0.5\n",
      "Trained batch 2329 batch loss 0.689214826 batch mAP 0.400726318 batch PCKh 0.625\n",
      "Trained batch 2330 batch loss 0.622849822 batch mAP 0.460968018 batch PCKh 0.4375\n",
      "Trained batch 2331 batch loss 0.632374942 batch mAP 0.429931641 batch PCKh 0\n",
      "Trained batch 2332 batch loss 0.596608818 batch mAP 0.459198 batch PCKh 0.1875\n",
      "Trained batch 2333 batch loss 0.639248312 batch mAP 0.412567139 batch PCKh 0.3125\n",
      "Trained batch 2334 batch loss 0.565484464 batch mAP 0.407287598 batch PCKh 0.5\n",
      "Trained batch 2335 batch loss 0.55349 batch mAP 0.42678833 batch PCKh 0.4375\n",
      "Trained batch 2336 batch loss 0.64013207 batch mAP 0.444091797 batch PCKh 0.875\n",
      "Trained batch 2337 batch loss 0.648531139 batch mAP 0.465148926 batch PCKh 0.3125\n",
      "Trained batch 2338 batch loss 0.700976 batch mAP 0.415405273 batch PCKh 0.5\n",
      "Trained batch 2339 batch loss 0.613429666 batch mAP 0.429107666 batch PCKh 0.625\n",
      "Trained batch 2340 batch loss 0.606989622 batch mAP 0.46081543 batch PCKh 0.375\n",
      "Trained batch 2341 batch loss 0.633113682 batch mAP 0.439147949 batch PCKh 0.625\n",
      "Trained batch 2342 batch loss 0.593298495 batch mAP 0.391723633 batch PCKh 0.4375\n",
      "Trained batch 2343 batch loss 0.601677239 batch mAP 0.521698 batch PCKh 0.5625\n",
      "Trained batch 2344 batch loss 0.570440412 batch mAP 0.493469238 batch PCKh 0.75\n",
      "Trained batch 2345 batch loss 0.640265107 batch mAP 0.52456665 batch PCKh 0.625\n",
      "Trained batch 2346 batch loss 0.604339361 batch mAP 0.488769531 batch PCKh 0.75\n",
      "Trained batch 2347 batch loss 0.570222616 batch mAP 0.523620605 batch PCKh 0.4375\n",
      "Trained batch 2348 batch loss 0.661358416 batch mAP 0.478851318 batch PCKh 0.1875\n",
      "Trained batch 2349 batch loss 0.617615044 batch mAP 0.539032 batch PCKh 0.3125\n",
      "Trained batch 2350 batch loss 0.597652256 batch mAP 0.465271 batch PCKh 0.5\n",
      "Trained batch 2351 batch loss 0.696220577 batch mAP 0.433288574 batch PCKh 0.0625\n",
      "Trained batch 2352 batch loss 0.659170032 batch mAP 0.445831299 batch PCKh 0.3125\n",
      "Trained batch 2353 batch loss 0.686464608 batch mAP 0.429962158 batch PCKh 0.375\n",
      "Trained batch 2354 batch loss 0.639887 batch mAP 0.450927734 batch PCKh 0.6875\n",
      "Trained batch 2355 batch loss 0.683279574 batch mAP 0.414764404 batch PCKh 0.0625\n",
      "Trained batch 2356 batch loss 0.631510794 batch mAP 0.400360107 batch PCKh 0.3125\n",
      "Trained batch 2357 batch loss 0.657190621 batch mAP 0.410247803 batch PCKh 0.3125\n",
      "Trained batch 2358 batch loss 0.614242256 batch mAP 0.415100098 batch PCKh 0.5625\n",
      "Trained batch 2359 batch loss 0.635148883 batch mAP 0.446044922 batch PCKh 0.1875\n",
      "Trained batch 2360 batch loss 0.682250619 batch mAP 0.422973633 batch PCKh 0\n",
      "Trained batch 2361 batch loss 0.732461035 batch mAP 0.449310303 batch PCKh 0.125\n",
      "Trained batch 2362 batch loss 0.643398345 batch mAP 0.452819824 batch PCKh 0.6875\n",
      "Trained batch 2363 batch loss 0.672071218 batch mAP 0.430206299 batch PCKh 0.125\n",
      "Trained batch 2364 batch loss 0.761995792 batch mAP 0.412658691 batch PCKh 0.5625\n",
      "Trained batch 2365 batch loss 0.713926 batch mAP 0.389221191 batch PCKh 0.4375\n",
      "Trained batch 2366 batch loss 0.632261872 batch mAP 0.430023193 batch PCKh 0.75\n",
      "Trained batch 2367 batch loss 0.596338272 batch mAP 0.405944824 batch PCKh 0.5625\n",
      "Trained batch 2368 batch loss 0.571729064 batch mAP 0.453613281 batch PCKh 0.4375\n",
      "Trained batch 2369 batch loss 0.619795918 batch mAP 0.448638916 batch PCKh 0.4375\n",
      "Trained batch 2370 batch loss 0.53774637 batch mAP 0.432128906 batch PCKh 0.5625\n",
      "Trained batch 2371 batch loss 0.590034962 batch mAP 0.427948 batch PCKh 0.5625\n",
      "Trained batch 2372 batch loss 0.723518252 batch mAP 0.44128418 batch PCKh 0.25\n",
      "Trained batch 2373 batch loss 0.670451045 batch mAP 0.443511963 batch PCKh 0\n",
      "Trained batch 2374 batch loss 0.628880858 batch mAP 0.400909424 batch PCKh 0.25\n",
      "Trained batch 2375 batch loss 0.656254113 batch mAP 0.428710938 batch PCKh 0.3125\n",
      "Trained batch 2376 batch loss 0.699640036 batch mAP 0.415161133 batch PCKh 0\n",
      "Trained batch 2377 batch loss 0.685639083 batch mAP 0.428009033 batch PCKh 0.1875\n",
      "Trained batch 2378 batch loss 0.685418725 batch mAP 0.446533203 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2379 batch loss 0.716544271 batch mAP 0.434570312 batch PCKh 0.625\n",
      "Trained batch 2380 batch loss 0.689507604 batch mAP 0.474945068 batch PCKh 0.5\n",
      "Trained batch 2381 batch loss 0.690380335 batch mAP 0.416717529 batch PCKh 0.1875\n",
      "Trained batch 2382 batch loss 0.509687841 batch mAP 0.418670654 batch PCKh 0\n",
      "Trained batch 2383 batch loss 0.558649898 batch mAP 0.446411133 batch PCKh 0.25\n",
      "Trained batch 2384 batch loss 0.710779667 batch mAP 0.460113525 batch PCKh 0.6875\n",
      "Trained batch 2385 batch loss 0.648463845 batch mAP 0.466491699 batch PCKh 0.5625\n",
      "Trained batch 2386 batch loss 0.645618379 batch mAP 0.421020508 batch PCKh 0.625\n",
      "Trained batch 2387 batch loss 0.596346 batch mAP 0.425201416 batch PCKh 0.75\n",
      "Trained batch 2388 batch loss 0.587380767 batch mAP 0.422027588 batch PCKh 0.75\n",
      "Trained batch 2389 batch loss 0.57963562 batch mAP 0.393768311 batch PCKh 0.5625\n",
      "Trained batch 2390 batch loss 0.580016494 batch mAP 0.404602051 batch PCKh 0.5625\n",
      "Trained batch 2391 batch loss 0.637367189 batch mAP 0.37802124 batch PCKh 0.5625\n",
      "Trained batch 2392 batch loss 0.650996327 batch mAP 0.389556885 batch PCKh 0.4375\n",
      "Trained batch 2393 batch loss 0.644581199 batch mAP 0.412506104 batch PCKh 0.25\n",
      "Trained batch 2394 batch loss 0.693256319 batch mAP 0.358825684 batch PCKh 0.375\n",
      "Trained batch 2395 batch loss 0.638439834 batch mAP 0.397583 batch PCKh 0\n",
      "Trained batch 2396 batch loss 0.623137832 batch mAP 0.464233398 batch PCKh 0.1875\n",
      "Trained batch 2397 batch loss 0.671845376 batch mAP 0.423248291 batch PCKh 0.125\n",
      "Trained batch 2398 batch loss 0.642395914 batch mAP 0.455108643 batch PCKh 0.5625\n",
      "Trained batch 2399 batch loss 0.685015798 batch mAP 0.307159424 batch PCKh 0\n",
      "Trained batch 2400 batch loss 0.717167914 batch mAP 0.449737549 batch PCKh 0.0625\n",
      "Trained batch 2401 batch loss 0.678908944 batch mAP 0.340026855 batch PCKh 0.0625\n",
      "Trained batch 2402 batch loss 0.713109374 batch mAP 0.375518799 batch PCKh 0.125\n",
      "Trained batch 2403 batch loss 0.7269454 batch mAP 0.386016846 batch PCKh 0\n",
      "Trained batch 2404 batch loss 0.687203109 batch mAP 0.416381836 batch PCKh 0\n",
      "Trained batch 2405 batch loss 0.747640669 batch mAP 0.384521484 batch PCKh 0\n",
      "Trained batch 2406 batch loss 0.722276509 batch mAP 0.344726562 batch PCKh 0.1875\n",
      "Trained batch 2407 batch loss 0.694022715 batch mAP 0.346374512 batch PCKh 0\n",
      "Trained batch 2408 batch loss 0.605013847 batch mAP 0.295654297 batch PCKh 0.3125\n",
      "Trained batch 2409 batch loss 0.647516966 batch mAP 0.241973877 batch PCKh 0.1875\n",
      "Trained batch 2410 batch loss 0.575992048 batch mAP 0.116699219 batch PCKh 0.3125\n",
      "Trained batch 2411 batch loss 0.635798514 batch mAP 0.368988037 batch PCKh 0.6875\n",
      "Trained batch 2412 batch loss 0.704333484 batch mAP 0.339019775 batch PCKh 0\n",
      "Trained batch 2413 batch loss 0.649914622 batch mAP 0.335357666 batch PCKh 0.5\n",
      "Trained batch 2414 batch loss 0.755969107 batch mAP 0.369873047 batch PCKh 0\n",
      "Trained batch 2415 batch loss 0.604070485 batch mAP 0.354858398 batch PCKh 0.1875\n",
      "Trained batch 2416 batch loss 0.600257576 batch mAP 0.245300293 batch PCKh 0.3125\n",
      "Trained batch 2417 batch loss 0.635236323 batch mAP 0.361450195 batch PCKh 0.25\n",
      "Trained batch 2418 batch loss 0.671885133 batch mAP 0.459777832 batch PCKh 0.3125\n",
      "Trained batch 2419 batch loss 0.604772568 batch mAP 0.366027832 batch PCKh 0.5\n",
      "Trained batch 2420 batch loss 0.596213341 batch mAP 0.43170166 batch PCKh 0.625\n",
      "Trained batch 2421 batch loss 0.601039 batch mAP 0.461517334 batch PCKh 0.6875\n",
      "Trained batch 2422 batch loss 0.580309272 batch mAP 0.454620361 batch PCKh 0.375\n",
      "Trained batch 2423 batch loss 0.592527211 batch mAP 0.466125488 batch PCKh 0.375\n",
      "Trained batch 2424 batch loss 0.590278924 batch mAP 0.446929932 batch PCKh 0.375\n",
      "Trained batch 2425 batch loss 0.632717371 batch mAP 0.451324463 batch PCKh 0.375\n",
      "Trained batch 2426 batch loss 0.605050683 batch mAP 0.474975586 batch PCKh 0.0625\n",
      "Trained batch 2427 batch loss 0.628293395 batch mAP 0.461425781 batch PCKh 0.1875\n",
      "Trained batch 2428 batch loss 0.626097441 batch mAP 0.475616455 batch PCKh 0.25\n",
      "Trained batch 2429 batch loss 0.630886793 batch mAP 0.452789307 batch PCKh 0.6875\n",
      "Trained batch 2430 batch loss 0.646415412 batch mAP 0.292755127 batch PCKh 0.5\n",
      "Trained batch 2431 batch loss 0.60194248 batch mAP 0.404968262 batch PCKh 0.4375\n",
      "Trained batch 2432 batch loss 0.610791504 batch mAP 0.378479 batch PCKh 0.5\n",
      "Trained batch 2433 batch loss 0.628222287 batch mAP 0.442626953 batch PCKh 0.625\n",
      "Trained batch 2434 batch loss 0.679259598 batch mAP 0.378265381 batch PCKh 0.1875\n",
      "Trained batch 2435 batch loss 0.641795754 batch mAP 0.349304199 batch PCKh 0.125\n",
      "Trained batch 2436 batch loss 0.621793747 batch mAP 0.373565674 batch PCKh 0.3125\n",
      "Trained batch 2437 batch loss 0.618960738 batch mAP 0.430633545 batch PCKh 0.0625\n",
      "Trained batch 2438 batch loss 0.526633263 batch mAP 0.526153564 batch PCKh 0.25\n",
      "Trained batch 2439 batch loss 0.601742 batch mAP 0.456970215 batch PCKh 0.1875\n",
      "Trained batch 2440 batch loss 0.677147925 batch mAP 0.442596436 batch PCKh 0.0625\n",
      "Trained batch 2441 batch loss 0.568873346 batch mAP 0.575073242 batch PCKh 0.25\n",
      "Trained batch 2442 batch loss 0.706017315 batch mAP 0.524108887 batch PCKh 0.25\n",
      "Trained batch 2443 batch loss 0.644266963 batch mAP 0.492034912 batch PCKh 0.1875\n",
      "Trained batch 2444 batch loss 0.692305803 batch mAP 0.450714111 batch PCKh 0.75\n",
      "Trained batch 2445 batch loss 0.655573785 batch mAP 0.485290527 batch PCKh 0.8125\n",
      "Trained batch 2446 batch loss 0.613786697 batch mAP 0.500305176 batch PCKh 0.6875\n",
      "Trained batch 2447 batch loss 0.638411164 batch mAP 0.437072754 batch PCKh 0.4375\n",
      "Trained batch 2448 batch loss 0.590894103 batch mAP 0.470855713 batch PCKh 0.4375\n",
      "Trained batch 2449 batch loss 0.659900784 batch mAP 0.448791504 batch PCKh 0.5625\n",
      "Trained batch 2450 batch loss 0.696365058 batch mAP 0.464630127 batch PCKh 0.1875\n",
      "Trained batch 2451 batch loss 0.712037921 batch mAP 0.434967041 batch PCKh 0\n",
      "Trained batch 2452 batch loss 0.61967051 batch mAP 0.386199951 batch PCKh 0.1875\n",
      "Trained batch 2453 batch loss 0.513506234 batch mAP 0.218109131 batch PCKh 0.1875\n",
      "Trained batch 2454 batch loss 0.703276515 batch mAP 0.367950439 batch PCKh 0\n",
      "Trained batch 2455 batch loss 0.635071278 batch mAP 0.350769043 batch PCKh 0.1875\n",
      "Trained batch 2456 batch loss 0.670806229 batch mAP 0.313812256 batch PCKh 0.3125\n",
      "Trained batch 2457 batch loss 0.655017257 batch mAP 0.25 batch PCKh 0.5\n",
      "Trained batch 2458 batch loss 0.534103394 batch mAP 0.342132568 batch PCKh 0.5\n",
      "Trained batch 2459 batch loss 0.656444311 batch mAP 0.382476807 batch PCKh 0.6875\n",
      "Trained batch 2460 batch loss 0.594404161 batch mAP 0.42175293 batch PCKh 0.5625\n",
      "Trained batch 2461 batch loss 0.702081084 batch mAP 0.470977783 batch PCKh 0.125\n",
      "Trained batch 2462 batch loss 0.754597962 batch mAP 0.481384277 batch PCKh 0.1875\n",
      "Trained batch 2463 batch loss 0.659885168 batch mAP 0.463623047 batch PCKh 0.5625\n",
      "Trained batch 2464 batch loss 0.636599422 batch mAP 0.464599609 batch PCKh 0.125\n",
      "Trained batch 2465 batch loss 0.625366449 batch mAP 0.475219727 batch PCKh 0.25\n",
      "Trained batch 2466 batch loss 0.671938896 batch mAP 0.417022705 batch PCKh 0.125\n",
      "Trained batch 2467 batch loss 0.653636932 batch mAP 0.372650146 batch PCKh 0\n",
      "Trained batch 2468 batch loss 0.617771685 batch mAP 0.402862549 batch PCKh 0.4375\n",
      "Trained batch 2469 batch loss 0.60635823 batch mAP 0.443115234 batch PCKh 0.1875\n",
      "Trained batch 2470 batch loss 0.593316615 batch mAP 0.454833984 batch PCKh 0.1875\n",
      "Trained batch 2471 batch loss 0.687887907 batch mAP 0.452056885 batch PCKh 0.125\n",
      "Trained batch 2472 batch loss 0.656605124 batch mAP 0.507202148 batch PCKh 0.75\n",
      "Trained batch 2473 batch loss 0.658957 batch mAP 0.491271973 batch PCKh 0.375\n",
      "Trained batch 2474 batch loss 0.612061381 batch mAP 0.47467041 batch PCKh 0.875\n",
      "Trained batch 2475 batch loss 0.645456076 batch mAP 0.498809814 batch PCKh 0.75\n",
      "Trained batch 2476 batch loss 0.701717675 batch mAP 0.400970459 batch PCKh 0.125\n",
      "Trained batch 2477 batch loss 0.675285339 batch mAP 0.466644287 batch PCKh 0.75\n",
      "Trained batch 2478 batch loss 0.568410158 batch mAP 0.466461182 batch PCKh 0.75\n",
      "Trained batch 2479 batch loss 0.629584 batch mAP 0.47668457 batch PCKh 0.4375\n",
      "Trained batch 2480 batch loss 0.684896 batch mAP 0.442749023 batch PCKh 0.625\n",
      "Trained batch 2481 batch loss 0.580360413 batch mAP 0.489501953 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2482 batch loss 0.573235631 batch mAP 0.494171143 batch PCKh 0.75\n",
      "Trained batch 2483 batch loss 0.591132164 batch mAP 0.539611816 batch PCKh 0.375\n",
      "Trained batch 2484 batch loss 0.547013521 batch mAP 0.51965332 batch PCKh 0.6875\n",
      "Trained batch 2485 batch loss 0.585570097 batch mAP 0.520202637 batch PCKh 0.6875\n",
      "Trained batch 2486 batch loss 0.59668225 batch mAP 0.514831543 batch PCKh 0.875\n",
      "Trained batch 2487 batch loss 0.594766736 batch mAP 0.502197266 batch PCKh 0.6875\n",
      "Trained batch 2488 batch loss 0.562418103 batch mAP 0.531494141 batch PCKh 0.5625\n",
      "Trained batch 2489 batch loss 0.561486 batch mAP 0.464447021 batch PCKh 0.1875\n",
      "Trained batch 2490 batch loss 0.554472387 batch mAP 0.520050049 batch PCKh 0.3125\n",
      "Trained batch 2491 batch loss 0.633509 batch mAP 0.482025146 batch PCKh 0.4375\n",
      "Trained batch 2492 batch loss 0.581554234 batch mAP 0.511688232 batch PCKh 0.4375\n",
      "Trained batch 2493 batch loss 0.566668153 batch mAP 0.560150146 batch PCKh 0.0625\n",
      "Trained batch 2494 batch loss 0.565175891 batch mAP 0.520751953 batch PCKh 0.5\n",
      "Trained batch 2495 batch loss 0.718378067 batch mAP 0.50567627 batch PCKh 0.1875\n",
      "Trained batch 2496 batch loss 0.573104501 batch mAP 0.502746582 batch PCKh 0.625\n",
      "Trained batch 2497 batch loss 0.5454368 batch mAP 0.534240723 batch PCKh 0.375\n",
      "Trained batch 2498 batch loss 0.560999811 batch mAP 0.502563477 batch PCKh 0.3125\n",
      "Trained batch 2499 batch loss 0.74514997 batch mAP 0.393676758 batch PCKh 0\n",
      "Trained batch 2500 batch loss 0.731056154 batch mAP 0.266601562 batch PCKh 0\n",
      "Trained batch 2501 batch loss 0.675932705 batch mAP 0.429473877 batch PCKh 0.1875\n",
      "Trained batch 2502 batch loss 0.668365 batch mAP 0.441040039 batch PCKh 0.375\n",
      "Trained batch 2503 batch loss 0.583687246 batch mAP 0.456237793 batch PCKh 0.5\n",
      "Trained batch 2504 batch loss 0.589298129 batch mAP 0.466094971 batch PCKh 0\n",
      "Trained batch 2505 batch loss 0.685447097 batch mAP 0.447601318 batch PCKh 0.25\n",
      "Trained batch 2506 batch loss 0.679439723 batch mAP 0.414581299 batch PCKh 0.4375\n",
      "Trained batch 2507 batch loss 0.63228482 batch mAP 0.33114624 batch PCKh 0\n",
      "Trained batch 2508 batch loss 0.642842591 batch mAP 0.310058594 batch PCKh 0.4375\n",
      "Trained batch 2509 batch loss 0.653585434 batch mAP 0.301849365 batch PCKh 0.4375\n",
      "Trained batch 2510 batch loss 0.559444 batch mAP 0.168182373 batch PCKh 0.125\n",
      "Trained batch 2511 batch loss 0.652525187 batch mAP 0.0827331543 batch PCKh 0.3125\n",
      "Trained batch 2512 batch loss 0.651447535 batch mAP 0.244720459 batch PCKh 0.4375\n",
      "Trained batch 2513 batch loss 0.683069229 batch mAP 0.265808105 batch PCKh 0\n",
      "Trained batch 2514 batch loss 0.745070457 batch mAP 0.350280762 batch PCKh 0.375\n",
      "Trained batch 2515 batch loss 0.680510402 batch mAP 0.33972168 batch PCKh 0.5625\n",
      "Trained batch 2516 batch loss 0.622067 batch mAP 0.30267334 batch PCKh 0.3125\n",
      "Trained batch 2517 batch loss 0.599459112 batch mAP 0.339019775 batch PCKh 0.375\n",
      "Trained batch 2518 batch loss 0.666859865 batch mAP 0.394805908 batch PCKh 0.5625\n",
      "Trained batch 2519 batch loss 0.665896058 batch mAP 0.442810059 batch PCKh 0.1875\n",
      "Trained batch 2520 batch loss 0.674013913 batch mAP 0.371002197 batch PCKh 0.1875\n",
      "Trained batch 2521 batch loss 0.693447709 batch mAP 0.347320557 batch PCKh 0.375\n",
      "Trained batch 2522 batch loss 0.644654393 batch mAP 0.356506348 batch PCKh 0.875\n",
      "Trained batch 2523 batch loss 0.657104611 batch mAP 0.404449463 batch PCKh 0.6875\n",
      "Trained batch 2524 batch loss 0.647122502 batch mAP 0.43170166 batch PCKh 0.625\n",
      "Trained batch 2525 batch loss 0.777605832 batch mAP 0.310638428 batch PCKh 0.3125\n",
      "Trained batch 2526 batch loss 0.758368909 batch mAP 0.315155029 batch PCKh 0\n",
      "Trained batch 2527 batch loss 0.791259408 batch mAP 0.265716553 batch PCKh 0.0625\n",
      "Trained batch 2528 batch loss 0.810844481 batch mAP 0.325134277 batch PCKh 0.0625\n",
      "Trained batch 2529 batch loss 0.619963169 batch mAP 0.438751221 batch PCKh 0.5\n",
      "Trained batch 2530 batch loss 0.638142884 batch mAP 0.4034729 batch PCKh 0.75\n",
      "Trained batch 2531 batch loss 0.663375735 batch mAP 0.411132812 batch PCKh 0.875\n",
      "Trained batch 2532 batch loss 0.638246357 batch mAP 0.407043457 batch PCKh 0.375\n",
      "Trained batch 2533 batch loss 0.61003077 batch mAP 0.404205322 batch PCKh 0.75\n",
      "Trained batch 2534 batch loss 0.512296 batch mAP 0.423156738 batch PCKh 0.125\n",
      "Trained batch 2535 batch loss 0.544221699 batch mAP 0.37008667 batch PCKh 0.1875\n",
      "Trained batch 2536 batch loss 0.475958318 batch mAP 0.387451172 batch PCKh 0\n",
      "Trained batch 2537 batch loss 0.590835869 batch mAP 0.392272949 batch PCKh 0.5\n",
      "Trained batch 2538 batch loss 0.641814828 batch mAP 0.38369751 batch PCKh 0.5625\n",
      "Trained batch 2539 batch loss 0.626720369 batch mAP 0.425018311 batch PCKh 0.6875\n",
      "Trained batch 2540 batch loss 0.558576107 batch mAP 0.39352417 batch PCKh 0.0625\n",
      "Trained batch 2541 batch loss 0.476586759 batch mAP 0.427063 batch PCKh 0.125\n",
      "Trained batch 2542 batch loss 0.531832576 batch mAP 0.418579102 batch PCKh 0.125\n",
      "Trained batch 2543 batch loss 0.506496 batch mAP 0.424316406 batch PCKh 0.25\n",
      "Trained batch 2544 batch loss 0.522752404 batch mAP 0.422088623 batch PCKh 0\n",
      "Trained batch 2545 batch loss 0.495593429 batch mAP 0.326934814 batch PCKh 0.5\n",
      "Trained batch 2546 batch loss 0.519768119 batch mAP 0.358184814 batch PCKh 0.625\n",
      "Trained batch 2547 batch loss 0.536994934 batch mAP 0.330871582 batch PCKh 0.5\n",
      "Trained batch 2548 batch loss 0.658444345 batch mAP 0.321594238 batch PCKh 0.375\n",
      "Trained batch 2549 batch loss 0.607671618 batch mAP 0.364563 batch PCKh 0.75\n",
      "Trained batch 2550 batch loss 0.638946414 batch mAP 0.335205078 batch PCKh 0.6875\n",
      "Trained batch 2551 batch loss 0.605350852 batch mAP 0.406555176 batch PCKh 0.375\n",
      "Trained batch 2552 batch loss 0.653903604 batch mAP 0.411254883 batch PCKh 0.1875\n",
      "Trained batch 2553 batch loss 0.751103222 batch mAP 0.40335083 batch PCKh 0\n",
      "Trained batch 2554 batch loss 0.682028413 batch mAP 0.433258057 batch PCKh 0.0625\n",
      "Trained batch 2555 batch loss 0.729335368 batch mAP 0.418884277 batch PCKh 0.3125\n",
      "Trained batch 2556 batch loss 0.684100747 batch mAP 0.404449463 batch PCKh 0.25\n",
      "Trained batch 2557 batch loss 0.688977718 batch mAP 0.372436523 batch PCKh 0.4375\n",
      "Trained batch 2558 batch loss 0.716602683 batch mAP 0.350524902 batch PCKh 0.3125\n",
      "Trained batch 2559 batch loss 0.724911094 batch mAP 0.319793701 batch PCKh 0.125\n",
      "Trained batch 2560 batch loss 0.644543 batch mAP 0.129638672 batch PCKh 0.1875\n",
      "Trained batch 2561 batch loss 0.676304 batch mAP 0.208129883 batch PCKh 0.625\n",
      "Trained batch 2562 batch loss 0.706486 batch mAP 0.244232178 batch PCKh 0.0625\n",
      "Trained batch 2563 batch loss 0.620593369 batch mAP 0.26348877 batch PCKh 0.125\n",
      "Trained batch 2564 batch loss 0.590985119 batch mAP 0.209747314 batch PCKh 0.25\n",
      "Trained batch 2565 batch loss 0.609734952 batch mAP 0.315612793 batch PCKh 0.5625\n",
      "Trained batch 2566 batch loss 0.553322077 batch mAP 0.354492188 batch PCKh 0.75\n",
      "Trained batch 2567 batch loss 0.562943816 batch mAP 0.337280273 batch PCKh 0.1875\n",
      "Trained batch 2568 batch loss 0.582437754 batch mAP 0.350646973 batch PCKh 0.625\n",
      "Trained batch 2569 batch loss 0.542171597 batch mAP 0.373291016 batch PCKh 0\n",
      "Trained batch 2570 batch loss 0.547287405 batch mAP 0.41305542 batch PCKh 0.5625\n",
      "Trained batch 2571 batch loss 0.754561782 batch mAP 0.384521484 batch PCKh 0\n",
      "Trained batch 2572 batch loss 0.897726178 batch mAP 0.108764648 batch PCKh 0\n",
      "Trained batch 2573 batch loss 0.752373457 batch mAP 0.205963135 batch PCKh 0.0625\n",
      "Trained batch 2574 batch loss 0.689153731 batch mAP 0.140655518 batch PCKh 0.4375\n",
      "Trained batch 2575 batch loss 0.638454318 batch mAP 0.0339050293 batch PCKh 0.5\n",
      "Trained batch 2576 batch loss 0.633664846 batch mAP 0.0184021 batch PCKh 0.375\n",
      "Trained batch 2577 batch loss 0.646034598 batch mAP 0.00900268555 batch PCKh 0.4375\n",
      "Trained batch 2578 batch loss 0.591797709 batch mAP 0.0166931152 batch PCKh 0.375\n",
      "Trained batch 2579 batch loss 0.659002542 batch mAP 0.19708252 batch PCKh 0.3125\n",
      "Trained batch 2580 batch loss 0.663475811 batch mAP 0.258575439 batch PCKh 0\n",
      "Trained batch 2581 batch loss 0.652716577 batch mAP 0.305725098 batch PCKh 0\n",
      "Trained batch 2582 batch loss 0.568688512 batch mAP 0.315582275 batch PCKh 0.5\n",
      "Trained batch 2583 batch loss 0.63563931 batch mAP 0.350036621 batch PCKh 0.375\n",
      "Trained batch 2584 batch loss 0.575866222 batch mAP 0.407684326 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2585 batch loss 0.553917527 batch mAP 0.373077393 batch PCKh 0.5\n",
      "Trained batch 2586 batch loss 0.663534045 batch mAP 0.414855957 batch PCKh 0.6875\n",
      "Trained batch 2587 batch loss 0.687280059 batch mAP 0.469207764 batch PCKh 0.125\n",
      "Trained batch 2588 batch loss 0.673649371 batch mAP 0.453155518 batch PCKh 0.5\n",
      "Trained batch 2589 batch loss 0.609722733 batch mAP 0.403869629 batch PCKh 0.4375\n",
      "Trained batch 2590 batch loss 0.669165492 batch mAP 0.437713623 batch PCKh 0\n",
      "Trained batch 2591 batch loss 0.68758297 batch mAP 0.431396484 batch PCKh 0.25\n",
      "Trained batch 2592 batch loss 0.637686133 batch mAP 0.351348877 batch PCKh 0.625\n",
      "Trained batch 2593 batch loss 0.620848894 batch mAP 0.429962158 batch PCKh 0.5625\n",
      "Trained batch 2594 batch loss 0.666088879 batch mAP 0.426605225 batch PCKh 0.6875\n",
      "Trained batch 2595 batch loss 0.713222921 batch mAP 0.371429443 batch PCKh 0\n",
      "Trained batch 2596 batch loss 0.596923411 batch mAP 0.452301025 batch PCKh 0.0625\n",
      "Trained batch 2597 batch loss 0.59819454 batch mAP 0.487548828 batch PCKh 0.25\n",
      "Trained batch 2598 batch loss 0.623391032 batch mAP 0.501434326 batch PCKh 0.5625\n",
      "Trained batch 2599 batch loss 0.604895 batch mAP 0.530426 batch PCKh 0.3125\n",
      "Trained batch 2600 batch loss 0.603308916 batch mAP 0.503082275 batch PCKh 0.0625\n",
      "Trained batch 2601 batch loss 0.621032953 batch mAP 0.494781494 batch PCKh 0.4375\n",
      "Trained batch 2602 batch loss 0.617432892 batch mAP 0.506469727 batch PCKh 0.5\n",
      "Trained batch 2603 batch loss 0.632918119 batch mAP 0.500732422 batch PCKh 0.625\n",
      "Trained batch 2604 batch loss 0.618326187 batch mAP 0.434204102 batch PCKh 0.1875\n",
      "Trained batch 2605 batch loss 0.688300252 batch mAP 0.463012695 batch PCKh 0.5\n",
      "Trained batch 2606 batch loss 0.641369462 batch mAP 0.482818604 batch PCKh 0.1875\n",
      "Trained batch 2607 batch loss 0.61255908 batch mAP 0.462341309 batch PCKh 0.375\n",
      "Trained batch 2608 batch loss 0.561585605 batch mAP 0.528167725 batch PCKh 0.4375\n",
      "Trained batch 2609 batch loss 0.63476932 batch mAP 0.501312256 batch PCKh 0.625\n",
      "Trained batch 2610 batch loss 0.689490139 batch mAP 0.454681396 batch PCKh 0.375\n",
      "Trained batch 2611 batch loss 0.656499445 batch mAP 0.462127686 batch PCKh 0.4375\n",
      "Trained batch 2612 batch loss 0.617931 batch mAP 0.443695068 batch PCKh 0.5\n",
      "Trained batch 2613 batch loss 0.679798841 batch mAP 0.396759033 batch PCKh 0.875\n",
      "Trained batch 2614 batch loss 0.602394581 batch mAP 0.391082764 batch PCKh 0.5\n",
      "Trained batch 2615 batch loss 0.602968752 batch mAP 0.367767334 batch PCKh 0.5625\n",
      "Trained batch 2616 batch loss 0.609698772 batch mAP 0.270111084 batch PCKh 0.5625\n",
      "Trained batch 2617 batch loss 0.615250349 batch mAP 0.26965332 batch PCKh 0.4375\n",
      "Trained batch 2618 batch loss 0.615154207 batch mAP 0.184295654 batch PCKh 0.625\n",
      "Trained batch 2619 batch loss 0.663960576 batch mAP 0.291564941 batch PCKh 0.375\n",
      "Trained batch 2620 batch loss 0.666549623 batch mAP 0.38949585 batch PCKh 0.75\n",
      "Trained batch 2621 batch loss 0.565120101 batch mAP 0.408447266 batch PCKh 0.6875\n",
      "Trained batch 2622 batch loss 0.67262876 batch mAP 0.426300049 batch PCKh 0.625\n",
      "Trained batch 2623 batch loss 0.713502467 batch mAP 0.400512695 batch PCKh 0.0625\n",
      "Trained batch 2624 batch loss 0.645918906 batch mAP 0.426025391 batch PCKh 0.1875\n",
      "Trained batch 2625 batch loss 0.666315198 batch mAP 0.426513672 batch PCKh 0.625\n",
      "Trained batch 2626 batch loss 0.605286121 batch mAP 0.470275879 batch PCKh 0.25\n",
      "Trained batch 2627 batch loss 0.573594391 batch mAP 0.482055664 batch PCKh 0.4375\n",
      "Trained batch 2628 batch loss 0.580695271 batch mAP 0.464386 batch PCKh 0.5625\n",
      "Trained batch 2629 batch loss 0.676682591 batch mAP 0.476470947 batch PCKh 0.1875\n",
      "Trained batch 2630 batch loss 0.646837234 batch mAP 0.506347656 batch PCKh 0.375\n",
      "Trained batch 2631 batch loss 0.682321191 batch mAP 0.475311279 batch PCKh 0.3125\n",
      "Trained batch 2632 batch loss 0.673164666 batch mAP 0.465667725 batch PCKh 0.375\n",
      "Trained batch 2633 batch loss 0.682463229 batch mAP 0.444824219 batch PCKh 0.3125\n",
      "Trained batch 2634 batch loss 0.63904053 batch mAP 0.487792969 batch PCKh 0.4375\n",
      "Trained batch 2635 batch loss 0.682568312 batch mAP 0.397033691 batch PCKh 0.5625\n",
      "Trained batch 2636 batch loss 0.622526407 batch mAP 0.430419922 batch PCKh 0.25\n",
      "Trained batch 2637 batch loss 0.672459722 batch mAP 0.348968506 batch PCKh 0.1875\n",
      "Trained batch 2638 batch loss 0.632423639 batch mAP 0.393890381 batch PCKh 0.6875\n",
      "Trained batch 2639 batch loss 0.551246047 batch mAP 0.422088623 batch PCKh 0.1875\n",
      "Trained batch 2640 batch loss 0.644355416 batch mAP 0.338134766 batch PCKh 0.375\n",
      "Trained batch 2641 batch loss 0.593800068 batch mAP 0.381286621 batch PCKh 0.6875\n",
      "Trained batch 2642 batch loss 0.582984746 batch mAP 0.471191406 batch PCKh 0.5\n",
      "Trained batch 2643 batch loss 0.668336749 batch mAP 0.471221924 batch PCKh 0.625\n",
      "Trained batch 2644 batch loss 0.662019074 batch mAP 0.462524414 batch PCKh 0.5\n",
      "Trained batch 2645 batch loss 0.563826621 batch mAP 0.52444458 batch PCKh 0.3125\n",
      "Trained batch 2646 batch loss 0.602490306 batch mAP 0.500030518 batch PCKh 0\n",
      "Trained batch 2647 batch loss 0.772555828 batch mAP 0.512146 batch PCKh 0.1875\n",
      "Trained batch 2648 batch loss 0.788350224 batch mAP 0.496154785 batch PCKh 0\n",
      "Trained batch 2649 batch loss 0.782644153 batch mAP 0.521392822 batch PCKh 0\n",
      "Trained batch 2650 batch loss 0.784567595 batch mAP 0.510803223 batch PCKh 0\n",
      "Trained batch 2651 batch loss 0.776927829 batch mAP 0.417114258 batch PCKh 0.125\n",
      "Trained batch 2652 batch loss 0.551939666 batch mAP 0.195556641 batch PCKh 0.5625\n",
      "Trained batch 2653 batch loss 0.569531143 batch mAP 0.189453125 batch PCKh 0.5\n",
      "Trained batch 2654 batch loss 0.621309638 batch mAP 0.294281 batch PCKh 0.625\n",
      "Trained batch 2655 batch loss 0.618509829 batch mAP 0.296203613 batch PCKh 0.625\n",
      "Trained batch 2656 batch loss 0.545983851 batch mAP 0.378723145 batch PCKh 0.5625\n",
      "Trained batch 2657 batch loss 0.62726897 batch mAP 0.434570312 batch PCKh 0.5625\n",
      "Trained batch 2658 batch loss 0.691387415 batch mAP 0.453887939 batch PCKh 0.5\n",
      "Trained batch 2659 batch loss 0.646789074 batch mAP 0.438995361 batch PCKh 0.375\n",
      "Trained batch 2660 batch loss 0.673667 batch mAP 0.419647217 batch PCKh 0.5625\n",
      "Trained batch 2661 batch loss 0.659495711 batch mAP 0.50402832 batch PCKh 0.125\n",
      "Trained batch 2662 batch loss 0.612591445 batch mAP 0.46182251 batch PCKh 0.125\n",
      "Trained batch 2663 batch loss 0.675347269 batch mAP 0.406921387 batch PCKh 0.875\n",
      "Trained batch 2664 batch loss 0.620862246 batch mAP 0.397399902 batch PCKh 0\n",
      "Trained batch 2665 batch loss 0.716613829 batch mAP 0.437896729 batch PCKh 0.3125\n",
      "Trained batch 2666 batch loss 0.728620172 batch mAP 0.434509277 batch PCKh 0\n",
      "Trained batch 2667 batch loss 0.675067663 batch mAP 0.424469 batch PCKh 0.125\n",
      "Trained batch 2668 batch loss 0.744633377 batch mAP 0.372558594 batch PCKh 0.4375\n",
      "Trained batch 2669 batch loss 0.685626805 batch mAP 0.403076172 batch PCKh 0.5625\n",
      "Trained batch 2670 batch loss 0.721926153 batch mAP 0.378356934 batch PCKh 0.625\n",
      "Trained batch 2671 batch loss 0.649441242 batch mAP 0.356933594 batch PCKh 0.125\n",
      "Trained batch 2672 batch loss 0.684631467 batch mAP 0.2059021 batch PCKh 0.5\n",
      "Trained batch 2673 batch loss 0.658374488 batch mAP 0.0862426758 batch PCKh 0.5\n",
      "Trained batch 2674 batch loss 0.648464203 batch mAP 0.264770508 batch PCKh 0.25\n",
      "Trained batch 2675 batch loss 0.579492867 batch mAP 0.286438 batch PCKh 0.5625\n",
      "Trained batch 2676 batch loss 0.602702379 batch mAP 0.233673096 batch PCKh 0.4375\n",
      "Trained batch 2677 batch loss 0.594899654 batch mAP 0.365386963 batch PCKh 0.0625\n",
      "Trained batch 2678 batch loss 0.68727088 batch mAP 0.419128418 batch PCKh 0.4375\n",
      "Trained batch 2679 batch loss 0.613219261 batch mAP 0.417602539 batch PCKh 0.6875\n",
      "Trained batch 2680 batch loss 0.690642834 batch mAP 0.372924805 batch PCKh 0\n",
      "Trained batch 2681 batch loss 0.615787864 batch mAP 0.421844482 batch PCKh 0.0625\n",
      "Trained batch 2682 batch loss 0.625457585 batch mAP 0.293731689 batch PCKh 0.5\n",
      "Trained batch 2683 batch loss 0.612793326 batch mAP 0.37197876 batch PCKh 0.25\n",
      "Trained batch 2684 batch loss 0.640636384 batch mAP 0.469818115 batch PCKh 0.75\n",
      "Trained batch 2685 batch loss 0.589759946 batch mAP 0.465576172 batch PCKh 0.25\n",
      "Trained batch 2686 batch loss 0.51846993 batch mAP 0.47769165 batch PCKh 0.625\n",
      "Trained batch 2687 batch loss 0.626175165 batch mAP 0.416107178 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2688 batch loss 0.632345796 batch mAP 0.427581787 batch PCKh 0.125\n",
      "Trained batch 2689 batch loss 0.622813225 batch mAP 0.408294678 batch PCKh 0.5\n",
      "Trained batch 2690 batch loss 0.660790801 batch mAP 0.423858643 batch PCKh 0.625\n",
      "Trained batch 2691 batch loss 0.622790933 batch mAP 0.381958 batch PCKh 0.125\n",
      "Trained batch 2692 batch loss 0.728034556 batch mAP 0.427215576 batch PCKh 0\n",
      "Trained batch 2693 batch loss 0.588085175 batch mAP 0.514709473 batch PCKh 0.3125\n",
      "Trained batch 2694 batch loss 0.602551043 batch mAP 0.479827881 batch PCKh 0.1875\n",
      "Trained batch 2695 batch loss 0.577787578 batch mAP 0.444824219 batch PCKh 0.375\n",
      "Trained batch 2696 batch loss 0.702150941 batch mAP 0.459686279 batch PCKh 0.625\n",
      "Trained batch 2697 batch loss 0.582464755 batch mAP 0.42755127 batch PCKh 0.5\n",
      "Trained batch 2698 batch loss 0.617142439 batch mAP 0.440734863 batch PCKh 0.125\n",
      "Trained batch 2699 batch loss 0.637771666 batch mAP 0.448272705 batch PCKh 0.1875\n",
      "Trained batch 2700 batch loss 0.60982132 batch mAP 0.445220947 batch PCKh 0.75\n",
      "Trained batch 2701 batch loss 0.679698706 batch mAP 0.43927002 batch PCKh 0.625\n",
      "Trained batch 2702 batch loss 0.744224727 batch mAP 0.421630859 batch PCKh 0.25\n",
      "Trained batch 2703 batch loss 0.667045 batch mAP 0.435333252 batch PCKh 0.25\n",
      "Trained batch 2704 batch loss 0.720952034 batch mAP 0.447021484 batch PCKh 0.75\n",
      "Trained batch 2705 batch loss 0.637859821 batch mAP 0.424438477 batch PCKh 0.125\n",
      "Trained batch 2706 batch loss 0.562135756 batch mAP 0.337677 batch PCKh 0.5625\n",
      "Trained batch 2707 batch loss 0.568567753 batch mAP 0.416107178 batch PCKh 0\n",
      "Trained batch 2708 batch loss 0.649325609 batch mAP 0.468811035 batch PCKh 0.5\n",
      "Trained batch 2709 batch loss 0.645307064 batch mAP 0.417633057 batch PCKh 0.625\n",
      "Trained batch 2710 batch loss 0.684259295 batch mAP 0.43182373 batch PCKh 0\n",
      "Trained batch 2711 batch loss 0.664787769 batch mAP 0.465118408 batch PCKh 0.0625\n",
      "Trained batch 2712 batch loss 0.604797602 batch mAP 0.428131104 batch PCKh 0.5\n",
      "Trained batch 2713 batch loss 0.640663207 batch mAP 0.395019531 batch PCKh 0.5625\n",
      "Trained batch 2714 batch loss 0.680733204 batch mAP 0.426330566 batch PCKh 0.625\n",
      "Trained batch 2715 batch loss 0.580987811 batch mAP 0.440429688 batch PCKh 0.25\n",
      "Trained batch 2716 batch loss 0.610756397 batch mAP 0.469909668 batch PCKh 0.5\n",
      "Trained batch 2717 batch loss 0.600228906 batch mAP 0.442596436 batch PCKh 0.5625\n",
      "Trained batch 2718 batch loss 0.665880442 batch mAP 0.489898682 batch PCKh 0.875\n",
      "Trained batch 2719 batch loss 0.683408082 batch mAP 0.447814941 batch PCKh 0.5\n",
      "Trained batch 2720 batch loss 0.697006464 batch mAP 0.447235107 batch PCKh 0.1875\n",
      "Trained batch 2721 batch loss 0.669488549 batch mAP 0.457550049 batch PCKh 0\n",
      "Trained batch 2722 batch loss 0.649852514 batch mAP 0.380004883 batch PCKh 0.375\n",
      "Trained batch 2723 batch loss 0.69506228 batch mAP 0.464324951 batch PCKh 0.5625\n",
      "Trained batch 2724 batch loss 0.682780921 batch mAP 0.44442749 batch PCKh 0.8125\n",
      "Trained batch 2725 batch loss 0.661399305 batch mAP 0.425842285 batch PCKh 0.6875\n",
      "Trained batch 2726 batch loss 0.676410496 batch mAP 0.422821045 batch PCKh 0.75\n",
      "Trained batch 2727 batch loss 0.692278087 batch mAP 0.417205811 batch PCKh 0.0625\n",
      "Trained batch 2728 batch loss 0.655071616 batch mAP 0.427429199 batch PCKh 0\n",
      "Trained batch 2729 batch loss 0.73127377 batch mAP 0.427886963 batch PCKh 0.375\n",
      "Trained batch 2730 batch loss 0.649648547 batch mAP 0.406738281 batch PCKh 0.125\n",
      "Trained batch 2731 batch loss 0.613501847 batch mAP 0.386688232 batch PCKh 0.625\n",
      "Trained batch 2732 batch loss 0.593337476 batch mAP 0.377197266 batch PCKh 0.5\n",
      "Trained batch 2733 batch loss 0.64416635 batch mAP 0.378814697 batch PCKh 0.625\n",
      "Trained batch 2734 batch loss 0.713396549 batch mAP 0.359985352 batch PCKh 0.75\n",
      "Trained batch 2735 batch loss 0.712863505 batch mAP 0.39465332 batch PCKh 0.1875\n",
      "Trained batch 2736 batch loss 0.666736066 batch mAP 0.387756348 batch PCKh 0\n",
      "Trained batch 2737 batch loss 0.655696511 batch mAP 0.448455811 batch PCKh 0.3125\n",
      "Trained batch 2738 batch loss 0.630254924 batch mAP 0.438262939 batch PCKh 0.25\n",
      "Trained batch 2739 batch loss 0.647489429 batch mAP 0.462249756 batch PCKh 0.5625\n",
      "Trained batch 2740 batch loss 0.605728745 batch mAP 0.432250977 batch PCKh 0.375\n",
      "Trained batch 2741 batch loss 0.5325495 batch mAP 0.452606201 batch PCKh 0.4375\n",
      "Trained batch 2742 batch loss 0.611359 batch mAP 0.415588379 batch PCKh 0.125\n",
      "Trained batch 2743 batch loss 0.675146341 batch mAP 0.470397949 batch PCKh 0.4375\n",
      "Trained batch 2744 batch loss 0.687796891 batch mAP 0.478851318 batch PCKh 0.5\n",
      "Trained batch 2745 batch loss 0.675506055 batch mAP 0.466430664 batch PCKh 0.5625\n",
      "Trained batch 2746 batch loss 0.665843606 batch mAP 0.46762085 batch PCKh 0.5625\n",
      "Trained batch 2747 batch loss 0.531634 batch mAP 0.48916626 batch PCKh 0.5\n",
      "Trained batch 2748 batch loss 0.537707567 batch mAP 0.428497314 batch PCKh 0.375\n",
      "Trained batch 2749 batch loss 0.529280305 batch mAP 0.422790527 batch PCKh 0.6875\n",
      "Trained batch 2750 batch loss 0.505340278 batch mAP 0.440185547 batch PCKh 0.6875\n",
      "Trained batch 2751 batch loss 0.496900201 batch mAP 0.431427 batch PCKh 0.25\n",
      "Trained batch 2752 batch loss 0.565613687 batch mAP 0.467590332 batch PCKh 0.375\n",
      "Trained batch 2753 batch loss 0.53640765 batch mAP 0.474639893 batch PCKh 0.75\n",
      "Trained batch 2754 batch loss 0.573767662 batch mAP 0.469940186 batch PCKh 0.6875\n",
      "Trained batch 2755 batch loss 0.631496668 batch mAP 0.457885742 batch PCKh 0.4375\n",
      "Trained batch 2756 batch loss 0.551374495 batch mAP 0.492218018 batch PCKh 0.625\n",
      "Trained batch 2757 batch loss 0.558182836 batch mAP 0.495239258 batch PCKh 0.4375\n",
      "Trained batch 2758 batch loss 0.520327747 batch mAP 0.548248291 batch PCKh 0.125\n",
      "Trained batch 2759 batch loss 0.607261181 batch mAP 0.471618652 batch PCKh 0.5625\n",
      "Trained batch 2760 batch loss 0.6184479 batch mAP 0.447967529 batch PCKh 0.375\n",
      "Trained batch 2761 batch loss 0.609542668 batch mAP 0.464111328 batch PCKh 0.5625\n",
      "Trained batch 2762 batch loss 0.624697626 batch mAP 0.378509521 batch PCKh 0.6875\n",
      "Trained batch 2763 batch loss 0.620841324 batch mAP 0.425354 batch PCKh 0.3125\n",
      "Trained batch 2764 batch loss 0.66683495 batch mAP 0.405517578 batch PCKh 0.875\n",
      "Trained batch 2765 batch loss 0.61701113 batch mAP 0.409118652 batch PCKh 0.4375\n",
      "Trained batch 2766 batch loss 0.557744563 batch mAP 0.457611084 batch PCKh 0.75\n",
      "Trained batch 2767 batch loss 0.57183826 batch mAP 0.40512085 batch PCKh 0.625\n",
      "Trained batch 2768 batch loss 0.526188314 batch mAP 0.420013428 batch PCKh 0.625\n",
      "Trained batch 2769 batch loss 0.663820148 batch mAP 0.428588867 batch PCKh 0.8125\n",
      "Trained batch 2770 batch loss 0.696578383 batch mAP 0.409698486 batch PCKh 0.625\n",
      "Trained batch 2771 batch loss 0.668311834 batch mAP 0.403961182 batch PCKh 0.25\n",
      "Trained batch 2772 batch loss 0.721302152 batch mAP 0.434265137 batch PCKh 0.125\n",
      "Trained batch 2773 batch loss 0.640128255 batch mAP 0.422698975 batch PCKh 0.1875\n",
      "Trained batch 2774 batch loss 0.692480326 batch mAP 0.475189209 batch PCKh 0.1875\n",
      "Trained batch 2775 batch loss 0.644747376 batch mAP 0.394958496 batch PCKh 0.4375\n",
      "Trained batch 2776 batch loss 0.627779305 batch mAP 0.447601318 batch PCKh 0.375\n",
      "Epoch 1 train loss 0.6800599694252014 train mAP 0.3543790578842163 train PCKh\n",
      "Validated batch 1 batch loss 0.619619727 batch mAP 0.449371338 batch PCKh 0.3125\n",
      "Validated batch 2 batch loss 0.640525401 batch mAP 0.412719727 batch PCKh 0.125\n",
      "Validated batch 3 batch loss 0.643751621 batch mAP 0.432678223 batch PCKh 0.5625\n",
      "Validated batch 4 batch loss 0.644692 batch mAP 0.362670898 batch PCKh 0.3125\n",
      "Validated batch 5 batch loss 0.6737082 batch mAP 0.372070312 batch PCKh 0.5\n",
      "Validated batch 6 batch loss 0.599353492 batch mAP 0.390197754 batch PCKh 0\n",
      "Validated batch 7 batch loss 0.681068897 batch mAP 0.46484375 batch PCKh 0.625\n",
      "Validated batch 8 batch loss 0.602664053 batch mAP 0.464263916 batch PCKh 0.75\n",
      "Validated batch 9 batch loss 0.648290873 batch mAP 0.461273193 batch PCKh 0.75\n",
      "Validated batch 10 batch loss 0.674931049 batch mAP 0.380828857 batch PCKh 0.6875\n",
      "Validated batch 11 batch loss 0.683917403 batch mAP 0.368469238 batch PCKh 0.3125\n",
      "Validated batch 12 batch loss 0.68716538 batch mAP 0.42401123 batch PCKh 0.5625\n",
      "Validated batch 13 batch loss 0.62766391 batch mAP 0.471038818 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 14 batch loss 0.692635596 batch mAP 0.344207764 batch PCKh 0\n",
      "Validated batch 15 batch loss 0.639163196 batch mAP 0.445098877 batch PCKh 0.625\n",
      "Validated batch 16 batch loss 0.665621161 batch mAP 0.416656494 batch PCKh 0.5625\n",
      "Validated batch 17 batch loss 0.686917782 batch mAP 0.389404297 batch PCKh 0.5625\n",
      "Validated batch 18 batch loss 0.592971921 batch mAP 0.470184326 batch PCKh 0.3125\n",
      "Validated batch 19 batch loss 0.580421627 batch mAP 0.531036377 batch PCKh 0.6875\n",
      "Validated batch 20 batch loss 0.769283473 batch mAP 0.438323975 batch PCKh 0.1875\n",
      "Validated batch 21 batch loss 0.688500106 batch mAP 0.402954102 batch PCKh 0.625\n",
      "Validated batch 22 batch loss 0.661777139 batch mAP 0.436218262 batch PCKh 0.125\n",
      "Validated batch 23 batch loss 0.707651317 batch mAP 0.440765381 batch PCKh 0.0625\n",
      "Validated batch 24 batch loss 0.577275634 batch mAP 0.457977295 batch PCKh 0.25\n",
      "Validated batch 25 batch loss 0.616261 batch mAP 0.452270508 batch PCKh 0.25\n",
      "Validated batch 26 batch loss 0.697540641 batch mAP 0.401550293 batch PCKh 0.1875\n",
      "Validated batch 27 batch loss 0.632666826 batch mAP 0.391021729 batch PCKh 0.25\n",
      "Validated batch 28 batch loss 0.680285811 batch mAP 0.387451172 batch PCKh 0.4375\n",
      "Validated batch 29 batch loss 0.649259865 batch mAP 0.391784668 batch PCKh 0\n",
      "Validated batch 30 batch loss 0.643777668 batch mAP 0.429504395 batch PCKh 0.125\n",
      "Validated batch 31 batch loss 0.625051141 batch mAP 0.450256348 batch PCKh 0.5\n",
      "Validated batch 32 batch loss 0.664376438 batch mAP 0.479614258 batch PCKh 0.1875\n",
      "Validated batch 33 batch loss 0.725942612 batch mAP 0.366027832 batch PCKh 0.125\n",
      "Validated batch 34 batch loss 0.723585427 batch mAP 0.433929443 batch PCKh 0.3125\n",
      "Validated batch 35 batch loss 0.57465589 batch mAP 0.474060059 batch PCKh 0.1875\n",
      "Validated batch 36 batch loss 0.734917 batch mAP 0.408172607 batch PCKh 0.25\n",
      "Validated batch 37 batch loss 0.650118291 batch mAP 0.421417236 batch PCKh 0.5625\n",
      "Validated batch 38 batch loss 0.560199738 batch mAP 0.476287842 batch PCKh 0.25\n",
      "Validated batch 39 batch loss 0.640761673 batch mAP 0.398040771 batch PCKh 0.4375\n",
      "Validated batch 40 batch loss 0.720179141 batch mAP 0.355377197 batch PCKh 0.5625\n",
      "Validated batch 41 batch loss 0.63680011 batch mAP 0.437713623 batch PCKh 0.1875\n",
      "Validated batch 42 batch loss 0.639719903 batch mAP 0.453338623 batch PCKh 0.1875\n",
      "Validated batch 43 batch loss 0.705975533 batch mAP 0.349243164 batch PCKh 0.25\n",
      "Validated batch 44 batch loss 0.585193634 batch mAP 0.463104248 batch PCKh 0.625\n",
      "Validated batch 45 batch loss 0.633323967 batch mAP 0.445800781 batch PCKh 0.5\n",
      "Validated batch 46 batch loss 0.706795096 batch mAP 0.49987793 batch PCKh 0.125\n",
      "Validated batch 47 batch loss 0.665530622 batch mAP 0.399536133 batch PCKh 0.625\n",
      "Validated batch 48 batch loss 0.596084476 batch mAP 0.489929199 batch PCKh 0.625\n",
      "Validated batch 49 batch loss 0.670370638 batch mAP 0.441650391 batch PCKh 0.5625\n",
      "Validated batch 50 batch loss 0.662071943 batch mAP 0.430999756 batch PCKh 0.6875\n",
      "Validated batch 51 batch loss 0.682231426 batch mAP 0.432525635 batch PCKh 0.875\n",
      "Validated batch 52 batch loss 0.61805582 batch mAP 0.410644531 batch PCKh 0.25\n",
      "Validated batch 53 batch loss 0.699362874 batch mAP 0.472290039 batch PCKh 0.125\n",
      "Validated batch 54 batch loss 0.568788946 batch mAP 0.500488281 batch PCKh 0.5625\n",
      "Validated batch 55 batch loss 0.662926912 batch mAP 0.446380615 batch PCKh 0.75\n",
      "Validated batch 56 batch loss 0.712533593 batch mAP 0.460906982 batch PCKh 0.125\n",
      "Validated batch 57 batch loss 0.759386778 batch mAP 0.398162842 batch PCKh 0.625\n",
      "Validated batch 58 batch loss 0.678135 batch mAP 0.448303223 batch PCKh 0.5\n",
      "Validated batch 59 batch loss 0.54269439 batch mAP 0.472290039 batch PCKh 0.6875\n",
      "Validated batch 60 batch loss 0.620746315 batch mAP 0.516906738 batch PCKh 0.6875\n",
      "Validated batch 61 batch loss 0.655811489 batch mAP 0.424407959 batch PCKh 0.125\n",
      "Validated batch 62 batch loss 0.64678973 batch mAP 0.403625488 batch PCKh 0.4375\n",
      "Validated batch 63 batch loss 0.640927494 batch mAP 0.395690918 batch PCKh 0.1875\n",
      "Validated batch 64 batch loss 0.627223969 batch mAP 0.463012695 batch PCKh 0.5625\n",
      "Validated batch 65 batch loss 0.740799785 batch mAP 0.448364258 batch PCKh 0.5625\n",
      "Validated batch 66 batch loss 0.667686343 batch mAP 0.430633545 batch PCKh 0.5\n",
      "Validated batch 67 batch loss 0.656252146 batch mAP 0.426940918 batch PCKh 0.375\n",
      "Validated batch 68 batch loss 0.688008487 batch mAP 0.388580322 batch PCKh 0.5\n",
      "Validated batch 69 batch loss 0.607864 batch mAP 0.448150635 batch PCKh 0.6875\n",
      "Validated batch 70 batch loss 0.558711886 batch mAP 0.406219482 batch PCKh 0.75\n",
      "Validated batch 71 batch loss 0.456345081 batch mAP 0.541656494 batch PCKh 0.25\n",
      "Validated batch 72 batch loss 0.694306731 batch mAP 0.445770264 batch PCKh 0.75\n",
      "Validated batch 73 batch loss 0.680302918 batch mAP 0.453735352 batch PCKh 0.6875\n",
      "Validated batch 74 batch loss 0.609341145 batch mAP 0.466430664 batch PCKh 0.4375\n",
      "Validated batch 75 batch loss 0.643962681 batch mAP 0.48727417 batch PCKh 0.25\n",
      "Validated batch 76 batch loss 0.666210055 batch mAP 0.421630859 batch PCKh 0.5\n",
      "Validated batch 77 batch loss 0.593913734 batch mAP 0.447296143 batch PCKh 0.5\n",
      "Validated batch 78 batch loss 0.643913269 batch mAP 0.375915527 batch PCKh 0.375\n",
      "Validated batch 79 batch loss 0.732346892 batch mAP 0.37020874 batch PCKh 0.625\n",
      "Validated batch 80 batch loss 0.601066232 batch mAP 0.458007812 batch PCKh 0.6875\n",
      "Validated batch 81 batch loss 0.634972 batch mAP 0.441986084 batch PCKh 0.25\n",
      "Validated batch 82 batch loss 0.609027803 batch mAP 0.443756104 batch PCKh 0.6875\n",
      "Validated batch 83 batch loss 0.714669585 batch mAP 0.439361572 batch PCKh 0.25\n",
      "Validated batch 84 batch loss 0.62244314 batch mAP 0.424102783 batch PCKh 0.0625\n",
      "Validated batch 85 batch loss 0.768209934 batch mAP 0.368896484 batch PCKh 0.0625\n",
      "Validated batch 86 batch loss 0.634360552 batch mAP 0.436187744 batch PCKh 0.25\n",
      "Validated batch 87 batch loss 0.67969662 batch mAP 0.391845703 batch PCKh 0.6875\n",
      "Validated batch 88 batch loss 0.664094806 batch mAP 0.402130127 batch PCKh 0.4375\n",
      "Validated batch 89 batch loss 0.667171538 batch mAP 0.421447754 batch PCKh 0.5625\n",
      "Validated batch 90 batch loss 0.612588584 batch mAP 0.441467285 batch PCKh 0.6875\n",
      "Validated batch 91 batch loss 0.580929041 batch mAP 0.46661377 batch PCKh 0.3125\n",
      "Validated batch 92 batch loss 0.616467178 batch mAP 0.475524902 batch PCKh 0.75\n",
      "Validated batch 93 batch loss 0.59468317 batch mAP 0.503356934 batch PCKh 0.75\n",
      "Validated batch 94 batch loss 0.632834375 batch mAP 0.455963135 batch PCKh 0.4375\n",
      "Validated batch 95 batch loss 0.683893621 batch mAP 0.447723389 batch PCKh 0.4375\n",
      "Validated batch 96 batch loss 0.622963071 batch mAP 0.4659729 batch PCKh 0.5\n",
      "Validated batch 97 batch loss 0.69232893 batch mAP 0.350372314 batch PCKh 0.0625\n",
      "Validated batch 98 batch loss 0.567251265 batch mAP 0.475799561 batch PCKh 0.1875\n",
      "Validated batch 99 batch loss 0.552999 batch mAP 0.467041016 batch PCKh 0.0625\n",
      "Validated batch 100 batch loss 0.666214287 batch mAP 0.418121338 batch PCKh 0.25\n",
      "Validated batch 101 batch loss 0.647782803 batch mAP 0.465484619 batch PCKh 0.8125\n",
      "Validated batch 102 batch loss 0.5775792 batch mAP 0.506378174 batch PCKh 0.25\n",
      "Validated batch 103 batch loss 0.761300564 batch mAP 0.434844971 batch PCKh 0\n",
      "Validated batch 104 batch loss 0.621888101 batch mAP 0.481567383 batch PCKh 0.75\n",
      "Validated batch 105 batch loss 0.658068478 batch mAP 0.466461182 batch PCKh 0.1875\n",
      "Validated batch 106 batch loss 0.646373034 batch mAP 0.44909668 batch PCKh 0.625\n",
      "Validated batch 107 batch loss 0.622541308 batch mAP 0.399383545 batch PCKh 0.375\n",
      "Validated batch 108 batch loss 0.588846087 batch mAP 0.479919434 batch PCKh 0.25\n",
      "Validated batch 109 batch loss 0.609991789 batch mAP 0.487548828 batch PCKh 0.625\n",
      "Validated batch 110 batch loss 0.697740316 batch mAP 0.437957764 batch PCKh 0.25\n",
      "Validated batch 111 batch loss 0.677957177 batch mAP 0.457824707 batch PCKh 0.1875\n",
      "Validated batch 112 batch loss 0.591548085 batch mAP 0.516265869 batch PCKh 0.1875\n",
      "Validated batch 113 batch loss 0.659605145 batch mAP 0.438781738 batch PCKh 0.375\n",
      "Validated batch 114 batch loss 0.574207783 batch mAP 0.50881958 batch PCKh 0.5\n",
      "Validated batch 115 batch loss 0.765941679 batch mAP 0.458465576 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 116 batch loss 0.564182758 batch mAP 0.429901123 batch PCKh 0\n",
      "Validated batch 117 batch loss 0.64740324 batch mAP 0.44418335 batch PCKh 0.3125\n",
      "Validated batch 118 batch loss 0.686361492 batch mAP 0.461425781 batch PCKh 0.1875\n",
      "Validated batch 119 batch loss 0.614665926 batch mAP 0.485839844 batch PCKh 0.125\n",
      "Validated batch 120 batch loss 0.67776221 batch mAP 0.461517334 batch PCKh 0.875\n",
      "Validated batch 121 batch loss 0.682558239 batch mAP 0.484802246 batch PCKh 0.75\n",
      "Validated batch 122 batch loss 0.697783 batch mAP 0.361450195 batch PCKh 0.375\n",
      "Validated batch 123 batch loss 0.721121311 batch mAP 0.39553833 batch PCKh 0.1875\n",
      "Validated batch 124 batch loss 0.697430849 batch mAP 0.479553223 batch PCKh 0.5625\n",
      "Validated batch 125 batch loss 0.63928324 batch mAP 0.463317871 batch PCKh 0.5625\n",
      "Validated batch 126 batch loss 0.638778746 batch mAP 0.447875977 batch PCKh 0.625\n",
      "Validated batch 127 batch loss 0.731263876 batch mAP 0.359649658 batch PCKh 0.0625\n",
      "Validated batch 128 batch loss 0.662023 batch mAP 0.435394287 batch PCKh 0.0625\n",
      "Validated batch 129 batch loss 0.57888025 batch mAP 0.482971191 batch PCKh 0.25\n",
      "Validated batch 130 batch loss 0.563265443 batch mAP 0.476623535 batch PCKh 0\n",
      "Validated batch 131 batch loss 0.629353 batch mAP 0.413909912 batch PCKh 0\n",
      "Validated batch 132 batch loss 0.669156194 batch mAP 0.319488525 batch PCKh 0.3125\n",
      "Validated batch 133 batch loss 0.627226174 batch mAP 0.399810791 batch PCKh 0.6875\n",
      "Validated batch 134 batch loss 0.613446712 batch mAP 0.438690186 batch PCKh 0.6875\n",
      "Validated batch 135 batch loss 0.642061114 batch mAP 0.443908691 batch PCKh 0.5\n",
      "Validated batch 136 batch loss 0.716091275 batch mAP 0.390045166 batch PCKh 0.5625\n",
      "Validated batch 137 batch loss 0.770620942 batch mAP 0.421844482 batch PCKh 0\n",
      "Validated batch 138 batch loss 0.709128 batch mAP 0.39654541 batch PCKh 0.4375\n",
      "Validated batch 139 batch loss 0.643825889 batch mAP 0.439086914 batch PCKh 0.3125\n",
      "Validated batch 140 batch loss 0.634025812 batch mAP 0.35534668 batch PCKh 0\n",
      "Validated batch 141 batch loss 0.606108367 batch mAP 0.460784912 batch PCKh 0.1875\n",
      "Validated batch 142 batch loss 0.582273245 batch mAP 0.451477051 batch PCKh 0.3125\n",
      "Validated batch 143 batch loss 0.622817934 batch mAP 0.414123535 batch PCKh 0.8125\n",
      "Validated batch 144 batch loss 0.683547258 batch mAP 0.389770508 batch PCKh 0.375\n",
      "Validated batch 145 batch loss 0.613233805 batch mAP 0.469604492 batch PCKh 0.3125\n",
      "Validated batch 146 batch loss 0.612829745 batch mAP 0.441162109 batch PCKh 0.5625\n",
      "Validated batch 147 batch loss 0.634029806 batch mAP 0.469055176 batch PCKh 0.625\n",
      "Validated batch 148 batch loss 0.640354335 batch mAP 0.448944092 batch PCKh 0.625\n",
      "Validated batch 149 batch loss 0.628591299 batch mAP 0.460388184 batch PCKh 0.125\n",
      "Validated batch 150 batch loss 0.702731371 batch mAP 0.40826416 batch PCKh 0.625\n",
      "Validated batch 151 batch loss 0.62275058 batch mAP 0.429656982 batch PCKh 0.75\n",
      "Validated batch 152 batch loss 0.708170116 batch mAP 0.344482422 batch PCKh 0.4375\n",
      "Validated batch 153 batch loss 0.63140142 batch mAP 0.396820068 batch PCKh 0.1875\n",
      "Validated batch 154 batch loss 0.750687242 batch mAP 0.408325195 batch PCKh 0.0625\n",
      "Validated batch 155 batch loss 0.708719134 batch mAP 0.406066895 batch PCKh 0.4375\n",
      "Validated batch 156 batch loss 0.666776299 batch mAP 0.490783691 batch PCKh 0.5625\n",
      "Validated batch 157 batch loss 0.695818424 batch mAP 0.513214111 batch PCKh 0.3125\n",
      "Validated batch 158 batch loss 0.636395574 batch mAP 0.437927246 batch PCKh 0.4375\n",
      "Validated batch 159 batch loss 0.651193142 batch mAP 0.409606934 batch PCKh 0.4375\n",
      "Validated batch 160 batch loss 0.61529094 batch mAP 0.473510742 batch PCKh 0.6875\n",
      "Validated batch 161 batch loss 0.697041094 batch mAP 0.44519043 batch PCKh 0.6875\n",
      "Validated batch 162 batch loss 0.743885398 batch mAP 0.426971436 batch PCKh 0.4375\n",
      "Validated batch 163 batch loss 0.693637729 batch mAP 0.407073975 batch PCKh 0.625\n",
      "Validated batch 164 batch loss 0.679296911 batch mAP 0.392364502 batch PCKh 0.3125\n",
      "Validated batch 165 batch loss 0.681758344 batch mAP 0.490905762 batch PCKh 0.5\n",
      "Validated batch 166 batch loss 0.694721 batch mAP 0.408538818 batch PCKh 0.375\n",
      "Validated batch 167 batch loss 0.669160128 batch mAP 0.433197021 batch PCKh 0.625\n",
      "Validated batch 168 batch loss 0.698191404 batch mAP 0.41229248 batch PCKh 0.6875\n",
      "Validated batch 169 batch loss 0.736963511 batch mAP 0.346710205 batch PCKh 0.625\n",
      "Validated batch 170 batch loss 0.692261517 batch mAP 0.478118896 batch PCKh 0.5\n",
      "Validated batch 171 batch loss 0.735815048 batch mAP 0.402374268 batch PCKh 0.4375\n",
      "Validated batch 172 batch loss 0.627709508 batch mAP 0.440368652 batch PCKh 0.125\n",
      "Validated batch 173 batch loss 0.676824629 batch mAP 0.458068848 batch PCKh 0.6875\n",
      "Validated batch 174 batch loss 0.545421362 batch mAP 0.472137451 batch PCKh 0.1875\n",
      "Validated batch 175 batch loss 0.638564944 batch mAP 0.435028076 batch PCKh 0.625\n",
      "Validated batch 176 batch loss 0.664065 batch mAP 0.446716309 batch PCKh 0.375\n",
      "Validated batch 177 batch loss 0.691716552 batch mAP 0.363891602 batch PCKh 0.1875\n",
      "Validated batch 178 batch loss 0.700171649 batch mAP 0.49621582 batch PCKh 0.1875\n",
      "Validated batch 179 batch loss 0.710763812 batch mAP 0.428527832 batch PCKh 0.5625\n",
      "Validated batch 180 batch loss 0.636257946 batch mAP 0.418273926 batch PCKh 0.5\n",
      "Validated batch 181 batch loss 0.645602345 batch mAP 0.4347229 batch PCKh 0.0625\n",
      "Validated batch 182 batch loss 0.673969388 batch mAP 0.425018311 batch PCKh 0.3125\n",
      "Validated batch 183 batch loss 0.704423428 batch mAP 0.373962402 batch PCKh 0.25\n",
      "Validated batch 184 batch loss 0.612151384 batch mAP 0.479980469 batch PCKh 0.6875\n",
      "Validated batch 185 batch loss 0.739369512 batch mAP 0.37677002 batch PCKh 0.125\n",
      "Validated batch 186 batch loss 0.741921544 batch mAP 0.389190674 batch PCKh 0.25\n",
      "Validated batch 187 batch loss 0.627733946 batch mAP 0.453735352 batch PCKh 0.625\n",
      "Validated batch 188 batch loss 0.657706261 batch mAP 0.463989258 batch PCKh 0.25\n",
      "Validated batch 189 batch loss 0.684362531 batch mAP 0.465332031 batch PCKh 0.625\n",
      "Validated batch 190 batch loss 0.584144175 batch mAP 0.465148926 batch PCKh 0.3125\n",
      "Validated batch 191 batch loss 0.611857533 batch mAP 0.50769043 batch PCKh 0.25\n",
      "Validated batch 192 batch loss 0.642641 batch mAP 0.467315674 batch PCKh 0.875\n",
      "Validated batch 193 batch loss 0.611414909 batch mAP 0.505065918 batch PCKh 0.625\n",
      "Validated batch 194 batch loss 0.609839797 batch mAP 0.480255127 batch PCKh 0.5\n",
      "Validated batch 195 batch loss 0.711463571 batch mAP 0.440979 batch PCKh 0.0625\n",
      "Validated batch 196 batch loss 0.638747811 batch mAP 0.485809326 batch PCKh 0.75\n",
      "Validated batch 197 batch loss 0.639337182 batch mAP 0.4480896 batch PCKh 0.5625\n",
      "Validated batch 198 batch loss 0.646103859 batch mAP 0.469909668 batch PCKh 0.5\n",
      "Validated batch 199 batch loss 0.62176013 batch mAP 0.505523682 batch PCKh 0.4375\n",
      "Validated batch 200 batch loss 0.609411 batch mAP 0.452667236 batch PCKh 0.5\n",
      "Validated batch 201 batch loss 0.589431465 batch mAP 0.436523438 batch PCKh 0.6875\n",
      "Validated batch 202 batch loss 0.648831189 batch mAP 0.436248779 batch PCKh 0.3125\n",
      "Validated batch 203 batch loss 0.667843938 batch mAP 0.478210449 batch PCKh 0.625\n",
      "Validated batch 204 batch loss 0.639204621 batch mAP 0.503601074 batch PCKh 0.375\n",
      "Validated batch 205 batch loss 0.686560392 batch mAP 0.436645508 batch PCKh 0.375\n",
      "Validated batch 206 batch loss 0.640991151 batch mAP 0.419525146 batch PCKh 0\n",
      "Validated batch 207 batch loss 0.669521213 batch mAP 0.325744629 batch PCKh 0.75\n",
      "Validated batch 208 batch loss 0.674052238 batch mAP 0.40222168 batch PCKh 0.1875\n",
      "Validated batch 209 batch loss 0.640420437 batch mAP 0.429473877 batch PCKh 0.5\n",
      "Validated batch 210 batch loss 0.675162613 batch mAP 0.381530762 batch PCKh 0.5625\n",
      "Validated batch 211 batch loss 0.558029115 batch mAP 0.430480957 batch PCKh 0.1875\n",
      "Validated batch 212 batch loss 0.645745516 batch mAP 0.431610107 batch PCKh 0.625\n",
      "Validated batch 213 batch loss 0.684249043 batch mAP 0.428924561 batch PCKh 0.1875\n",
      "Validated batch 214 batch loss 0.58027184 batch mAP 0.474945068 batch PCKh 0.6875\n",
      "Validated batch 215 batch loss 0.706273913 batch mAP 0.404418945 batch PCKh 0.1875\n",
      "Validated batch 216 batch loss 0.636641 batch mAP 0.411407471 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 217 batch loss 0.625848532 batch mAP 0.462493896 batch PCKh 0.125\n",
      "Validated batch 218 batch loss 0.686753154 batch mAP 0.453704834 batch PCKh 0.375\n",
      "Validated batch 219 batch loss 0.70519352 batch mAP 0.389831543 batch PCKh 0\n",
      "Validated batch 220 batch loss 0.696168184 batch mAP 0.425537109 batch PCKh 0.25\n",
      "Validated batch 221 batch loss 0.679839253 batch mAP 0.464904785 batch PCKh 0.3125\n",
      "Validated batch 222 batch loss 0.702376 batch mAP 0.402313232 batch PCKh 0.0625\n",
      "Validated batch 223 batch loss 0.758171 batch mAP 0.454071045 batch PCKh 0.1875\n",
      "Validated batch 224 batch loss 0.806188464 batch mAP 0.386474609 batch PCKh 0.125\n",
      "Validated batch 225 batch loss 0.7980721 batch mAP 0.382232666 batch PCKh 0.0625\n",
      "Validated batch 226 batch loss 0.662117958 batch mAP 0.450195312 batch PCKh 0.5625\n",
      "Validated batch 227 batch loss 0.608349323 batch mAP 0.497924805 batch PCKh 0.5\n",
      "Validated batch 228 batch loss 0.714118 batch mAP 0.384063721 batch PCKh 0.6875\n",
      "Validated batch 229 batch loss 0.596897 batch mAP 0.402740479 batch PCKh 0.4375\n",
      "Validated batch 230 batch loss 0.611555 batch mAP 0.445831299 batch PCKh 0.25\n",
      "Validated batch 231 batch loss 0.601472855 batch mAP 0.492553711 batch PCKh 0.6875\n",
      "Validated batch 232 batch loss 0.700328171 batch mAP 0.382019043 batch PCKh 0\n",
      "Validated batch 233 batch loss 0.747247577 batch mAP 0.466186523 batch PCKh 0.5\n",
      "Validated batch 234 batch loss 0.662011862 batch mAP 0.481872559 batch PCKh 0.1875\n",
      "Validated batch 235 batch loss 0.558486223 batch mAP 0.501159668 batch PCKh 0.5625\n",
      "Validated batch 236 batch loss 0.722359 batch mAP 0.450958252 batch PCKh 0.0625\n",
      "Validated batch 237 batch loss 0.66355 batch mAP 0.409362793 batch PCKh 0.375\n",
      "Validated batch 238 batch loss 0.588262141 batch mAP 0.405365 batch PCKh 0.5\n",
      "Validated batch 239 batch loss 0.623005033 batch mAP 0.420074463 batch PCKh 0.375\n",
      "Validated batch 240 batch loss 0.703609109 batch mAP 0.444396973 batch PCKh 0.3125\n",
      "Validated batch 241 batch loss 0.582742929 batch mAP 0.451690674 batch PCKh 0.5\n",
      "Validated batch 242 batch loss 0.695330501 batch mAP 0.520568848 batch PCKh 0.8125\n",
      "Validated batch 243 batch loss 0.634428 batch mAP 0.448455811 batch PCKh 0.375\n",
      "Validated batch 244 batch loss 0.66569072 batch mAP 0.459472656 batch PCKh 0.1875\n",
      "Validated batch 245 batch loss 0.5733217 batch mAP 0.456512451 batch PCKh 0.25\n",
      "Validated batch 246 batch loss 0.688495 batch mAP 0.378540039 batch PCKh 0.3125\n",
      "Validated batch 247 batch loss 0.545490444 batch mAP 0.491790771 batch PCKh 0.3125\n",
      "Validated batch 248 batch loss 0.690589488 batch mAP 0.468017578 batch PCKh 0.375\n",
      "Validated batch 249 batch loss 0.770053625 batch mAP 0.441986084 batch PCKh 0.125\n",
      "Validated batch 250 batch loss 0.622633159 batch mAP 0.469329834 batch PCKh 0.375\n",
      "Validated batch 251 batch loss 0.648548 batch mAP 0.462127686 batch PCKh 0.1875\n",
      "Validated batch 252 batch loss 0.575430691 batch mAP 0.501037598 batch PCKh 0.25\n",
      "Validated batch 253 batch loss 0.613992333 batch mAP 0.481719971 batch PCKh 0.6875\n",
      "Validated batch 254 batch loss 0.619017661 batch mAP 0.470825195 batch PCKh 0.5625\n",
      "Validated batch 255 batch loss 0.625950217 batch mAP 0.367980957 batch PCKh 0.3125\n",
      "Validated batch 256 batch loss 0.63215816 batch mAP 0.488128662 batch PCKh 0.3125\n",
      "Validated batch 257 batch loss 0.713463902 batch mAP 0.478637695 batch PCKh 0.8125\n",
      "Validated batch 258 batch loss 0.698386908 batch mAP 0.454589844 batch PCKh 0.0625\n",
      "Validated batch 259 batch loss 0.623577476 batch mAP 0.339355469 batch PCKh 0.25\n",
      "Validated batch 260 batch loss 0.747658968 batch mAP 0.312194824 batch PCKh 0.125\n",
      "Validated batch 261 batch loss 0.758428931 batch mAP 0.306610107 batch PCKh 0.4375\n",
      "Validated batch 262 batch loss 0.696649194 batch mAP 0.333648682 batch PCKh 0.5625\n",
      "Validated batch 263 batch loss 0.637354195 batch mAP 0.48651123 batch PCKh 0.75\n",
      "Validated batch 264 batch loss 0.644160628 batch mAP 0.442169189 batch PCKh 0.5\n",
      "Validated batch 265 batch loss 0.724627733 batch mAP 0.409729 batch PCKh 0.1875\n",
      "Validated batch 266 batch loss 0.705271482 batch mAP 0.417419434 batch PCKh 0.375\n",
      "Validated batch 267 batch loss 0.645356894 batch mAP 0.410644531 batch PCKh 0\n",
      "Validated batch 268 batch loss 0.632512808 batch mAP 0.445495605 batch PCKh 0.875\n",
      "Validated batch 269 batch loss 0.779283404 batch mAP 0.398284912 batch PCKh 0\n",
      "Validated batch 270 batch loss 0.631248236 batch mAP 0.430877686 batch PCKh 0.4375\n",
      "Validated batch 271 batch loss 0.674178958 batch mAP 0.399841309 batch PCKh 0.25\n",
      "Validated batch 272 batch loss 0.702183306 batch mAP 0.385528564 batch PCKh 0.8125\n",
      "Validated batch 273 batch loss 0.555304527 batch mAP 0.498657227 batch PCKh 0.5\n",
      "Validated batch 274 batch loss 0.519171476 batch mAP 0.496185303 batch PCKh 0.3125\n",
      "Validated batch 275 batch loss 0.613184452 batch mAP 0.488311768 batch PCKh 0.4375\n",
      "Validated batch 276 batch loss 0.647458673 batch mAP 0.475189209 batch PCKh 0.3125\n",
      "Validated batch 277 batch loss 0.695031822 batch mAP 0.498352051 batch PCKh 0.5625\n",
      "Validated batch 278 batch loss 0.616433263 batch mAP 0.491088867 batch PCKh 0.8125\n",
      "Validated batch 279 batch loss 0.692872 batch mAP 0.408355713 batch PCKh 0.5\n",
      "Validated batch 280 batch loss 0.600399733 batch mAP 0.480316162 batch PCKh 0.6875\n",
      "Validated batch 281 batch loss 0.710819602 batch mAP 0.438903809 batch PCKh 0.625\n",
      "Validated batch 282 batch loss 0.627395093 batch mAP 0.487304688 batch PCKh 0.5625\n",
      "Validated batch 283 batch loss 0.634591103 batch mAP 0.465087891 batch PCKh 0.8125\n",
      "Validated batch 284 batch loss 0.615956485 batch mAP 0.480255127 batch PCKh 0.5\n",
      "Validated batch 285 batch loss 0.619732738 batch mAP 0.417755127 batch PCKh 0.0625\n",
      "Validated batch 286 batch loss 0.686680317 batch mAP 0.405792236 batch PCKh 0.3125\n",
      "Validated batch 287 batch loss 0.696904182 batch mAP 0.391967773 batch PCKh 0.4375\n",
      "Validated batch 288 batch loss 0.704654694 batch mAP 0.419525146 batch PCKh 0.5\n",
      "Validated batch 289 batch loss 0.605066717 batch mAP 0.44317627 batch PCKh 0.3125\n",
      "Validated batch 290 batch loss 0.583337903 batch mAP 0.452301025 batch PCKh 0.25\n",
      "Validated batch 291 batch loss 0.620465279 batch mAP 0.502807617 batch PCKh 0.8125\n",
      "Validated batch 292 batch loss 0.68950367 batch mAP 0.4168396 batch PCKh 0.625\n",
      "Validated batch 293 batch loss 0.65593785 batch mAP 0.439697266 batch PCKh 0.25\n",
      "Validated batch 294 batch loss 0.617520452 batch mAP 0.475402832 batch PCKh 0.5\n",
      "Validated batch 295 batch loss 0.746645927 batch mAP 0.474121094 batch PCKh 0.125\n",
      "Validated batch 296 batch loss 0.62097317 batch mAP 0.422454834 batch PCKh 0.3125\n",
      "Validated batch 297 batch loss 0.628556132 batch mAP 0.490112305 batch PCKh 0.75\n",
      "Validated batch 298 batch loss 0.661015749 batch mAP 0.465606689 batch PCKh 0.6875\n",
      "Validated batch 299 batch loss 0.623114467 batch mAP 0.444732666 batch PCKh 0.625\n",
      "Validated batch 300 batch loss 0.598509431 batch mAP 0.476135254 batch PCKh 0.6875\n",
      "Validated batch 301 batch loss 0.652190924 batch mAP 0.481872559 batch PCKh 0.625\n",
      "Validated batch 302 batch loss 0.712969422 batch mAP 0.437347412 batch PCKh 0.4375\n",
      "Validated batch 303 batch loss 0.785267234 batch mAP 0.468078613 batch PCKh 0\n",
      "Validated batch 304 batch loss 0.578749478 batch mAP 0.435089111 batch PCKh 0.4375\n",
      "Validated batch 305 batch loss 0.660984874 batch mAP 0.394989 batch PCKh 0.4375\n",
      "Validated batch 306 batch loss 0.687517524 batch mAP 0.366333 batch PCKh 0.25\n",
      "Validated batch 307 batch loss 0.621732116 batch mAP 0.422943115 batch PCKh 0.0625\n",
      "Validated batch 308 batch loss 0.656419754 batch mAP 0.465454102 batch PCKh 0.375\n",
      "Validated batch 309 batch loss 0.640459895 batch mAP 0.413909912 batch PCKh 0.8125\n",
      "Validated batch 310 batch loss 0.671588123 batch mAP 0.427001953 batch PCKh 0.4375\n",
      "Validated batch 311 batch loss 0.731582522 batch mAP 0.386444092 batch PCKh 0.75\n",
      "Validated batch 312 batch loss 0.524073303 batch mAP 0.507476807 batch PCKh 0.4375\n",
      "Validated batch 313 batch loss 0.56825161 batch mAP 0.522369385 batch PCKh 0.6875\n",
      "Validated batch 314 batch loss 0.614409268 batch mAP 0.386108398 batch PCKh 0.75\n",
      "Validated batch 315 batch loss 0.636873245 batch mAP 0.393493652 batch PCKh 0.5\n",
      "Validated batch 316 batch loss 0.626757741 batch mAP 0.459503174 batch PCKh 0.5\n",
      "Validated batch 317 batch loss 0.583366752 batch mAP 0.501434326 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 318 batch loss 0.648585141 batch mAP 0.47064209 batch PCKh 0.6875\n",
      "Validated batch 319 batch loss 0.674296677 batch mAP 0.429351807 batch PCKh 0.5625\n",
      "Validated batch 320 batch loss 0.624148965 batch mAP 0.479736328 batch PCKh 0\n",
      "Validated batch 321 batch loss 0.728583097 batch mAP 0.512939453 batch PCKh 0.75\n",
      "Validated batch 322 batch loss 0.602007389 batch mAP 0.488922119 batch PCKh 0.0625\n",
      "Validated batch 323 batch loss 0.693904579 batch mAP 0.406188965 batch PCKh 0.375\n",
      "Validated batch 324 batch loss 0.649746299 batch mAP 0.46975708 batch PCKh 0.875\n",
      "Validated batch 325 batch loss 0.70730257 batch mAP 0.379730225 batch PCKh 0.75\n",
      "Validated batch 326 batch loss 0.607581317 batch mAP 0.489837646 batch PCKh 0.8125\n",
      "Validated batch 327 batch loss 0.62498647 batch mAP 0.481445312 batch PCKh 0.5\n",
      "Validated batch 328 batch loss 0.698475659 batch mAP 0.438385 batch PCKh 0.625\n",
      "Validated batch 329 batch loss 0.548879743 batch mAP 0.392547607 batch PCKh 0.25\n",
      "Validated batch 330 batch loss 0.700162411 batch mAP 0.424865723 batch PCKh 0.4375\n",
      "Validated batch 331 batch loss 0.739135265 batch mAP 0.486694336 batch PCKh 0\n",
      "Validated batch 332 batch loss 0.633737445 batch mAP 0.429931641 batch PCKh 0.5\n",
      "Validated batch 333 batch loss 0.677333832 batch mAP 0.450408936 batch PCKh 0.1875\n",
      "Validated batch 334 batch loss 0.665314794 batch mAP 0.468414307 batch PCKh 0.875\n",
      "Validated batch 335 batch loss 0.703463793 batch mAP 0.437744141 batch PCKh 0.375\n",
      "Validated batch 336 batch loss 0.673435509 batch mAP 0.439086914 batch PCKh 0.5625\n",
      "Validated batch 337 batch loss 0.697778344 batch mAP 0.410736084 batch PCKh 0.3125\n",
      "Validated batch 338 batch loss 0.699219048 batch mAP 0.414733887 batch PCKh 0.25\n",
      "Validated batch 339 batch loss 0.639093459 batch mAP 0.385253906 batch PCKh 0.6875\n",
      "Validated batch 340 batch loss 0.727545 batch mAP 0.396453857 batch PCKh 0\n",
      "Validated batch 341 batch loss 0.64503181 batch mAP 0.417266846 batch PCKh 0.75\n",
      "Validated batch 342 batch loss 0.65067029 batch mAP 0.444915771 batch PCKh 0.3125\n",
      "Validated batch 343 batch loss 0.665973186 batch mAP 0.441192627 batch PCKh 0.25\n",
      "Validated batch 344 batch loss 0.676202059 batch mAP 0.501617432 batch PCKh 0.3125\n",
      "Validated batch 345 batch loss 0.712080598 batch mAP 0.405731201 batch PCKh 0.1875\n",
      "Validated batch 346 batch loss 0.688521862 batch mAP 0.445983887 batch PCKh 0.4375\n",
      "Validated batch 347 batch loss 0.66111362 batch mAP 0.376098633 batch PCKh 0.25\n",
      "Validated batch 348 batch loss 0.677714586 batch mAP 0.388275146 batch PCKh 0.125\n",
      "Validated batch 349 batch loss 0.714200497 batch mAP 0.36618042 batch PCKh 0.5\n",
      "Validated batch 350 batch loss 0.666723251 batch mAP 0.453125 batch PCKh 0.0625\n",
      "Validated batch 351 batch loss 0.752236366 batch mAP 0.393035889 batch PCKh 0.125\n",
      "Validated batch 352 batch loss 0.681556225 batch mAP 0.315765381 batch PCKh 0\n",
      "Validated batch 353 batch loss 0.659039378 batch mAP 0.463775635 batch PCKh 0.5625\n",
      "Validated batch 354 batch loss 0.642011464 batch mAP 0.529541 batch PCKh 0.1875\n",
      "Validated batch 355 batch loss 0.645795286 batch mAP 0.454986572 batch PCKh 0.4375\n",
      "Validated batch 356 batch loss 0.751011968 batch mAP 0.384735107 batch PCKh 0.25\n",
      "Validated batch 357 batch loss 0.756764174 batch mAP 0.40246582 batch PCKh 0\n",
      "Validated batch 358 batch loss 0.56237793 batch mAP 0.46206665 batch PCKh 0.5\n",
      "Validated batch 359 batch loss 0.606792331 batch mAP 0.491943359 batch PCKh 0.1875\n",
      "Validated batch 360 batch loss 0.627352715 batch mAP 0.452941895 batch PCKh 0\n",
      "Validated batch 361 batch loss 0.729825377 batch mAP 0.434570312 batch PCKh 0.6875\n",
      "Validated batch 362 batch loss 0.607441306 batch mAP 0.446014404 batch PCKh 0.5\n",
      "Validated batch 363 batch loss 0.594860196 batch mAP 0.41809082 batch PCKh 0.5625\n",
      "Validated batch 364 batch loss 0.613027275 batch mAP 0.465332031 batch PCKh 0.3125\n",
      "Validated batch 365 batch loss 0.753444433 batch mAP 0.406707764 batch PCKh 0.125\n",
      "Validated batch 366 batch loss 0.644538164 batch mAP 0.473815918 batch PCKh 0.875\n",
      "Validated batch 367 batch loss 0.684347153 batch mAP 0.411895752 batch PCKh 0.6875\n",
      "Validated batch 368 batch loss 0.607104719 batch mAP 0.436553955 batch PCKh 0.5625\n",
      "Validated batch 369 batch loss 0.674649 batch mAP 0.376251221 batch PCKh 0.5625\n",
      "Epoch 1 val loss 0.6559008359909058 val mAP 0.4376949369907379 val PCKh\n",
      "Epoch 1 completed in 805.57 seconds\n",
      "Model /aiffel/aiffel/model_weight/GD08/y_model-epoch-1-loss-0.6559.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.646030068 batch mAP 0.466522217 batch PCKh 0.375\n",
      "Trained batch 2 batch loss 0.645562053 batch mAP 0.473327637 batch PCKh 0.3125\n",
      "Trained batch 3 batch loss 0.552873969 batch mAP 0.472717285 batch PCKh 0.3125\n",
      "Trained batch 4 batch loss 0.626777053 batch mAP 0.436035156 batch PCKh 0.4375\n",
      "Trained batch 5 batch loss 0.641587675 batch mAP 0.44720459 batch PCKh 0.5625\n",
      "Trained batch 6 batch loss 0.683591247 batch mAP 0.427734375 batch PCKh 0\n",
      "Trained batch 7 batch loss 0.759159923 batch mAP 0.494049072 batch PCKh 0.375\n",
      "Trained batch 8 batch loss 0.555147827 batch mAP 0.485565186 batch PCKh 0.75\n",
      "Trained batch 9 batch loss 0.6035254 batch mAP 0.42288208 batch PCKh 0.375\n",
      "Trained batch 10 batch loss 0.588249922 batch mAP 0.412414551 batch PCKh 0.5\n",
      "Trained batch 11 batch loss 0.661720872 batch mAP 0.392364502 batch PCKh 0.5625\n",
      "Trained batch 12 batch loss 0.643996894 batch mAP 0.413391113 batch PCKh 0.5625\n",
      "Trained batch 13 batch loss 0.69057548 batch mAP 0.435028076 batch PCKh 0.3125\n",
      "Trained batch 14 batch loss 0.555838823 batch mAP 0.464691162 batch PCKh 0.5625\n",
      "Trained batch 15 batch loss 0.695530653 batch mAP 0.444549561 batch PCKh 0.25\n",
      "Trained batch 16 batch loss 0.687149286 batch mAP 0.461334229 batch PCKh 0.6875\n",
      "Trained batch 17 batch loss 0.750074744 batch mAP 0.469543457 batch PCKh 0.125\n",
      "Trained batch 18 batch loss 0.592131 batch mAP 0.463531494 batch PCKh 0.3125\n",
      "Trained batch 19 batch loss 0.578429282 batch mAP 0.493865967 batch PCKh 0.6875\n",
      "Trained batch 20 batch loss 0.474869192 batch mAP 0.495544434 batch PCKh 0.125\n",
      "Trained batch 21 batch loss 0.545361876 batch mAP 0.45892334 batch PCKh 0.5625\n",
      "Trained batch 22 batch loss 0.476379931 batch mAP 0.487426758 batch PCKh 0.375\n",
      "Trained batch 23 batch loss 0.614387453 batch mAP 0.503112793 batch PCKh 0.4375\n",
      "Trained batch 24 batch loss 0.635051966 batch mAP 0.48916626 batch PCKh 0.1875\n",
      "Trained batch 25 batch loss 0.645337582 batch mAP 0.488739 batch PCKh 0.3125\n",
      "Trained batch 26 batch loss 0.561262608 batch mAP 0.490783691 batch PCKh 0.625\n",
      "Trained batch 27 batch loss 0.529388428 batch mAP 0.515075684 batch PCKh 0.25\n",
      "Trained batch 28 batch loss 0.554098964 batch mAP 0.510009766 batch PCKh 0.1875\n",
      "Trained batch 29 batch loss 0.641599894 batch mAP 0.470550537 batch PCKh 0.375\n",
      "Trained batch 30 batch loss 0.64350152 batch mAP 0.466033936 batch PCKh 0.75\n",
      "Trained batch 31 batch loss 0.639876306 batch mAP 0.490661621 batch PCKh 0.3125\n",
      "Trained batch 32 batch loss 0.646852314 batch mAP 0.485626221 batch PCKh 0.625\n",
      "Trained batch 33 batch loss 0.667151332 batch mAP 0.501159668 batch PCKh 0.8125\n",
      "Trained batch 34 batch loss 0.693511486 batch mAP 0.499908447 batch PCKh 0.3125\n",
      "Trained batch 35 batch loss 0.710936189 batch mAP 0.439178467 batch PCKh 0.3125\n",
      "Trained batch 36 batch loss 0.717918873 batch mAP 0.487792969 batch PCKh 0.1875\n",
      "Trained batch 37 batch loss 0.606311202 batch mAP 0.457702637 batch PCKh 0.3125\n",
      "Trained batch 38 batch loss 0.632761 batch mAP 0.432342529 batch PCKh 0.875\n",
      "Trained batch 39 batch loss 0.706458449 batch mAP 0.429748535 batch PCKh 0.1875\n",
      "Trained batch 40 batch loss 0.749321461 batch mAP 0.429626465 batch PCKh 0.3125\n",
      "Trained batch 41 batch loss 0.78779918 batch mAP 0.417877197 batch PCKh 0\n",
      "Trained batch 42 batch loss 0.64566195 batch mAP 0.204772949 batch PCKh 0.3125\n",
      "Trained batch 43 batch loss 0.764425933 batch mAP 0.316223145 batch PCKh 0.125\n",
      "Trained batch 44 batch loss 0.741007 batch mAP 0.127990723 batch PCKh 0.25\n",
      "Trained batch 45 batch loss 0.648266315 batch mAP 0.0609436035 batch PCKh 0.375\n",
      "Trained batch 46 batch loss 0.697982907 batch mAP 0.260559082 batch PCKh 0.1875\n",
      "Trained batch 47 batch loss 0.672232032 batch mAP 0.0787353516 batch PCKh 0.3125\n",
      "Trained batch 48 batch loss 0.636580586 batch mAP 0.144989014 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 49 batch loss 0.656171322 batch mAP 0.09375 batch PCKh 0.375\n",
      "Trained batch 50 batch loss 0.651836932 batch mAP 0.393157959 batch PCKh 0.625\n",
      "Trained batch 51 batch loss 0.691210806 batch mAP 0.42868042 batch PCKh 0.6875\n",
      "Trained batch 52 batch loss 0.654382825 batch mAP 0.459289551 batch PCKh 0.25\n",
      "Trained batch 53 batch loss 0.674205601 batch mAP 0.499389648 batch PCKh 0.3125\n",
      "Trained batch 54 batch loss 0.678750575 batch mAP 0.491851807 batch PCKh 0.3125\n",
      "Trained batch 55 batch loss 0.652952909 batch mAP 0.497680664 batch PCKh 0.3125\n",
      "Trained batch 56 batch loss 0.651897 batch mAP 0.431549072 batch PCKh 0.5\n",
      "Trained batch 57 batch loss 0.647147596 batch mAP 0.460266113 batch PCKh 0.25\n",
      "Trained batch 58 batch loss 0.671435475 batch mAP 0.39755249 batch PCKh 0.1875\n",
      "Trained batch 59 batch loss 0.648126245 batch mAP 0.327941895 batch PCKh 0.625\n",
      "Trained batch 60 batch loss 0.618261278 batch mAP 0.333709717 batch PCKh 0.1875\n",
      "Trained batch 61 batch loss 0.644802511 batch mAP 0.372741699 batch PCKh 0.75\n",
      "Trained batch 62 batch loss 0.573509455 batch mAP 0.459838867 batch PCKh 0.25\n",
      "Trained batch 63 batch loss 0.590516388 batch mAP 0.434570312 batch PCKh 0.625\n",
      "Trained batch 64 batch loss 0.607360601 batch mAP 0.446685791 batch PCKh 0.5\n",
      "Trained batch 65 batch loss 0.611633897 batch mAP 0.492340088 batch PCKh 0.625\n",
      "Trained batch 66 batch loss 0.619359672 batch mAP 0.478088379 batch PCKh 0.375\n",
      "Trained batch 67 batch loss 0.648134053 batch mAP 0.464935303 batch PCKh 0.6875\n",
      "Trained batch 68 batch loss 0.61870259 batch mAP 0.47076416 batch PCKh 0.25\n",
      "Trained batch 69 batch loss 0.605708957 batch mAP 0.489868164 batch PCKh 0.625\n",
      "Trained batch 70 batch loss 0.621933222 batch mAP 0.420837402 batch PCKh 0\n",
      "Trained batch 71 batch loss 0.60248965 batch mAP 0.453063965 batch PCKh 0.3125\n",
      "Trained batch 72 batch loss 0.563700259 batch mAP 0.497619629 batch PCKh 0.375\n",
      "Trained batch 73 batch loss 0.565178037 batch mAP 0.528686523 batch PCKh 0.625\n",
      "Trained batch 74 batch loss 0.596019626 batch mAP 0.528076172 batch PCKh 0.375\n",
      "Trained batch 75 batch loss 0.519149125 batch mAP 0.513824463 batch PCKh 0.4375\n",
      "Trained batch 76 batch loss 0.589605451 batch mAP 0.471008301 batch PCKh 0.5625\n",
      "Trained batch 77 batch loss 0.600852847 batch mAP 0.454833984 batch PCKh 0.4375\n",
      "Trained batch 78 batch loss 0.656852782 batch mAP 0.48046875 batch PCKh 0.3125\n",
      "Trained batch 79 batch loss 0.672986865 batch mAP 0.427429199 batch PCKh 0.1875\n",
      "Trained batch 80 batch loss 0.66060555 batch mAP 0.441345215 batch PCKh 0\n",
      "Trained batch 81 batch loss 0.671615601 batch mAP 0.469024658 batch PCKh 0.625\n",
      "Trained batch 82 batch loss 0.59281683 batch mAP 0.458984375 batch PCKh 0.4375\n",
      "Trained batch 83 batch loss 0.671946347 batch mAP 0.423797607 batch PCKh 0.375\n",
      "Trained batch 84 batch loss 0.646276772 batch mAP 0.4296875 batch PCKh 0.625\n",
      "Trained batch 85 batch loss 0.702596962 batch mAP 0.384613037 batch PCKh 0.3125\n",
      "Trained batch 86 batch loss 0.62572515 batch mAP 0.369232178 batch PCKh 0.375\n",
      "Trained batch 87 batch loss 0.665753722 batch mAP 0.310516357 batch PCKh 0.875\n",
      "Trained batch 88 batch loss 0.657993197 batch mAP 0.277313232 batch PCKh 0.375\n",
      "Trained batch 89 batch loss 0.635637403 batch mAP 0.219177246 batch PCKh 0.5\n",
      "Trained batch 90 batch loss 0.65568459 batch mAP 0.275054932 batch PCKh 0.4375\n",
      "Trained batch 91 batch loss 0.661568165 batch mAP 0.283874512 batch PCKh 0.1875\n",
      "Trained batch 92 batch loss 0.659914672 batch mAP 0.2159729 batch PCKh 0.375\n",
      "Trained batch 93 batch loss 0.57433939 batch mAP 0.131622314 batch PCKh 0.4375\n",
      "Trained batch 94 batch loss 0.591257811 batch mAP 0.0698547363 batch PCKh 0.375\n",
      "Trained batch 95 batch loss 0.584524333 batch mAP 0.265289307 batch PCKh 0.5\n",
      "Trained batch 96 batch loss 0.584773362 batch mAP 0.340454102 batch PCKh 0.5625\n",
      "Trained batch 97 batch loss 0.548106909 batch mAP 0.334686279 batch PCKh 0.5625\n",
      "Trained batch 98 batch loss 0.557442069 batch mAP 0.36517334 batch PCKh 0.5625\n",
      "Trained batch 99 batch loss 0.527391911 batch mAP 0.479156494 batch PCKh 0.25\n",
      "Trained batch 100 batch loss 0.734003603 batch mAP 0.302978516 batch PCKh 0.0625\n",
      "Trained batch 101 batch loss 0.684471667 batch mAP 0.3699646 batch PCKh 0.5625\n",
      "Trained batch 102 batch loss 0.683326 batch mAP 0.385223389 batch PCKh 0.5\n",
      "Trained batch 103 batch loss 0.668164074 batch mAP 0.45413208 batch PCKh 0.25\n",
      "Trained batch 104 batch loss 0.733192801 batch mAP 0.389160156 batch PCKh 0.4375\n",
      "Trained batch 105 batch loss 0.787998259 batch mAP 0.227142334 batch PCKh 0.375\n",
      "Trained batch 106 batch loss 0.747235179 batch mAP 0.366210938 batch PCKh 0.125\n",
      "Trained batch 107 batch loss 0.743055701 batch mAP 0.358154297 batch PCKh 0.0625\n",
      "Trained batch 108 batch loss 0.747630358 batch mAP 0.359283447 batch PCKh 0.1875\n",
      "Trained batch 109 batch loss 0.657437801 batch mAP 0.304260254 batch PCKh 0.3125\n",
      "Trained batch 110 batch loss 0.633659184 batch mAP 0.29397583 batch PCKh 0.0625\n",
      "Trained batch 111 batch loss 0.699258566 batch mAP 0.279602051 batch PCKh 0.4375\n",
      "Trained batch 112 batch loss 0.707235157 batch mAP 0.296691895 batch PCKh 0.1875\n",
      "Trained batch 113 batch loss 0.630256772 batch mAP 0.208984375 batch PCKh 0.375\n",
      "Trained batch 114 batch loss 0.596186578 batch mAP 0.168640137 batch PCKh 0.625\n",
      "Trained batch 115 batch loss 0.612793565 batch mAP 0.228179932 batch PCKh 0.4375\n",
      "Trained batch 116 batch loss 0.62366879 batch mAP 0.42300415 batch PCKh 0.1875\n",
      "Trained batch 117 batch loss 0.549701691 batch mAP 0.502044678 batch PCKh 0.75\n",
      "Trained batch 118 batch loss 0.695815086 batch mAP 0.418701172 batch PCKh 0.1875\n",
      "Trained batch 119 batch loss 0.607661963 batch mAP 0.46661377 batch PCKh 0.375\n",
      "Trained batch 120 batch loss 0.696990132 batch mAP 0.458984375 batch PCKh 0.0625\n",
      "Trained batch 121 batch loss 0.633081079 batch mAP 0.5027771 batch PCKh 0.4375\n",
      "Trained batch 122 batch loss 0.670187235 batch mAP 0.486724854 batch PCKh 0.375\n",
      "Trained batch 123 batch loss 0.639063954 batch mAP 0.504852295 batch PCKh 0.375\n",
      "Trained batch 124 batch loss 0.653931737 batch mAP 0.498382568 batch PCKh 0.375\n",
      "Trained batch 125 batch loss 0.672023 batch mAP 0.500061035 batch PCKh 0.8125\n",
      "Trained batch 126 batch loss 0.65462935 batch mAP 0.488769531 batch PCKh 0.5\n",
      "Trained batch 127 batch loss 0.607118726 batch mAP 0.43850708 batch PCKh 0.3125\n",
      "Trained batch 128 batch loss 0.642368674 batch mAP 0.475311279 batch PCKh 0.875\n",
      "Trained batch 129 batch loss 0.570391238 batch mAP 0.461212158 batch PCKh 0.75\n",
      "Trained batch 130 batch loss 0.605806589 batch mAP 0.380615234 batch PCKh 0.0625\n",
      "Trained batch 131 batch loss 0.626828313 batch mAP 0.225769043 batch PCKh 0.5\n",
      "Trained batch 132 batch loss 0.631459 batch mAP 0.359344482 batch PCKh 0\n",
      "Trained batch 133 batch loss 0.626730084 batch mAP 0.458953857 batch PCKh 0.3125\n",
      "Trained batch 134 batch loss 0.752463102 batch mAP 0.507751465 batch PCKh 0.1875\n",
      "Trained batch 135 batch loss 0.678414702 batch mAP 0.54119873 batch PCKh 0.1875\n",
      "Trained batch 136 batch loss 0.674445152 batch mAP 0.508911133 batch PCKh 0\n",
      "Trained batch 137 batch loss 0.701979101 batch mAP 0.498748779 batch PCKh 0.25\n",
      "Trained batch 138 batch loss 0.660620451 batch mAP 0.48916626 batch PCKh 0.1875\n",
      "Trained batch 139 batch loss 0.730358303 batch mAP 0.413879395 batch PCKh 0.6875\n",
      "Trained batch 140 batch loss 0.653924465 batch mAP 0.401763916 batch PCKh 0.875\n",
      "Trained batch 141 batch loss 0.6510607 batch mAP 0.347717285 batch PCKh 0.3125\n",
      "Trained batch 142 batch loss 0.610104442 batch mAP 0.35534668 batch PCKh 0.625\n",
      "Trained batch 143 batch loss 0.658423305 batch mAP 0.30078125 batch PCKh 0.625\n",
      "Trained batch 144 batch loss 0.599961758 batch mAP 0.167327881 batch PCKh 0.125\n",
      "Trained batch 145 batch loss 0.590153396 batch mAP 0.24887085 batch PCKh 0.25\n",
      "Trained batch 146 batch loss 0.624696195 batch mAP 0.402740479 batch PCKh 0.375\n",
      "Trained batch 147 batch loss 0.647041917 batch mAP 0.396606445 batch PCKh 0.5625\n",
      "Trained batch 148 batch loss 0.676351964 batch mAP 0.455566406 batch PCKh 0.1875\n",
      "Trained batch 149 batch loss 0.652436 batch mAP 0.445281982 batch PCKh 0.4375\n",
      "Trained batch 150 batch loss 0.643035 batch mAP 0.468109131 batch PCKh 0.625\n",
      "Trained batch 151 batch loss 0.591861606 batch mAP 0.484008789 batch PCKh 0.4375\n",
      "Trained batch 152 batch loss 0.612862945 batch mAP 0.474304199 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 153 batch loss 0.622575641 batch mAP 0.492645264 batch PCKh 0.0625\n",
      "Trained batch 154 batch loss 0.649939299 batch mAP 0.486602783 batch PCKh 0.3125\n",
      "Trained batch 155 batch loss 0.595257699 batch mAP 0.488983154 batch PCKh 0.4375\n",
      "Trained batch 156 batch loss 0.642121851 batch mAP 0.471893311 batch PCKh 0.8125\n",
      "Trained batch 157 batch loss 0.630932868 batch mAP 0.408996582 batch PCKh 0.625\n",
      "Trained batch 158 batch loss 0.613053143 batch mAP 0.390045166 batch PCKh 0.0625\n",
      "Trained batch 159 batch loss 0.571960032 batch mAP 0.42300415 batch PCKh 0.3125\n",
      "Trained batch 160 batch loss 0.644550323 batch mAP 0.493377686 batch PCKh 0.5625\n",
      "Trained batch 161 batch loss 0.535369754 batch mAP 0.476654053 batch PCKh 0.6875\n",
      "Trained batch 162 batch loss 0.505489111 batch mAP 0.439147949 batch PCKh 0.75\n",
      "Trained batch 163 batch loss 0.502728164 batch mAP 0.474884033 batch PCKh 0.625\n",
      "Trained batch 164 batch loss 0.53475672 batch mAP 0.506958 batch PCKh 0.8125\n",
      "Trained batch 165 batch loss 0.517155766 batch mAP 0.4793396 batch PCKh 0.75\n",
      "Trained batch 166 batch loss 0.661043465 batch mAP 0.351898193 batch PCKh 0.4375\n",
      "Trained batch 167 batch loss 0.668527961 batch mAP 0.443847656 batch PCKh 0.5625\n",
      "Trained batch 168 batch loss 0.65633142 batch mAP 0.438659668 batch PCKh 0.5\n",
      "Trained batch 169 batch loss 0.671475768 batch mAP 0.406890869 batch PCKh 0.875\n",
      "Trained batch 170 batch loss 0.694165111 batch mAP 0.385162354 batch PCKh 0.1875\n",
      "Trained batch 171 batch loss 0.695078433 batch mAP 0.448059082 batch PCKh 0.625\n",
      "Trained batch 172 batch loss 0.670859575 batch mAP 0.414215088 batch PCKh 0.25\n",
      "Trained batch 173 batch loss 0.635832608 batch mAP 0.407653809 batch PCKh 0.25\n",
      "Trained batch 174 batch loss 0.65943 batch mAP 0.343170166 batch PCKh 0.1875\n",
      "Trained batch 175 batch loss 0.664976597 batch mAP 0.286560059 batch PCKh 0.75\n",
      "Trained batch 176 batch loss 0.689613163 batch mAP 0.314117432 batch PCKh 0.5625\n",
      "Trained batch 177 batch loss 0.691402555 batch mAP 0.169586182 batch PCKh 0.875\n",
      "Trained batch 178 batch loss 0.660504282 batch mAP 0.0761413574 batch PCKh 0.1875\n",
      "Trained batch 179 batch loss 0.657129943 batch mAP 0.0919494629 batch PCKh 0.5\n",
      "Trained batch 180 batch loss 0.584187269 batch mAP 0.111694336 batch PCKh 0.125\n",
      "Trained batch 181 batch loss 0.636644125 batch mAP 0.305206299 batch PCKh 0.125\n",
      "Trained batch 182 batch loss 0.753118098 batch mAP 0.395446777 batch PCKh 0.125\n",
      "Trained batch 183 batch loss 0.737207294 batch mAP 0.389801025 batch PCKh 0.125\n",
      "Trained batch 184 batch loss 0.711291313 batch mAP 0.441711426 batch PCKh 0.8125\n",
      "Trained batch 185 batch loss 0.723109305 batch mAP 0.444335938 batch PCKh 0\n",
      "Trained batch 186 batch loss 0.70773375 batch mAP 0.466918945 batch PCKh 0.375\n",
      "Trained batch 187 batch loss 0.655855894 batch mAP 0.451416016 batch PCKh 0.3125\n",
      "Trained batch 188 batch loss 0.629228234 batch mAP 0.451446533 batch PCKh 0.5\n",
      "Trained batch 189 batch loss 0.65318507 batch mAP 0.460449219 batch PCKh 0.875\n",
      "Trained batch 190 batch loss 0.625133872 batch mAP 0.456054688 batch PCKh 0.875\n",
      "Trained batch 191 batch loss 0.641534328 batch mAP 0.419494629 batch PCKh 0.75\n",
      "Trained batch 192 batch loss 0.598270893 batch mAP 0.391662598 batch PCKh 0.625\n",
      "Trained batch 193 batch loss 0.651194215 batch mAP 0.430511475 batch PCKh 0.6875\n",
      "Trained batch 194 batch loss 0.570591 batch mAP 0.416320801 batch PCKh 0.625\n",
      "Trained batch 195 batch loss 0.677146316 batch mAP 0.325073242 batch PCKh 0.5625\n",
      "Trained batch 196 batch loss 0.694668531 batch mAP 0.407104492 batch PCKh 0.5625\n",
      "Trained batch 197 batch loss 0.689216852 batch mAP 0.435699463 batch PCKh 0.1875\n",
      "Trained batch 198 batch loss 0.6745013 batch mAP 0.459960938 batch PCKh 0.625\n",
      "Trained batch 199 batch loss 0.778695 batch mAP 0.389373779 batch PCKh 0\n",
      "Trained batch 200 batch loss 0.77439332 batch mAP 0.425292969 batch PCKh 0.125\n",
      "Trained batch 201 batch loss 0.775274694 batch mAP 0.382904053 batch PCKh 0.3125\n",
      "Trained batch 202 batch loss 0.774423122 batch mAP 0.438690186 batch PCKh 0.25\n",
      "Trained batch 203 batch loss 0.532303512 batch mAP 0.237365723 batch PCKh 0.5625\n",
      "Trained batch 204 batch loss 0.662072659 batch mAP 0.377655029 batch PCKh 0.1875\n",
      "Trained batch 205 batch loss 0.625897408 batch mAP 0.241973877 batch PCKh 0.125\n",
      "Trained batch 206 batch loss 0.717949331 batch mAP 0.32232666 batch PCKh 0\n",
      "Trained batch 207 batch loss 0.722903073 batch mAP 0.149719238 batch PCKh 0\n",
      "Trained batch 208 batch loss 0.611426234 batch mAP 0.0512390137 batch PCKh 0.3125\n",
      "Trained batch 209 batch loss 0.709737659 batch mAP 0.341674805 batch PCKh 0\n",
      "Trained batch 210 batch loss 0.68622911 batch mAP 0.284545898 batch PCKh 0.25\n",
      "Trained batch 211 batch loss 0.704925954 batch mAP 0.357299805 batch PCKh 0\n",
      "Trained batch 212 batch loss 0.720195949 batch mAP 0.399627686 batch PCKh 0.3125\n",
      "Trained batch 213 batch loss 0.778683245 batch mAP 0.409912109 batch PCKh 0.125\n",
      "Trained batch 214 batch loss 0.63741225 batch mAP 0.226776123 batch PCKh 0.375\n",
      "Trained batch 215 batch loss 0.619025707 batch mAP 0.243011475 batch PCKh 0.3125\n",
      "Trained batch 216 batch loss 0.535575211 batch mAP 0.350708 batch PCKh 0.25\n",
      "Trained batch 217 batch loss 0.494113743 batch mAP 0.460235596 batch PCKh 0.375\n",
      "Trained batch 218 batch loss 0.549205661 batch mAP 0.438690186 batch PCKh 0.6875\n",
      "Trained batch 219 batch loss 0.481353074 batch mAP 0.49029541 batch PCKh 0.25\n",
      "Trained batch 220 batch loss 0.480514407 batch mAP 0.499969482 batch PCKh 0\n",
      "Trained batch 221 batch loss 0.642340064 batch mAP 0.44921875 batch PCKh 0.625\n",
      "Trained batch 222 batch loss 0.671069503 batch mAP 0.330932617 batch PCKh 0.5\n",
      "Trained batch 223 batch loss 0.642350078 batch mAP 0.356933594 batch PCKh 0.625\n",
      "Trained batch 224 batch loss 0.755160511 batch mAP 0.12701416 batch PCKh 0.125\n",
      "Trained batch 225 batch loss 0.623395085 batch mAP 0.347137451 batch PCKh 0.75\n",
      "Trained batch 226 batch loss 0.612033367 batch mAP 0.337432861 batch PCKh 0.1875\n",
      "Trained batch 227 batch loss 0.737998605 batch mAP 0.311584473 batch PCKh 0.5\n",
      "Trained batch 228 batch loss 0.650784969 batch mAP 0.391937256 batch PCKh 0.75\n",
      "Trained batch 229 batch loss 0.629501939 batch mAP 0.419464111 batch PCKh 0.5625\n",
      "Trained batch 230 batch loss 0.668429255 batch mAP 0.498901367 batch PCKh 0.375\n",
      "Trained batch 231 batch loss 0.780664086 batch mAP 0.455200195 batch PCKh 0\n",
      "Trained batch 232 batch loss 0.668808103 batch mAP 0.496368408 batch PCKh 0.4375\n",
      "Trained batch 233 batch loss 0.634124756 batch mAP 0.437774658 batch PCKh 0.5\n",
      "Trained batch 234 batch loss 0.626475692 batch mAP 0.434326172 batch PCKh 0.6875\n",
      "Trained batch 235 batch loss 0.625267863 batch mAP 0.383056641 batch PCKh 0.5\n",
      "Trained batch 236 batch loss 0.717397451 batch mAP 0.433898926 batch PCKh 0.6875\n",
      "Trained batch 237 batch loss 0.715243 batch mAP 0.455444336 batch PCKh 0.0625\n",
      "Trained batch 238 batch loss 0.675015569 batch mAP 0.412628174 batch PCKh 0.5\n",
      "Trained batch 239 batch loss 0.739611626 batch mAP 0.40737915 batch PCKh 0.1875\n",
      "Trained batch 240 batch loss 0.599756837 batch mAP 0.40234375 batch PCKh 0.1875\n",
      "Trained batch 241 batch loss 0.670354903 batch mAP 0.438873291 batch PCKh 0.3125\n",
      "Trained batch 242 batch loss 0.59241128 batch mAP 0.442626953 batch PCKh 0.1875\n",
      "Trained batch 243 batch loss 0.602425277 batch mAP 0.436065674 batch PCKh 0.375\n",
      "Trained batch 244 batch loss 0.618650436 batch mAP 0.456176758 batch PCKh 0.1875\n",
      "Trained batch 245 batch loss 0.601878822 batch mAP 0.489044189 batch PCKh 0.375\n",
      "Trained batch 246 batch loss 0.520685554 batch mAP 0.44833374 batch PCKh 0.1875\n",
      "Trained batch 247 batch loss 0.503392637 batch mAP 0.430328369 batch PCKh 0.1875\n",
      "Trained batch 248 batch loss 0.588660061 batch mAP 0.426513672 batch PCKh 0.1875\n",
      "Trained batch 249 batch loss 0.554688871 batch mAP 0.409759521 batch PCKh 0.125\n",
      "Trained batch 250 batch loss 0.56045866 batch mAP 0.4659729 batch PCKh 0.375\n",
      "Trained batch 251 batch loss 0.702249885 batch mAP 0.388000488 batch PCKh 0.0625\n",
      "Trained batch 252 batch loss 0.637566447 batch mAP 0.502838135 batch PCKh 0.1875\n",
      "Trained batch 253 batch loss 0.628665924 batch mAP 0.503692627 batch PCKh 0.3125\n",
      "Trained batch 254 batch loss 0.648671508 batch mAP 0.396057129 batch PCKh 0.5625\n",
      "Trained batch 255 batch loss 0.668928087 batch mAP 0.409881592 batch PCKh 0\n",
      "Trained batch 256 batch loss 0.671139479 batch mAP 0.42565918 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 257 batch loss 0.713271 batch mAP 0.448120117 batch PCKh 0.75\n",
      "Trained batch 258 batch loss 0.663538 batch mAP 0.430603027 batch PCKh 0.625\n",
      "Trained batch 259 batch loss 0.622926176 batch mAP 0.44744873 batch PCKh 0.75\n",
      "Trained batch 260 batch loss 0.622670233 batch mAP 0.441070557 batch PCKh 0.5625\n",
      "Trained batch 261 batch loss 0.600261 batch mAP 0.427093506 batch PCKh 0.625\n",
      "Trained batch 262 batch loss 0.685355783 batch mAP 0.372314453 batch PCKh 0.4375\n",
      "Trained batch 263 batch loss 0.695520401 batch mAP 0.446807861 batch PCKh 0.6875\n",
      "Trained batch 264 batch loss 0.672185421 batch mAP 0.4296875 batch PCKh 0.5\n",
      "Trained batch 265 batch loss 0.58958745 batch mAP 0.452514648 batch PCKh 0.5625\n",
      "Trained batch 266 batch loss 0.636887074 batch mAP 0.45022583 batch PCKh 0.5625\n",
      "Trained batch 267 batch loss 0.659144044 batch mAP 0.419311523 batch PCKh 0.3125\n",
      "Trained batch 268 batch loss 0.623919547 batch mAP 0.41305542 batch PCKh 0.375\n",
      "Trained batch 269 batch loss 0.662948549 batch mAP 0.465393066 batch PCKh 0.75\n",
      "Trained batch 270 batch loss 0.613346696 batch mAP 0.380310059 batch PCKh 0.5625\n",
      "Trained batch 271 batch loss 0.691265643 batch mAP 0.431915283 batch PCKh 0.75\n",
      "Trained batch 272 batch loss 0.660760522 batch mAP 0.409454346 batch PCKh 0.6875\n",
      "Trained batch 273 batch loss 0.629205 batch mAP 0.427825928 batch PCKh 0.8125\n",
      "Trained batch 274 batch loss 0.633175552 batch mAP 0.428985596 batch PCKh 0.8125\n",
      "Trained batch 275 batch loss 0.613432586 batch mAP 0.45715332 batch PCKh 0.25\n",
      "Trained batch 276 batch loss 0.622391164 batch mAP 0.421478271 batch PCKh 0.1875\n",
      "Trained batch 277 batch loss 0.593995392 batch mAP 0.464904785 batch PCKh 0.625\n",
      "Trained batch 278 batch loss 0.604299784 batch mAP 0.443573 batch PCKh 0\n",
      "Trained batch 279 batch loss 0.612393618 batch mAP 0.47454834 batch PCKh 0.625\n",
      "Trained batch 280 batch loss 0.593267 batch mAP 0.431030273 batch PCKh 0.5625\n",
      "Trained batch 281 batch loss 0.631963909 batch mAP 0.486694336 batch PCKh 0.5625\n",
      "Trained batch 282 batch loss 0.599183083 batch mAP 0.455047607 batch PCKh 0.625\n",
      "Trained batch 283 batch loss 0.689553 batch mAP 0.45614624 batch PCKh 0.875\n",
      "Trained batch 284 batch loss 0.717929423 batch mAP 0.498626709 batch PCKh 0.4375\n",
      "Trained batch 285 batch loss 0.676719844 batch mAP 0.370300293 batch PCKh 0.125\n",
      "Trained batch 286 batch loss 0.634110808 batch mAP 0.443054199 batch PCKh 0\n",
      "Trained batch 287 batch loss 0.635845244 batch mAP 0.455749512 batch PCKh 0.5\n",
      "Trained batch 288 batch loss 0.66997683 batch mAP 0.490875244 batch PCKh 0.25\n",
      "Trained batch 289 batch loss 0.644138098 batch mAP 0.455810547 batch PCKh 0.3125\n",
      "Trained batch 290 batch loss 0.671928227 batch mAP 0.490020752 batch PCKh 0.1875\n",
      "Trained batch 291 batch loss 0.588659167 batch mAP 0.424407959 batch PCKh 0.5625\n",
      "Trained batch 292 batch loss 0.644216478 batch mAP 0.428466797 batch PCKh 0.25\n",
      "Trained batch 293 batch loss 0.713078678 batch mAP 0.44732666 batch PCKh 0.6875\n",
      "Trained batch 294 batch loss 0.5821172 batch mAP 0.458251953 batch PCKh 0.25\n",
      "Trained batch 295 batch loss 0.570577621 batch mAP 0.442596436 batch PCKh 0.375\n",
      "Trained batch 296 batch loss 0.526261628 batch mAP 0.436584473 batch PCKh 0.5\n",
      "Trained batch 297 batch loss 0.59851 batch mAP 0.457641602 batch PCKh 0.75\n",
      "Trained batch 298 batch loss 0.638073325 batch mAP 0.4402771 batch PCKh 0.5\n",
      "Trained batch 299 batch loss 0.70390594 batch mAP 0.387786865 batch PCKh 0.1875\n",
      "Trained batch 300 batch loss 0.647683263 batch mAP 0.393829346 batch PCKh 0.625\n",
      "Trained batch 301 batch loss 0.704228044 batch mAP 0.421539307 batch PCKh 0\n",
      "Trained batch 302 batch loss 0.623600602 batch mAP 0.406036377 batch PCKh 0.25\n",
      "Trained batch 303 batch loss 0.645914793 batch mAP 0.422424316 batch PCKh 0.3125\n",
      "Trained batch 304 batch loss 0.662249446 batch mAP 0.454803467 batch PCKh 0.1875\n",
      "Trained batch 305 batch loss 0.69031024 batch mAP 0.479370117 batch PCKh 0.25\n",
      "Trained batch 306 batch loss 0.685432792 batch mAP 0.447784424 batch PCKh 0.375\n",
      "Trained batch 307 batch loss 0.780095935 batch mAP 0.44317627 batch PCKh 0.1875\n",
      "Trained batch 308 batch loss 0.690240741 batch mAP 0.471679688 batch PCKh 0.25\n",
      "Trained batch 309 batch loss 0.722395182 batch mAP 0.435974121 batch PCKh 0.25\n",
      "Trained batch 310 batch loss 0.623832881 batch mAP 0.459136963 batch PCKh 0.75\n",
      "Trained batch 311 batch loss 0.629415751 batch mAP 0.450408936 batch PCKh 0.4375\n",
      "Trained batch 312 batch loss 0.60545373 batch mAP 0.432006836 batch PCKh 0\n",
      "Trained batch 313 batch loss 0.645220101 batch mAP 0.348846436 batch PCKh 0\n",
      "Trained batch 314 batch loss 0.58770287 batch mAP 0.328857422 batch PCKh 0.5\n",
      "Trained batch 315 batch loss 0.612481296 batch mAP 0.366485596 batch PCKh 0.625\n",
      "Trained batch 316 batch loss 0.648655891 batch mAP 0.424499512 batch PCKh 0.5625\n",
      "Trained batch 317 batch loss 0.620717168 batch mAP 0.415771484 batch PCKh 0.4375\n",
      "Trained batch 318 batch loss 0.591452539 batch mAP 0.427215576 batch PCKh 0.5\n",
      "Trained batch 319 batch loss 0.675534248 batch mAP 0.450897217 batch PCKh 0.5625\n",
      "Trained batch 320 batch loss 0.596900582 batch mAP 0.389312744 batch PCKh 0.25\n",
      "Trained batch 321 batch loss 0.571117043 batch mAP 0.404205322 batch PCKh 0.4375\n",
      "Trained batch 322 batch loss 0.751652896 batch mAP 0.458251953 batch PCKh 0.3125\n",
      "Trained batch 323 batch loss 0.700633764 batch mAP 0.425689697 batch PCKh 0.5625\n",
      "Trained batch 324 batch loss 0.733093679 batch mAP 0.407653809 batch PCKh 0\n",
      "Trained batch 325 batch loss 0.559155 batch mAP 0.43560791 batch PCKh 0.25\n",
      "Trained batch 326 batch loss 0.590991199 batch mAP 0.451141357 batch PCKh 0.125\n",
      "Trained batch 327 batch loss 0.629704237 batch mAP 0.454956055 batch PCKh 0.75\n",
      "Trained batch 328 batch loss 0.679101288 batch mAP 0.477813721 batch PCKh 0\n",
      "Trained batch 329 batch loss 0.695949078 batch mAP 0.435577393 batch PCKh 0.125\n",
      "Trained batch 330 batch loss 0.62991 batch mAP 0.457458496 batch PCKh 0.3125\n",
      "Trained batch 331 batch loss 0.629268587 batch mAP 0.473907471 batch PCKh 0.625\n",
      "Trained batch 332 batch loss 0.542827964 batch mAP 0.47644043 batch PCKh 0.375\n",
      "Trained batch 333 batch loss 0.617457867 batch mAP 0.491882324 batch PCKh 0.375\n",
      "Trained batch 334 batch loss 0.565205455 batch mAP 0.490966797 batch PCKh 0.3125\n",
      "Trained batch 335 batch loss 0.639369369 batch mAP 0.501678467 batch PCKh 0.5\n",
      "Trained batch 336 batch loss 0.644062161 batch mAP 0.504058838 batch PCKh 0.375\n",
      "Trained batch 337 batch loss 0.620405853 batch mAP 0.533050537 batch PCKh 0.3125\n",
      "Trained batch 338 batch loss 0.58956027 batch mAP 0.544403076 batch PCKh 0.5625\n",
      "Trained batch 339 batch loss 0.582131 batch mAP 0.534973145 batch PCKh 0.5\n",
      "Trained batch 340 batch loss 0.528922 batch mAP 0.490692139 batch PCKh 0.4375\n",
      "Trained batch 341 batch loss 0.599107444 batch mAP 0.508026123 batch PCKh 0.375\n",
      "Trained batch 342 batch loss 0.545656681 batch mAP 0.542144775 batch PCKh 0.375\n",
      "Trained batch 343 batch loss 0.58249855 batch mAP 0.519683838 batch PCKh 0.8125\n",
      "Trained batch 344 batch loss 0.595064044 batch mAP 0.471099854 batch PCKh 0.5625\n",
      "Trained batch 345 batch loss 0.661085427 batch mAP 0.391540527 batch PCKh 0.4375\n",
      "Trained batch 346 batch loss 0.632875562 batch mAP 0.331237793 batch PCKh 0.75\n",
      "Trained batch 347 batch loss 0.698120713 batch mAP 0.345489502 batch PCKh 0.5\n",
      "Trained batch 348 batch loss 0.664290667 batch mAP 0.203125 batch PCKh 0.5\n",
      "Trained batch 349 batch loss 0.575196445 batch mAP 0.505584717 batch PCKh 0.5625\n",
      "Trained batch 350 batch loss 0.664650798 batch mAP 0.464386 batch PCKh 0.3125\n",
      "Trained batch 351 batch loss 0.667986333 batch mAP 0.400024414 batch PCKh 0.3125\n",
      "Trained batch 352 batch loss 0.717351079 batch mAP 0.302124023 batch PCKh 0.5\n",
      "Trained batch 353 batch loss 0.8382954 batch mAP 0.368103027 batch PCKh 0\n",
      "Trained batch 354 batch loss 0.6930933 batch mAP 0.388214111 batch PCKh 0\n",
      "Trained batch 355 batch loss 0.684311628 batch mAP 0.43460083 batch PCKh 0.3125\n",
      "Trained batch 356 batch loss 0.651138484 batch mAP 0.404449463 batch PCKh 0.4375\n",
      "Trained batch 357 batch loss 0.694083214 batch mAP 0.372314453 batch PCKh 0.6875\n",
      "Trained batch 358 batch loss 0.701657832 batch mAP 0.332702637 batch PCKh 0.3125\n",
      "Trained batch 359 batch loss 0.611187 batch mAP 0.263824463 batch PCKh 0.4375\n",
      "Trained batch 360 batch loss 0.59212172 batch mAP 0.28414917 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 361 batch loss 0.605831 batch mAP 0.328094482 batch PCKh 0.4375\n",
      "Trained batch 362 batch loss 0.654099345 batch mAP 0.35144043 batch PCKh 0.25\n",
      "Trained batch 363 batch loss 0.57848537 batch mAP 0.357513428 batch PCKh 0.125\n",
      "Trained batch 364 batch loss 0.542662263 batch mAP 0.370605469 batch PCKh 0.3125\n",
      "Trained batch 365 batch loss 0.671949744 batch mAP 0.483215332 batch PCKh 0.75\n",
      "Trained batch 366 batch loss 0.690475464 batch mAP 0.497680664 batch PCKh 0.1875\n",
      "Trained batch 367 batch loss 0.668181896 batch mAP 0.495636 batch PCKh 0.5\n",
      "Trained batch 368 batch loss 0.615139127 batch mAP 0.516235352 batch PCKh 0.5625\n",
      "Trained batch 369 batch loss 0.569168 batch mAP 0.472625732 batch PCKh 0.4375\n",
      "Trained batch 370 batch loss 0.59697473 batch mAP 0.438385 batch PCKh 0.1875\n",
      "Trained batch 371 batch loss 0.585342526 batch mAP 0.467163086 batch PCKh 0.375\n",
      "Trained batch 372 batch loss 0.557559967 batch mAP 0.437255859 batch PCKh 0.1875\n",
      "Trained batch 373 batch loss 0.534796 batch mAP 0.498657227 batch PCKh 0.4375\n",
      "Trained batch 374 batch loss 0.510447919 batch mAP 0.481079102 batch PCKh 0.1875\n",
      "Trained batch 375 batch loss 0.526612282 batch mAP 0.529327393 batch PCKh 0.3125\n",
      "Trained batch 376 batch loss 0.526217818 batch mAP 0.532470703 batch PCKh 0.375\n",
      "Trained batch 377 batch loss 0.498175979 batch mAP 0.526550293 batch PCKh 0.5\n",
      "Trained batch 378 batch loss 0.700135469 batch mAP 0.350067139 batch PCKh 0.375\n",
      "Trained batch 379 batch loss 0.737331152 batch mAP 0.130126953 batch PCKh 0.125\n",
      "Trained batch 380 batch loss 0.780408502 batch mAP 0.0919799805 batch PCKh 0.25\n",
      "Trained batch 381 batch loss 0.821006536 batch mAP 0.0793762207 batch PCKh 0.5\n",
      "Trained batch 382 batch loss 0.756832 batch mAP 0.102783203 batch PCKh 0.6875\n",
      "Trained batch 383 batch loss 0.738058329 batch mAP 0.241424561 batch PCKh 0.5625\n",
      "Trained batch 384 batch loss 0.677538872 batch mAP 0.419128418 batch PCKh 0.875\n",
      "Trained batch 385 batch loss 0.66532886 batch mAP 0.386474609 batch PCKh 0.625\n",
      "Trained batch 386 batch loss 0.622958779 batch mAP 0.373046875 batch PCKh 0.6875\n",
      "Trained batch 387 batch loss 0.702164054 batch mAP 0.367004395 batch PCKh 0.625\n",
      "Trained batch 388 batch loss 0.795124531 batch mAP 0.357269287 batch PCKh 0.6875\n",
      "Trained batch 389 batch loss 0.763009846 batch mAP 0.304992676 batch PCKh 0.1875\n",
      "Trained batch 390 batch loss 0.727392912 batch mAP 0.226776123 batch PCKh 0.0625\n",
      "Trained batch 391 batch loss 0.700364769 batch mAP 0.0760498047 batch PCKh 0.125\n",
      "Trained batch 392 batch loss 0.713074148 batch mAP 0.147491455 batch PCKh 0.1875\n",
      "Trained batch 393 batch loss 0.686658 batch mAP 0.104827881 batch PCKh 0\n",
      "Trained batch 394 batch loss 0.618146718 batch mAP 0.173065186 batch PCKh 0\n",
      "Trained batch 395 batch loss 0.619843721 batch mAP 0.0975647 batch PCKh 0.75\n",
      "Trained batch 396 batch loss 0.620545268 batch mAP 0.2394104 batch PCKh 0.125\n",
      "Trained batch 397 batch loss 0.624801695 batch mAP 0.274108887 batch PCKh 0.5625\n",
      "Trained batch 398 batch loss 0.615900755 batch mAP 0.306091309 batch PCKh 0.75\n",
      "Trained batch 399 batch loss 0.68056041 batch mAP 0.354675293 batch PCKh 0\n",
      "Trained batch 400 batch loss 0.698754907 batch mAP 0.340362549 batch PCKh 0\n",
      "Trained batch 401 batch loss 0.613342285 batch mAP 0.341308594 batch PCKh 0\n",
      "Trained batch 402 batch loss 0.616493344 batch mAP 0.365020752 batch PCKh 0.125\n",
      "Trained batch 403 batch loss 0.648320436 batch mAP 0.34588623 batch PCKh 0.1875\n",
      "Trained batch 404 batch loss 0.643417537 batch mAP 0.377105713 batch PCKh 0.4375\n",
      "Trained batch 405 batch loss 0.699418247 batch mAP 0.33404541 batch PCKh 0.6875\n",
      "Trained batch 406 batch loss 0.606711328 batch mAP 0.416412354 batch PCKh 0.75\n",
      "Trained batch 407 batch loss 0.687812388 batch mAP 0.420532227 batch PCKh 0.5625\n",
      "Trained batch 408 batch loss 0.63 batch mAP 0.477783203 batch PCKh 0.75\n",
      "Trained batch 409 batch loss 0.703206658 batch mAP 0.39239502 batch PCKh 0.125\n",
      "Trained batch 410 batch loss 0.658164501 batch mAP 0.444641113 batch PCKh 0.5625\n",
      "Trained batch 411 batch loss 0.727132082 batch mAP 0.49017334 batch PCKh 0.625\n",
      "Trained batch 412 batch loss 0.689843059 batch mAP 0.459960938 batch PCKh 0.5625\n",
      "Trained batch 413 batch loss 0.625615835 batch mAP 0.501220703 batch PCKh 0.375\n",
      "Trained batch 414 batch loss 0.645959616 batch mAP 0.421905518 batch PCKh 0.1875\n",
      "Trained batch 415 batch loss 0.687389731 batch mAP 0.415252686 batch PCKh 0.5625\n",
      "Trained batch 416 batch loss 0.623992383 batch mAP 0.408660889 batch PCKh 0.75\n",
      "Trained batch 417 batch loss 0.6156708 batch mAP 0.436187744 batch PCKh 0.75\n",
      "Trained batch 418 batch loss 0.635273337 batch mAP 0.439758301 batch PCKh 0.75\n",
      "Trained batch 419 batch loss 0.571869969 batch mAP 0.466094971 batch PCKh 0.5625\n",
      "Trained batch 420 batch loss 0.673777461 batch mAP 0.44732666 batch PCKh 0.125\n",
      "Trained batch 421 batch loss 0.553229 batch mAP 0.451812744 batch PCKh 0.5625\n",
      "Trained batch 422 batch loss 0.647716761 batch mAP 0.477172852 batch PCKh 0.4375\n",
      "Trained batch 423 batch loss 0.656186104 batch mAP 0.426269531 batch PCKh 0.6875\n",
      "Trained batch 424 batch loss 0.625168681 batch mAP 0.471496582 batch PCKh 0.5625\n",
      "Trained batch 425 batch loss 0.616355956 batch mAP 0.460876465 batch PCKh 0.375\n",
      "Trained batch 426 batch loss 0.599224746 batch mAP 0.460571289 batch PCKh 0.4375\n",
      "Trained batch 427 batch loss 0.674307644 batch mAP 0.46395874 batch PCKh 0.375\n",
      "Trained batch 428 batch loss 0.597854674 batch mAP 0.508667 batch PCKh 0.4375\n",
      "Trained batch 429 batch loss 0.684824288 batch mAP 0.451477051 batch PCKh 0.75\n",
      "Trained batch 430 batch loss 0.608686566 batch mAP 0.46270752 batch PCKh 0.5\n",
      "Trained batch 431 batch loss 0.599152565 batch mAP 0.454772949 batch PCKh 0.0625\n",
      "Trained batch 432 batch loss 0.603063583 batch mAP 0.415740967 batch PCKh 0.125\n",
      "Trained batch 433 batch loss 0.624613822 batch mAP 0.51272583 batch PCKh 0.1875\n",
      "Trained batch 434 batch loss 0.620679617 batch mAP 0.43560791 batch PCKh 0.375\n",
      "Trained batch 435 batch loss 0.59895432 batch mAP 0.46472168 batch PCKh 0.3125\n",
      "Trained batch 436 batch loss 0.61907506 batch mAP 0.448303223 batch PCKh 0.75\n",
      "Trained batch 437 batch loss 0.589505434 batch mAP 0.436737061 batch PCKh 0.625\n",
      "Trained batch 438 batch loss 0.54977119 batch mAP 0.430999756 batch PCKh 0.5625\n",
      "Trained batch 439 batch loss 0.58466512 batch mAP 0.469665527 batch PCKh 0.3125\n",
      "Trained batch 440 batch loss 0.626814067 batch mAP 0.460388184 batch PCKh 0.5625\n",
      "Trained batch 441 batch loss 0.615611434 batch mAP 0.439086914 batch PCKh 0.6875\n",
      "Trained batch 442 batch loss 0.581932068 batch mAP 0.451293945 batch PCKh 0.25\n",
      "Trained batch 443 batch loss 0.592462897 batch mAP 0.455993652 batch PCKh 0.25\n",
      "Trained batch 444 batch loss 0.569477856 batch mAP 0.466796875 batch PCKh 0.5\n",
      "Trained batch 445 batch loss 0.658483565 batch mAP 0.371368408 batch PCKh 0.75\n",
      "Trained batch 446 batch loss 0.60387969 batch mAP 0.360076904 batch PCKh 0.6875\n",
      "Trained batch 447 batch loss 0.665908456 batch mAP 0.199920654 batch PCKh 0\n",
      "Trained batch 448 batch loss 0.713721216 batch mAP 0.324737549 batch PCKh 0\n",
      "Trained batch 449 batch loss 0.693643451 batch mAP 0.474029541 batch PCKh 0.75\n",
      "Trained batch 450 batch loss 0.626675844 batch mAP 0.526367188 batch PCKh 0.5\n",
      "Trained batch 451 batch loss 0.701492131 batch mAP 0.420379639 batch PCKh 0.3125\n",
      "Trained batch 452 batch loss 0.648513436 batch mAP 0.503448486 batch PCKh 0.25\n",
      "Trained batch 453 batch loss 0.668106079 batch mAP 0.448883057 batch PCKh 0.125\n",
      "Trained batch 454 batch loss 0.629421413 batch mAP 0.44543457 batch PCKh 0.25\n",
      "Trained batch 455 batch loss 0.61604178 batch mAP 0.382324219 batch PCKh 0.5625\n",
      "Trained batch 456 batch loss 0.587427735 batch mAP 0.456512451 batch PCKh 0.25\n",
      "Trained batch 457 batch loss 0.61048305 batch mAP 0.400939941 batch PCKh 0.5625\n",
      "Trained batch 458 batch loss 0.54908812 batch mAP 0.414611816 batch PCKh 0.75\n",
      "Trained batch 459 batch loss 0.539688647 batch mAP 0.451965332 batch PCKh 0.25\n",
      "Trained batch 460 batch loss 0.597303033 batch mAP 0.479797363 batch PCKh 0.75\n",
      "Trained batch 461 batch loss 0.553023219 batch mAP 0.469360352 batch PCKh 0.625\n",
      "Trained batch 462 batch loss 0.570766509 batch mAP 0.468292236 batch PCKh 0.5625\n",
      "Trained batch 463 batch loss 0.627589345 batch mAP 0.421264648 batch PCKh 0\n",
      "Trained batch 464 batch loss 0.660615683 batch mAP 0.420196533 batch PCKh 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 465 batch loss 0.662349105 batch mAP 0.395813 batch PCKh 0.8125\n",
      "Trained batch 466 batch loss 0.70552218 batch mAP 0.414886475 batch PCKh 0.8125\n",
      "Trained batch 467 batch loss 0.66529 batch mAP 0.378753662 batch PCKh 0.8125\n",
      "Trained batch 468 batch loss 0.646053433 batch mAP 0.257080078 batch PCKh 0.375\n",
      "Trained batch 469 batch loss 0.579949856 batch mAP 0.314361572 batch PCKh 0.375\n",
      "Trained batch 470 batch loss 0.556906044 batch mAP 0.317993164 batch PCKh 0.5\n",
      "Trained batch 471 batch loss 0.629086 batch mAP 0.399353027 batch PCKh 0\n",
      "Trained batch 472 batch loss 0.609191775 batch mAP 0.390197754 batch PCKh 0.0625\n",
      "Trained batch 473 batch loss 0.651519179 batch mAP 0.405517578 batch PCKh 0.125\n",
      "Trained batch 474 batch loss 0.662833333 batch mAP 0.387939453 batch PCKh 0.8125\n",
      "Trained batch 475 batch loss 0.58722651 batch mAP 0.362854 batch PCKh 0.3125\n",
      "Trained batch 476 batch loss 0.538491368 batch mAP 0.402435303 batch PCKh 0.3125\n",
      "Trained batch 477 batch loss 0.551555216 batch mAP 0.388885498 batch PCKh 0.5625\n",
      "Trained batch 478 batch loss 0.61976707 batch mAP 0.399658203 batch PCKh 0.625\n",
      "Trained batch 479 batch loss 0.569663048 batch mAP 0.379821777 batch PCKh 0.125\n",
      "Trained batch 480 batch loss 0.635306716 batch mAP 0.389953613 batch PCKh 0.25\n",
      "Trained batch 481 batch loss 0.63572681 batch mAP 0.408203125 batch PCKh 0.875\n",
      "Trained batch 482 batch loss 0.584390521 batch mAP 0.420684814 batch PCKh 0.5\n",
      "Trained batch 483 batch loss 0.639249206 batch mAP 0.385650635 batch PCKh 0.5625\n",
      "Trained batch 484 batch loss 0.542519689 batch mAP 0.292449951 batch PCKh 0.4375\n",
      "Trained batch 485 batch loss 0.541949809 batch mAP 0.267059326 batch PCKh 0.1875\n",
      "Trained batch 486 batch loss 0.539149284 batch mAP 0.325744629 batch PCKh 0.25\n",
      "Trained batch 487 batch loss 0.517157912 batch mAP 0.217773438 batch PCKh 0.25\n",
      "Trained batch 488 batch loss 0.629276633 batch mAP 0.312530518 batch PCKh 0.875\n",
      "Trained batch 489 batch loss 0.693296671 batch mAP 0.303009033 batch PCKh 0.6875\n",
      "Trained batch 490 batch loss 0.575843215 batch mAP 0.438446045 batch PCKh 0.5\n",
      "Trained batch 491 batch loss 0.546779394 batch mAP 0.376800537 batch PCKh 0.125\n",
      "Trained batch 492 batch loss 0.656961322 batch mAP 0.329803467 batch PCKh 0.5625\n",
      "Trained batch 493 batch loss 0.676712632 batch mAP 0.375823975 batch PCKh 0.75\n",
      "Trained batch 494 batch loss 0.589556694 batch mAP 0.512756348 batch PCKh 0.5\n",
      "Trained batch 495 batch loss 0.691148818 batch mAP 0.495178223 batch PCKh 0.5625\n",
      "Trained batch 496 batch loss 0.628702044 batch mAP 0.44052124 batch PCKh 0.3125\n",
      "Trained batch 497 batch loss 0.62874 batch mAP 0.428497314 batch PCKh 0.25\n",
      "Trained batch 498 batch loss 0.54435885 batch mAP 0.295898438 batch PCKh 0.6875\n",
      "Trained batch 499 batch loss 0.624446213 batch mAP 0.375640869 batch PCKh 0.5625\n",
      "Trained batch 500 batch loss 0.624076843 batch mAP 0.437683105 batch PCKh 0.5625\n",
      "Trained batch 501 batch loss 0.700043738 batch mAP 0.459625244 batch PCKh 0.25\n",
      "Trained batch 502 batch loss 0.623497 batch mAP 0.419677734 batch PCKh 0.3125\n",
      "Trained batch 503 batch loss 0.634106636 batch mAP 0.454528809 batch PCKh 0.875\n",
      "Trained batch 504 batch loss 0.687809467 batch mAP 0.461395264 batch PCKh 0.25\n",
      "Trained batch 505 batch loss 0.622801065 batch mAP 0.485565186 batch PCKh 0.5\n",
      "Trained batch 506 batch loss 0.591829538 batch mAP 0.479553223 batch PCKh 0.75\n",
      "Trained batch 507 batch loss 0.638566196 batch mAP 0.467315674 batch PCKh 0.0625\n",
      "Trained batch 508 batch loss 0.645617604 batch mAP 0.450683594 batch PCKh 0.125\n",
      "Trained batch 509 batch loss 0.612397909 batch mAP 0.473510742 batch PCKh 0.75\n",
      "Trained batch 510 batch loss 0.629269063 batch mAP 0.448272705 batch PCKh 0.1875\n",
      "Trained batch 511 batch loss 0.615562916 batch mAP 0.443359375 batch PCKh 0.1875\n",
      "Trained batch 512 batch loss 0.525421441 batch mAP 0.461395264 batch PCKh 0.625\n",
      "Trained batch 513 batch loss 0.521032631 batch mAP 0.444854736 batch PCKh 0.125\n",
      "Trained batch 514 batch loss 0.626884937 batch mAP 0.454315186 batch PCKh 0.4375\n",
      "Trained batch 515 batch loss 0.599190116 batch mAP 0.427337646 batch PCKh 0.1875\n",
      "Trained batch 516 batch loss 0.635962307 batch mAP 0.445770264 batch PCKh 0.8125\n",
      "Trained batch 517 batch loss 0.653935254 batch mAP 0.422241211 batch PCKh 0.625\n",
      "Trained batch 518 batch loss 0.613444686 batch mAP 0.451385498 batch PCKh 0.6875\n",
      "Trained batch 519 batch loss 0.56851995 batch mAP 0.490234375 batch PCKh 0.5625\n",
      "Trained batch 520 batch loss 0.654673278 batch mAP 0.448791504 batch PCKh 0.375\n",
      "Trained batch 521 batch loss 0.651761 batch mAP 0.43359375 batch PCKh 0.6875\n",
      "Trained batch 522 batch loss 0.563833952 batch mAP 0.494781494 batch PCKh 0.75\n",
      "Trained batch 523 batch loss 0.719769239 batch mAP 0.342865 batch PCKh 0.8125\n",
      "Trained batch 524 batch loss 0.618036091 batch mAP 0.457611084 batch PCKh 0.75\n",
      "Trained batch 525 batch loss 0.63587 batch mAP 0.428344727 batch PCKh 0.3125\n",
      "Trained batch 526 batch loss 0.555737138 batch mAP 0.466491699 batch PCKh 0.625\n",
      "Trained batch 527 batch loss 0.6052984 batch mAP 0.408874512 batch PCKh 0.4375\n",
      "Trained batch 528 batch loss 0.480317295 batch mAP 0.300231934 batch PCKh 0.5\n",
      "Trained batch 529 batch loss 0.591531396 batch mAP 0.41897583 batch PCKh 0.25\n",
      "Trained batch 530 batch loss 0.556775749 batch mAP 0.39175415 batch PCKh 0.375\n",
      "Trained batch 531 batch loss 0.618052244 batch mAP 0.422729492 batch PCKh 0.25\n",
      "Trained batch 532 batch loss 0.597263753 batch mAP 0.393463135 batch PCKh 0.25\n",
      "Trained batch 533 batch loss 0.564045966 batch mAP 0.399230957 batch PCKh 0.25\n",
      "Trained batch 534 batch loss 0.613009393 batch mAP 0.346282959 batch PCKh 0\n",
      "Trained batch 535 batch loss 0.620651722 batch mAP 0.360748291 batch PCKh 0.5\n",
      "Trained batch 536 batch loss 0.661958635 batch mAP 0.432098389 batch PCKh 0.125\n",
      "Trained batch 537 batch loss 0.606475055 batch mAP 0.385284424 batch PCKh 0.0625\n",
      "Trained batch 538 batch loss 0.547392547 batch mAP 0.402374268 batch PCKh 0.625\n",
      "Trained batch 539 batch loss 0.652762 batch mAP 0.455657959 batch PCKh 0.1875\n",
      "Trained batch 540 batch loss 0.607735574 batch mAP 0.441619873 batch PCKh 0.5625\n",
      "Trained batch 541 batch loss 0.687678099 batch mAP 0.467224121 batch PCKh 0.375\n",
      "Trained batch 542 batch loss 0.660318136 batch mAP 0.464599609 batch PCKh 0.25\n",
      "Trained batch 543 batch loss 0.60526669 batch mAP 0.435760498 batch PCKh 0.0625\n",
      "Trained batch 544 batch loss 0.672462165 batch mAP 0.330627441 batch PCKh 0.625\n",
      "Trained batch 545 batch loss 0.643729091 batch mAP 0.397521973 batch PCKh 0.125\n",
      "Trained batch 546 batch loss 0.700663507 batch mAP 0.368988037 batch PCKh 0.5\n",
      "Trained batch 547 batch loss 0.545633376 batch mAP 0.376708984 batch PCKh 0.4375\n",
      "Trained batch 548 batch loss 0.670121372 batch mAP 0.408630371 batch PCKh 0\n",
      "Trained batch 549 batch loss 0.540072918 batch mAP 0.468261719 batch PCKh 0.1875\n",
      "Trained batch 550 batch loss 0.548799932 batch mAP 0.49822998 batch PCKh 0.125\n",
      "Trained batch 551 batch loss 0.58618021 batch mAP 0.493682861 batch PCKh 0.5625\n",
      "Trained batch 552 batch loss 0.581165552 batch mAP 0.447601318 batch PCKh 0.75\n",
      "Trained batch 553 batch loss 0.664887607 batch mAP 0.447509766 batch PCKh 0.625\n",
      "Trained batch 554 batch loss 0.666490376 batch mAP 0.470581055 batch PCKh 0.375\n",
      "Trained batch 555 batch loss 0.670574784 batch mAP 0.417053223 batch PCKh 0.625\n",
      "Trained batch 556 batch loss 0.705172718 batch mAP 0.453735352 batch PCKh 0.25\n",
      "Trained batch 557 batch loss 0.712899566 batch mAP 0.413330078 batch PCKh 0.0625\n",
      "Trained batch 558 batch loss 0.65072155 batch mAP 0.459350586 batch PCKh 0.0625\n",
      "Trained batch 559 batch loss 0.619149685 batch mAP 0.443817139 batch PCKh 0.375\n",
      "Trained batch 560 batch loss 0.64747262 batch mAP 0.481445312 batch PCKh 0.3125\n",
      "Trained batch 561 batch loss 0.599992096 batch mAP 0.493713379 batch PCKh 0.1875\n",
      "Trained batch 562 batch loss 0.555082381 batch mAP 0.447814941 batch PCKh 0.6875\n",
      "Trained batch 563 batch loss 0.557309389 batch mAP 0.424591064 batch PCKh 0.1875\n",
      "Trained batch 564 batch loss 0.656376064 batch mAP 0.422271729 batch PCKh 0.1875\n",
      "Trained batch 565 batch loss 0.676968336 batch mAP 0.41897583 batch PCKh 0.1875\n",
      "Trained batch 566 batch loss 0.649368525 batch mAP 0.48236084 batch PCKh 0.5625\n",
      "Trained batch 567 batch loss 0.654869497 batch mAP 0.467224121 batch PCKh 0.375\n",
      "Trained batch 568 batch loss 0.637492537 batch mAP 0.506713867 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 569 batch loss 0.682140589 batch mAP 0.473938 batch PCKh 0.1875\n",
      "Trained batch 570 batch loss 0.661887109 batch mAP 0.508453369 batch PCKh 0.125\n",
      "Trained batch 571 batch loss 0.65692085 batch mAP 0.461486816 batch PCKh 0.3125\n",
      "Trained batch 572 batch loss 0.616127253 batch mAP 0.499969482 batch PCKh 0.1875\n",
      "Trained batch 573 batch loss 0.732350945 batch mAP 0.500091553 batch PCKh 0\n",
      "Trained batch 574 batch loss 0.763359249 batch mAP 0.461151123 batch PCKh 0\n",
      "Trained batch 575 batch loss 0.682831943 batch mAP 0.449493408 batch PCKh 0.125\n",
      "Trained batch 576 batch loss 0.696774483 batch mAP 0.425231934 batch PCKh 0.4375\n",
      "Trained batch 577 batch loss 0.602331281 batch mAP 0.402008057 batch PCKh 0.25\n",
      "Trained batch 578 batch loss 0.674004197 batch mAP 0.442230225 batch PCKh 0.125\n",
      "Trained batch 579 batch loss 0.580788255 batch mAP 0.436920166 batch PCKh 0.375\n",
      "Trained batch 580 batch loss 0.621197641 batch mAP 0.46105957 batch PCKh 0.3125\n",
      "Trained batch 581 batch loss 0.761449754 batch mAP 0.432556152 batch PCKh 0.375\n",
      "Trained batch 582 batch loss 0.747813225 batch mAP 0.44619751 batch PCKh 0.4375\n",
      "Trained batch 583 batch loss 0.695509255 batch mAP 0.422729492 batch PCKh 0.375\n",
      "Trained batch 584 batch loss 0.665606081 batch mAP 0.421936035 batch PCKh 0.875\n",
      "Trained batch 585 batch loss 0.601117432 batch mAP 0.421264648 batch PCKh 0.4375\n",
      "Trained batch 586 batch loss 0.61360997 batch mAP 0.426818848 batch PCKh 0.3125\n",
      "Trained batch 587 batch loss 0.615997136 batch mAP 0.439697266 batch PCKh 0.1875\n",
      "Trained batch 588 batch loss 0.619577169 batch mAP 0.453674316 batch PCKh 0.4375\n",
      "Trained batch 589 batch loss 0.602740765 batch mAP 0.466461182 batch PCKh 0.625\n",
      "Trained batch 590 batch loss 0.577284694 batch mAP 0.44821167 batch PCKh 0.5625\n",
      "Trained batch 591 batch loss 0.576243639 batch mAP 0.463195801 batch PCKh 0.3125\n",
      "Trained batch 592 batch loss 0.614849508 batch mAP 0.497009277 batch PCKh 0.25\n",
      "Trained batch 593 batch loss 0.654833436 batch mAP 0.465209961 batch PCKh 0.4375\n",
      "Trained batch 594 batch loss 0.618993521 batch mAP 0.496246338 batch PCKh 0.6875\n",
      "Trained batch 595 batch loss 0.649295211 batch mAP 0.500701904 batch PCKh 0.5\n",
      "Trained batch 596 batch loss 0.663878441 batch mAP 0.472137451 batch PCKh 0.1875\n",
      "Trained batch 597 batch loss 0.656912863 batch mAP 0.339691162 batch PCKh 0.5625\n",
      "Trained batch 598 batch loss 0.652189732 batch mAP 0.475769043 batch PCKh 0.1875\n",
      "Trained batch 599 batch loss 0.538811684 batch mAP 0.505493164 batch PCKh 0.3125\n",
      "Trained batch 600 batch loss 0.531831324 batch mAP 0.554260254 batch PCKh 0.125\n",
      "Trained batch 601 batch loss 0.645576477 batch mAP 0.587615967 batch PCKh 0.6875\n",
      "Trained batch 602 batch loss 0.614245832 batch mAP 0.576965332 batch PCKh 0.25\n",
      "Trained batch 603 batch loss 0.569625497 batch mAP 0.575897217 batch PCKh 0.625\n",
      "Trained batch 604 batch loss 0.598757148 batch mAP 0.56439209 batch PCKh 0.1875\n",
      "Trained batch 605 batch loss 0.636886299 batch mAP 0.486999512 batch PCKh 0.375\n",
      "Trained batch 606 batch loss 0.634651303 batch mAP 0.448181152 batch PCKh 0.1875\n",
      "Trained batch 607 batch loss 0.581663 batch mAP 0.497161865 batch PCKh 0.375\n",
      "Trained batch 608 batch loss 0.753843725 batch mAP 0.555664062 batch PCKh 0\n",
      "Trained batch 609 batch loss 0.756839216 batch mAP 0.4793396 batch PCKh 0.1875\n",
      "Trained batch 610 batch loss 0.72784555 batch mAP 0.54296875 batch PCKh 0.0625\n",
      "Trained batch 611 batch loss 0.746430457 batch mAP 0.529571533 batch PCKh 0\n",
      "Trained batch 612 batch loss 0.774299 batch mAP 0.456848145 batch PCKh 0.0625\n",
      "Trained batch 613 batch loss 0.61174 batch mAP 0.47164917 batch PCKh 0.125\n",
      "Trained batch 614 batch loss 0.536374 batch mAP 0.270141602 batch PCKh 0.625\n",
      "Trained batch 615 batch loss 0.578021586 batch mAP 0.228302 batch PCKh 0.6875\n",
      "Trained batch 616 batch loss 0.619704545 batch mAP 0.233093262 batch PCKh 0.625\n",
      "Trained batch 617 batch loss 0.59500736 batch mAP 0.252868652 batch PCKh 0.5\n",
      "Trained batch 618 batch loss 0.572374165 batch mAP 0.395050049 batch PCKh 0.5625\n",
      "Trained batch 619 batch loss 0.643817782 batch mAP 0.428466797 batch PCKh 0.3125\n",
      "Trained batch 620 batch loss 0.610563397 batch mAP 0.476989746 batch PCKh 0.6875\n",
      "Trained batch 621 batch loss 0.650973678 batch mAP 0.4168396 batch PCKh 0.5\n",
      "Trained batch 622 batch loss 0.618743539 batch mAP 0.477020264 batch PCKh 0.1875\n",
      "Trained batch 623 batch loss 0.606638193 batch mAP 0.477081299 batch PCKh 0.1875\n",
      "Trained batch 624 batch loss 0.630708575 batch mAP 0.471466064 batch PCKh 0.1875\n",
      "Trained batch 625 batch loss 0.642149568 batch mAP 0.428741455 batch PCKh 0.75\n",
      "Trained batch 626 batch loss 0.628275692 batch mAP 0.48828125 batch PCKh 0.3125\n",
      "Trained batch 627 batch loss 0.702263594 batch mAP 0.488342285 batch PCKh 0.5\n",
      "Trained batch 628 batch loss 0.6928913 batch mAP 0.487640381 batch PCKh 0\n",
      "Trained batch 629 batch loss 0.692811847 batch mAP 0.434448242 batch PCKh 0.25\n",
      "Trained batch 630 batch loss 0.707957208 batch mAP 0.479125977 batch PCKh 0.75\n",
      "Trained batch 631 batch loss 0.735266089 batch mAP 0.432067871 batch PCKh 0.3125\n",
      "Trained batch 632 batch loss 0.658052266 batch mAP 0.409057617 batch PCKh 0.375\n",
      "Trained batch 633 batch loss 0.624688864 batch mAP 0.180114746 batch PCKh 0.6875\n",
      "Trained batch 634 batch loss 0.599821091 batch mAP 0.0416564941 batch PCKh 0.5625\n",
      "Trained batch 635 batch loss 0.612691224 batch mAP 0.098449707 batch PCKh 0.25\n",
      "Trained batch 636 batch loss 0.597806811 batch mAP 0.237335205 batch PCKh 0.4375\n",
      "Trained batch 637 batch loss 0.604450822 batch mAP 0.231964111 batch PCKh 0.4375\n",
      "Trained batch 638 batch loss 0.543473542 batch mAP 0.269805908 batch PCKh 0.375\n",
      "Trained batch 639 batch loss 0.652673304 batch mAP 0.406433105 batch PCKh 0.3125\n",
      "Trained batch 640 batch loss 0.626163125 batch mAP 0.453216553 batch PCKh 0.375\n",
      "Trained batch 641 batch loss 0.621302783 batch mAP 0.547515869 batch PCKh 0.375\n",
      "Trained batch 642 batch loss 0.690670729 batch mAP 0.414581299 batch PCKh 0\n",
      "Trained batch 643 batch loss 0.571685731 batch mAP 0.470031738 batch PCKh 0.3125\n",
      "Trained batch 644 batch loss 0.642563939 batch mAP 0.469451904 batch PCKh 0.25\n",
      "Trained batch 645 batch loss 0.552073419 batch mAP 0.531799316 batch PCKh 0.5\n",
      "Trained batch 646 batch loss 0.592433691 batch mAP 0.562744141 batch PCKh 0.625\n",
      "Trained batch 647 batch loss 0.614836156 batch mAP 0.487762451 batch PCKh 0.4375\n",
      "Trained batch 648 batch loss 0.578607142 batch mAP 0.439056396 batch PCKh 0.75\n",
      "Trained batch 649 batch loss 0.599619 batch mAP 0.484924316 batch PCKh 0.5625\n",
      "Trained batch 650 batch loss 0.559969664 batch mAP 0.470550537 batch PCKh 0.5625\n",
      "Trained batch 651 batch loss 0.594189644 batch mAP 0.228515625 batch PCKh 0.3125\n",
      "Trained batch 652 batch loss 0.607974112 batch mAP 0.24786377 batch PCKh 0.1875\n",
      "Trained batch 653 batch loss 0.604587436 batch mAP 0.384033203 batch PCKh 0.25\n",
      "Trained batch 654 batch loss 0.645098209 batch mAP 0.338256836 batch PCKh 0.75\n",
      "Trained batch 655 batch loss 0.724165559 batch mAP 0.392486572 batch PCKh 0.1875\n",
      "Trained batch 656 batch loss 0.648873 batch mAP 0.455688477 batch PCKh 0.375\n",
      "Trained batch 657 batch loss 0.655882776 batch mAP 0.506958 batch PCKh 0.0625\n",
      "Trained batch 658 batch loss 0.581462502 batch mAP 0.541656494 batch PCKh 0.6875\n",
      "Trained batch 659 batch loss 0.552999198 batch mAP 0.543640137 batch PCKh 0.75\n",
      "Trained batch 660 batch loss 0.675492823 batch mAP 0.511291504 batch PCKh 0.75\n",
      "Trained batch 661 batch loss 0.673642039 batch mAP 0.48135376 batch PCKh 0.5\n",
      "Trained batch 662 batch loss 0.6379897 batch mAP 0.479614258 batch PCKh 0.4375\n",
      "Trained batch 663 batch loss 0.60029155 batch mAP 0.412017822 batch PCKh 0.6875\n",
      "Trained batch 664 batch loss 0.60677737 batch mAP 0.376861572 batch PCKh 0.3125\n",
      "Trained batch 665 batch loss 0.632427 batch mAP 0.457122803 batch PCKh 0.1875\n",
      "Trained batch 666 batch loss 0.648614705 batch mAP 0.383178711 batch PCKh 0.5\n",
      "Trained batch 667 batch loss 0.703828692 batch mAP 0.387390137 batch PCKh 0.625\n",
      "Trained batch 668 batch loss 0.687380791 batch mAP 0.398498535 batch PCKh 0.3125\n",
      "Trained batch 669 batch loss 0.694348037 batch mAP 0.427093506 batch PCKh 0.25\n",
      "Trained batch 670 batch loss 0.705115676 batch mAP 0.436340332 batch PCKh 0.1875\n",
      "Trained batch 671 batch loss 0.705573201 batch mAP 0.465911865 batch PCKh 0.5625\n",
      "Trained batch 672 batch loss 0.569805622 batch mAP 0.463470459 batch PCKh 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 673 batch loss 0.519440413 batch mAP 0.398376465 batch PCKh 0.5625\n",
      "Trained batch 674 batch loss 0.616256833 batch mAP 0.402404785 batch PCKh 0.5\n",
      "Trained batch 675 batch loss 0.699292779 batch mAP 0.431762695 batch PCKh 0.5625\n",
      "Trained batch 676 batch loss 0.667702198 batch mAP 0.437957764 batch PCKh 0.1875\n",
      "Trained batch 677 batch loss 0.642387509 batch mAP 0.50692749 batch PCKh 0.5625\n",
      "Trained batch 678 batch loss 0.583853245 batch mAP 0.468475342 batch PCKh 0.125\n",
      "Trained batch 679 batch loss 0.625474334 batch mAP 0.44354248 batch PCKh 0.125\n",
      "Trained batch 680 batch loss 0.689007699 batch mAP 0.424804688 batch PCKh 0\n",
      "Trained batch 681 batch loss 0.63139987 batch mAP 0.424041748 batch PCKh 0.1875\n",
      "Trained batch 682 batch loss 0.613212824 batch mAP 0.304992676 batch PCKh 0.75\n",
      "Trained batch 683 batch loss 0.596839666 batch mAP 0.243103027 batch PCKh 0.6875\n",
      "Trained batch 684 batch loss 0.637022138 batch mAP 0.370361328 batch PCKh 0.5625\n",
      "Trained batch 685 batch loss 0.570307076 batch mAP 0.387390137 batch PCKh 0.6875\n",
      "Trained batch 686 batch loss 0.647702217 batch mAP 0.450927734 batch PCKh 0.75\n",
      "Trained batch 687 batch loss 0.611433327 batch mAP 0.440185547 batch PCKh 0.8125\n",
      "Trained batch 688 batch loss 0.601419747 batch mAP 0.388366699 batch PCKh 0.1875\n",
      "Trained batch 689 batch loss 0.683688402 batch mAP 0.42199707 batch PCKh 0.625\n",
      "Trained batch 690 batch loss 0.726815104 batch mAP 0.384613037 batch PCKh 0\n",
      "Trained batch 691 batch loss 0.53367877 batch mAP 0.504333496 batch PCKh 0.1875\n",
      "Trained batch 692 batch loss 0.613912 batch mAP 0.487091064 batch PCKh 0.375\n",
      "Trained batch 693 batch loss 0.61812 batch mAP 0.457275391 batch PCKh 0.625\n",
      "Trained batch 694 batch loss 0.635624468 batch mAP 0.442657471 batch PCKh 0.0625\n",
      "Trained batch 695 batch loss 0.623557925 batch mAP 0.44720459 batch PCKh 0.25\n",
      "Trained batch 696 batch loss 0.634969 batch mAP 0.466491699 batch PCKh 0.625\n",
      "Trained batch 697 batch loss 0.656079054 batch mAP 0.442871094 batch PCKh 0.0625\n",
      "Trained batch 698 batch loss 0.694792747 batch mAP 0.438079834 batch PCKh 0.6875\n",
      "Trained batch 699 batch loss 0.619214535 batch mAP 0.41708374 batch PCKh 0.0625\n",
      "Trained batch 700 batch loss 0.61351788 batch mAP 0.382629395 batch PCKh 0.625\n",
      "Trained batch 701 batch loss 0.593949258 batch mAP 0.391693115 batch PCKh 0.75\n",
      "Trained batch 702 batch loss 0.629152298 batch mAP 0.465271 batch PCKh 0.625\n",
      "Trained batch 703 batch loss 0.638239801 batch mAP 0.327178955 batch PCKh 0.5625\n",
      "Trained batch 704 batch loss 0.735829592 batch mAP 0.445983887 batch PCKh 0.5625\n",
      "Trained batch 705 batch loss 0.629525065 batch mAP 0.442504883 batch PCKh 0.75\n",
      "Trained batch 706 batch loss 0.558282554 batch mAP 0.466766357 batch PCKh 0.125\n",
      "Trained batch 707 batch loss 0.620385349 batch mAP 0.490386963 batch PCKh 0.625\n",
      "Trained batch 708 batch loss 0.634185 batch mAP 0.443756104 batch PCKh 0.25\n",
      "Trained batch 709 batch loss 0.609840751 batch mAP 0.497680664 batch PCKh 0.625\n",
      "Trained batch 710 batch loss 0.642236114 batch mAP 0.47088623 batch PCKh 0.5625\n",
      "Trained batch 711 batch loss 0.551632166 batch mAP 0.452423096 batch PCKh 0.0625\n",
      "Trained batch 712 batch loss 0.617391825 batch mAP 0.423797607 batch PCKh 0.625\n",
      "Trained batch 713 batch loss 0.620494783 batch mAP 0.476348877 batch PCKh 0.375\n",
      "Trained batch 714 batch loss 0.585824966 batch mAP 0.476409912 batch PCKh 0.1875\n",
      "Trained batch 715 batch loss 0.571824312 batch mAP 0.390991211 batch PCKh 0.5625\n",
      "Trained batch 716 batch loss 0.605161369 batch mAP 0.52142334 batch PCKh 0.1875\n",
      "Trained batch 717 batch loss 0.61514473 batch mAP 0.518066406 batch PCKh 0.4375\n",
      "Trained batch 718 batch loss 0.607576728 batch mAP 0.458343506 batch PCKh 0.3125\n",
      "Trained batch 719 batch loss 0.666227102 batch mAP 0.504150391 batch PCKh 0.5625\n",
      "Trained batch 720 batch loss 0.661286354 batch mAP 0.480560303 batch PCKh 0.375\n",
      "Trained batch 721 batch loss 0.727279067 batch mAP 0.422027588 batch PCKh 0.75\n",
      "Trained batch 722 batch loss 0.775202572 batch mAP 0.454650879 batch PCKh 0.375\n",
      "Trained batch 723 batch loss 0.788939178 batch mAP 0.42678833 batch PCKh 0.125\n",
      "Trained batch 724 batch loss 0.681316614 batch mAP 0.437316895 batch PCKh 0.625\n",
      "Trained batch 725 batch loss 0.737851143 batch mAP 0.458282471 batch PCKh 0.625\n",
      "Trained batch 726 batch loss 0.641294122 batch mAP 0.436798096 batch PCKh 0.125\n",
      "Trained batch 727 batch loss 0.688262343 batch mAP 0.459289551 batch PCKh 0.5\n",
      "Trained batch 728 batch loss 0.713336289 batch mAP 0.414520264 batch PCKh 0.4375\n",
      "Trained batch 729 batch loss 0.612801492 batch mAP 0.373962402 batch PCKh 0.4375\n",
      "Trained batch 730 batch loss 0.588775396 batch mAP 0.349365234 batch PCKh 0.6875\n",
      "Trained batch 731 batch loss 0.558850884 batch mAP 0.390625 batch PCKh 0.4375\n",
      "Trained batch 732 batch loss 0.50302583 batch mAP 0.352874756 batch PCKh 0.1875\n",
      "Trained batch 733 batch loss 0.463494271 batch mAP 0.359924316 batch PCKh 0\n",
      "Trained batch 734 batch loss 0.49804464 batch mAP 0.373657227 batch PCKh 0.6875\n",
      "Trained batch 735 batch loss 0.474981 batch mAP 0.385894775 batch PCKh 0.5625\n",
      "Trained batch 736 batch loss 0.512707 batch mAP 0.413116455 batch PCKh 0.625\n",
      "Trained batch 737 batch loss 0.540545285 batch mAP 0.472167969 batch PCKh 0.5625\n",
      "Trained batch 738 batch loss 0.637681127 batch mAP 0.447662354 batch PCKh 0.75\n",
      "Trained batch 739 batch loss 0.665962875 batch mAP 0.338531494 batch PCKh 0.125\n",
      "Trained batch 740 batch loss 0.692134619 batch mAP 0.335021973 batch PCKh 0.125\n",
      "Trained batch 741 batch loss 0.567274094 batch mAP 0.455200195 batch PCKh 0.5625\n",
      "Trained batch 742 batch loss 0.585754752 batch mAP 0.367736816 batch PCKh 0.25\n",
      "Trained batch 743 batch loss 0.655023813 batch mAP 0.450195312 batch PCKh 0\n",
      "Trained batch 744 batch loss 0.625802517 batch mAP 0.477417 batch PCKh 0.1875\n",
      "Trained batch 745 batch loss 0.66196239 batch mAP 0.402435303 batch PCKh 0\n",
      "Trained batch 746 batch loss 0.683224678 batch mAP 0.472106934 batch PCKh 0.3125\n",
      "Trained batch 747 batch loss 0.625926316 batch mAP 0.468688965 batch PCKh 0.8125\n",
      "Trained batch 748 batch loss 0.643792391 batch mAP 0.46383667 batch PCKh 0.625\n",
      "Trained batch 749 batch loss 0.63954246 batch mAP 0.453674316 batch PCKh 0.375\n",
      "Trained batch 750 batch loss 0.664397299 batch mAP 0.452148438 batch PCKh 0.1875\n",
      "Trained batch 751 batch loss 0.57043767 batch mAP 0.445953369 batch PCKh 0.375\n",
      "Trained batch 752 batch loss 0.593848467 batch mAP 0.479736328 batch PCKh 0.8125\n",
      "Trained batch 753 batch loss 0.617105126 batch mAP 0.488555908 batch PCKh 0.75\n",
      "Trained batch 754 batch loss 0.582049251 batch mAP 0.531768799 batch PCKh 0.5\n",
      "Trained batch 755 batch loss 0.564002454 batch mAP 0.524169922 batch PCKh 0.3125\n",
      "Trained batch 756 batch loss 0.605710566 batch mAP 0.558776855 batch PCKh 0.25\n",
      "Trained batch 757 batch loss 0.573770881 batch mAP 0.570373535 batch PCKh 0.4375\n",
      "Trained batch 758 batch loss 0.644362092 batch mAP 0.477386475 batch PCKh 0.3125\n",
      "Trained batch 759 batch loss 0.688518763 batch mAP 0.482788086 batch PCKh 0.4375\n",
      "Trained batch 760 batch loss 0.690153718 batch mAP 0.402984619 batch PCKh 0.125\n",
      "Trained batch 761 batch loss 0.639114499 batch mAP 0.481689453 batch PCKh 0.5625\n",
      "Trained batch 762 batch loss 0.714602351 batch mAP 0.302703857 batch PCKh 0.125\n",
      "Trained batch 763 batch loss 0.658649325 batch mAP 0.515960693 batch PCKh 0.375\n",
      "Trained batch 764 batch loss 0.618780255 batch mAP 0.508911133 batch PCKh 0.375\n",
      "Trained batch 765 batch loss 0.633216918 batch mAP 0.488555908 batch PCKh 0.1875\n",
      "Trained batch 766 batch loss 0.685267866 batch mAP 0.473083496 batch PCKh 0.3125\n",
      "Trained batch 767 batch loss 0.669122577 batch mAP 0.45501709 batch PCKh 0.75\n",
      "Trained batch 768 batch loss 0.67007184 batch mAP 0.439697266 batch PCKh 0.25\n",
      "Trained batch 769 batch loss 0.667366445 batch mAP 0.443054199 batch PCKh 0.6875\n",
      "Trained batch 770 batch loss 0.6118837 batch mAP 0.435150146 batch PCKh 0.8125\n",
      "Trained batch 771 batch loss 0.685477376 batch mAP 0.42477417 batch PCKh 0.1875\n",
      "Trained batch 772 batch loss 0.584183872 batch mAP 0.406188965 batch PCKh 0.1875\n",
      "Trained batch 773 batch loss 0.619758844 batch mAP 0.443389893 batch PCKh 0.1875\n",
      "Trained batch 774 batch loss 0.583486855 batch mAP 0.384460449 batch PCKh 0.5625\n",
      "Trained batch 775 batch loss 0.662184477 batch mAP 0.457977295 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 776 batch loss 0.578327119 batch mAP 0.452362061 batch PCKh 0.1875\n",
      "Trained batch 777 batch loss 0.59088707 batch mAP 0.469451904 batch PCKh 0.75\n",
      "Trained batch 778 batch loss 0.579615 batch mAP 0.528442383 batch PCKh 0.625\n",
      "Trained batch 779 batch loss 0.634575546 batch mAP 0.512512207 batch PCKh 0.8125\n",
      "Trained batch 780 batch loss 0.670250654 batch mAP 0.439361572 batch PCKh 0.875\n",
      "Trained batch 781 batch loss 0.514765263 batch mAP 0.50402832 batch PCKh 0.1875\n",
      "Trained batch 782 batch loss 0.434246123 batch mAP 0.45135498 batch PCKh 0.1875\n",
      "Trained batch 783 batch loss 0.564083278 batch mAP 0.470794678 batch PCKh 0.6875\n",
      "Trained batch 784 batch loss 0.549562275 batch mAP 0.475006104 batch PCKh 0.5625\n",
      "Trained batch 785 batch loss 0.695382833 batch mAP 0.474731445 batch PCKh 0.1875\n",
      "Trained batch 786 batch loss 0.723614097 batch mAP 0.342376709 batch PCKh 0.1875\n",
      "Trained batch 787 batch loss 0.656154871 batch mAP 0.424285889 batch PCKh 0.375\n",
      "Trained batch 788 batch loss 0.653966546 batch mAP 0.4324646 batch PCKh 0.3125\n",
      "Trained batch 789 batch loss 0.656027079 batch mAP 0.483642578 batch PCKh 0.25\n",
      "Trained batch 790 batch loss 0.625965297 batch mAP 0.504730225 batch PCKh 0.625\n",
      "Trained batch 791 batch loss 0.654947042 batch mAP 0.516723633 batch PCKh 0.375\n",
      "Trained batch 792 batch loss 0.707145631 batch mAP 0.4637146 batch PCKh 0\n",
      "Trained batch 793 batch loss 0.693338275 batch mAP 0.410827637 batch PCKh 0.1875\n",
      "Trained batch 794 batch loss 0.644370615 batch mAP 0.460632324 batch PCKh 0.5\n",
      "Trained batch 795 batch loss 0.648061275 batch mAP 0.455627441 batch PCKh 0.0625\n",
      "Trained batch 796 batch loss 0.568290591 batch mAP 0.501037598 batch PCKh 0.5\n",
      "Trained batch 797 batch loss 0.613344431 batch mAP 0.489318848 batch PCKh 0.875\n",
      "Trained batch 798 batch loss 0.518877327 batch mAP 0.436462402 batch PCKh 0.8125\n",
      "Trained batch 799 batch loss 0.575092256 batch mAP 0.4581604 batch PCKh 0.75\n",
      "Trained batch 800 batch loss 0.583046496 batch mAP 0.472473145 batch PCKh 0.8125\n",
      "Trained batch 801 batch loss 0.66307652 batch mAP 0.440612793 batch PCKh 0.125\n",
      "Trained batch 802 batch loss 0.664222836 batch mAP 0.478027344 batch PCKh 0.5625\n",
      "Trained batch 803 batch loss 0.596561313 batch mAP 0.503936768 batch PCKh 0.75\n",
      "Trained batch 804 batch loss 0.573153496 batch mAP 0.519134521 batch PCKh 0.25\n",
      "Trained batch 805 batch loss 0.644331276 batch mAP 0.477600098 batch PCKh 0.3125\n",
      "Trained batch 806 batch loss 0.66665417 batch mAP 0.46673584 batch PCKh 0.6875\n",
      "Trained batch 807 batch loss 0.746029 batch mAP 0.387908936 batch PCKh 0.3125\n",
      "Trained batch 808 batch loss 0.666964889 batch mAP 0.478759766 batch PCKh 0.5\n",
      "Trained batch 809 batch loss 0.607016265 batch mAP 0.507415771 batch PCKh 0.5\n",
      "Trained batch 810 batch loss 0.684301853 batch mAP 0.429351807 batch PCKh 0.75\n",
      "Trained batch 811 batch loss 0.575686038 batch mAP 0.431030273 batch PCKh 0.4375\n",
      "Trained batch 812 batch loss 0.749070525 batch mAP 0.411010742 batch PCKh 0.5\n",
      "Trained batch 813 batch loss 0.654454827 batch mAP 0.401184082 batch PCKh 0.5625\n",
      "Trained batch 814 batch loss 0.700292766 batch mAP 0.322814941 batch PCKh 0.6875\n",
      "Trained batch 815 batch loss 0.7062 batch mAP 0.315643311 batch PCKh 0.4375\n",
      "Trained batch 816 batch loss 0.675881147 batch mAP 0.282775879 batch PCKh 0.3125\n",
      "Trained batch 817 batch loss 0.745104253 batch mAP 0.337371826 batch PCKh 0.4375\n",
      "Trained batch 818 batch loss 0.691309154 batch mAP 0.278076172 batch PCKh 0.625\n",
      "Trained batch 819 batch loss 0.638704717 batch mAP 0.219726562 batch PCKh 0.75\n",
      "Trained batch 820 batch loss 0.722761154 batch mAP 0.229309082 batch PCKh 0.375\n",
      "Trained batch 821 batch loss 0.737319291 batch mAP 0.241943359 batch PCKh 0.375\n",
      "Trained batch 822 batch loss 0.671189189 batch mAP 0.137542725 batch PCKh 0.3125\n",
      "Trained batch 823 batch loss 0.664456606 batch mAP 0.163909912 batch PCKh 0.5625\n",
      "Trained batch 824 batch loss 0.674438238 batch mAP 0.27142334 batch PCKh 0.4375\n",
      "Trained batch 825 batch loss 0.708346844 batch mAP 0.302825928 batch PCKh 0.3125\n",
      "Trained batch 826 batch loss 0.752172232 batch mAP 0.321044922 batch PCKh 0.625\n",
      "Trained batch 827 batch loss 0.667619467 batch mAP 0.327606201 batch PCKh 0.5625\n",
      "Trained batch 828 batch loss 0.651923418 batch mAP 0.359466553 batch PCKh 0.3125\n",
      "Trained batch 829 batch loss 0.630823374 batch mAP 0.385650635 batch PCKh 0.625\n",
      "Trained batch 830 batch loss 0.706388712 batch mAP 0.352294922 batch PCKh 0.0625\n",
      "Trained batch 831 batch loss 0.635236561 batch mAP 0.399932861 batch PCKh 0.6875\n",
      "Trained batch 832 batch loss 0.659007788 batch mAP 0.374816895 batch PCKh 0.625\n",
      "Trained batch 833 batch loss 0.654614 batch mAP 0.386810303 batch PCKh 0.6875\n",
      "Trained batch 834 batch loss 0.663817644 batch mAP 0.409240723 batch PCKh 0.6875\n",
      "Trained batch 835 batch loss 0.71186173 batch mAP 0.382385254 batch PCKh 0.1875\n",
      "Trained batch 836 batch loss 0.701459527 batch mAP 0.402038574 batch PCKh 0.625\n",
      "Trained batch 837 batch loss 0.577134848 batch mAP 0.443725586 batch PCKh 0.4375\n",
      "Trained batch 838 batch loss 0.571099401 batch mAP 0.456634521 batch PCKh 0.4375\n",
      "Trained batch 839 batch loss 0.616099536 batch mAP 0.501190186 batch PCKh 0.3125\n",
      "Trained batch 840 batch loss 0.589417577 batch mAP 0.466766357 batch PCKh 0.1875\n",
      "Trained batch 841 batch loss 0.603660226 batch mAP 0.40625 batch PCKh 0.25\n",
      "Trained batch 842 batch loss 0.555035472 batch mAP 0.410675049 batch PCKh 0.4375\n",
      "Trained batch 843 batch loss 0.505875409 batch mAP 0.374481201 batch PCKh 0.3125\n",
      "Trained batch 844 batch loss 0.51924926 batch mAP 0.368255615 batch PCKh 0.25\n",
      "Trained batch 845 batch loss 0.543746233 batch mAP 0.416778564 batch PCKh 0.625\n",
      "Trained batch 846 batch loss 0.543325126 batch mAP 0.377227783 batch PCKh 0.5\n",
      "Trained batch 847 batch loss 0.557388186 batch mAP 0.307250977 batch PCKh 0.5625\n",
      "Trained batch 848 batch loss 0.562623 batch mAP 0.353759766 batch PCKh 0.8125\n",
      "Trained batch 849 batch loss 0.583580911 batch mAP 0.319091797 batch PCKh 0.875\n",
      "Trained batch 850 batch loss 0.497312039 batch mAP 0.321624756 batch PCKh 0\n",
      "Trained batch 851 batch loss 0.449684262 batch mAP 0.442810059 batch PCKh 0\n",
      "Trained batch 852 batch loss 0.471615493 batch mAP 0.440216064 batch PCKh 0.75\n",
      "Trained batch 853 batch loss 0.473852932 batch mAP 0.461151123 batch PCKh 0\n",
      "Trained batch 854 batch loss 0.516051888 batch mAP 0.435119629 batch PCKh 0\n",
      "Trained batch 855 batch loss 0.46876353 batch mAP 0.46572876 batch PCKh 0\n",
      "Trained batch 856 batch loss 0.426106751 batch mAP 0.483062744 batch PCKh 0\n",
      "Trained batch 857 batch loss 0.532765388 batch mAP 0.485412598 batch PCKh 0.375\n",
      "Trained batch 858 batch loss 0.613123119 batch mAP 0.480804443 batch PCKh 0.625\n",
      "Trained batch 859 batch loss 0.59873879 batch mAP 0.476226807 batch PCKh 0.4375\n",
      "Trained batch 860 batch loss 0.674582839 batch mAP 0.486877441 batch PCKh 0.375\n",
      "Trained batch 861 batch loss 0.578754544 batch mAP 0.499389648 batch PCKh 0.25\n",
      "Trained batch 862 batch loss 0.688159108 batch mAP 0.493591309 batch PCKh 0.375\n",
      "Trained batch 863 batch loss 0.568699181 batch mAP 0.512329102 batch PCKh 0.1875\n",
      "Trained batch 864 batch loss 0.633269966 batch mAP 0.495574951 batch PCKh 0.125\n",
      "Trained batch 865 batch loss 0.550157309 batch mAP 0.374908447 batch PCKh 0.4375\n",
      "Trained batch 866 batch loss 0.58102 batch mAP 0.402954102 batch PCKh 0.5625\n",
      "Trained batch 867 batch loss 0.656508684 batch mAP 0.434234619 batch PCKh 0.25\n",
      "Trained batch 868 batch loss 0.614743114 batch mAP 0.44631958 batch PCKh 0.8125\n",
      "Trained batch 869 batch loss 0.659237564 batch mAP 0.431762695 batch PCKh 0.6875\n",
      "Trained batch 870 batch loss 0.683124661 batch mAP 0.447418213 batch PCKh 0.5625\n",
      "Trained batch 871 batch loss 0.718136251 batch mAP 0.435150146 batch PCKh 0.8125\n",
      "Trained batch 872 batch loss 0.577355266 batch mAP 0.487823486 batch PCKh 0.5\n",
      "Trained batch 873 batch loss 0.622262 batch mAP 0.450897217 batch PCKh 0.125\n",
      "Trained batch 874 batch loss 0.550449193 batch mAP 0.393280029 batch PCKh 0.25\n",
      "Trained batch 875 batch loss 0.649636149 batch mAP 0.475463867 batch PCKh 0.1875\n",
      "Trained batch 876 batch loss 0.570907235 batch mAP 0.508209229 batch PCKh 0.4375\n",
      "Trained batch 877 batch loss 0.590843201 batch mAP 0.521026611 batch PCKh 0.4375\n",
      "Trained batch 878 batch loss 0.605387449 batch mAP 0.517547607 batch PCKh 0.5\n",
      "Trained batch 879 batch loss 0.661008358 batch mAP 0.493927 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 880 batch loss 0.654423654 batch mAP 0.548400879 batch PCKh 0.5\n",
      "Trained batch 881 batch loss 0.60596931 batch mAP 0.534301758 batch PCKh 0.25\n",
      "Trained batch 882 batch loss 0.597341478 batch mAP 0.465545654 batch PCKh 0.5\n",
      "Trained batch 883 batch loss 0.631170809 batch mAP 0.480896 batch PCKh 0.5625\n",
      "Trained batch 884 batch loss 0.633670211 batch mAP 0.479553223 batch PCKh 0.3125\n",
      "Trained batch 885 batch loss 0.684710205 batch mAP 0.48550415 batch PCKh 0.5\n",
      "Trained batch 886 batch loss 0.58725661 batch mAP 0.509521484 batch PCKh 0.75\n",
      "Trained batch 887 batch loss 0.632401466 batch mAP 0.478485107 batch PCKh 0.1875\n",
      "Trained batch 888 batch loss 0.69041729 batch mAP 0.471557617 batch PCKh 0.5\n",
      "Trained batch 889 batch loss 0.655956 batch mAP 0.447875977 batch PCKh 0.625\n",
      "Trained batch 890 batch loss 0.676130593 batch mAP 0.488067627 batch PCKh 0.375\n",
      "Trained batch 891 batch loss 0.715478301 batch mAP 0.414276123 batch PCKh 0.0625\n",
      "Trained batch 892 batch loss 0.656633377 batch mAP 0.448486328 batch PCKh 0.0625\n",
      "Trained batch 893 batch loss 0.679165244 batch mAP 0.423309326 batch PCKh 0.75\n",
      "Trained batch 894 batch loss 0.674004912 batch mAP 0.476745605 batch PCKh 0.5\n",
      "Trained batch 895 batch loss 0.658570468 batch mAP 0.449066162 batch PCKh 0.5625\n",
      "Trained batch 896 batch loss 0.725428343 batch mAP 0.442047119 batch PCKh 0.125\n",
      "Trained batch 897 batch loss 0.715174556 batch mAP 0.374542236 batch PCKh 0.1875\n",
      "Trained batch 898 batch loss 0.541219413 batch mAP 0.386932373 batch PCKh 0.3125\n",
      "Trained batch 899 batch loss 0.552688718 batch mAP 0.378601074 batch PCKh 0.0625\n",
      "Trained batch 900 batch loss 0.590314627 batch mAP 0.337097168 batch PCKh 0.1875\n",
      "Trained batch 901 batch loss 0.609067798 batch mAP 0.3828125 batch PCKh 0.125\n",
      "Trained batch 902 batch loss 0.634642839 batch mAP 0.350860596 batch PCKh 0.3125\n",
      "Trained batch 903 batch loss 0.621437728 batch mAP 0.427276611 batch PCKh 0.125\n",
      "Trained batch 904 batch loss 0.584477901 batch mAP 0.432281494 batch PCKh 0.25\n",
      "Trained batch 905 batch loss 0.546962738 batch mAP 0.465362549 batch PCKh 0.1875\n",
      "Trained batch 906 batch loss 0.518930256 batch mAP 0.480865479 batch PCKh 0.1875\n",
      "Trained batch 907 batch loss 0.550881386 batch mAP 0.50680542 batch PCKh 0.5625\n",
      "Trained batch 908 batch loss 0.651495457 batch mAP 0.338104248 batch PCKh 0.125\n",
      "Trained batch 909 batch loss 0.596090436 batch mAP 0.462860107 batch PCKh 0.375\n",
      "Trained batch 910 batch loss 0.584372044 batch mAP 0.485473633 batch PCKh 0.8125\n",
      "Trained batch 911 batch loss 0.639269114 batch mAP 0.469940186 batch PCKh 0.6875\n",
      "Trained batch 912 batch loss 0.545713305 batch mAP 0.540405273 batch PCKh 0.3125\n",
      "Trained batch 913 batch loss 0.55561024 batch mAP 0.558898926 batch PCKh 0.75\n",
      "Trained batch 914 batch loss 0.530703068 batch mAP 0.545806885 batch PCKh 0.375\n",
      "Trained batch 915 batch loss 0.577116728 batch mAP 0.514007568 batch PCKh 0.625\n",
      "Trained batch 916 batch loss 0.668889046 batch mAP 0.573120117 batch PCKh 0.3125\n",
      "Trained batch 917 batch loss 0.562609076 batch mAP 0.566803 batch PCKh 0.6875\n",
      "Trained batch 918 batch loss 0.549506307 batch mAP 0.540374756 batch PCKh 0.5\n",
      "Trained batch 919 batch loss 0.591024876 batch mAP 0.491821289 batch PCKh 0.625\n",
      "Trained batch 920 batch loss 0.643358231 batch mAP 0.458984375 batch PCKh 0.0625\n",
      "Trained batch 921 batch loss 0.645403683 batch mAP 0.413818359 batch PCKh 0.25\n",
      "Trained batch 922 batch loss 0.654462874 batch mAP 0.484008789 batch PCKh 0.3125\n",
      "Trained batch 923 batch loss 0.650823593 batch mAP 0.444732666 batch PCKh 0.5\n",
      "Trained batch 924 batch loss 0.658624113 batch mAP 0.433654785 batch PCKh 0.0625\n",
      "Trained batch 925 batch loss 0.621185303 batch mAP 0.425109863 batch PCKh 0.5\n",
      "Trained batch 926 batch loss 0.617207646 batch mAP 0.427734375 batch PCKh 0.375\n",
      "Trained batch 927 batch loss 0.572094 batch mAP 0.495239258 batch PCKh 0.25\n",
      "Trained batch 928 batch loss 0.732889652 batch mAP 0.451599121 batch PCKh 0.3125\n",
      "Trained batch 929 batch loss 0.694610596 batch mAP 0.496887207 batch PCKh 0.1875\n",
      "Trained batch 930 batch loss 0.637913525 batch mAP 0.487823486 batch PCKh 0.3125\n",
      "Trained batch 931 batch loss 0.588823855 batch mAP 0.510986328 batch PCKh 0.4375\n",
      "Trained batch 932 batch loss 0.716309845 batch mAP 0.467895508 batch PCKh 0.6875\n",
      "Trained batch 933 batch loss 0.695161 batch mAP 0.413726807 batch PCKh 0.625\n",
      "Trained batch 934 batch loss 0.638870239 batch mAP 0.431060791 batch PCKh 0.375\n",
      "Trained batch 935 batch loss 0.624908 batch mAP 0.431945801 batch PCKh 0.75\n",
      "Trained batch 936 batch loss 0.512773156 batch mAP 0.463653564 batch PCKh 0.375\n",
      "Trained batch 937 batch loss 0.583387136 batch mAP 0.473510742 batch PCKh 0.5\n",
      "Trained batch 938 batch loss 0.585449159 batch mAP 0.465118408 batch PCKh 0.25\n",
      "Trained batch 939 batch loss 0.535695612 batch mAP 0.460601807 batch PCKh 0.625\n",
      "Trained batch 940 batch loss 0.680413902 batch mAP 0.440795898 batch PCKh 0.1875\n",
      "Trained batch 941 batch loss 0.602131724 batch mAP 0.463104248 batch PCKh 0.125\n",
      "Trained batch 942 batch loss 0.630293727 batch mAP 0.445983887 batch PCKh 0.3125\n",
      "Trained batch 943 batch loss 0.630821705 batch mAP 0.47857666 batch PCKh 0.25\n",
      "Trained batch 944 batch loss 0.713182271 batch mAP 0.460388184 batch PCKh 0\n",
      "Trained batch 945 batch loss 0.671899319 batch mAP 0.443054199 batch PCKh 0.3125\n",
      "Trained batch 946 batch loss 0.687559664 batch mAP 0.474487305 batch PCKh 0.5625\n",
      "Trained batch 947 batch loss 0.640922189 batch mAP 0.430725098 batch PCKh 0.5625\n",
      "Trained batch 948 batch loss 0.64364 batch mAP 0.46307373 batch PCKh 0.125\n",
      "Trained batch 949 batch loss 0.669109 batch mAP 0.467926025 batch PCKh 0.6875\n",
      "Trained batch 950 batch loss 0.586833417 batch mAP 0.455871582 batch PCKh 0.5\n",
      "Trained batch 951 batch loss 0.502136946 batch mAP 0.472015381 batch PCKh 0.375\n",
      "Trained batch 952 batch loss 0.57394743 batch mAP 0.462554932 batch PCKh 0.1875\n",
      "Trained batch 953 batch loss 0.660206676 batch mAP 0.489044189 batch PCKh 0.6875\n",
      "Trained batch 954 batch loss 0.644151151 batch mAP 0.507720947 batch PCKh 0.5625\n",
      "Trained batch 955 batch loss 0.645887375 batch mAP 0.474212646 batch PCKh 0.875\n",
      "Trained batch 956 batch loss 0.646870255 batch mAP 0.437164307 batch PCKh 0.3125\n",
      "Trained batch 957 batch loss 0.560199201 batch mAP 0.445343018 batch PCKh 0.1875\n",
      "Trained batch 958 batch loss 0.648139656 batch mAP 0.383178711 batch PCKh 0.1875\n",
      "Trained batch 959 batch loss 0.594108462 batch mAP 0.414855957 batch PCKh 0\n",
      "Trained batch 960 batch loss 0.666023314 batch mAP 0.417419434 batch PCKh 0\n",
      "Trained batch 961 batch loss 0.762123 batch mAP 0.437744141 batch PCKh 0\n",
      "Trained batch 962 batch loss 0.599601626 batch mAP 0.480438232 batch PCKh 0.5625\n",
      "Trained batch 963 batch loss 0.646503389 batch mAP 0.50289917 batch PCKh 0.25\n",
      "Trained batch 964 batch loss 0.624562263 batch mAP 0.495117188 batch PCKh 0.4375\n",
      "Trained batch 965 batch loss 0.579500914 batch mAP 0.477172852 batch PCKh 0.375\n",
      "Trained batch 966 batch loss 0.540667772 batch mAP 0.455749512 batch PCKh 0.25\n",
      "Trained batch 967 batch loss 0.570143223 batch mAP 0.389953613 batch PCKh 0.8125\n",
      "Trained batch 968 batch loss 0.580187678 batch mAP 0.35144043 batch PCKh 0.875\n",
      "Trained batch 969 batch loss 0.565009356 batch mAP 0.392700195 batch PCKh 0.4375\n",
      "Trained batch 970 batch loss 0.511296868 batch mAP 0.458679199 batch PCKh 0.3125\n",
      "Trained batch 971 batch loss 0.530317485 batch mAP 0.467865 batch PCKh 0.8125\n",
      "Trained batch 972 batch loss 0.562255263 batch mAP 0.471557617 batch PCKh 0.5\n",
      "Trained batch 973 batch loss 0.620134771 batch mAP 0.425323486 batch PCKh 0.875\n",
      "Trained batch 974 batch loss 0.626836061 batch mAP 0.488555908 batch PCKh 0.5625\n",
      "Trained batch 975 batch loss 0.675079584 batch mAP 0.533660889 batch PCKh 0.1875\n",
      "Trained batch 976 batch loss 0.639232814 batch mAP 0.489624023 batch PCKh 0.3125\n",
      "Trained batch 977 batch loss 0.654179215 batch mAP 0.496673584 batch PCKh 0.3125\n",
      "Trained batch 978 batch loss 0.596551538 batch mAP 0.53994751 batch PCKh 0.1875\n",
      "Trained batch 979 batch loss 0.536199 batch mAP 0.51071167 batch PCKh 0.125\n",
      "Trained batch 980 batch loss 0.552064538 batch mAP 0.46585083 batch PCKh 0.6875\n",
      "Trained batch 981 batch loss 0.563610196 batch mAP 0.420928955 batch PCKh 0.5625\n",
      "Trained batch 982 batch loss 0.562956 batch mAP 0.487365723 batch PCKh 0.25\n",
      "Trained batch 983 batch loss 0.557595789 batch mAP 0.524841309 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 984 batch loss 0.592963219 batch mAP 0.496887207 batch PCKh 0.75\n",
      "Trained batch 985 batch loss 0.707774162 batch mAP 0.456054688 batch PCKh 0.4375\n",
      "Trained batch 986 batch loss 0.722367287 batch mAP 0.403808594 batch PCKh 0.5\n",
      "Trained batch 987 batch loss 0.625436187 batch mAP 0.451049805 batch PCKh 0.4375\n",
      "Trained batch 988 batch loss 0.631592512 batch mAP 0.475006104 batch PCKh 0.5625\n",
      "Trained batch 989 batch loss 0.642113924 batch mAP 0.465118408 batch PCKh 0.6875\n",
      "Trained batch 990 batch loss 0.610998392 batch mAP 0.44342041 batch PCKh 0.4375\n",
      "Trained batch 991 batch loss 0.486794561 batch mAP 0.430633545 batch PCKh 0.6875\n",
      "Trained batch 992 batch loss 0.502313 batch mAP 0.35256958 batch PCKh 0.375\n",
      "Trained batch 993 batch loss 0.493450314 batch mAP 0.363464355 batch PCKh 0.75\n",
      "Trained batch 994 batch loss 0.460906625 batch mAP 0.29006958 batch PCKh 0.5\n",
      "Trained batch 995 batch loss 0.467671663 batch mAP 0.365142822 batch PCKh 0.6875\n",
      "Trained batch 996 batch loss 0.455472559 batch mAP 0.384155273 batch PCKh 0.5\n",
      "Trained batch 997 batch loss 0.51350522 batch mAP 0.530944824 batch PCKh 0.1875\n",
      "Trained batch 998 batch loss 0.540984035 batch mAP 0.539306641 batch PCKh 0.625\n",
      "Trained batch 999 batch loss 0.634166 batch mAP 0.447052 batch PCKh 0.75\n",
      "Trained batch 1000 batch loss 0.662043095 batch mAP 0.45199585 batch PCKh 0.4375\n",
      "Trained batch 1001 batch loss 0.630653143 batch mAP 0.496307373 batch PCKh 0.625\n",
      "Trained batch 1002 batch loss 0.530566931 batch mAP 0.492095947 batch PCKh 0.375\n",
      "Trained batch 1003 batch loss 0.547129273 batch mAP 0.504974365 batch PCKh 0.3125\n",
      "Trained batch 1004 batch loss 0.516439915 batch mAP 0.521118164 batch PCKh 0.625\n",
      "Trained batch 1005 batch loss 0.675734043 batch mAP 0.532409668 batch PCKh 0.375\n",
      "Trained batch 1006 batch loss 0.642893314 batch mAP 0.543548584 batch PCKh 0.625\n",
      "Trained batch 1007 batch loss 0.667742729 batch mAP 0.445648193 batch PCKh 0.125\n",
      "Trained batch 1008 batch loss 0.668502867 batch mAP 0.460235596 batch PCKh 0.25\n",
      "Trained batch 1009 batch loss 0.70253706 batch mAP 0.518615723 batch PCKh 0.25\n",
      "Trained batch 1010 batch loss 0.703949928 batch mAP 0.450866699 batch PCKh 0.3125\n",
      "Trained batch 1011 batch loss 0.656323135 batch mAP 0.521362305 batch PCKh 0.4375\n",
      "Trained batch 1012 batch loss 0.639622629 batch mAP 0.459564209 batch PCKh 0.75\n",
      "Trained batch 1013 batch loss 0.622688472 batch mAP 0.449737549 batch PCKh 0.4375\n",
      "Trained batch 1014 batch loss 0.626856148 batch mAP 0.44708252 batch PCKh 0.6875\n",
      "Trained batch 1015 batch loss 0.652750611 batch mAP 0.420196533 batch PCKh 0.375\n",
      "Trained batch 1016 batch loss 0.667034209 batch mAP 0.421661377 batch PCKh 0.1875\n",
      "Trained batch 1017 batch loss 0.683984756 batch mAP 0.39074707 batch PCKh 0.1875\n",
      "Trained batch 1018 batch loss 0.687582731 batch mAP 0.375152588 batch PCKh 0.1875\n",
      "Trained batch 1019 batch loss 0.705373764 batch mAP 0.381530762 batch PCKh 0\n",
      "Trained batch 1020 batch loss 0.648030758 batch mAP 0.399475098 batch PCKh 0.5\n",
      "Trained batch 1021 batch loss 0.60132724 batch mAP 0.387390137 batch PCKh 0.5\n",
      "Trained batch 1022 batch loss 0.599097669 batch mAP 0.353118896 batch PCKh 0.0625\n",
      "Trained batch 1023 batch loss 0.598606288 batch mAP 0.346252441 batch PCKh 0\n",
      "Trained batch 1024 batch loss 0.624123096 batch mAP 0.399292 batch PCKh 0.25\n",
      "Trained batch 1025 batch loss 0.616157532 batch mAP 0.273193359 batch PCKh 0.1875\n",
      "Trained batch 1026 batch loss 0.641748965 batch mAP 0.247009277 batch PCKh 0.4375\n",
      "Trained batch 1027 batch loss 0.638004541 batch mAP 0.189453125 batch PCKh 0.625\n",
      "Trained batch 1028 batch loss 0.570751548 batch mAP 0.237243652 batch PCKh 0.6875\n",
      "Trained batch 1029 batch loss 0.530863702 batch mAP 0.265319824 batch PCKh 0.125\n",
      "Trained batch 1030 batch loss 0.577262819 batch mAP 0.364074707 batch PCKh 0.1875\n",
      "Trained batch 1031 batch loss 0.680534184 batch mAP 0.336547852 batch PCKh 0.1875\n",
      "Trained batch 1032 batch loss 0.622356415 batch mAP 0.364196777 batch PCKh 0.1875\n",
      "Trained batch 1033 batch loss 0.657425582 batch mAP 0.422149658 batch PCKh 0.3125\n",
      "Trained batch 1034 batch loss 0.528263211 batch mAP 0.424835205 batch PCKh 0.5\n",
      "Trained batch 1035 batch loss 0.603913605 batch mAP 0.449584961 batch PCKh 0.5625\n",
      "Trained batch 1036 batch loss 0.646640062 batch mAP 0.431854248 batch PCKh 0.375\n",
      "Trained batch 1037 batch loss 0.541944742 batch mAP 0.469604492 batch PCKh 0.5625\n",
      "Trained batch 1038 batch loss 0.599087894 batch mAP 0.479980469 batch PCKh 0.6875\n",
      "Trained batch 1039 batch loss 0.595856309 batch mAP 0.387573242 batch PCKh 0.625\n",
      "Trained batch 1040 batch loss 0.61298573 batch mAP 0.420684814 batch PCKh 0.75\n",
      "Trained batch 1041 batch loss 0.675769806 batch mAP 0.32333374 batch PCKh 0.25\n",
      "Trained batch 1042 batch loss 0.70015645 batch mAP 0.41885376 batch PCKh 0.625\n",
      "Trained batch 1043 batch loss 0.679023623 batch mAP 0.404327393 batch PCKh 0.1875\n",
      "Trained batch 1044 batch loss 0.719956756 batch mAP 0.369934082 batch PCKh 0.75\n",
      "Trained batch 1045 batch loss 0.619940341 batch mAP 0.403717041 batch PCKh 0.1875\n",
      "Trained batch 1046 batch loss 0.538675547 batch mAP 0.378448486 batch PCKh 0.125\n",
      "Trained batch 1047 batch loss 0.51365447 batch mAP 0.347167969 batch PCKh 0.3125\n",
      "Trained batch 1048 batch loss 0.479947388 batch mAP 0.38470459 batch PCKh 0.125\n",
      "Trained batch 1049 batch loss 0.537169874 batch mAP 0.394805908 batch PCKh 0.5\n",
      "Trained batch 1050 batch loss 0.64353925 batch mAP 0.382049561 batch PCKh 0\n",
      "Trained batch 1051 batch loss 0.631994486 batch mAP 0.450653076 batch PCKh 0.125\n",
      "Trained batch 1052 batch loss 0.671944 batch mAP 0.435150146 batch PCKh 0.125\n",
      "Trained batch 1053 batch loss 0.672189474 batch mAP 0.416717529 batch PCKh 0.5\n",
      "Trained batch 1054 batch loss 0.698724747 batch mAP 0.432830811 batch PCKh 0.375\n",
      "Trained batch 1055 batch loss 0.65313518 batch mAP 0.47164917 batch PCKh 0.25\n",
      "Trained batch 1056 batch loss 0.680781186 batch mAP 0.419006348 batch PCKh 0.5\n",
      "Trained batch 1057 batch loss 0.633667707 batch mAP 0.322113037 batch PCKh 0.25\n",
      "Trained batch 1058 batch loss 0.614078104 batch mAP 0.369781494 batch PCKh 0.5\n",
      "Trained batch 1059 batch loss 0.569025517 batch mAP 0.280426025 batch PCKh 0.0625\n",
      "Trained batch 1060 batch loss 0.662356555 batch mAP 0.357025146 batch PCKh 0.3125\n",
      "Trained batch 1061 batch loss 0.709385276 batch mAP 0.39541626 batch PCKh 0.0625\n",
      "Trained batch 1062 batch loss 0.640885472 batch mAP 0.411438 batch PCKh 0.6875\n",
      "Trained batch 1063 batch loss 0.608113527 batch mAP 0.428314209 batch PCKh 0.3125\n",
      "Trained batch 1064 batch loss 0.593964279 batch mAP 0.438598633 batch PCKh 0\n",
      "Trained batch 1065 batch loss 0.643987179 batch mAP 0.371643066 batch PCKh 0.25\n",
      "Trained batch 1066 batch loss 0.638157248 batch mAP 0.373168945 batch PCKh 0.125\n",
      "Trained batch 1067 batch loss 0.634237289 batch mAP 0.397796631 batch PCKh 0.25\n",
      "Trained batch 1068 batch loss 0.663721263 batch mAP 0.440856934 batch PCKh 0\n",
      "Trained batch 1069 batch loss 0.575932324 batch mAP 0.441864 batch PCKh 0.1875\n",
      "Trained batch 1070 batch loss 0.603611946 batch mAP 0.455688477 batch PCKh 0.75\n",
      "Trained batch 1071 batch loss 0.615449548 batch mAP 0.431335449 batch PCKh 0.625\n",
      "Trained batch 1072 batch loss 0.634736419 batch mAP 0.454803467 batch PCKh 0.875\n",
      "Trained batch 1073 batch loss 0.647827506 batch mAP 0.439239502 batch PCKh 0.8125\n",
      "Trained batch 1074 batch loss 0.608502328 batch mAP 0.48626709 batch PCKh 0.6875\n",
      "Trained batch 1075 batch loss 0.623321652 batch mAP 0.460083 batch PCKh 0.5\n",
      "Trained batch 1076 batch loss 0.654699206 batch mAP 0.455841064 batch PCKh 0.1875\n",
      "Trained batch 1077 batch loss 0.622896552 batch mAP 0.486877441 batch PCKh 0.75\n",
      "Trained batch 1078 batch loss 0.571700811 batch mAP 0.492614746 batch PCKh 0.6875\n",
      "Trained batch 1079 batch loss 0.620180845 batch mAP 0.479064941 batch PCKh 0.375\n",
      "Trained batch 1080 batch loss 0.66652739 batch mAP 0.439544678 batch PCKh 0.125\n",
      "Trained batch 1081 batch loss 0.659060955 batch mAP 0.449279785 batch PCKh 0.3125\n",
      "Trained batch 1082 batch loss 0.649703 batch mAP 0.516815186 batch PCKh 0.375\n",
      "Trained batch 1083 batch loss 0.630596519 batch mAP 0.526306152 batch PCKh 0.3125\n",
      "Trained batch 1084 batch loss 0.64664495 batch mAP 0.493499756 batch PCKh 0.4375\n",
      "Trained batch 1085 batch loss 0.613568664 batch mAP 0.523590088 batch PCKh 0.625\n",
      "Trained batch 1086 batch loss 0.601745248 batch mAP 0.493042 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1087 batch loss 0.579597 batch mAP 0.512756348 batch PCKh 0.6875\n",
      "Trained batch 1088 batch loss 0.581539869 batch mAP 0.492767334 batch PCKh 0.5\n",
      "Trained batch 1089 batch loss 0.611487508 batch mAP 0.466064453 batch PCKh 0.1875\n",
      "Trained batch 1090 batch loss 0.602180302 batch mAP 0.481445312 batch PCKh 0.375\n",
      "Trained batch 1091 batch loss 0.660045266 batch mAP 0.433990479 batch PCKh 0.25\n",
      "Trained batch 1092 batch loss 0.565170825 batch mAP 0.456420898 batch PCKh 0.1875\n",
      "Trained batch 1093 batch loss 0.579086065 batch mAP 0.491790771 batch PCKh 0.25\n",
      "Trained batch 1094 batch loss 0.558814049 batch mAP 0.495574951 batch PCKh 0.1875\n",
      "Trained batch 1095 batch loss 0.620279074 batch mAP 0.488525391 batch PCKh 0.3125\n",
      "Trained batch 1096 batch loss 0.645779 batch mAP 0.424499512 batch PCKh 0\n",
      "Trained batch 1097 batch loss 0.57790333 batch mAP 0.561553955 batch PCKh 0.25\n",
      "Trained batch 1098 batch loss 0.646611035 batch mAP 0.50604248 batch PCKh 0.625\n",
      "Trained batch 1099 batch loss 0.583786428 batch mAP 0.502105713 batch PCKh 0.25\n",
      "Trained batch 1100 batch loss 0.680379272 batch mAP 0.428192139 batch PCKh 0.75\n",
      "Trained batch 1101 batch loss 0.589936852 batch mAP 0.539276123 batch PCKh 0.4375\n",
      "Trained batch 1102 batch loss 0.627773166 batch mAP 0.480712891 batch PCKh 0.5625\n",
      "Trained batch 1103 batch loss 0.628172874 batch mAP 0.485961914 batch PCKh 0.75\n",
      "Trained batch 1104 batch loss 0.587450206 batch mAP 0.525634766 batch PCKh 0.5625\n",
      "Trained batch 1105 batch loss 0.659184337 batch mAP 0.525512695 batch PCKh 0.1875\n",
      "Trained batch 1106 batch loss 0.649109244 batch mAP 0.530426 batch PCKh 0.125\n",
      "Trained batch 1107 batch loss 0.602824 batch mAP 0.517669678 batch PCKh 0\n",
      "Trained batch 1108 batch loss 0.566102386 batch mAP 0.479766846 batch PCKh 0.3125\n",
      "Trained batch 1109 batch loss 0.574139297 batch mAP 0.441131592 batch PCKh 0.0625\n",
      "Trained batch 1110 batch loss 0.649926662 batch mAP 0.436126709 batch PCKh 0.3125\n",
      "Trained batch 1111 batch loss 0.652742147 batch mAP 0.467468262 batch PCKh 0.125\n",
      "Trained batch 1112 batch loss 0.604868591 batch mAP 0.359710693 batch PCKh 0.3125\n",
      "Trained batch 1113 batch loss 0.554381549 batch mAP 0.434234619 batch PCKh 0.5\n",
      "Trained batch 1114 batch loss 0.596674442 batch mAP 0.472076416 batch PCKh 0.625\n",
      "Trained batch 1115 batch loss 0.635100722 batch mAP 0.485290527 batch PCKh 0.375\n",
      "Trained batch 1116 batch loss 0.619973779 batch mAP 0.466217041 batch PCKh 0.1875\n",
      "Trained batch 1117 batch loss 0.729048908 batch mAP 0.425537109 batch PCKh 0\n",
      "Trained batch 1118 batch loss 0.693530083 batch mAP 0.471374512 batch PCKh 0.625\n",
      "Trained batch 1119 batch loss 0.592912138 batch mAP 0.444610596 batch PCKh 0.4375\n",
      "Trained batch 1120 batch loss 0.593338549 batch mAP 0.421234131 batch PCKh 0.3125\n",
      "Trained batch 1121 batch loss 0.609825492 batch mAP 0.384002686 batch PCKh 0.5625\n",
      "Trained batch 1122 batch loss 0.677198946 batch mAP 0.386444092 batch PCKh 0.25\n",
      "Trained batch 1123 batch loss 0.579296231 batch mAP 0.36618042 batch PCKh 0\n",
      "Trained batch 1124 batch loss 0.513083041 batch mAP 0.276031494 batch PCKh 0.1875\n",
      "Trained batch 1125 batch loss 0.649505794 batch mAP 0.413146973 batch PCKh 0.625\n",
      "Trained batch 1126 batch loss 0.605848074 batch mAP 0.4559021 batch PCKh 0.5\n",
      "Trained batch 1127 batch loss 0.628316522 batch mAP 0.496917725 batch PCKh 0.5625\n",
      "Trained batch 1128 batch loss 0.588515341 batch mAP 0.507293701 batch PCKh 0.375\n",
      "Trained batch 1129 batch loss 0.703401327 batch mAP 0.399200439 batch PCKh 0.1875\n",
      "Trained batch 1130 batch loss 0.639051616 batch mAP 0.432952881 batch PCKh 0\n",
      "Trained batch 1131 batch loss 0.580172777 batch mAP 0.469604492 batch PCKh 0.5\n",
      "Trained batch 1132 batch loss 0.599345446 batch mAP 0.477355957 batch PCKh 0.4375\n",
      "Trained batch 1133 batch loss 0.597276926 batch mAP 0.503326416 batch PCKh 0.4375\n",
      "Trained batch 1134 batch loss 0.614051104 batch mAP 0.498321533 batch PCKh 0.5625\n",
      "Trained batch 1135 batch loss 0.682374 batch mAP 0.437957764 batch PCKh 0.375\n",
      "Trained batch 1136 batch loss 0.655492961 batch mAP 0.472747803 batch PCKh 0.5625\n",
      "Trained batch 1137 batch loss 0.499248803 batch mAP 0.383422852 batch PCKh 0.25\n",
      "Trained batch 1138 batch loss 0.606667042 batch mAP 0.341217041 batch PCKh 0.5625\n",
      "Trained batch 1139 batch loss 0.577373862 batch mAP 0.447509766 batch PCKh 0.25\n",
      "Trained batch 1140 batch loss 0.601976 batch mAP 0.465576172 batch PCKh 0.1875\n",
      "Trained batch 1141 batch loss 0.601494551 batch mAP 0.426239 batch PCKh 0.6875\n",
      "Trained batch 1142 batch loss 0.576356888 batch mAP 0.466125488 batch PCKh 0.5625\n",
      "Trained batch 1143 batch loss 0.638049 batch mAP 0.449310303 batch PCKh 0\n",
      "Trained batch 1144 batch loss 0.587711573 batch mAP 0.44329834 batch PCKh 0.4375\n",
      "Trained batch 1145 batch loss 0.607135713 batch mAP 0.417938232 batch PCKh 0.3125\n",
      "Trained batch 1146 batch loss 0.52895391 batch mAP 0.428161621 batch PCKh 0.3125\n",
      "Trained batch 1147 batch loss 0.493354082 batch mAP 0.413879395 batch PCKh 0.3125\n",
      "Trained batch 1148 batch loss 0.513914108 batch mAP 0.448638916 batch PCKh 0.3125\n",
      "Trained batch 1149 batch loss 0.539097 batch mAP 0.51297 batch PCKh 0.6875\n",
      "Trained batch 1150 batch loss 0.509918869 batch mAP 0.502990723 batch PCKh 0.375\n",
      "Trained batch 1151 batch loss 0.529032707 batch mAP 0.495422363 batch PCKh 0.75\n",
      "Trained batch 1152 batch loss 0.682992399 batch mAP 0.323059082 batch PCKh 0.25\n",
      "Trained batch 1153 batch loss 0.618269324 batch mAP 0.446960449 batch PCKh 0.5625\n",
      "Trained batch 1154 batch loss 0.589265943 batch mAP 0.344207764 batch PCKh 0.3125\n",
      "Trained batch 1155 batch loss 0.534481645 batch mAP 0.467071533 batch PCKh 0.375\n",
      "Trained batch 1156 batch loss 0.572131515 batch mAP 0.354797363 batch PCKh 0.4375\n",
      "Trained batch 1157 batch loss 0.570122242 batch mAP 0.411438 batch PCKh 0.125\n",
      "Trained batch 1158 batch loss 0.614963293 batch mAP 0.393981934 batch PCKh 0.625\n",
      "Trained batch 1159 batch loss 0.596300066 batch mAP 0.430236816 batch PCKh 0.625\n",
      "Trained batch 1160 batch loss 0.565044641 batch mAP 0.448364258 batch PCKh 0.5\n",
      "Trained batch 1161 batch loss 0.637666285 batch mAP 0.432220459 batch PCKh 0.4375\n",
      "Trained batch 1162 batch loss 0.651490092 batch mAP 0.499908447 batch PCKh 0.1875\n",
      "Trained batch 1163 batch loss 0.722769499 batch mAP 0.509277344 batch PCKh 0\n",
      "Trained batch 1164 batch loss 0.691485465 batch mAP 0.491882324 batch PCKh 0.25\n",
      "Trained batch 1165 batch loss 0.699999809 batch mAP 0.519439697 batch PCKh 0.25\n",
      "Trained batch 1166 batch loss 0.60019505 batch mAP 0.511627197 batch PCKh 0.125\n",
      "Trained batch 1167 batch loss 0.659731209 batch mAP 0.528533936 batch PCKh 0.3125\n",
      "Trained batch 1168 batch loss 0.642796159 batch mAP 0.446136475 batch PCKh 0.4375\n",
      "Trained batch 1169 batch loss 0.67142731 batch mAP 0.488555908 batch PCKh 0.1875\n",
      "Trained batch 1170 batch loss 0.672003806 batch mAP 0.497009277 batch PCKh 0.25\n",
      "Trained batch 1171 batch loss 0.563025177 batch mAP 0.465820312 batch PCKh 0\n",
      "Trained batch 1172 batch loss 0.625218272 batch mAP 0.434661865 batch PCKh 0.625\n",
      "Trained batch 1173 batch loss 0.618268 batch mAP 0.391357422 batch PCKh 0.3125\n",
      "Trained batch 1174 batch loss 0.628794372 batch mAP 0.498474121 batch PCKh 0.3125\n",
      "Trained batch 1175 batch loss 0.589391887 batch mAP 0.468719482 batch PCKh 0.375\n",
      "Trained batch 1176 batch loss 0.558752537 batch mAP 0.547943115 batch PCKh 0.5625\n",
      "Trained batch 1177 batch loss 0.482827902 batch mAP 0.528198242 batch PCKh 0.0625\n",
      "Trained batch 1178 batch loss 0.489214838 batch mAP 0.520446777 batch PCKh 0.5\n",
      "Trained batch 1179 batch loss 0.52286768 batch mAP 0.504486084 batch PCKh 0\n",
      "Trained batch 1180 batch loss 0.622300744 batch mAP 0.514068604 batch PCKh 0.1875\n",
      "Trained batch 1181 batch loss 0.610331237 batch mAP 0.454467773 batch PCKh 0.3125\n",
      "Trained batch 1182 batch loss 0.649260402 batch mAP 0.472381592 batch PCKh 0.25\n",
      "Trained batch 1183 batch loss 0.616580367 batch mAP 0.517211914 batch PCKh 0.25\n",
      "Trained batch 1184 batch loss 0.631884038 batch mAP 0.563018799 batch PCKh 0.1875\n",
      "Trained batch 1185 batch loss 0.604349613 batch mAP 0.547058105 batch PCKh 0.25\n",
      "Trained batch 1186 batch loss 0.687131107 batch mAP 0.592712402 batch PCKh 0.25\n",
      "Trained batch 1187 batch loss 0.557514429 batch mAP 0.561309814 batch PCKh 0.4375\n",
      "Trained batch 1188 batch loss 0.741378963 batch mAP 0.518157959 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1189 batch loss 0.692500949 batch mAP 0.457580566 batch PCKh 0.25\n",
      "Trained batch 1190 batch loss 0.683485091 batch mAP 0.429718018 batch PCKh 0.25\n",
      "Trained batch 1191 batch loss 0.627037644 batch mAP 0.396240234 batch PCKh 0.25\n",
      "Trained batch 1192 batch loss 0.624847353 batch mAP 0.0268859863 batch PCKh 0.125\n",
      "Trained batch 1193 batch loss 0.678654611 batch mAP 0.0369567871 batch PCKh 0.125\n",
      "Trained batch 1194 batch loss 0.548614264 batch mAP 0.00631713867 batch PCKh 0\n",
      "Trained batch 1195 batch loss 0.624620378 batch mAP 0.0182495117 batch PCKh 0.5\n",
      "Trained batch 1196 batch loss 0.647725224 batch mAP 0.0522155762 batch PCKh 0.25\n",
      "Trained batch 1197 batch loss 0.638543367 batch mAP 0.177337646 batch PCKh 0.1875\n",
      "Trained batch 1198 batch loss 0.656164587 batch mAP 0.319854736 batch PCKh 0.3125\n",
      "Trained batch 1199 batch loss 0.62085712 batch mAP 0.402771 batch PCKh 0.5\n",
      "Trained batch 1200 batch loss 0.657392442 batch mAP 0.418304443 batch PCKh 0.125\n",
      "Trained batch 1201 batch loss 0.450653672 batch mAP 0.517120361 batch PCKh 0.0625\n",
      "Trained batch 1202 batch loss 0.537489116 batch mAP 0.527069092 batch PCKh 0.125\n",
      "Trained batch 1203 batch loss 0.472593725 batch mAP 0.367462158 batch PCKh 0\n",
      "Trained batch 1204 batch loss 0.418095 batch mAP 0.336639404 batch PCKh 0\n",
      "Trained batch 1205 batch loss 0.470893681 batch mAP 0.441436768 batch PCKh 0.375\n",
      "Trained batch 1206 batch loss 0.557737052 batch mAP 0.52532959 batch PCKh 0.4375\n",
      "Trained batch 1207 batch loss 0.659718096 batch mAP 0.619171143 batch PCKh 0.5625\n",
      "Trained batch 1208 batch loss 0.67674458 batch mAP 0.540802 batch PCKh 0.5625\n",
      "Trained batch 1209 batch loss 0.770036 batch mAP 0.165283203 batch PCKh 0.4375\n",
      "Trained batch 1210 batch loss 0.721857548 batch mAP 0.212219238 batch PCKh 0\n",
      "Trained batch 1211 batch loss 0.751138806 batch mAP 0.195495605 batch PCKh 0\n",
      "Trained batch 1212 batch loss 0.721362948 batch mAP 0.263366699 batch PCKh 0.25\n",
      "Trained batch 1213 batch loss 0.674327493 batch mAP 0.255096436 batch PCKh 0.25\n",
      "Trained batch 1214 batch loss 0.683231354 batch mAP 0.279876709 batch PCKh 0.1875\n",
      "Trained batch 1215 batch loss 0.660975218 batch mAP 0.309356689 batch PCKh 0.3125\n",
      "Trained batch 1216 batch loss 0.66323936 batch mAP 0.389099121 batch PCKh 0.25\n",
      "Trained batch 1217 batch loss 0.730621278 batch mAP 0.420288086 batch PCKh 0.125\n",
      "Trained batch 1218 batch loss 0.636983216 batch mAP 0.457580566 batch PCKh 0.25\n",
      "Trained batch 1219 batch loss 0.69191128 batch mAP 0.403198242 batch PCKh 0.3125\n",
      "Trained batch 1220 batch loss 0.703374505 batch mAP 0.392944336 batch PCKh 0.75\n",
      "Trained batch 1221 batch loss 0.630050898 batch mAP 0.19342041 batch PCKh 0.6875\n",
      "Trained batch 1222 batch loss 0.609982193 batch mAP 0.105255127 batch PCKh 0.25\n",
      "Trained batch 1223 batch loss 0.55178982 batch mAP 0.099029541 batch PCKh 0.5625\n",
      "Trained batch 1224 batch loss 0.558199167 batch mAP 0.169158936 batch PCKh 0.625\n",
      "Trained batch 1225 batch loss 0.593493462 batch mAP 0.224456787 batch PCKh 0.4375\n",
      "Trained batch 1226 batch loss 0.597382665 batch mAP 0.262390137 batch PCKh 0.6875\n",
      "Trained batch 1227 batch loss 0.596636772 batch mAP 0.279144287 batch PCKh 0.25\n",
      "Trained batch 1228 batch loss 0.563821733 batch mAP 0.365112305 batch PCKh 0.375\n",
      "Trained batch 1229 batch loss 0.646068394 batch mAP 0.40914917 batch PCKh 0.4375\n",
      "Trained batch 1230 batch loss 0.693863153 batch mAP 0.436553955 batch PCKh 0.8125\n",
      "Trained batch 1231 batch loss 0.644471288 batch mAP 0.45614624 batch PCKh 0.4375\n",
      "Trained batch 1232 batch loss 0.635385454 batch mAP 0.455993652 batch PCKh 0.3125\n",
      "Trained batch 1233 batch loss 0.583491921 batch mAP 0.399200439 batch PCKh 0.375\n",
      "Trained batch 1234 batch loss 0.62521863 batch mAP 0.32824707 batch PCKh 0.25\n",
      "Trained batch 1235 batch loss 0.622969925 batch mAP 0.328704834 batch PCKh 0.5625\n",
      "Trained batch 1236 batch loss 0.612513542 batch mAP 0.31628418 batch PCKh 0.4375\n",
      "Trained batch 1237 batch loss 0.668238044 batch mAP 0.307006836 batch PCKh 0.0625\n",
      "Trained batch 1238 batch loss 0.562574923 batch mAP 0.30267334 batch PCKh 0.5\n",
      "Trained batch 1239 batch loss 0.568936229 batch mAP 0.342956543 batch PCKh 0.4375\n",
      "Trained batch 1240 batch loss 0.560666203 batch mAP 0.407653809 batch PCKh 0.5625\n",
      "Trained batch 1241 batch loss 0.599736214 batch mAP 0.42086792 batch PCKh 0.4375\n",
      "Trained batch 1242 batch loss 0.628285766 batch mAP 0.484008789 batch PCKh 0.25\n",
      "Trained batch 1243 batch loss 0.627780259 batch mAP 0.435852051 batch PCKh 0.625\n",
      "Trained batch 1244 batch loss 0.604463279 batch mAP 0.321044922 batch PCKh 0.75\n",
      "Trained batch 1245 batch loss 0.532289743 batch mAP 0.326324463 batch PCKh 0.6875\n",
      "Trained batch 1246 batch loss 0.518577099 batch mAP 0.179840088 batch PCKh 0.6875\n",
      "Trained batch 1247 batch loss 0.729411066 batch mAP 0.129516602 batch PCKh 0.75\n",
      "Trained batch 1248 batch loss 0.65417248 batch mAP 0.1875 batch PCKh 0.625\n",
      "Trained batch 1249 batch loss 0.60999608 batch mAP 0.360137939 batch PCKh 0.625\n",
      "Trained batch 1250 batch loss 0.601291776 batch mAP 0.459411621 batch PCKh 0.25\n",
      "Trained batch 1251 batch loss 0.701490939 batch mAP 0.496490479 batch PCKh 0.5625\n",
      "Trained batch 1252 batch loss 0.60407865 batch mAP 0.483612061 batch PCKh 0\n",
      "Trained batch 1253 batch loss 0.613548875 batch mAP 0.466918945 batch PCKh 0.625\n",
      "Trained batch 1254 batch loss 0.605653167 batch mAP 0.468963623 batch PCKh 0.3125\n",
      "Trained batch 1255 batch loss 0.582081437 batch mAP 0.41998291 batch PCKh 0.25\n",
      "Trained batch 1256 batch loss 0.584599137 batch mAP 0.4140625 batch PCKh 0.3125\n",
      "Trained batch 1257 batch loss 0.499834746 batch mAP 0.334991455 batch PCKh 0.4375\n",
      "Trained batch 1258 batch loss 0.566173196 batch mAP 0.396179199 batch PCKh 0.3125\n",
      "Trained batch 1259 batch loss 0.599586904 batch mAP 0.422607422 batch PCKh 0.1875\n",
      "Trained batch 1260 batch loss 0.585135102 batch mAP 0.439056396 batch PCKh 0.1875\n",
      "Trained batch 1261 batch loss 0.665617347 batch mAP 0.442016602 batch PCKh 0.125\n",
      "Trained batch 1262 batch loss 0.646641195 batch mAP 0.446044922 batch PCKh 0.1875\n",
      "Trained batch 1263 batch loss 0.654283583 batch mAP 0.415924072 batch PCKh 0.625\n",
      "Trained batch 1264 batch loss 0.626788318 batch mAP 0.424499512 batch PCKh 0.8125\n",
      "Trained batch 1265 batch loss 0.617307425 batch mAP 0.450714111 batch PCKh 0.8125\n",
      "Trained batch 1266 batch loss 0.605816722 batch mAP 0.528503418 batch PCKh 0.75\n",
      "Trained batch 1267 batch loss 0.75331825 batch mAP 0.484069824 batch PCKh 0.125\n",
      "Trained batch 1268 batch loss 0.763644636 batch mAP 0.461120605 batch PCKh 0\n",
      "Trained batch 1269 batch loss 0.736011 batch mAP 0.451751709 batch PCKh 0.125\n",
      "Trained batch 1270 batch loss 0.773756921 batch mAP 0.343811035 batch PCKh 0.0625\n",
      "Trained batch 1271 batch loss 0.70811522 batch mAP 0.401977539 batch PCKh 0.5625\n",
      "Trained batch 1272 batch loss 0.603353143 batch mAP 0.427612305 batch PCKh 0.75\n",
      "Trained batch 1273 batch loss 0.650372088 batch mAP 0.41027832 batch PCKh 0.75\n",
      "Trained batch 1274 batch loss 0.539745569 batch mAP 0.405090332 batch PCKh 0.4375\n",
      "Trained batch 1275 batch loss 0.580626369 batch mAP 0.264373779 batch PCKh 0.75\n",
      "Trained batch 1276 batch loss 0.527569413 batch mAP 0.332946777 batch PCKh 0.25\n",
      "Trained batch 1277 batch loss 0.474452585 batch mAP 0.314880371 batch PCKh 0\n",
      "Trained batch 1278 batch loss 0.462472796 batch mAP 0.336486816 batch PCKh 0\n",
      "Trained batch 1279 batch loss 0.580970049 batch mAP 0.394378662 batch PCKh 0.625\n",
      "Trained batch 1280 batch loss 0.571965933 batch mAP 0.381958 batch PCKh 0\n",
      "Trained batch 1281 batch loss 0.617365 batch mAP 0.45489502 batch PCKh 0.625\n",
      "Trained batch 1282 batch loss 0.570750713 batch mAP 0.289001465 batch PCKh 0.3125\n",
      "Trained batch 1283 batch loss 0.498142272 batch mAP 0.372131348 batch PCKh 0.1875\n",
      "Trained batch 1284 batch loss 0.501055241 batch mAP 0.453308105 batch PCKh 0\n",
      "Trained batch 1285 batch loss 0.458674371 batch mAP 0.497283936 batch PCKh 0\n",
      "Trained batch 1286 batch loss 0.44217813 batch mAP 0.473999023 batch PCKh 0\n",
      "Trained batch 1287 batch loss 0.514130473 batch mAP 0.462493896 batch PCKh 0.5\n",
      "Trained batch 1288 batch loss 0.486708134 batch mAP 0.431793213 batch PCKh 0.5\n",
      "Trained batch 1289 batch loss 0.566796064 batch mAP 0.333496094 batch PCKh 0.6875\n",
      "Trained batch 1290 batch loss 0.565843225 batch mAP 0.411376953 batch PCKh 0.6875\n",
      "Trained batch 1291 batch loss 0.530566752 batch mAP 0.398681641 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1292 batch loss 0.583757222 batch mAP 0.295013428 batch PCKh 0.6875\n",
      "Trained batch 1293 batch loss 0.621229291 batch mAP 0.253967285 batch PCKh 0.375\n",
      "Trained batch 1294 batch loss 0.63330847 batch mAP 0.353851318 batch PCKh 0.25\n",
      "Trained batch 1295 batch loss 0.709977746 batch mAP 0.431854248 batch PCKh 0\n",
      "Trained batch 1296 batch loss 0.670811415 batch mAP 0.474060059 batch PCKh 0.125\n",
      "Trained batch 1297 batch loss 0.611356 batch mAP 0.446411133 batch PCKh 0.375\n",
      "Trained batch 1298 batch loss 0.622163415 batch mAP 0.447967529 batch PCKh 0.625\n",
      "Trained batch 1299 batch loss 0.615218759 batch mAP 0.412353516 batch PCKh 0.3125\n",
      "Trained batch 1300 batch loss 0.68065542 batch mAP 0.431640625 batch PCKh 0.1875\n",
      "Trained batch 1301 batch loss 0.640777349 batch mAP 0.261169434 batch PCKh 0.625\n",
      "Trained batch 1302 batch loss 0.61789763 batch mAP 0.259735107 batch PCKh 0\n",
      "Trained batch 1303 batch loss 0.611837626 batch mAP 0.164093018 batch PCKh 0.3125\n",
      "Trained batch 1304 batch loss 0.634742439 batch mAP 0.210021973 batch PCKh 0.5\n",
      "Trained batch 1305 batch loss 0.579051 batch mAP 0.163024902 batch PCKh 0.0625\n",
      "Trained batch 1306 batch loss 0.59941268 batch mAP 0.345611572 batch PCKh 0.375\n",
      "Trained batch 1307 batch loss 0.651361823 batch mAP 0.396850586 batch PCKh 0.4375\n",
      "Trained batch 1308 batch loss 0.594240904 batch mAP 0.448761 batch PCKh 0.25\n",
      "Trained batch 1309 batch loss 0.629219592 batch mAP 0.480682373 batch PCKh 0.3125\n",
      "Trained batch 1310 batch loss 0.64226079 batch mAP 0.474121094 batch PCKh 0.3125\n",
      "Trained batch 1311 batch loss 0.586410403 batch mAP 0.519561768 batch PCKh 0.375\n",
      "Trained batch 1312 batch loss 0.645064712 batch mAP 0.484405518 batch PCKh 0.25\n",
      "Trained batch 1313 batch loss 0.618552089 batch mAP 0.52331543 batch PCKh 0.1875\n",
      "Trained batch 1314 batch loss 0.591920435 batch mAP 0.46762085 batch PCKh 0.625\n",
      "Trained batch 1315 batch loss 0.589203119 batch mAP 0.546264648 batch PCKh 0.625\n",
      "Trained batch 1316 batch loss 0.617552519 batch mAP 0.548736572 batch PCKh 0.3125\n",
      "Trained batch 1317 batch loss 0.628426254 batch mAP 0.508026123 batch PCKh 0.25\n",
      "Trained batch 1318 batch loss 0.697133899 batch mAP 0.4609375 batch PCKh 0.625\n",
      "Trained batch 1319 batch loss 0.656664491 batch mAP 0.522064209 batch PCKh 0.625\n",
      "Trained batch 1320 batch loss 0.686844707 batch mAP 0.418121338 batch PCKh 0.5625\n",
      "Trained batch 1321 batch loss 0.671945572 batch mAP 0.440032959 batch PCKh 0.625\n",
      "Trained batch 1322 batch loss 0.588750899 batch mAP 0.464019775 batch PCKh 0.5\n",
      "Trained batch 1323 batch loss 0.597306907 batch mAP 0.450378418 batch PCKh 0.5625\n",
      "Trained batch 1324 batch loss 0.705226302 batch mAP 0.492614746 batch PCKh 0.3125\n",
      "Trained batch 1325 batch loss 0.579943419 batch mAP 0.483459473 batch PCKh 0.5625\n",
      "Trained batch 1326 batch loss 0.583766043 batch mAP 0.419189453 batch PCKh 0\n",
      "Trained batch 1327 batch loss 0.581154168 batch mAP 0.471008301 batch PCKh 0.5\n",
      "Trained batch 1328 batch loss 0.58912611 batch mAP 0.471618652 batch PCKh 0.5625\n",
      "Trained batch 1329 batch loss 0.660685062 batch mAP 0.445007324 batch PCKh 0.1875\n",
      "Trained batch 1330 batch loss 0.649151802 batch mAP 0.478240967 batch PCKh 0.125\n",
      "Trained batch 1331 batch loss 0.665783763 batch mAP 0.452056885 batch PCKh 0.3125\n",
      "Trained batch 1332 batch loss 0.70770067 batch mAP 0.42666626 batch PCKh 0.5\n",
      "Trained batch 1333 batch loss 0.637648106 batch mAP 0.449859619 batch PCKh 0\n",
      "Trained batch 1334 batch loss 0.610684872 batch mAP 0.453308105 batch PCKh 0.3125\n",
      "Trained batch 1335 batch loss 0.687997401 batch mAP 0.478729248 batch PCKh 0.5\n",
      "Trained batch 1336 batch loss 0.670527399 batch mAP 0.460266113 batch PCKh 0.5\n",
      "Trained batch 1337 batch loss 0.701559186 batch mAP 0.473083496 batch PCKh 0.5\n",
      "Trained batch 1338 batch loss 0.657135248 batch mAP 0.477966309 batch PCKh 0.375\n",
      "Trained batch 1339 batch loss 0.665810227 batch mAP 0.506500244 batch PCKh 0.25\n",
      "Trained batch 1340 batch loss 0.673237 batch mAP 0.507507324 batch PCKh 0.625\n",
      "Trained batch 1341 batch loss 0.652313828 batch mAP 0.491973877 batch PCKh 0.5625\n",
      "Trained batch 1342 batch loss 0.67528 batch mAP 0.514801 batch PCKh 0.1875\n",
      "Trained batch 1343 batch loss 0.620654523 batch mAP 0.481170654 batch PCKh 0.4375\n",
      "Trained batch 1344 batch loss 0.567165554 batch mAP 0.451324463 batch PCKh 0.4375\n",
      "Trained batch 1345 batch loss 0.669104815 batch mAP 0.467834473 batch PCKh 0.5\n",
      "Trained batch 1346 batch loss 0.654792547 batch mAP 0.409301758 batch PCKh 0.3125\n",
      "Trained batch 1347 batch loss 0.54949981 batch mAP 0.519714355 batch PCKh 0.125\n",
      "Trained batch 1348 batch loss 0.570181489 batch mAP 0.467346191 batch PCKh 0.1875\n",
      "Trained batch 1349 batch loss 0.694917798 batch mAP 0.358825684 batch PCKh 0.375\n",
      "Trained batch 1350 batch loss 0.733450353 batch mAP 0.300048828 batch PCKh 0\n",
      "Trained batch 1351 batch loss 0.642608345 batch mAP 0.406982422 batch PCKh 0.5\n",
      "Trained batch 1352 batch loss 0.621920109 batch mAP 0.404174805 batch PCKh 0.1875\n",
      "Trained batch 1353 batch loss 0.651991 batch mAP 0.402618408 batch PCKh 0.5\n",
      "Trained batch 1354 batch loss 0.710065126 batch mAP 0.438995361 batch PCKh 0.3125\n",
      "Trained batch 1355 batch loss 0.718637943 batch mAP 0.427886963 batch PCKh 0.8125\n",
      "Trained batch 1356 batch loss 0.767820597 batch mAP 0.446258545 batch PCKh 0.625\n",
      "Trained batch 1357 batch loss 0.693179369 batch mAP 0.424255371 batch PCKh 0.5625\n",
      "Trained batch 1358 batch loss 0.701204419 batch mAP 0.424041748 batch PCKh 0.5\n",
      "Trained batch 1359 batch loss 0.584030867 batch mAP 0.348968506 batch PCKh 0.75\n",
      "Trained batch 1360 batch loss 0.554521799 batch mAP 0.461242676 batch PCKh 0.5625\n",
      "Trained batch 1361 batch loss 0.650613129 batch mAP 0.429260254 batch PCKh 0.625\n",
      "Trained batch 1362 batch loss 0.668536186 batch mAP 0.410827637 batch PCKh 0.6875\n",
      "Trained batch 1363 batch loss 0.55348742 batch mAP 0.424194336 batch PCKh 0.5625\n",
      "Trained batch 1364 batch loss 0.59525 batch mAP 0.460144043 batch PCKh 0.6875\n",
      "Trained batch 1365 batch loss 0.619285822 batch mAP 0.444030762 batch PCKh 0.4375\n",
      "Trained batch 1366 batch loss 0.665991 batch mAP 0.424163818 batch PCKh 0\n",
      "Trained batch 1367 batch loss 0.618707538 batch mAP 0.45916748 batch PCKh 0.375\n",
      "Trained batch 1368 batch loss 0.665096104 batch mAP 0.439300537 batch PCKh 0.25\n",
      "Trained batch 1369 batch loss 0.705182076 batch mAP 0.463562 batch PCKh 0.75\n",
      "Trained batch 1370 batch loss 0.657206595 batch mAP 0.470214844 batch PCKh 0.1875\n",
      "Trained batch 1371 batch loss 0.632313609 batch mAP 0.466888428 batch PCKh 0.5\n",
      "Trained batch 1372 batch loss 0.703375101 batch mAP 0.492156982 batch PCKh 0\n",
      "Trained batch 1373 batch loss 0.713045359 batch mAP 0.513092041 batch PCKh 0.1875\n",
      "Trained batch 1374 batch loss 0.751895547 batch mAP 0.484283447 batch PCKh 0\n",
      "Trained batch 1375 batch loss 0.684132576 batch mAP 0.505065918 batch PCKh 0.0625\n",
      "Trained batch 1376 batch loss 0.689712286 batch mAP 0.521484375 batch PCKh 0.4375\n",
      "Trained batch 1377 batch loss 0.737257123 batch mAP 0.445068359 batch PCKh 0.1875\n",
      "Trained batch 1378 batch loss 0.708334 batch mAP 0.478912354 batch PCKh 0.4375\n",
      "Trained batch 1379 batch loss 0.712957621 batch mAP 0.517395 batch PCKh 0.375\n",
      "Trained batch 1380 batch loss 0.706194 batch mAP 0.49029541 batch PCKh 0.8125\n",
      "Trained batch 1381 batch loss 0.709578097 batch mAP 0.479400635 batch PCKh 0.125\n",
      "Trained batch 1382 batch loss 0.718539715 batch mAP 0.444671631 batch PCKh 0.4375\n",
      "Trained batch 1383 batch loss 0.641773939 batch mAP 0.468688965 batch PCKh 0.5\n",
      "Trained batch 1384 batch loss 0.669137955 batch mAP 0.522583 batch PCKh 0.625\n",
      "Trained batch 1385 batch loss 0.645590067 batch mAP 0.486480713 batch PCKh 0.5625\n",
      "Trained batch 1386 batch loss 0.634006739 batch mAP 0.499725342 batch PCKh 0.5\n",
      "Trained batch 1387 batch loss 0.685478449 batch mAP 0.475128174 batch PCKh 0.5\n",
      "Trained batch 1388 batch loss 0.69405067 batch mAP 0.498016357 batch PCKh 0.625\n",
      "Trained batch 1389 batch loss 0.757858098 batch mAP 0.436676025 batch PCKh 0.25\n",
      "Trained batch 1390 batch loss 0.702002764 batch mAP 0.450775146 batch PCKh 0.125\n",
      "Trained batch 1391 batch loss 0.694760144 batch mAP 0.425048828 batch PCKh 0\n",
      "Trained batch 1392 batch loss 0.589605808 batch mAP 0.516845703 batch PCKh 0.0625\n",
      "Trained batch 1393 batch loss 0.669955194 batch mAP 0.511779785 batch PCKh 0.1875\n",
      "Trained batch 1394 batch loss 0.629252493 batch mAP 0.468994141 batch PCKh 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1395 batch loss 0.615258098 batch mAP 0.488647461 batch PCKh 0.4375\n",
      "Trained batch 1396 batch loss 0.547255576 batch mAP 0.452606201 batch PCKh 0.3125\n",
      "Trained batch 1397 batch loss 0.533257484 batch mAP 0.448242188 batch PCKh 0.3125\n",
      "Trained batch 1398 batch loss 0.639283538 batch mAP 0.480255127 batch PCKh 0.6875\n",
      "Trained batch 1399 batch loss 0.637103498 batch mAP 0.474243164 batch PCKh 0.5\n",
      "Trained batch 1400 batch loss 0.61992389 batch mAP 0.456542969 batch PCKh 0.1875\n",
      "Trained batch 1401 batch loss 0.615359306 batch mAP 0.470977783 batch PCKh 0.4375\n",
      "Trained batch 1402 batch loss 0.590350509 batch mAP 0.485412598 batch PCKh 0.625\n",
      "Trained batch 1403 batch loss 0.477203727 batch mAP 0.483184814 batch PCKh 0.25\n",
      "Trained batch 1404 batch loss 0.580265701 batch mAP 0.449035645 batch PCKh 0.75\n",
      "Trained batch 1405 batch loss 0.47950083 batch mAP 0.468292236 batch PCKh 0.1875\n",
      "Trained batch 1406 batch loss 0.535553694 batch mAP 0.437957764 batch PCKh 0.75\n",
      "Trained batch 1407 batch loss 0.543461502 batch mAP 0.461517334 batch PCKh 0.5\n",
      "Trained batch 1408 batch loss 0.515175104 batch mAP 0.506378174 batch PCKh 0.6875\n",
      "Trained batch 1409 batch loss 0.641331077 batch mAP 0.495178223 batch PCKh 0.6875\n",
      "Trained batch 1410 batch loss 0.627412081 batch mAP 0.494140625 batch PCKh 0.625\n",
      "Trained batch 1411 batch loss 0.63791 batch mAP 0.51171875 batch PCKh 0.4375\n",
      "Trained batch 1412 batch loss 0.470826745 batch mAP 0.539642334 batch PCKh 0.5625\n",
      "Trained batch 1413 batch loss 0.521457314 batch mAP 0.521698 batch PCKh 0.25\n",
      "Trained batch 1414 batch loss 0.507314682 batch mAP 0.489837646 batch PCKh 0.5625\n",
      "Trained batch 1415 batch loss 0.594462395 batch mAP 0.464996338 batch PCKh 0.625\n",
      "Trained batch 1416 batch loss 0.63260138 batch mAP 0.449981689 batch PCKh 0.1875\n",
      "Trained batch 1417 batch loss 0.586596668 batch mAP 0.476593018 batch PCKh 0.75\n",
      "Trained batch 1418 batch loss 0.643434346 batch mAP 0.423522949 batch PCKh 0.4375\n",
      "Trained batch 1419 batch loss 0.665287316 batch mAP 0.446777344 batch PCKh 0.6875\n",
      "Trained batch 1420 batch loss 0.616323829 batch mAP 0.441589355 batch PCKh 0\n",
      "Trained batch 1421 batch loss 0.561811209 batch mAP 0.437408447 batch PCKh 0.6875\n",
      "Trained batch 1422 batch loss 0.55045855 batch mAP 0.463623047 batch PCKh 0.8125\n",
      "Trained batch 1423 batch loss 0.514236152 batch mAP 0.447021484 batch PCKh 0.6875\n",
      "Trained batch 1424 batch loss 0.558507 batch mAP 0.43270874 batch PCKh 0.625\n",
      "Trained batch 1425 batch loss 0.65817976 batch mAP 0.405761719 batch PCKh 0.5625\n",
      "Trained batch 1426 batch loss 0.666554272 batch mAP 0.40246582 batch PCKh 0.5625\n",
      "Trained batch 1427 batch loss 0.667649925 batch mAP 0.413146973 batch PCKh 0.1875\n",
      "Trained batch 1428 batch loss 0.685856342 batch mAP 0.417694092 batch PCKh 0.1875\n",
      "Trained batch 1429 batch loss 0.634495258 batch mAP 0.461517334 batch PCKh 0.1875\n",
      "Trained batch 1430 batch loss 0.634810209 batch mAP 0.422119141 batch PCKh 0.375\n",
      "Trained batch 1431 batch loss 0.610329509 batch mAP 0.437408447 batch PCKh 0.25\n",
      "Trained batch 1432 batch loss 0.611063242 batch mAP 0.428070068 batch PCKh 0\n",
      "Trained batch 1433 batch loss 0.657250762 batch mAP 0.432189941 batch PCKh 0.4375\n",
      "Trained batch 1434 batch loss 0.579568446 batch mAP 0.430358887 batch PCKh 0.5625\n",
      "Trained batch 1435 batch loss 0.589260459 batch mAP 0.462524414 batch PCKh 0.5625\n",
      "Trained batch 1436 batch loss 0.671739221 batch mAP 0.442779541 batch PCKh 0.4375\n",
      "Trained batch 1437 batch loss 0.641556621 batch mAP 0.423675537 batch PCKh 0\n",
      "Trained batch 1438 batch loss 0.604608655 batch mAP 0.392211914 batch PCKh 0.5625\n",
      "Trained batch 1439 batch loss 0.654751599 batch mAP 0.450622559 batch PCKh 0.125\n",
      "Trained batch 1440 batch loss 0.677459121 batch mAP 0.43258667 batch PCKh 0.5\n",
      "Trained batch 1441 batch loss 0.643976688 batch mAP 0.443023682 batch PCKh 0.3125\n",
      "Trained batch 1442 batch loss 0.67899394 batch mAP 0.428710938 batch PCKh 0.75\n",
      "Trained batch 1443 batch loss 0.643394 batch mAP 0.424987793 batch PCKh 0.6875\n",
      "Trained batch 1444 batch loss 0.574909449 batch mAP 0.388885498 batch PCKh 0.5\n",
      "Trained batch 1445 batch loss 0.592953 batch mAP 0.455108643 batch PCKh 0.6875\n",
      "Trained batch 1446 batch loss 0.580554128 batch mAP 0.507171631 batch PCKh 0.3125\n",
      "Trained batch 1447 batch loss 0.567115426 batch mAP 0.458648682 batch PCKh 0.5625\n",
      "Trained batch 1448 batch loss 0.574971259 batch mAP 0.446136475 batch PCKh 0.375\n",
      "Trained batch 1449 batch loss 0.574705601 batch mAP 0.44631958 batch PCKh 0.5\n",
      "Trained batch 1450 batch loss 0.518792093 batch mAP 0.47442627 batch PCKh 0.625\n",
      "Trained batch 1451 batch loss 0.517534137 batch mAP 0.531646729 batch PCKh 0.75\n",
      "Trained batch 1452 batch loss 0.564740539 batch mAP 0.502563477 batch PCKh 0.75\n",
      "Trained batch 1453 batch loss 0.579031289 batch mAP 0.509399414 batch PCKh 0.5\n",
      "Trained batch 1454 batch loss 0.637728572 batch mAP 0.493743896 batch PCKh 0.875\n",
      "Trained batch 1455 batch loss 0.607281446 batch mAP 0.408782959 batch PCKh 0.625\n",
      "Trained batch 1456 batch loss 0.557964087 batch mAP 0.533111572 batch PCKh 0.75\n",
      "Trained batch 1457 batch loss 0.624143124 batch mAP 0.517730713 batch PCKh 0.75\n",
      "Trained batch 1458 batch loss 0.582774282 batch mAP 0.489074707 batch PCKh 0.5625\n",
      "Trained batch 1459 batch loss 0.591504812 batch mAP 0.540435791 batch PCKh 0.75\n",
      "Trained batch 1460 batch loss 0.50577569 batch mAP 0.501983643 batch PCKh 0.4375\n",
      "Trained batch 1461 batch loss 0.610321522 batch mAP 0.511047363 batch PCKh 0.5\n",
      "Trained batch 1462 batch loss 0.590303302 batch mAP 0.510131836 batch PCKh 0.75\n",
      "Trained batch 1463 batch loss 0.580595255 batch mAP 0.53326416 batch PCKh 0.5625\n",
      "Trained batch 1464 batch loss 0.619066358 batch mAP 0.524200439 batch PCKh 0.3125\n",
      "Trained batch 1465 batch loss 0.557817817 batch mAP 0.526489258 batch PCKh 0.6875\n",
      "Trained batch 1466 batch loss 0.55405879 batch mAP 0.492797852 batch PCKh 0.4375\n",
      "Trained batch 1467 batch loss 0.633523 batch mAP 0.502471924 batch PCKh 0.5\n",
      "Trained batch 1468 batch loss 0.678721547 batch mAP 0.519958496 batch PCKh 0.125\n",
      "Trained batch 1469 batch loss 0.650910914 batch mAP 0.47644043 batch PCKh 0.625\n",
      "Trained batch 1470 batch loss 0.654760122 batch mAP 0.45199585 batch PCKh 0.0625\n",
      "Trained batch 1471 batch loss 0.679375052 batch mAP 0.444213867 batch PCKh 0.4375\n",
      "Trained batch 1472 batch loss 0.6177513 batch mAP 0.451324463 batch PCKh 0.375\n",
      "Trained batch 1473 batch loss 0.627925515 batch mAP 0.447052 batch PCKh 0.75\n",
      "Trained batch 1474 batch loss 0.611605763 batch mAP 0.423400879 batch PCKh 0.1875\n",
      "Trained batch 1475 batch loss 0.660171151 batch mAP 0.476379395 batch PCKh 0.5625\n",
      "Trained batch 1476 batch loss 0.595209718 batch mAP 0.503631592 batch PCKh 0.4375\n",
      "Trained batch 1477 batch loss 0.617692173 batch mAP 0.467926025 batch PCKh 0.4375\n",
      "Trained batch 1478 batch loss 0.540144682 batch mAP 0.50680542 batch PCKh 0.375\n",
      "Trained batch 1479 batch loss 0.578331292 batch mAP 0.540924072 batch PCKh 0.375\n",
      "Trained batch 1480 batch loss 0.553195596 batch mAP 0.535675049 batch PCKh 0.6875\n",
      "Trained batch 1481 batch loss 0.645002365 batch mAP 0.543579102 batch PCKh 0.0625\n",
      "Trained batch 1482 batch loss 0.631992161 batch mAP 0.486602783 batch PCKh 0.625\n",
      "Trained batch 1483 batch loss 0.578354 batch mAP 0.497589111 batch PCKh 0.5\n",
      "Trained batch 1484 batch loss 0.644668877 batch mAP 0.443450928 batch PCKh 0.4375\n",
      "Trained batch 1485 batch loss 0.578481376 batch mAP 0.462493896 batch PCKh 0.375\n",
      "Trained batch 1486 batch loss 0.613760531 batch mAP 0.474243164 batch PCKh 0.375\n",
      "Trained batch 1487 batch loss 0.622056484 batch mAP 0.485626221 batch PCKh 0.6875\n",
      "Trained batch 1488 batch loss 0.587839603 batch mAP 0.468139648 batch PCKh 0.6875\n",
      "Trained batch 1489 batch loss 0.517376423 batch mAP 0.487792969 batch PCKh 0.125\n",
      "Trained batch 1490 batch loss 0.593019 batch mAP 0.426544189 batch PCKh 0.5\n",
      "Trained batch 1491 batch loss 0.618295729 batch mAP 0.471740723 batch PCKh 0.75\n",
      "Trained batch 1492 batch loss 0.604509234 batch mAP 0.479309082 batch PCKh 0.125\n",
      "Trained batch 1493 batch loss 0.55477643 batch mAP 0.505004883 batch PCKh 0.8125\n",
      "Trained batch 1494 batch loss 0.639972031 batch mAP 0.480163574 batch PCKh 0.625\n",
      "Trained batch 1495 batch loss 0.655412853 batch mAP 0.439605713 batch PCKh 0.625\n",
      "Trained batch 1496 batch loss 0.628806949 batch mAP 0.471344 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1497 batch loss 0.596006572 batch mAP 0.528808594 batch PCKh 0.3125\n",
      "Trained batch 1498 batch loss 0.627079368 batch mAP 0.513916 batch PCKh 0.4375\n",
      "Trained batch 1499 batch loss 0.537799239 batch mAP 0.485565186 batch PCKh 0.5\n",
      "Trained batch 1500 batch loss 0.586714625 batch mAP 0.463684082 batch PCKh 0.75\n",
      "Trained batch 1501 batch loss 0.593860388 batch mAP 0.472686768 batch PCKh 0.3125\n",
      "Trained batch 1502 batch loss 0.658240139 batch mAP 0.479766846 batch PCKh 0.25\n",
      "Trained batch 1503 batch loss 0.571973205 batch mAP 0.47555542 batch PCKh 0.25\n",
      "Trained batch 1504 batch loss 0.624691963 batch mAP 0.501556396 batch PCKh 0.5\n",
      "Trained batch 1505 batch loss 0.60868609 batch mAP 0.468048096 batch PCKh 0.4375\n",
      "Trained batch 1506 batch loss 0.591273844 batch mAP 0.470275879 batch PCKh 0.375\n",
      "Trained batch 1507 batch loss 0.58273381 batch mAP 0.473754883 batch PCKh 0.8125\n",
      "Trained batch 1508 batch loss 0.645666 batch mAP 0.463623047 batch PCKh 0.4375\n",
      "Trained batch 1509 batch loss 0.6502648 batch mAP 0.465118408 batch PCKh 0.1875\n",
      "Trained batch 1510 batch loss 0.580175459 batch mAP 0.497955322 batch PCKh 0.5\n",
      "Trained batch 1511 batch loss 0.518055499 batch mAP 0.539764404 batch PCKh 0.25\n",
      "Trained batch 1512 batch loss 0.558856249 batch mAP 0.493438721 batch PCKh 0.4375\n",
      "Trained batch 1513 batch loss 0.553444862 batch mAP 0.424407959 batch PCKh 0.3125\n",
      "Trained batch 1514 batch loss 0.599854887 batch mAP 0.444946289 batch PCKh 0.5625\n",
      "Trained batch 1515 batch loss 0.540158153 batch mAP 0.538818359 batch PCKh 0.1875\n",
      "Trained batch 1516 batch loss 0.613463402 batch mAP 0.546386719 batch PCKh 0.875\n",
      "Trained batch 1517 batch loss 0.584562182 batch mAP 0.532012939 batch PCKh 0.1875\n",
      "Trained batch 1518 batch loss 0.504492342 batch mAP 0.498809814 batch PCKh 0\n",
      "Trained batch 1519 batch loss 0.538227499 batch mAP 0.518035889 batch PCKh 0.25\n",
      "Trained batch 1520 batch loss 0.574381948 batch mAP 0.519805908 batch PCKh 0\n",
      "Trained batch 1521 batch loss 0.652683854 batch mAP 0.555389404 batch PCKh 0\n",
      "Trained batch 1522 batch loss 0.629846752 batch mAP 0.451293945 batch PCKh 0.125\n",
      "Trained batch 1523 batch loss 0.77307 batch mAP 0.386261 batch PCKh 0.125\n",
      "Trained batch 1524 batch loss 0.831700146 batch mAP 0.368560791 batch PCKh 0\n",
      "Trained batch 1525 batch loss 0.705822 batch mAP 0.367889404 batch PCKh 0.5\n",
      "Trained batch 1526 batch loss 0.546819806 batch mAP 0.00372314453 batch PCKh 0.25\n",
      "Trained batch 1527 batch loss 0.579029918 batch mAP 0.000427246094 batch PCKh 0\n",
      "Trained batch 1528 batch loss 0.518779874 batch mAP 0.000244140625 batch PCKh 0.25\n",
      "Trained batch 1529 batch loss 0.526487708 batch mAP 3.05175781e-05 batch PCKh 0.3125\n",
      "Trained batch 1530 batch loss 0.530768573 batch mAP 0.000213623047 batch PCKh 0.6875\n",
      "Trained batch 1531 batch loss 0.57378155 batch mAP 0.0428466797 batch PCKh 0.125\n",
      "Trained batch 1532 batch loss 0.587491035 batch mAP 0.0252380371 batch PCKh 0.3125\n",
      "Trained batch 1533 batch loss 0.596830785 batch mAP 0.0882873535 batch PCKh 0.1875\n",
      "Trained batch 1534 batch loss 0.64269042 batch mAP 0.294830322 batch PCKh 0.1875\n",
      "Trained batch 1535 batch loss 0.735818 batch mAP 0.439331055 batch PCKh 0.25\n",
      "Trained batch 1536 batch loss 0.721804082 batch mAP 0.421295166 batch PCKh 0.125\n",
      "Trained batch 1537 batch loss 0.610509515 batch mAP 0.459106445 batch PCKh 0.75\n",
      "Trained batch 1538 batch loss 0.670870543 batch mAP 0.411529541 batch PCKh 0.125\n",
      "Trained batch 1539 batch loss 0.613834262 batch mAP 0.494567871 batch PCKh 0.25\n",
      "Trained batch 1540 batch loss 0.618936 batch mAP 0.467498779 batch PCKh 0.3125\n",
      "Trained batch 1541 batch loss 0.610888362 batch mAP 0.447174072 batch PCKh 0.1875\n",
      "Trained batch 1542 batch loss 0.638211727 batch mAP 0.535339355 batch PCKh 0.6875\n",
      "Trained batch 1543 batch loss 0.593751073 batch mAP 0.468597412 batch PCKh 0.125\n",
      "Trained batch 1544 batch loss 0.624540389 batch mAP 0.491027832 batch PCKh 0.125\n",
      "Trained batch 1545 batch loss 0.587715149 batch mAP 0.388275146 batch PCKh 0.25\n",
      "Trained batch 1546 batch loss 0.573176265 batch mAP 0.209655762 batch PCKh 0.375\n",
      "Trained batch 1547 batch loss 0.582830846 batch mAP 0.261871338 batch PCKh 0.5625\n",
      "Trained batch 1548 batch loss 0.603008091 batch mAP 0.221588135 batch PCKh 0.1875\n",
      "Trained batch 1549 batch loss 0.761045456 batch mAP 0.147888184 batch PCKh 0.25\n",
      "Trained batch 1550 batch loss 0.67440331 batch mAP 0.408874512 batch PCKh 0.3125\n",
      "Trained batch 1551 batch loss 0.7696684 batch mAP 0.442749023 batch PCKh 0\n",
      "Trained batch 1552 batch loss 0.69936 batch mAP 0.493408203 batch PCKh 0\n",
      "Trained batch 1553 batch loss 0.638518155 batch mAP 0.441772461 batch PCKh 0.6875\n",
      "Trained batch 1554 batch loss 0.564196169 batch mAP 0.401763916 batch PCKh 0.875\n",
      "Trained batch 1555 batch loss 0.543682814 batch mAP 0.383575439 batch PCKh 0.5\n",
      "Trained batch 1556 batch loss 0.575009763 batch mAP 0.349884033 batch PCKh 0.4375\n",
      "Trained batch 1557 batch loss 0.605065405 batch mAP 0.307769775 batch PCKh 0.3125\n",
      "Trained batch 1558 batch loss 0.59475553 batch mAP 0.157470703 batch PCKh 0.375\n",
      "Trained batch 1559 batch loss 0.55592 batch mAP 0.223144531 batch PCKh 0.375\n",
      "Trained batch 1560 batch loss 0.61804986 batch mAP 0.381073 batch PCKh 0.25\n",
      "Trained batch 1561 batch loss 0.687554479 batch mAP 0.359283447 batch PCKh 0.4375\n",
      "Trained batch 1562 batch loss 0.715658069 batch mAP 0.389648438 batch PCKh 0.8125\n",
      "Trained batch 1563 batch loss 0.658475697 batch mAP 0.308258057 batch PCKh 0.0625\n",
      "Trained batch 1564 batch loss 0.618312359 batch mAP 0.39175415 batch PCKh 0.5625\n",
      "Trained batch 1565 batch loss 0.655262768 batch mAP 0.377655029 batch PCKh 0.5\n",
      "Trained batch 1566 batch loss 0.587502718 batch mAP 0.386199951 batch PCKh 0\n",
      "Trained batch 1567 batch loss 0.590981603 batch mAP 0.344146729 batch PCKh 0.375\n",
      "Trained batch 1568 batch loss 0.514009118 batch mAP 0.255584717 batch PCKh 0.25\n",
      "Trained batch 1569 batch loss 0.639752626 batch mAP 0.332763672 batch PCKh 0.125\n",
      "Trained batch 1570 batch loss 0.590478539 batch mAP 0.199279785 batch PCKh 0.75\n",
      "Trained batch 1571 batch loss 0.535608292 batch mAP 0.290344238 batch PCKh 0.5625\n",
      "Trained batch 1572 batch loss 0.530177 batch mAP 0.346740723 batch PCKh 0.1875\n",
      "Trained batch 1573 batch loss 0.515223265 batch mAP 0.441375732 batch PCKh 0.25\n",
      "Trained batch 1574 batch loss 0.60281837 batch mAP 0.491638184 batch PCKh 0.1875\n",
      "Trained batch 1575 batch loss 0.554318726 batch mAP 0.532959 batch PCKh 0.625\n",
      "Trained batch 1576 batch loss 0.555696487 batch mAP 0.551269531 batch PCKh 0.875\n",
      "Trained batch 1577 batch loss 0.596249521 batch mAP 0.481811523 batch PCKh 0.4375\n",
      "Trained batch 1578 batch loss 0.51477468 batch mAP 0.578674316 batch PCKh 0.4375\n",
      "Trained batch 1579 batch loss 0.610515594 batch mAP 0.492340088 batch PCKh 0.3125\n",
      "Trained batch 1580 batch loss 0.530400157 batch mAP 0.533935547 batch PCKh 0.625\n",
      "Trained batch 1581 batch loss 0.557487905 batch mAP 0.438537598 batch PCKh 0.25\n",
      "Trained batch 1582 batch loss 0.621572733 batch mAP 0.363555908 batch PCKh 0.5\n",
      "Trained batch 1583 batch loss 0.621988773 batch mAP 0.369628906 batch PCKh 0.125\n",
      "Trained batch 1584 batch loss 0.58868289 batch mAP 0.468536377 batch PCKh 0.625\n",
      "Trained batch 1585 batch loss 0.553966105 batch mAP 0.454528809 batch PCKh 0.0625\n",
      "Trained batch 1586 batch loss 0.506642222 batch mAP 0.455657959 batch PCKh 0.25\n",
      "Trained batch 1587 batch loss 0.66035521 batch mAP 0.463104248 batch PCKh 0.5\n",
      "Trained batch 1588 batch loss 0.58253777 batch mAP 0.500701904 batch PCKh 0.375\n",
      "Trained batch 1589 batch loss 0.648295522 batch mAP 0.50201416 batch PCKh 0.1875\n",
      "Trained batch 1590 batch loss 0.689030528 batch mAP 0.468353271 batch PCKh 0.5625\n",
      "Trained batch 1591 batch loss 0.61772877 batch mAP 0.470367432 batch PCKh 0\n",
      "Trained batch 1592 batch loss 0.652912736 batch mAP 0.4715271 batch PCKh 0.125\n",
      "Trained batch 1593 batch loss 0.621396184 batch mAP 0.471588135 batch PCKh 0.3125\n",
      "Trained batch 1594 batch loss 0.720795095 batch mAP 0.40411377 batch PCKh 0.3125\n",
      "Trained batch 1595 batch loss 0.689876437 batch mAP 0.455566406 batch PCKh 0.125\n",
      "Trained batch 1596 batch loss 0.636941373 batch mAP 0.408630371 batch PCKh 0.1875\n",
      "Trained batch 1597 batch loss 0.585915327 batch mAP 0.368286133 batch PCKh 0.125\n",
      "Trained batch 1598 batch loss 0.601526201 batch mAP 0.380981445 batch PCKh 0.375\n",
      "Trained batch 1599 batch loss 0.616721869 batch mAP 0.378448486 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1600 batch loss 0.613143921 batch mAP 0.397460938 batch PCKh 0.375\n",
      "Trained batch 1601 batch loss 0.605071187 batch mAP 0.401123047 batch PCKh 0.25\n",
      "Trained batch 1602 batch loss 0.63153851 batch mAP 0.377319336 batch PCKh 0.125\n",
      "Trained batch 1603 batch loss 0.640416622 batch mAP 0.40927124 batch PCKh 0.5625\n",
      "Trained batch 1604 batch loss 0.611585379 batch mAP 0.425109863 batch PCKh 0.4375\n",
      "Trained batch 1605 batch loss 0.568492174 batch mAP 0.456329346 batch PCKh 0.75\n",
      "Trained batch 1606 batch loss 0.655286908 batch mAP 0.453613281 batch PCKh 0.625\n",
      "Trained batch 1607 batch loss 0.590007722 batch mAP 0.467956543 batch PCKh 0.5625\n",
      "Trained batch 1608 batch loss 0.633148074 batch mAP 0.449371338 batch PCKh 0.875\n",
      "Trained batch 1609 batch loss 0.667738259 batch mAP 0.472564697 batch PCKh 0.8125\n",
      "Trained batch 1610 batch loss 0.662204087 batch mAP 0.447967529 batch PCKh 0.875\n",
      "Trained batch 1611 batch loss 0.765280068 batch mAP 0.399353027 batch PCKh 0.0625\n",
      "Trained batch 1612 batch loss 0.677698135 batch mAP 0.419433594 batch PCKh 0.125\n",
      "Trained batch 1613 batch loss 0.598222852 batch mAP 0.430969238 batch PCKh 0.0625\n",
      "Trained batch 1614 batch loss 0.591011047 batch mAP 0.371124268 batch PCKh 0.375\n",
      "Trained batch 1615 batch loss 0.684744954 batch mAP 0.446838379 batch PCKh 0\n",
      "Trained batch 1616 batch loss 0.666202128 batch mAP 0.450561523 batch PCKh 0.5\n",
      "Trained batch 1617 batch loss 0.746641457 batch mAP 0.442504883 batch PCKh 0\n",
      "Trained batch 1618 batch loss 0.613905132 batch mAP 0.437683105 batch PCKh 0.6875\n",
      "Trained batch 1619 batch loss 0.597583532 batch mAP 0.428924561 batch PCKh 0.75\n",
      "Trained batch 1620 batch loss 0.650203705 batch mAP 0.437042236 batch PCKh 0.5625\n",
      "Trained batch 1621 batch loss 0.539675057 batch mAP 0.41293335 batch PCKh 0.5625\n",
      "Trained batch 1622 batch loss 0.597521305 batch mAP 0.442993164 batch PCKh 0.5625\n",
      "Trained batch 1623 batch loss 0.620788693 batch mAP 0.496948242 batch PCKh 0.375\n",
      "Trained batch 1624 batch loss 0.637938 batch mAP 0.489501953 batch PCKh 0.3125\n",
      "Trained batch 1625 batch loss 0.802207291 batch mAP 0.410247803 batch PCKh 0.0625\n",
      "Trained batch 1626 batch loss 0.677646279 batch mAP 0.467590332 batch PCKh 0.0625\n",
      "Trained batch 1627 batch loss 0.717038393 batch mAP 0.489715576 batch PCKh 0.25\n",
      "Trained batch 1628 batch loss 0.620541573 batch mAP 0.524627686 batch PCKh 0.125\n",
      "Trained batch 1629 batch loss 0.67398268 batch mAP 0.501403809 batch PCKh 0.1875\n",
      "Trained batch 1630 batch loss 0.568880439 batch mAP 0.518371582 batch PCKh 0.1875\n",
      "Trained batch 1631 batch loss 0.602185965 batch mAP 0.549865723 batch PCKh 0.25\n",
      "Trained batch 1632 batch loss 0.589757919 batch mAP 0.5128479 batch PCKh 0.4375\n",
      "Trained batch 1633 batch loss 0.655415773 batch mAP 0.494903564 batch PCKh 0.1875\n",
      "Trained batch 1634 batch loss 0.647107422 batch mAP 0.535339355 batch PCKh 0.4375\n",
      "Trained batch 1635 batch loss 0.648975492 batch mAP 0.5262146 batch PCKh 0.1875\n",
      "Trained batch 1636 batch loss 0.567945242 batch mAP 0.505462646 batch PCKh 0.1875\n",
      "Trained batch 1637 batch loss 0.672396183 batch mAP 0.41897583 batch PCKh 0.6875\n",
      "Trained batch 1638 batch loss 0.723319 batch mAP 0.450256348 batch PCKh 0.875\n",
      "Trained batch 1639 batch loss 0.644070387 batch mAP 0.471832275 batch PCKh 0.5\n",
      "Trained batch 1640 batch loss 0.691156 batch mAP 0.379455566 batch PCKh 0.625\n",
      "Trained batch 1641 batch loss 0.74729228 batch mAP 0.426940918 batch PCKh 0.5\n",
      "Trained batch 1642 batch loss 0.696710467 batch mAP 0.438110352 batch PCKh 0.25\n",
      "Trained batch 1643 batch loss 0.62816745 batch mAP 0.406036377 batch PCKh 0.75\n",
      "Trained batch 1644 batch loss 0.656816 batch mAP 0.389068604 batch PCKh 0\n",
      "Trained batch 1645 batch loss 0.547052264 batch mAP 0.313110352 batch PCKh 0.125\n",
      "Trained batch 1646 batch loss 0.578181744 batch mAP 0.307922363 batch PCKh 0.0625\n",
      "Trained batch 1647 batch loss 0.554538906 batch mAP 0.223968506 batch PCKh 0.125\n",
      "Trained batch 1648 batch loss 0.52947861 batch mAP 0.312408447 batch PCKh 0.4375\n",
      "Trained batch 1649 batch loss 0.58490479 batch mAP 0.304504395 batch PCKh 0.75\n",
      "Trained batch 1650 batch loss 0.575674295 batch mAP 0.104614258 batch PCKh 0.3125\n",
      "Trained batch 1651 batch loss 0.605812967 batch mAP 0.106048584 batch PCKh 0.25\n",
      "Trained batch 1652 batch loss 0.610413969 batch mAP 0.248474121 batch PCKh 0.5\n",
      "Trained batch 1653 batch loss 0.577674389 batch mAP 0.29675293 batch PCKh 0.3125\n",
      "Trained batch 1654 batch loss 0.537426829 batch mAP 0.281677246 batch PCKh 0.375\n",
      "Trained batch 1655 batch loss 0.565856278 batch mAP 0.399017334 batch PCKh 0.75\n",
      "Trained batch 1656 batch loss 0.619713843 batch mAP 0.473510742 batch PCKh 0.75\n",
      "Trained batch 1657 batch loss 0.712492049 batch mAP 0.404022217 batch PCKh 0.5\n",
      "Trained batch 1658 batch loss 0.614708424 batch mAP 0.302124023 batch PCKh 0.3125\n",
      "Trained batch 1659 batch loss 0.67530179 batch mAP 0.488891602 batch PCKh 0.5625\n",
      "Trained batch 1660 batch loss 0.65291822 batch mAP 0.442016602 batch PCKh 0.1875\n",
      "Trained batch 1661 batch loss 0.67134434 batch mAP 0.479858398 batch PCKh 0.875\n",
      "Trained batch 1662 batch loss 0.579210877 batch mAP 0.461517334 batch PCKh 0.3125\n",
      "Trained batch 1663 batch loss 0.684864521 batch mAP 0.422149658 batch PCKh 0.6875\n",
      "Trained batch 1664 batch loss 0.63033855 batch mAP 0.465576172 batch PCKh 0.375\n",
      "Trained batch 1665 batch loss 0.487918615 batch mAP 0.459289551 batch PCKh 0.25\n",
      "Trained batch 1666 batch loss 0.609692931 batch mAP 0.454498291 batch PCKh 0.3125\n",
      "Trained batch 1667 batch loss 0.543739557 batch mAP 0.449035645 batch PCKh 0.3125\n",
      "Trained batch 1668 batch loss 0.511958838 batch mAP 0.432067871 batch PCKh 0.1875\n",
      "Trained batch 1669 batch loss 0.632476807 batch mAP 0.410522461 batch PCKh 0.375\n",
      "Trained batch 1670 batch loss 0.570669889 batch mAP 0.4503479 batch PCKh 0.75\n",
      "Trained batch 1671 batch loss 0.61722517 batch mAP 0.475891113 batch PCKh 0.375\n",
      "Trained batch 1672 batch loss 0.587012708 batch mAP 0.471435547 batch PCKh 0.4375\n",
      "Trained batch 1673 batch loss 0.619888783 batch mAP 0.46307373 batch PCKh 0\n",
      "Trained batch 1674 batch loss 0.731190443 batch mAP 0.377990723 batch PCKh 0.0625\n",
      "Trained batch 1675 batch loss 0.676670671 batch mAP 0.502716064 batch PCKh 0.375\n",
      "Trained batch 1676 batch loss 0.610296488 batch mAP 0.495605469 batch PCKh 0.375\n",
      "Trained batch 1677 batch loss 0.652560055 batch mAP 0.515991211 batch PCKh 0.5\n",
      "Trained batch 1678 batch loss 0.634558797 batch mAP 0.490600586 batch PCKh 0.4375\n",
      "Trained batch 1679 batch loss 0.62252748 batch mAP 0.483459473 batch PCKh 0.5625\n",
      "Trained batch 1680 batch loss 0.603122354 batch mAP 0.49395752 batch PCKh 0.625\n",
      "Trained batch 1681 batch loss 0.616456509 batch mAP 0.48638916 batch PCKh 0.375\n",
      "Trained batch 1682 batch loss 0.58633405 batch mAP 0.429534912 batch PCKh 0.25\n",
      "Trained batch 1683 batch loss 0.64455986 batch mAP 0.419403076 batch PCKh 0.4375\n",
      "Trained batch 1684 batch loss 0.673136711 batch mAP 0.459503174 batch PCKh 0.5\n",
      "Trained batch 1685 batch loss 0.627194881 batch mAP 0.388397217 batch PCKh 0.6875\n",
      "Trained batch 1686 batch loss 0.6524508 batch mAP 0.396148682 batch PCKh 0.1875\n",
      "Trained batch 1687 batch loss 0.600473404 batch mAP 0.405303955 batch PCKh 0.5\n",
      "Trained batch 1688 batch loss 0.592893064 batch mAP 0.427764893 batch PCKh 0.4375\n",
      "Trained batch 1689 batch loss 0.616081476 batch mAP 0.421783447 batch PCKh 0.25\n",
      "Trained batch 1690 batch loss 0.593535185 batch mAP 0.452453613 batch PCKh 0.625\n",
      "Trained batch 1691 batch loss 0.655931711 batch mAP 0.437927246 batch PCKh 0.625\n",
      "Trained batch 1692 batch loss 0.667787135 batch mAP 0.448120117 batch PCKh 0.0625\n",
      "Trained batch 1693 batch loss 0.713424563 batch mAP 0.424377441 batch PCKh 0.25\n",
      "Trained batch 1694 batch loss 0.628068149 batch mAP 0.457489 batch PCKh 0.1875\n",
      "Trained batch 1695 batch loss 0.599514425 batch mAP 0.456848145 batch PCKh 0.625\n",
      "Trained batch 1696 batch loss 0.593485951 batch mAP 0.432159424 batch PCKh 0.5\n",
      "Trained batch 1697 batch loss 0.616264939 batch mAP 0.467315674 batch PCKh 0.0625\n",
      "Trained batch 1698 batch loss 0.615178287 batch mAP 0.48059082 batch PCKh 0.25\n",
      "Trained batch 1699 batch loss 0.63813448 batch mAP 0.457000732 batch PCKh 0\n",
      "Trained batch 1700 batch loss 0.633353055 batch mAP 0.394165039 batch PCKh 0.375\n",
      "Trained batch 1701 batch loss 0.590386093 batch mAP 0.464233398 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1702 batch loss 0.550322175 batch mAP 0.450164795 batch PCKh 0.4375\n",
      "Trained batch 1703 batch loss 0.556532145 batch mAP 0.414245605 batch PCKh 0.375\n",
      "Trained batch 1704 batch loss 0.515179396 batch mAP 0.404571533 batch PCKh 0\n",
      "Trained batch 1705 batch loss 0.620120883 batch mAP 0.43560791 batch PCKh 0.25\n",
      "Trained batch 1706 batch loss 0.629932642 batch mAP 0.458435059 batch PCKh 0.625\n",
      "Trained batch 1707 batch loss 0.552444458 batch mAP 0.462188721 batch PCKh 0.375\n",
      "Trained batch 1708 batch loss 0.672383428 batch mAP 0.416748047 batch PCKh 0.5625\n",
      "Trained batch 1709 batch loss 0.633784354 batch mAP 0.437072754 batch PCKh 0.1875\n",
      "Trained batch 1710 batch loss 0.605033517 batch mAP 0.451629639 batch PCKh 0.4375\n",
      "Trained batch 1711 batch loss 0.607689142 batch mAP 0.492095947 batch PCKh 0.3125\n",
      "Trained batch 1712 batch loss 0.547025502 batch mAP 0.5234375 batch PCKh 0.8125\n",
      "Trained batch 1713 batch loss 0.685100436 batch mAP 0.478820801 batch PCKh 0.375\n",
      "Trained batch 1714 batch loss 0.612929463 batch mAP 0.475463867 batch PCKh 0.8125\n",
      "Trained batch 1715 batch loss 0.642352 batch mAP 0.461334229 batch PCKh 0.4375\n",
      "Trained batch 1716 batch loss 0.654763103 batch mAP 0.464874268 batch PCKh 0.75\n",
      "Trained batch 1717 batch loss 0.620179296 batch mAP 0.427612305 batch PCKh 0.625\n",
      "Trained batch 1718 batch loss 0.548891425 batch mAP 0.453094482 batch PCKh 0.75\n",
      "Trained batch 1719 batch loss 0.538229 batch mAP 0.450164795 batch PCKh 0.5625\n",
      "Trained batch 1720 batch loss 0.586111486 batch mAP 0.465209961 batch PCKh 0.4375\n",
      "Trained batch 1721 batch loss 0.62578392 batch mAP 0.4375 batch PCKh 0.625\n",
      "Trained batch 1722 batch loss 0.642912388 batch mAP 0.422302246 batch PCKh 0.625\n",
      "Trained batch 1723 batch loss 0.635326505 batch mAP 0.458557129 batch PCKh 0.25\n",
      "Trained batch 1724 batch loss 0.628200829 batch mAP 0.467407227 batch PCKh 0.125\n",
      "Trained batch 1725 batch loss 0.629122198 batch mAP 0.415130615 batch PCKh 0.0625\n",
      "Trained batch 1726 batch loss 0.67920506 batch mAP 0.438598633 batch PCKh 0.625\n",
      "Trained batch 1727 batch loss 0.629959 batch mAP 0.396911621 batch PCKh 0.25\n",
      "Trained batch 1728 batch loss 0.600784719 batch mAP 0.428222656 batch PCKh 0.3125\n",
      "Trained batch 1729 batch loss 0.55539155 batch mAP 0.371826172 batch PCKh 0.8125\n",
      "Trained batch 1730 batch loss 0.516880035 batch mAP 0.397003174 batch PCKh 0.5\n",
      "Trained batch 1731 batch loss 0.499461561 batch mAP 0.472961426 batch PCKh 0.25\n",
      "Trained batch 1732 batch loss 0.650653183 batch mAP 0.45425415 batch PCKh 0.4375\n",
      "Trained batch 1733 batch loss 0.541827619 batch mAP 0.460418701 batch PCKh 0.3125\n",
      "Trained batch 1734 batch loss 0.738373101 batch mAP 0.455993652 batch PCKh 0.125\n",
      "Trained batch 1735 batch loss 0.66258055 batch mAP 0.440917969 batch PCKh 0.0625\n",
      "Trained batch 1736 batch loss 0.464332819 batch mAP 0.477813721 batch PCKh 0.125\n",
      "Trained batch 1737 batch loss 0.449148715 batch mAP 0.477355957 batch PCKh 0\n",
      "Trained batch 1738 batch loss 0.518125653 batch mAP 0.46661377 batch PCKh 0.1875\n",
      "Trained batch 1739 batch loss 0.475821167 batch mAP 0.477355957 batch PCKh 0\n",
      "Trained batch 1740 batch loss 0.446742952 batch mAP 0.520050049 batch PCKh 0.3125\n",
      "Trained batch 1741 batch loss 0.672213197 batch mAP 0.459259033 batch PCKh 0.1875\n",
      "Trained batch 1742 batch loss 0.600657 batch mAP 0.476501465 batch PCKh 0.6875\n",
      "Trained batch 1743 batch loss 0.708076954 batch mAP 0.413452148 batch PCKh 0.5\n",
      "Trained batch 1744 batch loss 0.721820354 batch mAP 0.414154053 batch PCKh 0.3125\n",
      "Trained batch 1745 batch loss 0.705399811 batch mAP 0.342987061 batch PCKh 0.125\n",
      "Trained batch 1746 batch loss 0.800848842 batch mAP 0.401428223 batch PCKh 0\n",
      "Trained batch 1747 batch loss 0.831472754 batch mAP 0.42401123 batch PCKh 0\n",
      "Trained batch 1748 batch loss 0.556889534 batch mAP 0.458648682 batch PCKh 0.3125\n",
      "Trained batch 1749 batch loss 0.604774356 batch mAP 0.419555664 batch PCKh 0.0625\n",
      "Trained batch 1750 batch loss 0.551337779 batch mAP 0.406097412 batch PCKh 0.5625\n",
      "Trained batch 1751 batch loss 0.555782139 batch mAP 0.239013672 batch PCKh 0.25\n",
      "Trained batch 1752 batch loss 0.655412436 batch mAP 0.274841309 batch PCKh 0.5625\n",
      "Trained batch 1753 batch loss 0.62400496 batch mAP 0.14654541 batch PCKh 0.25\n",
      "Trained batch 1754 batch loss 0.633917928 batch mAP 0.060760498 batch PCKh 0.6875\n",
      "Trained batch 1755 batch loss 0.620520592 batch mAP 0.0835571289 batch PCKh 0.375\n",
      "Trained batch 1756 batch loss 0.599733114 batch mAP 0.110137939 batch PCKh 0.25\n",
      "Trained batch 1757 batch loss 0.645369172 batch mAP 0.266082764 batch PCKh 0.5\n",
      "Trained batch 1758 batch loss 0.626335561 batch mAP 0.331542969 batch PCKh 0.3125\n",
      "Trained batch 1759 batch loss 0.636849403 batch mAP 0.414642334 batch PCKh 0.3125\n",
      "Trained batch 1760 batch loss 0.61754334 batch mAP 0.480072021 batch PCKh 0.1875\n",
      "Trained batch 1761 batch loss 0.653702796 batch mAP 0.397125244 batch PCKh 0.6875\n",
      "Trained batch 1762 batch loss 0.663312197 batch mAP 0.394195557 batch PCKh 0.375\n",
      "Trained batch 1763 batch loss 0.625128388 batch mAP 0.423339844 batch PCKh 0.8125\n",
      "Trained batch 1764 batch loss 0.617321432 batch mAP 0.416748047 batch PCKh 0.625\n",
      "Trained batch 1765 batch loss 0.628422737 batch mAP 0.416107178 batch PCKh 0.6875\n",
      "Trained batch 1766 batch loss 0.639323354 batch mAP 0.441833496 batch PCKh 0.25\n",
      "Trained batch 1767 batch loss 0.552867651 batch mAP 0.56350708 batch PCKh 0.1875\n",
      "Trained batch 1768 batch loss 0.571238279 batch mAP 0.560333252 batch PCKh 0.3125\n",
      "Trained batch 1769 batch loss 0.606900096 batch mAP 0.385009766 batch PCKh 0.4375\n",
      "Trained batch 1770 batch loss 0.573700547 batch mAP 0.527404785 batch PCKh 0.1875\n",
      "Trained batch 1771 batch loss 0.558504939 batch mAP 0.482879639 batch PCKh 0.5625\n",
      "Trained batch 1772 batch loss 0.597838163 batch mAP 0.599456787 batch PCKh 0.6875\n",
      "Trained batch 1773 batch loss 0.68210417 batch mAP 0.533355713 batch PCKh 0.5625\n",
      "Trained batch 1774 batch loss 0.629514 batch mAP 0.559051514 batch PCKh 0.0625\n",
      "Trained batch 1775 batch loss 0.570543468 batch mAP 0.564941406 batch PCKh 0.6875\n",
      "Trained batch 1776 batch loss 0.619043887 batch mAP 0.543518066 batch PCKh 0.375\n",
      "Trained batch 1777 batch loss 0.417166948 batch mAP 0.504303 batch PCKh 0.375\n",
      "Trained batch 1778 batch loss 0.504059672 batch mAP 0.519348145 batch PCKh 0.0625\n",
      "Trained batch 1779 batch loss 0.619270563 batch mAP 0.601532 batch PCKh 0.5625\n",
      "Trained batch 1780 batch loss 0.607642174 batch mAP 0.590362549 batch PCKh 0.375\n",
      "Trained batch 1781 batch loss 0.668233871 batch mAP 0.534820557 batch PCKh 0.125\n",
      "Trained batch 1782 batch loss 0.571379721 batch mAP 0.566253662 batch PCKh 0.5625\n",
      "Trained batch 1783 batch loss 0.656443775 batch mAP 0.570983887 batch PCKh 0.6875\n",
      "Trained batch 1784 batch loss 0.652162373 batch mAP 0.502716064 batch PCKh 0\n",
      "Trained batch 1785 batch loss 0.633646309 batch mAP 0.5 batch PCKh 0.75\n",
      "Trained batch 1786 batch loss 0.576938093 batch mAP 0.508911133 batch PCKh 0.5625\n",
      "Trained batch 1787 batch loss 0.531970084 batch mAP 0.493682861 batch PCKh 0.75\n",
      "Trained batch 1788 batch loss 0.661017597 batch mAP 0.530883789 batch PCKh 0.625\n",
      "Trained batch 1789 batch loss 0.62680459 batch mAP 0.453582764 batch PCKh 0.5625\n",
      "Trained batch 1790 batch loss 0.611797154 batch mAP 0.441497803 batch PCKh 0.25\n",
      "Trained batch 1791 batch loss 0.582384288 batch mAP 0.452087402 batch PCKh 0.125\n",
      "Trained batch 1792 batch loss 0.578429759 batch mAP 0.489044189 batch PCKh 0.125\n",
      "Trained batch 1793 batch loss 0.566954315 batch mAP 0.449401855 batch PCKh 0.3125\n",
      "Trained batch 1794 batch loss 0.6192047 batch mAP 0.493286133 batch PCKh 0.75\n",
      "Trained batch 1795 batch loss 0.569215894 batch mAP 0.515289307 batch PCKh 0.75\n",
      "Trained batch 1796 batch loss 0.613704562 batch mAP 0.49697876 batch PCKh 0.5625\n",
      "Trained batch 1797 batch loss 0.753800571 batch mAP 0.445739746 batch PCKh 0.25\n",
      "Trained batch 1798 batch loss 0.669875622 batch mAP 0.497894287 batch PCKh 0.5625\n",
      "Trained batch 1799 batch loss 0.623427033 batch mAP 0.464691162 batch PCKh 0.625\n",
      "Trained batch 1800 batch loss 0.664513052 batch mAP 0.451141357 batch PCKh 0.6875\n",
      "Trained batch 1801 batch loss 0.703714848 batch mAP 0.442504883 batch PCKh 0.4375\n",
      "Trained batch 1802 batch loss 0.653007925 batch mAP 0.472747803 batch PCKh 0.6875\n",
      "Trained batch 1803 batch loss 0.749310315 batch mAP 0.440460205 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1804 batch loss 0.615597546 batch mAP 0.440368652 batch PCKh 0.75\n",
      "Trained batch 1805 batch loss 0.620957494 batch mAP 0.363769531 batch PCKh 0.25\n",
      "Trained batch 1806 batch loss 0.649699152 batch mAP 0.339111328 batch PCKh 0.5\n",
      "Trained batch 1807 batch loss 0.604548275 batch mAP 0.335998535 batch PCKh 0.375\n",
      "Trained batch 1808 batch loss 0.692925453 batch mAP 0.402038574 batch PCKh 0.25\n",
      "Trained batch 1809 batch loss 0.640361667 batch mAP 0.370452881 batch PCKh 0.6875\n",
      "Trained batch 1810 batch loss 0.67350173 batch mAP 0.435516357 batch PCKh 0.4375\n",
      "Trained batch 1811 batch loss 0.569867 batch mAP 0.353851318 batch PCKh 0.75\n",
      "Trained batch 1812 batch loss 0.666622639 batch mAP 0.41015625 batch PCKh 0.375\n",
      "Trained batch 1813 batch loss 0.644883573 batch mAP 0.444976807 batch PCKh 0.6875\n",
      "Trained batch 1814 batch loss 0.579463243 batch mAP 0.446990967 batch PCKh 0.0625\n",
      "Trained batch 1815 batch loss 0.562378109 batch mAP 0.421539307 batch PCKh 0.4375\n",
      "Trained batch 1816 batch loss 0.652492464 batch mAP 0.438110352 batch PCKh 0.25\n",
      "Trained batch 1817 batch loss 0.546959043 batch mAP 0.446960449 batch PCKh 0.5625\n",
      "Trained batch 1818 batch loss 0.587895274 batch mAP 0.478271484 batch PCKh 0.5625\n",
      "Trained batch 1819 batch loss 0.560686111 batch mAP 0.465515137 batch PCKh 0.375\n",
      "Trained batch 1820 batch loss 0.502957582 batch mAP 0.432922363 batch PCKh 0.5\n",
      "Trained batch 1821 batch loss 0.59200716 batch mAP 0.536193848 batch PCKh 0.625\n",
      "Trained batch 1822 batch loss 0.539295733 batch mAP 0.548736572 batch PCKh 0.625\n",
      "Trained batch 1823 batch loss 0.624905348 batch mAP 0.462402344 batch PCKh 0.5\n",
      "Trained batch 1824 batch loss 0.606426477 batch mAP 0.514373779 batch PCKh 0.4375\n",
      "Trained batch 1825 batch loss 0.624049127 batch mAP 0.537658691 batch PCKh 0.25\n",
      "Trained batch 1826 batch loss 0.668994904 batch mAP 0.501464844 batch PCKh 0.3125\n",
      "Trained batch 1827 batch loss 0.670194149 batch mAP 0.498413086 batch PCKh 0.25\n",
      "Trained batch 1828 batch loss 0.680643201 batch mAP 0.51965332 batch PCKh 0.25\n",
      "Trained batch 1829 batch loss 0.606599271 batch mAP 0.549438477 batch PCKh 0.25\n",
      "Trained batch 1830 batch loss 0.550803959 batch mAP 0.591186523 batch PCKh 0.25\n",
      "Trained batch 1831 batch loss 0.557519495 batch mAP 0.567169189 batch PCKh 0.25\n",
      "Trained batch 1832 batch loss 0.557690799 batch mAP 0.548706055 batch PCKh 0.4375\n",
      "Trained batch 1833 batch loss 0.603494525 batch mAP 0.534423828 batch PCKh 0.375\n",
      "Trained batch 1834 batch loss 0.597623 batch mAP 0.53414917 batch PCKh 0.5625\n",
      "Trained batch 1835 batch loss 0.566226363 batch mAP 0.504821777 batch PCKh 0.25\n",
      "Trained batch 1836 batch loss 0.567091525 batch mAP 0.545532227 batch PCKh 0.625\n",
      "Trained batch 1837 batch loss 0.608759284 batch mAP 0.497528076 batch PCKh 0.125\n",
      "Trained batch 1838 batch loss 0.597273469 batch mAP 0.497436523 batch PCKh 0.5\n",
      "Trained batch 1839 batch loss 0.627425611 batch mAP 0.477325439 batch PCKh 0.3125\n",
      "Trained batch 1840 batch loss 0.603959084 batch mAP 0.476135254 batch PCKh 0.6875\n",
      "Trained batch 1841 batch loss 0.57643348 batch mAP 0.494415283 batch PCKh 0.6875\n",
      "Trained batch 1842 batch loss 0.580797672 batch mAP 0.500610352 batch PCKh 0.5625\n",
      "Trained batch 1843 batch loss 0.55095762 batch mAP 0.524688721 batch PCKh 0.6875\n",
      "Trained batch 1844 batch loss 0.582511783 batch mAP 0.489471436 batch PCKh 0.1875\n",
      "Trained batch 1845 batch loss 0.613269 batch mAP 0.483764648 batch PCKh 0.375\n",
      "Trained batch 1846 batch loss 0.615696 batch mAP 0.45614624 batch PCKh 0.875\n",
      "Trained batch 1847 batch loss 0.613248467 batch mAP 0.450012207 batch PCKh 0.75\n",
      "Trained batch 1848 batch loss 0.57230413 batch mAP 0.383178711 batch PCKh 0.1875\n",
      "Trained batch 1849 batch loss 0.544163108 batch mAP 0.327148438 batch PCKh 0.5625\n",
      "Trained batch 1850 batch loss 0.652247846 batch mAP 0.339324951 batch PCKh 0.25\n",
      "Trained batch 1851 batch loss 0.568341553 batch mAP 0.415588379 batch PCKh 0.6875\n",
      "Trained batch 1852 batch loss 0.613594174 batch mAP 0.450622559 batch PCKh 0.625\n",
      "Trained batch 1853 batch loss 0.574553668 batch mAP 0.405487061 batch PCKh 0.5625\n",
      "Trained batch 1854 batch loss 0.588697076 batch mAP 0.510742188 batch PCKh 0.375\n",
      "Trained batch 1855 batch loss 0.625323772 batch mAP 0.464416504 batch PCKh 0.25\n",
      "Trained batch 1856 batch loss 0.565152109 batch mAP 0.496887207 batch PCKh 0.375\n",
      "Trained batch 1857 batch loss 0.594230413 batch mAP 0.496673584 batch PCKh 0.3125\n",
      "Trained batch 1858 batch loss 0.563269675 batch mAP 0.507629395 batch PCKh 0.125\n",
      "Trained batch 1859 batch loss 0.5967 batch mAP 0.517150879 batch PCKh 0.125\n",
      "Trained batch 1860 batch loss 0.610246778 batch mAP 0.516296387 batch PCKh 0.0625\n",
      "Trained batch 1861 batch loss 0.592705607 batch mAP 0.501861572 batch PCKh 0.1875\n",
      "Trained batch 1862 batch loss 0.62699157 batch mAP 0.462524414 batch PCKh 0.0625\n",
      "Trained batch 1863 batch loss 0.681101084 batch mAP 0.37713623 batch PCKh 0.3125\n",
      "Trained batch 1864 batch loss 0.665811896 batch mAP 0.486877441 batch PCKh 0.25\n",
      "Trained batch 1865 batch loss 0.638735294 batch mAP 0.42678833 batch PCKh 0.6875\n",
      "Trained batch 1866 batch loss 0.573744237 batch mAP 0.492675781 batch PCKh 0.5\n",
      "Trained batch 1867 batch loss 0.583936095 batch mAP 0.449157715 batch PCKh 0.75\n",
      "Trained batch 1868 batch loss 0.540115237 batch mAP 0.501831055 batch PCKh 0.5625\n",
      "Trained batch 1869 batch loss 0.599121332 batch mAP 0.446685791 batch PCKh 0.375\n",
      "Trained batch 1870 batch loss 0.63084203 batch mAP 0.416931152 batch PCKh 0.5\n",
      "Trained batch 1871 batch loss 0.596146345 batch mAP 0.432006836 batch PCKh 0.25\n",
      "Trained batch 1872 batch loss 0.633662283 batch mAP 0.392089844 batch PCKh 0.75\n",
      "Trained batch 1873 batch loss 0.636855245 batch mAP 0.389892578 batch PCKh 0.3125\n",
      "Trained batch 1874 batch loss 0.64050436 batch mAP 0.435516357 batch PCKh 0.0625\n",
      "Trained batch 1875 batch loss 0.645367622 batch mAP 0.444122314 batch PCKh 0.25\n",
      "Trained batch 1876 batch loss 0.680471718 batch mAP 0.389526367 batch PCKh 0.375\n",
      "Trained batch 1877 batch loss 0.595965862 batch mAP 0.419372559 batch PCKh 0.75\n",
      "Trained batch 1878 batch loss 0.512925386 batch mAP 0.410400391 batch PCKh 0.625\n",
      "Trained batch 1879 batch loss 0.684223413 batch mAP 0.420013428 batch PCKh 0.5\n",
      "Trained batch 1880 batch loss 0.585194647 batch mAP 0.388183594 batch PCKh 0.3125\n",
      "Trained batch 1881 batch loss 0.602695227 batch mAP 0.372558594 batch PCKh 0.375\n",
      "Trained batch 1882 batch loss 0.635631204 batch mAP 0.381317139 batch PCKh 0.5\n",
      "Trained batch 1883 batch loss 0.62954396 batch mAP 0.417480469 batch PCKh 0.5625\n",
      "Trained batch 1884 batch loss 0.653742313 batch mAP 0.463409424 batch PCKh 0.125\n",
      "Trained batch 1885 batch loss 0.621920347 batch mAP 0.485168457 batch PCKh 0.1875\n",
      "Trained batch 1886 batch loss 0.597955227 batch mAP 0.44128418 batch PCKh 0.1875\n",
      "Trained batch 1887 batch loss 0.582066476 batch mAP 0.372528076 batch PCKh 0.625\n",
      "Trained batch 1888 batch loss 0.569040179 batch mAP 0.395751953 batch PCKh 0.625\n",
      "Trained batch 1889 batch loss 0.569176495 batch mAP 0.371063232 batch PCKh 0.6875\n",
      "Trained batch 1890 batch loss 0.487396538 batch mAP 0.481567383 batch PCKh 0.5\n",
      "Trained batch 1891 batch loss 0.609405756 batch mAP 0.48425293 batch PCKh 0.4375\n",
      "Trained batch 1892 batch loss 0.626827955 batch mAP 0.485900879 batch PCKh 0.625\n",
      "Trained batch 1893 batch loss 0.630151272 batch mAP 0.462219238 batch PCKh 0.4375\n",
      "Trained batch 1894 batch loss 0.619893074 batch mAP 0.44039917 batch PCKh 0.4375\n",
      "Trained batch 1895 batch loss 0.632440448 batch mAP 0.403015137 batch PCKh 0.3125\n",
      "Trained batch 1896 batch loss 0.710476875 batch mAP 0.406463623 batch PCKh 0\n",
      "Trained batch 1897 batch loss 0.632486761 batch mAP 0.395111084 batch PCKh 0.1875\n",
      "Trained batch 1898 batch loss 0.627736747 batch mAP 0.443664551 batch PCKh 0.75\n",
      "Trained batch 1899 batch loss 0.610879898 batch mAP 0.45010376 batch PCKh 0.8125\n",
      "Trained batch 1900 batch loss 0.518965781 batch mAP 0.509307861 batch PCKh 0.8125\n",
      "Trained batch 1901 batch loss 0.486321718 batch mAP 0.248077393 batch PCKh 0.5625\n",
      "Trained batch 1902 batch loss 0.521975875 batch mAP 0.31829834 batch PCKh 0.3125\n",
      "Trained batch 1903 batch loss 0.527523458 batch mAP 0.295532227 batch PCKh 0.8125\n",
      "Trained batch 1904 batch loss 0.49236 batch mAP 0.245849609 batch PCKh 0.8125\n",
      "Trained batch 1905 batch loss 0.465686381 batch mAP 0.266937256 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1906 batch loss 0.50545907 batch mAP 0.427612305 batch PCKh 0.6875\n",
      "Trained batch 1907 batch loss 0.595622599 batch mAP 0.468811035 batch PCKh 0.5\n",
      "Trained batch 1908 batch loss 0.489998072 batch mAP 0.453369141 batch PCKh 0.6875\n",
      "Trained batch 1909 batch loss 0.54920435 batch mAP 0.518066406 batch PCKh 0.625\n",
      "Trained batch 1910 batch loss 0.574652195 batch mAP 0.49130249 batch PCKh 0.4375\n",
      "Trained batch 1911 batch loss 0.639143229 batch mAP 0.50604248 batch PCKh 0.5625\n",
      "Trained batch 1912 batch loss 0.67550838 batch mAP 0.467163086 batch PCKh 0.75\n",
      "Trained batch 1913 batch loss 0.631097317 batch mAP 0.450164795 batch PCKh 0.125\n",
      "Trained batch 1914 batch loss 0.635630071 batch mAP 0.443084717 batch PCKh 0.6875\n",
      "Trained batch 1915 batch loss 0.718961 batch mAP 0.346832275 batch PCKh 0\n",
      "Trained batch 1916 batch loss 0.75071454 batch mAP 0.221679688 batch PCKh 0\n",
      "Trained batch 1917 batch loss 0.824700236 batch mAP 0.231628418 batch PCKh 0\n",
      "Trained batch 1918 batch loss 0.774377823 batch mAP 0.395813 batch PCKh 0.0625\n",
      "Trained batch 1919 batch loss 0.695931315 batch mAP 0.43939209 batch PCKh 0.125\n",
      "Trained batch 1920 batch loss 0.583971679 batch mAP 0.421112061 batch PCKh 0.4375\n",
      "Trained batch 1921 batch loss 0.738823891 batch mAP 0.378723145 batch PCKh 0\n",
      "Trained batch 1922 batch loss 0.612349391 batch mAP 0.376098633 batch PCKh 0.1875\n",
      "Trained batch 1923 batch loss 0.725722671 batch mAP 0.349212646 batch PCKh 0.125\n",
      "Trained batch 1924 batch loss 0.630578756 batch mAP 0.216308594 batch PCKh 0.4375\n",
      "Trained batch 1925 batch loss 0.66962254 batch mAP 0.330841064 batch PCKh 0.125\n",
      "Trained batch 1926 batch loss 0.607136846 batch mAP 0.170135498 batch PCKh 0.6875\n",
      "Trained batch 1927 batch loss 0.536423802 batch mAP 0.150634766 batch PCKh 0.6875\n",
      "Trained batch 1928 batch loss 0.53136915 batch mAP 0.347991943 batch PCKh 0.0625\n",
      "Trained batch 1929 batch loss 0.547278047 batch mAP 0.332061768 batch PCKh 0.375\n",
      "Trained batch 1930 batch loss 0.615935564 batch mAP 0.332550049 batch PCKh 0.25\n",
      "Trained batch 1931 batch loss 0.658769906 batch mAP 0.40725708 batch PCKh 0.3125\n",
      "Trained batch 1932 batch loss 0.682525396 batch mAP 0.432006836 batch PCKh 0.4375\n",
      "Trained batch 1933 batch loss 0.689984143 batch mAP 0.347961426 batch PCKh 0.125\n",
      "Trained batch 1934 batch loss 0.683306932 batch mAP 0.383880615 batch PCKh 0\n",
      "Trained batch 1935 batch loss 0.609205723 batch mAP 0.487915039 batch PCKh 0.625\n",
      "Trained batch 1936 batch loss 0.638937473 batch mAP 0.399414062 batch PCKh 0.6875\n",
      "Trained batch 1937 batch loss 0.603218079 batch mAP 0.496154785 batch PCKh 0.625\n",
      "Trained batch 1938 batch loss 0.537736535 batch mAP 0.481842041 batch PCKh 0.4375\n",
      "Trained batch 1939 batch loss 0.531829119 batch mAP 0.484039307 batch PCKh 0.4375\n",
      "Trained batch 1940 batch loss 0.613182962 batch mAP 0.554595947 batch PCKh 0.625\n",
      "Trained batch 1941 batch loss 0.514148712 batch mAP 0.487976074 batch PCKh 0\n",
      "Trained batch 1942 batch loss 0.515471101 batch mAP 0.444824219 batch PCKh 0.125\n",
      "Trained batch 1943 batch loss 0.543503046 batch mAP 0.50491333 batch PCKh 0.3125\n",
      "Trained batch 1944 batch loss 0.579478264 batch mAP 0.524810791 batch PCKh 0.5\n",
      "Trained batch 1945 batch loss 0.566729367 batch mAP 0.434204102 batch PCKh 0.625\n",
      "Trained batch 1946 batch loss 0.598922551 batch mAP 0.485168457 batch PCKh 0.375\n",
      "Trained batch 1947 batch loss 0.580672264 batch mAP 0.498779297 batch PCKh 0.5\n",
      "Trained batch 1948 batch loss 0.593221843 batch mAP 0.51348877 batch PCKh 0.5\n",
      "Trained batch 1949 batch loss 0.536393762 batch mAP 0.553161621 batch PCKh 0.4375\n",
      "Trained batch 1950 batch loss 0.611293614 batch mAP 0.533905 batch PCKh 0.625\n",
      "Trained batch 1951 batch loss 0.568132281 batch mAP 0.506073 batch PCKh 0.3125\n",
      "Trained batch 1952 batch loss 0.654946 batch mAP 0.505828857 batch PCKh 0.25\n",
      "Trained batch 1953 batch loss 0.636637807 batch mAP 0.504730225 batch PCKh 0.625\n",
      "Trained batch 1954 batch loss 0.609594226 batch mAP 0.487701416 batch PCKh 0.6875\n",
      "Trained batch 1955 batch loss 0.585845 batch mAP 0.569976807 batch PCKh 0.25\n",
      "Trained batch 1956 batch loss 0.576911569 batch mAP 0.54675293 batch PCKh 0.25\n",
      "Trained batch 1957 batch loss 0.595765114 batch mAP 0.567871094 batch PCKh 0.3125\n",
      "Trained batch 1958 batch loss 0.582829475 batch mAP 0.527648926 batch PCKh 0.5625\n",
      "Trained batch 1959 batch loss 0.622897565 batch mAP 0.511444092 batch PCKh 0.3125\n",
      "Trained batch 1960 batch loss 0.579405427 batch mAP 0.490600586 batch PCKh 0.125\n",
      "Trained batch 1961 batch loss 0.564641416 batch mAP 0.38369751 batch PCKh 0.5625\n",
      "Trained batch 1962 batch loss 0.557727098 batch mAP 0.497192383 batch PCKh 0.5\n",
      "Trained batch 1963 batch loss 0.594083786 batch mAP 0.452087402 batch PCKh 0.25\n",
      "Trained batch 1964 batch loss 0.59680742 batch mAP 0.445678711 batch PCKh 0.1875\n",
      "Trained batch 1965 batch loss 0.637682736 batch mAP 0.443664551 batch PCKh 0.3125\n",
      "Trained batch 1966 batch loss 0.590350211 batch mAP 0.458618164 batch PCKh 0.125\n",
      "Trained batch 1967 batch loss 0.598010898 batch mAP 0.442016602 batch PCKh 0.1875\n",
      "Trained batch 1968 batch loss 0.500286043 batch mAP 0.421478271 batch PCKh 0\n",
      "Trained batch 1969 batch loss 0.609389186 batch mAP 0.353179932 batch PCKh 0.0625\n",
      "Trained batch 1970 batch loss 0.573163152 batch mAP 0.325958252 batch PCKh 0.4375\n",
      "Trained batch 1971 batch loss 0.469080836 batch mAP 0.395019531 batch PCKh 0.0625\n",
      "Trained batch 1972 batch loss 0.445923835 batch mAP 0.405853271 batch PCKh 0\n",
      "Trained batch 1973 batch loss 0.457794726 batch mAP 0.275634766 batch PCKh 0\n",
      "Trained batch 1974 batch loss 0.589111865 batch mAP 0.235290527 batch PCKh 0.1875\n",
      "Trained batch 1975 batch loss 0.598426402 batch mAP 0.176300049 batch PCKh 0.1875\n",
      "Trained batch 1976 batch loss 0.720216393 batch mAP 0.2918396 batch PCKh 0.375\n",
      "Trained batch 1977 batch loss 0.634941459 batch mAP 0.264373779 batch PCKh 0.75\n",
      "Trained batch 1978 batch loss 0.606330693 batch mAP 0.337738037 batch PCKh 0.625\n",
      "Trained batch 1979 batch loss 0.607630253 batch mAP 0.356536865 batch PCKh 0.125\n",
      "Trained batch 1980 batch loss 0.650220394 batch mAP 0.423858643 batch PCKh 0.125\n",
      "Trained batch 1981 batch loss 0.599281371 batch mAP 0.40435791 batch PCKh 0\n",
      "Trained batch 1982 batch loss 0.623293757 batch mAP 0.44329834 batch PCKh 0.8125\n",
      "Trained batch 1983 batch loss 0.626263261 batch mAP 0.437591553 batch PCKh 0.125\n",
      "Trained batch 1984 batch loss 0.584890723 batch mAP 0.388641357 batch PCKh 0.8125\n",
      "Trained batch 1985 batch loss 0.512797236 batch mAP 0.157897949 batch PCKh 0.5625\n",
      "Trained batch 1986 batch loss 0.50394094 batch mAP 0.245178223 batch PCKh 0.3125\n",
      "Trained batch 1987 batch loss 0.569960475 batch mAP 0.211761475 batch PCKh 0.25\n",
      "Trained batch 1988 batch loss 0.532450438 batch mAP 0.266387939 batch PCKh 0.1875\n",
      "Trained batch 1989 batch loss 0.550067663 batch mAP 0.261016846 batch PCKh 0.75\n",
      "Trained batch 1990 batch loss 0.605941772 batch mAP 0.324768066 batch PCKh 0.4375\n",
      "Trained batch 1991 batch loss 0.583913147 batch mAP 0.203979492 batch PCKh 0.0625\n",
      "Trained batch 1992 batch loss 0.606503189 batch mAP 0.277557373 batch PCKh 0\n",
      "Trained batch 1993 batch loss 0.70388484 batch mAP 0.350524902 batch PCKh 0\n",
      "Trained batch 1994 batch loss 0.659449458 batch mAP 0.387084961 batch PCKh 0.5625\n",
      "Trained batch 1995 batch loss 0.597992897 batch mAP 0.49118042 batch PCKh 0\n",
      "Trained batch 1996 batch loss 0.567280889 batch mAP 0.476318359 batch PCKh 0.375\n",
      "Trained batch 1997 batch loss 0.609760761 batch mAP 0.428649902 batch PCKh 0.25\n",
      "Trained batch 1998 batch loss 0.570835829 batch mAP 0.431945801 batch PCKh 0.75\n",
      "Trained batch 1999 batch loss 0.590636492 batch mAP 0.476837158 batch PCKh 0.4375\n",
      "Trained batch 2000 batch loss 0.541560113 batch mAP 0.400665283 batch PCKh 0.25\n",
      "Trained batch 2001 batch loss 0.562567472 batch mAP 0.375793457 batch PCKh 0.4375\n",
      "Trained batch 2002 batch loss 0.548103929 batch mAP 0.420806885 batch PCKh 0.1875\n",
      "Trained batch 2003 batch loss 0.570518136 batch mAP 0.40524292 batch PCKh 0.4375\n",
      "Trained batch 2004 batch loss 0.638795257 batch mAP 0.458190918 batch PCKh 0.5\n",
      "Trained batch 2005 batch loss 0.69235 batch mAP 0.488037109 batch PCKh 0.125\n",
      "Trained batch 2006 batch loss 0.605478168 batch mAP 0.479736328 batch PCKh 0.25\n",
      "Trained batch 2007 batch loss 0.639000177 batch mAP 0.521850586 batch PCKh 0.125\n",
      "Trained batch 2008 batch loss 0.779567659 batch mAP 0.447937 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2009 batch loss 0.762608886 batch mAP 0.463226318 batch PCKh 0\n",
      "Trained batch 2010 batch loss 0.754622281 batch mAP 0.4453125 batch PCKh 0\n",
      "Trained batch 2011 batch loss 0.705994725 batch mAP 0.449462891 batch PCKh 0.125\n",
      "Trained batch 2012 batch loss 0.654528856 batch mAP 0.408294678 batch PCKh 0.75\n",
      "Trained batch 2013 batch loss 0.632333875 batch mAP 0.361724854 batch PCKh 0.75\n",
      "Trained batch 2014 batch loss 0.687734306 batch mAP 0.288024902 batch PCKh 0.75\n",
      "Trained batch 2015 batch loss 0.629037 batch mAP 0.160858154 batch PCKh 0.75\n",
      "Trained batch 2016 batch loss 0.642242432 batch mAP 0.112854004 batch PCKh 0.375\n",
      "Trained batch 2017 batch loss 0.701375961 batch mAP 0.0481262207 batch PCKh 0.8125\n",
      "Trained batch 2018 batch loss 0.605989933 batch mAP 0.0533752441 batch PCKh 0.8125\n",
      "Trained batch 2019 batch loss 0.656184435 batch mAP 0.0397644043 batch PCKh 0.4375\n",
      "Trained batch 2020 batch loss 0.674658179 batch mAP 0.228485107 batch PCKh 0.8125\n",
      "Trained batch 2021 batch loss 0.603689551 batch mAP 0.304626465 batch PCKh 0.75\n",
      "Trained batch 2022 batch loss 0.664453745 batch mAP 0.334564209 batch PCKh 0.1875\n",
      "Trained batch 2023 batch loss 0.633583367 batch mAP 0.395263672 batch PCKh 0.5\n",
      "Trained batch 2024 batch loss 0.610390663 batch mAP 0.459869385 batch PCKh 0.75\n",
      "Trained batch 2025 batch loss 0.57440871 batch mAP 0.488891602 batch PCKh 0.3125\n",
      "Trained batch 2026 batch loss 0.559651434 batch mAP 0.497619629 batch PCKh 0.25\n",
      "Trained batch 2027 batch loss 0.607485294 batch mAP 0.569915771 batch PCKh 0.25\n",
      "Trained batch 2028 batch loss 0.721943259 batch mAP 0.457489 batch PCKh 0\n",
      "Trained batch 2029 batch loss 0.629410267 batch mAP 0.309204102 batch PCKh 0.125\n",
      "Trained batch 2030 batch loss 0.656266 batch mAP 0.45501709 batch PCKh 0.4375\n",
      "Trained batch 2031 batch loss 0.623935759 batch mAP 0.44644165 batch PCKh 0.3125\n",
      "Trained batch 2032 batch loss 0.62845 batch mAP 0.291442871 batch PCKh 0.5625\n",
      "Trained batch 2033 batch loss 0.605820894 batch mAP 0.158691406 batch PCKh 0.3125\n",
      "Trained batch 2034 batch loss 0.605758369 batch mAP 0.179321289 batch PCKh 0.625\n",
      "Trained batch 2035 batch loss 0.575988412 batch mAP 0.438323975 batch PCKh 0.8125\n",
      "Trained batch 2036 batch loss 0.658075392 batch mAP 0.382324219 batch PCKh 0.4375\n",
      "Trained batch 2037 batch loss 0.567458391 batch mAP 0.508178711 batch PCKh 0.75\n",
      "Trained batch 2038 batch loss 0.560667634 batch mAP 0.523590088 batch PCKh 0.8125\n",
      "Trained batch 2039 batch loss 0.619156241 batch mAP 0.506958 batch PCKh 0.5625\n",
      "Trained batch 2040 batch loss 0.583153605 batch mAP 0.509307861 batch PCKh 0.4375\n",
      "Trained batch 2041 batch loss 0.627768397 batch mAP 0.510131836 batch PCKh 0.4375\n",
      "Trained batch 2042 batch loss 0.642637372 batch mAP 0.448577881 batch PCKh 0.1875\n",
      "Trained batch 2043 batch loss 0.561190069 batch mAP 0.439239502 batch PCKh 0.625\n",
      "Trained batch 2044 batch loss 0.619825184 batch mAP 0.449768066 batch PCKh 0.1875\n",
      "Trained batch 2045 batch loss 0.69159 batch mAP 0.473815918 batch PCKh 0.1875\n",
      "Trained batch 2046 batch loss 0.697248459 batch mAP 0.473724365 batch PCKh 0.75\n",
      "Trained batch 2047 batch loss 0.670778215 batch mAP 0.498138428 batch PCKh 0\n",
      "Trained batch 2048 batch loss 0.708630443 batch mAP 0.468048096 batch PCKh 0.5625\n",
      "Trained batch 2049 batch loss 0.663119733 batch mAP 0.476226807 batch PCKh 0.1875\n",
      "Trained batch 2050 batch loss 0.627151549 batch mAP 0.437225342 batch PCKh 0.6875\n",
      "Trained batch 2051 batch loss 0.630577266 batch mAP 0.400115967 batch PCKh 0.5\n",
      "Trained batch 2052 batch loss 0.663179755 batch mAP 0.464172363 batch PCKh 0.875\n",
      "Trained batch 2053 batch loss 0.61852324 batch mAP 0.348968506 batch PCKh 0.4375\n",
      "Trained batch 2054 batch loss 0.694669 batch mAP 0.433563232 batch PCKh 0\n",
      "Trained batch 2055 batch loss 0.68100071 batch mAP 0.44442749 batch PCKh 0.5625\n",
      "Trained batch 2056 batch loss 0.622471809 batch mAP 0.487792969 batch PCKh 0.75\n",
      "Trained batch 2057 batch loss 0.578454375 batch mAP 0.437744141 batch PCKh 0.6875\n",
      "Trained batch 2058 batch loss 0.676469207 batch mAP 0.461242676 batch PCKh 0.125\n",
      "Trained batch 2059 batch loss 0.691791 batch mAP 0.439788818 batch PCKh 0.25\n",
      "Trained batch 2060 batch loss 0.675283253 batch mAP 0.493713379 batch PCKh 0.6875\n",
      "Trained batch 2061 batch loss 0.683482349 batch mAP 0.45098877 batch PCKh 0.25\n",
      "Trained batch 2062 batch loss 0.638204336 batch mAP 0.446685791 batch PCKh 0.375\n",
      "Trained batch 2063 batch loss 0.629811287 batch mAP 0.513305664 batch PCKh 0.4375\n",
      "Trained batch 2064 batch loss 0.611599386 batch mAP 0.51373291 batch PCKh 0.5625\n",
      "Trained batch 2065 batch loss 0.692053556 batch mAP 0.487701416 batch PCKh 0.4375\n",
      "Trained batch 2066 batch loss 0.648812056 batch mAP 0.47756958 batch PCKh 0.875\n",
      "Trained batch 2067 batch loss 0.587114215 batch mAP 0.489257812 batch PCKh 0.8125\n",
      "Trained batch 2068 batch loss 0.630942702 batch mAP 0.472717285 batch PCKh 0.8125\n",
      "Trained batch 2069 batch loss 0.593887269 batch mAP 0.499755859 batch PCKh 0.625\n",
      "Trained batch 2070 batch loss 0.604268551 batch mAP 0.478179932 batch PCKh 0.3125\n",
      "Trained batch 2071 batch loss 0.701691329 batch mAP 0.475738525 batch PCKh 0.4375\n",
      "Trained batch 2072 batch loss 0.529159069 batch mAP 0.432617188 batch PCKh 0\n",
      "Trained batch 2073 batch loss 0.683600664 batch mAP 0.467865 batch PCKh 0.5625\n",
      "Trained batch 2074 batch loss 0.597704887 batch mAP 0.46182251 batch PCKh 0.6875\n",
      "Trained batch 2075 batch loss 0.677977502 batch mAP 0.442626953 batch PCKh 0.5\n",
      "Trained batch 2076 batch loss 0.621167839 batch mAP 0.485443115 batch PCKh 0.5\n",
      "Trained batch 2077 batch loss 0.616088271 batch mAP 0.477874756 batch PCKh 0.375\n",
      "Trained batch 2078 batch loss 0.642361164 batch mAP 0.48739624 batch PCKh 0.5\n",
      "Trained batch 2079 batch loss 0.565327168 batch mAP 0.465942383 batch PCKh 0.75\n",
      "Trained batch 2080 batch loss 0.542673349 batch mAP 0.521850586 batch PCKh 0\n",
      "Trained batch 2081 batch loss 0.529889703 batch mAP 0.546630859 batch PCKh 0.6875\n",
      "Trained batch 2082 batch loss 0.597504437 batch mAP 0.519439697 batch PCKh 0.5\n",
      "Trained batch 2083 batch loss 0.558131 batch mAP 0.521179199 batch PCKh 0.125\n",
      "Trained batch 2084 batch loss 0.614611626 batch mAP 0.517883301 batch PCKh 0.125\n",
      "Trained batch 2085 batch loss 0.604687691 batch mAP 0.512481689 batch PCKh 0.125\n",
      "Trained batch 2086 batch loss 0.627295732 batch mAP 0.557861328 batch PCKh 0.1875\n",
      "Trained batch 2087 batch loss 0.588298202 batch mAP 0.496307373 batch PCKh 0.3125\n",
      "Trained batch 2088 batch loss 0.598603606 batch mAP 0.473632812 batch PCKh 0.6875\n",
      "Trained batch 2089 batch loss 0.627020955 batch mAP 0.49710083 batch PCKh 0.375\n",
      "Trained batch 2090 batch loss 0.511692584 batch mAP 0.548553467 batch PCKh 0.5625\n",
      "Trained batch 2091 batch loss 0.559966743 batch mAP 0.518951416 batch PCKh 0.1875\n",
      "Trained batch 2092 batch loss 0.574551165 batch mAP 0.523407 batch PCKh 0.6875\n",
      "Trained batch 2093 batch loss 0.619479537 batch mAP 0.550476074 batch PCKh 0.375\n",
      "Trained batch 2094 batch loss 0.743342161 batch mAP 0.518859863 batch PCKh 0.625\n",
      "Trained batch 2095 batch loss 0.62859 batch mAP 0.507171631 batch PCKh 0.5\n",
      "Trained batch 2096 batch loss 0.67611891 batch mAP 0.492767334 batch PCKh 0.25\n",
      "Trained batch 2097 batch loss 0.728697658 batch mAP 0.451751709 batch PCKh 0.3125\n",
      "Trained batch 2098 batch loss 0.553375602 batch mAP 0.511444092 batch PCKh 0.5\n",
      "Trained batch 2099 batch loss 0.591544032 batch mAP 0.541442871 batch PCKh 0.5\n",
      "Trained batch 2100 batch loss 0.543005586 batch mAP 0.511871338 batch PCKh 0.0625\n",
      "Trained batch 2101 batch loss 0.621893048 batch mAP 0.50112915 batch PCKh 0.5625\n",
      "Trained batch 2102 batch loss 0.572067559 batch mAP 0.474456787 batch PCKh 0.25\n",
      "Trained batch 2103 batch loss 0.666239858 batch mAP 0.504333496 batch PCKh 0.0625\n",
      "Trained batch 2104 batch loss 0.602553725 batch mAP 0.46105957 batch PCKh 0.1875\n",
      "Trained batch 2105 batch loss 0.608715773 batch mAP 0.487854 batch PCKh 0.3125\n",
      "Trained batch 2106 batch loss 0.622732639 batch mAP 0.493103027 batch PCKh 0.875\n",
      "Trained batch 2107 batch loss 0.648667037 batch mAP 0.462860107 batch PCKh 0.1875\n",
      "Trained batch 2108 batch loss 0.598372579 batch mAP 0.441467285 batch PCKh 0.75\n",
      "Trained batch 2109 batch loss 0.618866861 batch mAP 0.478057861 batch PCKh 0.4375\n",
      "Trained batch 2110 batch loss 0.639792442 batch mAP 0.541503906 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2111 batch loss 0.587926567 batch mAP 0.465118408 batch PCKh 0.0625\n",
      "Trained batch 2112 batch loss 0.614230454 batch mAP 0.485198975 batch PCKh 0.5\n",
      "Trained batch 2113 batch loss 0.636435568 batch mAP 0.498779297 batch PCKh 0.5\n",
      "Trained batch 2114 batch loss 0.654051602 batch mAP 0.479095459 batch PCKh 0.625\n",
      "Trained batch 2115 batch loss 0.642626464 batch mAP 0.461364746 batch PCKh 0.1875\n",
      "Trained batch 2116 batch loss 0.653522432 batch mAP 0.469177246 batch PCKh 0.25\n",
      "Trained batch 2117 batch loss 0.682667613 batch mAP 0.482269287 batch PCKh 0.5\n",
      "Trained batch 2118 batch loss 0.574371636 batch mAP 0.476135254 batch PCKh 0.75\n",
      "Trained batch 2119 batch loss 0.63930577 batch mAP 0.457122803 batch PCKh 0.25\n",
      "Trained batch 2120 batch loss 0.637591839 batch mAP 0.449066162 batch PCKh 0.375\n",
      "Trained batch 2121 batch loss 0.680536389 batch mAP 0.511230469 batch PCKh 0.0625\n",
      "Trained batch 2122 batch loss 0.666831136 batch mAP 0.423339844 batch PCKh 0.6875\n",
      "Trained batch 2123 batch loss 0.636360705 batch mAP 0.43762207 batch PCKh 0.375\n",
      "Trained batch 2124 batch loss 0.581411242 batch mAP 0.436645508 batch PCKh 0.375\n",
      "Trained batch 2125 batch loss 0.624191165 batch mAP 0.442657471 batch PCKh 0.75\n",
      "Trained batch 2126 batch loss 0.583830237 batch mAP 0.442962646 batch PCKh 0\n",
      "Trained batch 2127 batch loss 0.642364264 batch mAP 0.400268555 batch PCKh 0.0625\n",
      "Trained batch 2128 batch loss 0.695264041 batch mAP 0.445495605 batch PCKh 0.3125\n",
      "Trained batch 2129 batch loss 0.591072559 batch mAP 0.460479736 batch PCKh 0.1875\n",
      "Trained batch 2130 batch loss 0.606907427 batch mAP 0.513031 batch PCKh 0.4375\n",
      "Trained batch 2131 batch loss 0.614691436 batch mAP 0.518920898 batch PCKh 0.3125\n",
      "Trained batch 2132 batch loss 0.647766352 batch mAP 0.530303955 batch PCKh 0.1875\n",
      "Trained batch 2133 batch loss 0.656979084 batch mAP 0.50289917 batch PCKh 0.375\n",
      "Trained batch 2134 batch loss 0.632634938 batch mAP 0.462982178 batch PCKh 0.1875\n",
      "Trained batch 2135 batch loss 0.643643081 batch mAP 0.382507324 batch PCKh 0.1875\n",
      "Trained batch 2136 batch loss 0.525941908 batch mAP 0.148223877 batch PCKh 0\n",
      "Trained batch 2137 batch loss 0.512913227 batch mAP 0.190917969 batch PCKh 0.1875\n",
      "Trained batch 2138 batch loss 0.635770261 batch mAP 0.0885925293 batch PCKh 0\n",
      "Trained batch 2139 batch loss 0.628330171 batch mAP 0.201812744 batch PCKh 0\n",
      "Trained batch 2140 batch loss 0.681814551 batch mAP 0.396881104 batch PCKh 0.8125\n",
      "Trained batch 2141 batch loss 0.728858769 batch mAP 0.335754395 batch PCKh 0\n",
      "Trained batch 2142 batch loss 0.708601 batch mAP 0.469360352 batch PCKh 0.375\n",
      "Trained batch 2143 batch loss 0.646840036 batch mAP 0.49041748 batch PCKh 0.3125\n",
      "Trained batch 2144 batch loss 0.547565401 batch mAP 0.406799316 batch PCKh 0.5625\n",
      "Trained batch 2145 batch loss 0.492644101 batch mAP 0.40625 batch PCKh 0.5\n",
      "Trained batch 2146 batch loss 0.546393037 batch mAP 0.469085693 batch PCKh 0.375\n",
      "Trained batch 2147 batch loss 0.562365055 batch mAP 0.502197266 batch PCKh 0.625\n",
      "Trained batch 2148 batch loss 0.523947299 batch mAP 0.465087891 batch PCKh 0.0625\n",
      "Trained batch 2149 batch loss 0.548130274 batch mAP 0.388580322 batch PCKh 0.4375\n",
      "Trained batch 2150 batch loss 0.600016296 batch mAP 0.361572266 batch PCKh 0.375\n",
      "Trained batch 2151 batch loss 0.520197 batch mAP 0.386810303 batch PCKh 0.5625\n",
      "Trained batch 2152 batch loss 0.511684656 batch mAP 0.461334229 batch PCKh 0.5\n",
      "Trained batch 2153 batch loss 0.517745733 batch mAP 0.517669678 batch PCKh 0.4375\n",
      "Trained batch 2154 batch loss 0.49991858 batch mAP 0.558624268 batch PCKh 0.375\n",
      "Trained batch 2155 batch loss 0.523289561 batch mAP 0.506591797 batch PCKh 0.25\n",
      "Trained batch 2156 batch loss 0.545596242 batch mAP 0.464935303 batch PCKh 0.6875\n",
      "Trained batch 2157 batch loss 0.524477661 batch mAP 0.291229248 batch PCKh 0.125\n",
      "Trained batch 2158 batch loss 0.564371169 batch mAP 0.368408203 batch PCKh 0.875\n",
      "Trained batch 2159 batch loss 0.620311618 batch mAP 0.25390625 batch PCKh 0.875\n",
      "Trained batch 2160 batch loss 0.587640882 batch mAP 0.300689697 batch PCKh 0.875\n",
      "Trained batch 2161 batch loss 0.56360507 batch mAP 0.429473877 batch PCKh 0.6875\n",
      "Trained batch 2162 batch loss 0.628848314 batch mAP 0.395172119 batch PCKh 0.875\n",
      "Trained batch 2163 batch loss 0.625937581 batch mAP 0.381073 batch PCKh 0.75\n",
      "Trained batch 2164 batch loss 0.682376504 batch mAP 0.408721924 batch PCKh 0.875\n",
      "Trained batch 2165 batch loss 0.612475574 batch mAP 0.476348877 batch PCKh 0.75\n",
      "Trained batch 2166 batch loss 0.568173528 batch mAP 0.444519043 batch PCKh 0.625\n",
      "Trained batch 2167 batch loss 0.722073257 batch mAP 0.435089111 batch PCKh 0.5625\n",
      "Trained batch 2168 batch loss 0.603746295 batch mAP 0.41003418 batch PCKh 0.125\n",
      "Trained batch 2169 batch loss 0.668492675 batch mAP 0.430511475 batch PCKh 0.0625\n",
      "Trained batch 2170 batch loss 0.567794323 batch mAP 0.291778564 batch PCKh 0.25\n",
      "Trained batch 2171 batch loss 0.69791311 batch mAP 0.203887939 batch PCKh 0.3125\n",
      "Trained batch 2172 batch loss 0.642194033 batch mAP 0.129150391 batch PCKh 0.4375\n",
      "Trained batch 2173 batch loss 0.506372571 batch mAP 0.0772094727 batch PCKh 0.25\n",
      "Trained batch 2174 batch loss 0.620377183 batch mAP 0.0716247559 batch PCKh 0.6875\n",
      "Trained batch 2175 batch loss 0.605258942 batch mAP 0.0422973633 batch PCKh 0.6875\n",
      "Trained batch 2176 batch loss 0.522675097 batch mAP 0.0133666992 batch PCKh 0.5625\n",
      "Trained batch 2177 batch loss 0.431749851 batch mAP 0.0157775879 batch PCKh 0.4375\n",
      "Trained batch 2178 batch loss 0.505866408 batch mAP 0.0271911621 batch PCKh 0.6875\n",
      "Trained batch 2179 batch loss 0.557601869 batch mAP 0.067779541 batch PCKh 0\n",
      "Trained batch 2180 batch loss 0.485466123 batch mAP 0.160980225 batch PCKh 0.5625\n",
      "Trained batch 2181 batch loss 0.716405034 batch mAP 0.424499512 batch PCKh 0\n",
      "Trained batch 2182 batch loss 0.874017358 batch mAP 0.381347656 batch PCKh 0\n",
      "Trained batch 2183 batch loss 0.721022666 batch mAP 0.450500488 batch PCKh 0\n",
      "Trained batch 2184 batch loss 0.679070234 batch mAP 0.450469971 batch PCKh 0.25\n",
      "Trained batch 2185 batch loss 0.587822497 batch mAP 0.32635498 batch PCKh 0.0625\n",
      "Trained batch 2186 batch loss 0.617636085 batch mAP 0.384246826 batch PCKh 0.3125\n",
      "Trained batch 2187 batch loss 0.55328083 batch mAP 0.370361328 batch PCKh 0.6875\n",
      "Trained batch 2188 batch loss 0.576619744 batch mAP 0.443481445 batch PCKh 0.1875\n",
      "Trained batch 2189 batch loss 0.659379363 batch mAP 0.475189209 batch PCKh 0\n",
      "Trained batch 2190 batch loss 0.620305896 batch mAP 0.48349 batch PCKh 0.125\n",
      "Trained batch 2191 batch loss 0.581871808 batch mAP 0.470062256 batch PCKh 0.1875\n",
      "Trained batch 2192 batch loss 0.582000613 batch mAP 0.46496582 batch PCKh 0.75\n",
      "Trained batch 2193 batch loss 0.575263381 batch mAP 0.491210938 batch PCKh 0.1875\n",
      "Trained batch 2194 batch loss 0.548217416 batch mAP 0.513519287 batch PCKh 0.5625\n",
      "Trained batch 2195 batch loss 0.613046646 batch mAP 0.436035156 batch PCKh 0.4375\n",
      "Trained batch 2196 batch loss 0.582820892 batch mAP 0.294830322 batch PCKh 0.25\n",
      "Trained batch 2197 batch loss 0.592418969 batch mAP 0.525115967 batch PCKh 0.625\n",
      "Trained batch 2198 batch loss 0.622587323 batch mAP 0.325317383 batch PCKh 0.25\n",
      "Trained batch 2199 batch loss 0.607009709 batch mAP 0.509094238 batch PCKh 0.5\n",
      "Trained batch 2200 batch loss 0.608736038 batch mAP 0.4581604 batch PCKh 0.0625\n",
      "Trained batch 2201 batch loss 0.63872081 batch mAP 0.496459961 batch PCKh 0.8125\n",
      "Trained batch 2202 batch loss 0.663183 batch mAP 0.316375732 batch PCKh 0.3125\n",
      "Trained batch 2203 batch loss 0.551243305 batch mAP 0.520965576 batch PCKh 0.375\n",
      "Trained batch 2204 batch loss 0.655370951 batch mAP 0.255310059 batch PCKh 0.875\n",
      "Trained batch 2205 batch loss 0.639329672 batch mAP 0.446228027 batch PCKh 0.125\n",
      "Trained batch 2206 batch loss 0.583105087 batch mAP 0.51776123 batch PCKh 0.1875\n",
      "Trained batch 2207 batch loss 0.538429737 batch mAP 0.603393555 batch PCKh 0.375\n",
      "Trained batch 2208 batch loss 0.601076961 batch mAP 0.53805542 batch PCKh 0.4375\n",
      "Trained batch 2209 batch loss 0.564808547 batch mAP 0.610687256 batch PCKh 0.3125\n",
      "Trained batch 2210 batch loss 0.537979126 batch mAP 0.609283447 batch PCKh 0.1875\n",
      "Trained batch 2211 batch loss 0.604688168 batch mAP 0.539459229 batch PCKh 0.3125\n",
      "Trained batch 2212 batch loss 0.646684468 batch mAP 0.566436768 batch PCKh 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2213 batch loss 0.627735 batch mAP 0.537597656 batch PCKh 0.3125\n",
      "Trained batch 2214 batch loss 0.557778955 batch mAP 0.559967041 batch PCKh 0.1875\n",
      "Trained batch 2215 batch loss 0.59350419 batch mAP 0.519622803 batch PCKh 0.3125\n",
      "Trained batch 2216 batch loss 0.539061308 batch mAP 0.55380249 batch PCKh 0.5\n",
      "Trained batch 2217 batch loss 0.588483572 batch mAP 0.553894043 batch PCKh 0.4375\n",
      "Trained batch 2218 batch loss 0.614388645 batch mAP 0.534332275 batch PCKh 0.5625\n",
      "Trained batch 2219 batch loss 0.597799897 batch mAP 0.552246094 batch PCKh 0.25\n",
      "Trained batch 2220 batch loss 0.529976428 batch mAP 0.592041 batch PCKh 0.1875\n",
      "Trained batch 2221 batch loss 0.611746132 batch mAP 0.562652588 batch PCKh 0.125\n",
      "Trained batch 2222 batch loss 0.562561274 batch mAP 0.520324707 batch PCKh 0.5625\n",
      "Trained batch 2223 batch loss 0.596031785 batch mAP 0.476013184 batch PCKh 0.1875\n",
      "Trained batch 2224 batch loss 0.633949697 batch mAP 0.484710693 batch PCKh 0.5625\n",
      "Trained batch 2225 batch loss 0.639981568 batch mAP 0.500671387 batch PCKh 0.5625\n",
      "Trained batch 2226 batch loss 0.557891071 batch mAP 0.493896484 batch PCKh 0.5625\n",
      "Trained batch 2227 batch loss 0.513507128 batch mAP 0.472900391 batch PCKh 0.25\n",
      "Trained batch 2228 batch loss 0.507419348 batch mAP 0.449157715 batch PCKh 0.3125\n",
      "Trained batch 2229 batch loss 0.5671857 batch mAP 0.438232422 batch PCKh 0.3125\n",
      "Trained batch 2230 batch loss 0.656553864 batch mAP 0.391845703 batch PCKh 0.1875\n",
      "Trained batch 2231 batch loss 0.568614066 batch mAP 0.438171387 batch PCKh 0.0625\n",
      "Trained batch 2232 batch loss 0.552732468 batch mAP 0.405822754 batch PCKh 0.375\n",
      "Trained batch 2233 batch loss 0.577016234 batch mAP 0.473266602 batch PCKh 0.4375\n",
      "Trained batch 2234 batch loss 0.523082554 batch mAP 0.488189697 batch PCKh 0.125\n",
      "Trained batch 2235 batch loss 0.578867197 batch mAP 0.477111816 batch PCKh 0.3125\n",
      "Trained batch 2236 batch loss 0.543590128 batch mAP 0.516937256 batch PCKh 0\n",
      "Trained batch 2237 batch loss 0.536202133 batch mAP 0.438537598 batch PCKh 0.5625\n",
      "Trained batch 2238 batch loss 0.580094337 batch mAP 0.462677 batch PCKh 0.0625\n",
      "Trained batch 2239 batch loss 0.516795 batch mAP 0.525512695 batch PCKh 0.25\n",
      "Trained batch 2240 batch loss 0.631425798 batch mAP 0.502960205 batch PCKh 0\n",
      "Trained batch 2241 batch loss 0.624778211 batch mAP 0.500091553 batch PCKh 0.3125\n",
      "Trained batch 2242 batch loss 0.628545463 batch mAP 0.474090576 batch PCKh 0.1875\n",
      "Trained batch 2243 batch loss 0.543890119 batch mAP 0.534423828 batch PCKh 0.5\n",
      "Trained batch 2244 batch loss 0.642530322 batch mAP 0.541778564 batch PCKh 0.3125\n",
      "Trained batch 2245 batch loss 0.608924031 batch mAP 0.558837891 batch PCKh 0.25\n",
      "Trained batch 2246 batch loss 0.611815333 batch mAP 0.570098877 batch PCKh 0.6875\n",
      "Trained batch 2247 batch loss 0.647773623 batch mAP 0.587677 batch PCKh 0.25\n",
      "Trained batch 2248 batch loss 0.651367188 batch mAP 0.537200928 batch PCKh 0.25\n",
      "Trained batch 2249 batch loss 0.584427595 batch mAP 0.594787598 batch PCKh 0.25\n",
      "Trained batch 2250 batch loss 0.651695 batch mAP 0.492431641 batch PCKh 0.5\n",
      "Trained batch 2251 batch loss 0.687742591 batch mAP 0.452209473 batch PCKh 0\n",
      "Trained batch 2252 batch loss 0.659922957 batch mAP 0.558654785 batch PCKh 0.125\n",
      "Trained batch 2253 batch loss 0.664390802 batch mAP 0.567718506 batch PCKh 0.3125\n",
      "Trained batch 2254 batch loss 0.614131868 batch mAP 0.597442627 batch PCKh 0.6875\n",
      "Trained batch 2255 batch loss 0.631112814 batch mAP 0.526245117 batch PCKh 0.25\n",
      "Trained batch 2256 batch loss 0.572974622 batch mAP 0.513214111 batch PCKh 0.5\n",
      "Trained batch 2257 batch loss 0.658280492 batch mAP 0.488037109 batch PCKh 0.375\n",
      "Trained batch 2258 batch loss 0.61918658 batch mAP 0.509185791 batch PCKh 0.5\n",
      "Trained batch 2259 batch loss 0.66040349 batch mAP 0.454864502 batch PCKh 0.6875\n",
      "Trained batch 2260 batch loss 0.612008333 batch mAP 0.447906494 batch PCKh 0.6875\n",
      "Trained batch 2261 batch loss 0.644443393 batch mAP 0.424316406 batch PCKh 0.375\n",
      "Trained batch 2262 batch loss 0.624062657 batch mAP 0.418426514 batch PCKh 0.4375\n",
      "Trained batch 2263 batch loss 0.605923414 batch mAP 0.307250977 batch PCKh 0.125\n",
      "Trained batch 2264 batch loss 0.702260733 batch mAP 0.225738525 batch PCKh 0.875\n",
      "Trained batch 2265 batch loss 0.633046448 batch mAP 0.233184814 batch PCKh 0.375\n",
      "Trained batch 2266 batch loss 0.518132091 batch mAP 0.295227051 batch PCKh 0.1875\n",
      "Trained batch 2267 batch loss 0.539475799 batch mAP 0.379730225 batch PCKh 0.5625\n",
      "Trained batch 2268 batch loss 0.504159451 batch mAP 0.319702148 batch PCKh 0.625\n",
      "Trained batch 2269 batch loss 0.522473216 batch mAP 0.230712891 batch PCKh 0.5\n",
      "Trained batch 2270 batch loss 0.605374038 batch mAP 0.383483887 batch PCKh 0.1875\n",
      "Trained batch 2271 batch loss 0.603578746 batch mAP 0.27911377 batch PCKh 0.6875\n",
      "Trained batch 2272 batch loss 0.707563162 batch mAP 0.399871826 batch PCKh 0.375\n",
      "Trained batch 2273 batch loss 0.611363351 batch mAP 0.344177246 batch PCKh 0.3125\n",
      "Trained batch 2274 batch loss 0.711139381 batch mAP 0.385345459 batch PCKh 0.6875\n",
      "Trained batch 2275 batch loss 0.706356108 batch mAP 0.342163086 batch PCKh 0.0625\n",
      "Trained batch 2276 batch loss 0.696774244 batch mAP 0.310699463 batch PCKh 0.0625\n",
      "Trained batch 2277 batch loss 0.632416606 batch mAP 0.413269043 batch PCKh 0.6875\n",
      "Trained batch 2278 batch loss 0.662301421 batch mAP 0.437194824 batch PCKh 0.0625\n",
      "Trained batch 2279 batch loss 0.707292676 batch mAP 0.470092773 batch PCKh 0\n",
      "Trained batch 2280 batch loss 0.667076826 batch mAP 0.461730957 batch PCKh 0.4375\n",
      "Trained batch 2281 batch loss 0.658199966 batch mAP 0.521881104 batch PCKh 0.0625\n",
      "Trained batch 2282 batch loss 0.671128929 batch mAP 0.508270264 batch PCKh 0.3125\n",
      "Trained batch 2283 batch loss 0.618426561 batch mAP 0.545349121 batch PCKh 0.1875\n",
      "Trained batch 2284 batch loss 0.679295778 batch mAP 0.474182129 batch PCKh 0.3125\n",
      "Trained batch 2285 batch loss 0.637429357 batch mAP 0.45892334 batch PCKh 0.6875\n",
      "Trained batch 2286 batch loss 0.636339664 batch mAP 0.480255127 batch PCKh 0.5625\n",
      "Trained batch 2287 batch loss 0.584475 batch mAP 0.466949463 batch PCKh 0.375\n",
      "Trained batch 2288 batch loss 0.630824327 batch mAP 0.508972168 batch PCKh 0.6875\n",
      "Trained batch 2289 batch loss 0.689872086 batch mAP 0.499755859 batch PCKh 0.375\n",
      "Trained batch 2290 batch loss 0.664756 batch mAP 0.436950684 batch PCKh 0.5\n",
      "Trained batch 2291 batch loss 0.634922206 batch mAP 0.426239 batch PCKh 0.125\n",
      "Trained batch 2292 batch loss 0.606094837 batch mAP 0.441589355 batch PCKh 0.875\n",
      "Trained batch 2293 batch loss 0.694523573 batch mAP 0.421875 batch PCKh 0.0625\n",
      "Trained batch 2294 batch loss 0.654631257 batch mAP 0.448944092 batch PCKh 0.5625\n",
      "Trained batch 2295 batch loss 0.70928359 batch mAP 0.386169434 batch PCKh 0.1875\n",
      "Trained batch 2296 batch loss 0.624692082 batch mAP 0.423736572 batch PCKh 0.5\n",
      "Trained batch 2297 batch loss 0.638135195 batch mAP 0.422912598 batch PCKh 0.3125\n",
      "Trained batch 2298 batch loss 0.651654 batch mAP 0.400817871 batch PCKh 0.5625\n",
      "Trained batch 2299 batch loss 0.603832543 batch mAP 0.372161865 batch PCKh 0\n",
      "Trained batch 2300 batch loss 0.652972758 batch mAP 0.396820068 batch PCKh 0.75\n",
      "Trained batch 2301 batch loss 0.585443258 batch mAP 0.432128906 batch PCKh 0.5625\n",
      "Trained batch 2302 batch loss 0.738271356 batch mAP 0.45501709 batch PCKh 0.5625\n",
      "Trained batch 2303 batch loss 0.516563892 batch mAP 0.49508667 batch PCKh 0.5625\n",
      "Trained batch 2304 batch loss 0.498855799 batch mAP 0.469085693 batch PCKh 0.5\n",
      "Trained batch 2305 batch loss 0.515893638 batch mAP 0.486328125 batch PCKh 0.75\n",
      "Trained batch 2306 batch loss 0.508015513 batch mAP 0.493804932 batch PCKh 0.5625\n",
      "Trained batch 2307 batch loss 0.534934402 batch mAP 0.521820068 batch PCKh 0.625\n",
      "Trained batch 2308 batch loss 0.476280928 batch mAP 0.511138916 batch PCKh 0.625\n",
      "Trained batch 2309 batch loss 0.521194756 batch mAP 0.47479248 batch PCKh 0.75\n",
      "Trained batch 2310 batch loss 0.561553597 batch mAP 0.431213379 batch PCKh 0.4375\n",
      "Trained batch 2311 batch loss 0.649183929 batch mAP 0.368042 batch PCKh 0.6875\n",
      "Trained batch 2312 batch loss 0.667007625 batch mAP 0.37109375 batch PCKh 0.6875\n",
      "Trained batch 2313 batch loss 0.619241714 batch mAP 0.397918701 batch PCKh 0.5625\n",
      "Trained batch 2314 batch loss 0.696868062 batch mAP 0.292938232 batch PCKh 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2315 batch loss 0.715861261 batch mAP 0.445098877 batch PCKh 0.1875\n",
      "Trained batch 2316 batch loss 0.637218714 batch mAP 0.492523193 batch PCKh 0.3125\n",
      "Trained batch 2317 batch loss 0.737714827 batch mAP 0.491241455 batch PCKh 0.4375\n",
      "Trained batch 2318 batch loss 0.549781203 batch mAP 0.439575195 batch PCKh 0.25\n",
      "Trained batch 2319 batch loss 0.572127759 batch mAP 0.344604492 batch PCKh 0.375\n",
      "Trained batch 2320 batch loss 0.538410306 batch mAP 0.282073975 batch PCKh 0.5\n",
      "Trained batch 2321 batch loss 0.565596223 batch mAP 0.272399902 batch PCKh 0.3125\n",
      "Trained batch 2322 batch loss 0.580832541 batch mAP 0.223876953 batch PCKh 0.375\n",
      "Trained batch 2323 batch loss 0.621549428 batch mAP 0.303863525 batch PCKh 0.3125\n",
      "Trained batch 2324 batch loss 0.672528744 batch mAP 0.339233398 batch PCKh 0.4375\n",
      "Trained batch 2325 batch loss 0.629433632 batch mAP 0.35144043 batch PCKh 0.75\n",
      "Trained batch 2326 batch loss 0.588381708 batch mAP 0.347290039 batch PCKh 0.75\n",
      "Trained batch 2327 batch loss 0.662446499 batch mAP 0.443084717 batch PCKh 0.1875\n",
      "Trained batch 2328 batch loss 0.650524735 batch mAP 0.463012695 batch PCKh 0.25\n",
      "Trained batch 2329 batch loss 0.582806885 batch mAP 0.463623047 batch PCKh 0.125\n",
      "Trained batch 2330 batch loss 0.573393404 batch mAP 0.517852783 batch PCKh 0.875\n",
      "Trained batch 2331 batch loss 0.658901811 batch mAP 0.457000732 batch PCKh 0.5625\n",
      "Trained batch 2332 batch loss 0.724555552 batch mAP 0.297515869 batch PCKh 0.3125\n",
      "Trained batch 2333 batch loss 0.730653048 batch mAP 0.325500488 batch PCKh 0.375\n",
      "Trained batch 2334 batch loss 0.635804176 batch mAP 0.220275879 batch PCKh 0.875\n",
      "Trained batch 2335 batch loss 0.536389649 batch mAP 0.487426758 batch PCKh 0.75\n",
      "Trained batch 2336 batch loss 0.627536058 batch mAP 0.509613037 batch PCKh 0.875\n",
      "Trained batch 2337 batch loss 0.674934 batch mAP 0.495819092 batch PCKh 0.875\n",
      "Trained batch 2338 batch loss 0.630133331 batch mAP 0.484588623 batch PCKh 0.5\n",
      "Trained batch 2339 batch loss 0.538566053 batch mAP 0.513519287 batch PCKh 0.6875\n",
      "Trained batch 2340 batch loss 0.537211537 batch mAP 0.472076416 batch PCKh 0\n",
      "Trained batch 2341 batch loss 0.541199207 batch mAP 0.527893066 batch PCKh 0.5\n",
      "Trained batch 2342 batch loss 0.566573441 batch mAP 0.481384277 batch PCKh 0.375\n",
      "Trained batch 2343 batch loss 0.507051885 batch mAP 0.505645752 batch PCKh 0.4375\n",
      "Trained batch 2344 batch loss 0.458735883 batch mAP 0.494506836 batch PCKh 0.25\n",
      "Trained batch 2345 batch loss 0.442023575 batch mAP 0.507995605 batch PCKh 0\n",
      "Trained batch 2346 batch loss 0.523666263 batch mAP 0.477630615 batch PCKh 0.25\n",
      "Trained batch 2347 batch loss 0.595441461 batch mAP 0.488800049 batch PCKh 0.5625\n",
      "Trained batch 2348 batch loss 0.608622432 batch mAP 0.487976074 batch PCKh 0.4375\n",
      "Trained batch 2349 batch loss 0.59279412 batch mAP 0.50592041 batch PCKh 0.8125\n",
      "Trained batch 2350 batch loss 0.553789675 batch mAP 0.495330811 batch PCKh 0.4375\n",
      "Trained batch 2351 batch loss 0.592495 batch mAP 0.471191406 batch PCKh 0.1875\n",
      "Trained batch 2352 batch loss 0.604494035 batch mAP 0.4503479 batch PCKh 0.6875\n",
      "Trained batch 2353 batch loss 0.592229426 batch mAP 0.426025391 batch PCKh 0.5\n",
      "Trained batch 2354 batch loss 0.519038081 batch mAP 0.463500977 batch PCKh 0.3125\n",
      "Trained batch 2355 batch loss 0.547501564 batch mAP 0.532562256 batch PCKh 0.3125\n",
      "Trained batch 2356 batch loss 0.517723858 batch mAP 0.528503418 batch PCKh 0.4375\n",
      "Trained batch 2357 batch loss 0.616508603 batch mAP 0.431213379 batch PCKh 0.375\n",
      "Trained batch 2358 batch loss 0.609446168 batch mAP 0.447906494 batch PCKh 0.125\n",
      "Trained batch 2359 batch loss 0.618471861 batch mAP 0.374053955 batch PCKh 0.5625\n",
      "Trained batch 2360 batch loss 0.604476094 batch mAP 0.414367676 batch PCKh 0.4375\n",
      "Trained batch 2361 batch loss 0.604877651 batch mAP 0.521606445 batch PCKh 0.25\n",
      "Trained batch 2362 batch loss 0.616824806 batch mAP 0.542205811 batch PCKh 0.1875\n",
      "Trained batch 2363 batch loss 0.654375076 batch mAP 0.390808105 batch PCKh 0.625\n",
      "Trained batch 2364 batch loss 0.617448568 batch mAP 0.414550781 batch PCKh 0.25\n",
      "Trained batch 2365 batch loss 0.513108909 batch mAP 0.449646 batch PCKh 0\n",
      "Trained batch 2366 batch loss 0.584225416 batch mAP 0.436645508 batch PCKh 0.625\n",
      "Trained batch 2367 batch loss 0.531442404 batch mAP 0.362426758 batch PCKh 0.5\n",
      "Trained batch 2368 batch loss 0.525469661 batch mAP 0.338439941 batch PCKh 0.5\n",
      "Trained batch 2369 batch loss 0.545928955 batch mAP 0.407867432 batch PCKh 0.625\n",
      "Trained batch 2370 batch loss 0.59136641 batch mAP 0.424346924 batch PCKh 0.4375\n",
      "Trained batch 2371 batch loss 0.580055475 batch mAP 0.424743652 batch PCKh 0.0625\n",
      "Trained batch 2372 batch loss 0.631257415 batch mAP 0.48046875 batch PCKh 0.5\n",
      "Trained batch 2373 batch loss 0.606983066 batch mAP 0.435577393 batch PCKh 0.625\n",
      "Trained batch 2374 batch loss 0.668565452 batch mAP 0.476654053 batch PCKh 0.3125\n",
      "Trained batch 2375 batch loss 0.677901 batch mAP 0.49331665 batch PCKh 0.5625\n",
      "Trained batch 2376 batch loss 0.748060107 batch mAP 0.471557617 batch PCKh 0.125\n",
      "Trained batch 2377 batch loss 0.715518892 batch mAP 0.491607666 batch PCKh 0.0625\n",
      "Trained batch 2378 batch loss 0.664747953 batch mAP 0.44317627 batch PCKh 0.1875\n",
      "Trained batch 2379 batch loss 0.641195774 batch mAP 0.427978516 batch PCKh 0.3125\n",
      "Trained batch 2380 batch loss 0.637278736 batch mAP 0.401184082 batch PCKh 0.5625\n",
      "Trained batch 2381 batch loss 0.592066288 batch mAP 0.304412842 batch PCKh 0.6875\n",
      "Trained batch 2382 batch loss 0.598439515 batch mAP 0.248901367 batch PCKh 0.5\n",
      "Trained batch 2383 batch loss 0.648358822 batch mAP 0.254180908 batch PCKh 0.75\n",
      "Trained batch 2384 batch loss 0.555842042 batch mAP 0.185577393 batch PCKh 0.3125\n",
      "Trained batch 2385 batch loss 0.592867374 batch mAP 0.237731934 batch PCKh 0.25\n",
      "Trained batch 2386 batch loss 0.73387742 batch mAP 0.218597412 batch PCKh 0.375\n",
      "Trained batch 2387 batch loss 0.649637461 batch mAP 0.149291992 batch PCKh 0.1875\n",
      "Trained batch 2388 batch loss 0.654960334 batch mAP 0.0979614258 batch PCKh 0.5\n",
      "Trained batch 2389 batch loss 0.621516585 batch mAP 0.114440918 batch PCKh 0.625\n",
      "Trained batch 2390 batch loss 0.640710831 batch mAP 0.248168945 batch PCKh 0.25\n",
      "Trained batch 2391 batch loss 0.593625247 batch mAP 0.319061279 batch PCKh 0.25\n",
      "Trained batch 2392 batch loss 0.618841231 batch mAP 0.410095215 batch PCKh 0.375\n",
      "Trained batch 2393 batch loss 0.633349776 batch mAP 0.428466797 batch PCKh 0.1875\n",
      "Trained batch 2394 batch loss 0.662313521 batch mAP 0.392150879 batch PCKh 0.625\n",
      "Trained batch 2395 batch loss 0.664473355 batch mAP 0.347808838 batch PCKh 0.5\n",
      "Trained batch 2396 batch loss 0.623172283 batch mAP 0.386779785 batch PCKh 0.75\n",
      "Trained batch 2397 batch loss 0.623889565 batch mAP 0.345184326 batch PCKh 0.5625\n",
      "Trained batch 2398 batch loss 0.626805305 batch mAP 0.369934082 batch PCKh 0.3125\n",
      "Trained batch 2399 batch loss 0.664289355 batch mAP 0.336181641 batch PCKh 0.125\n",
      "Trained batch 2400 batch loss 0.58453083 batch mAP 0.438049316 batch PCKh 0.75\n",
      "Trained batch 2401 batch loss 0.667798519 batch mAP 0.417938232 batch PCKh 0.5625\n",
      "Trained batch 2402 batch loss 0.594323277 batch mAP 0.432403564 batch PCKh 0.8125\n",
      "Trained batch 2403 batch loss 0.631369352 batch mAP 0.442047119 batch PCKh 0.375\n",
      "Trained batch 2404 batch loss 0.584928 batch mAP 0.516174316 batch PCKh 0.0625\n",
      "Trained batch 2405 batch loss 0.613249719 batch mAP 0.492980957 batch PCKh 0.8125\n",
      "Trained batch 2406 batch loss 0.621094704 batch mAP 0.458007812 batch PCKh 0.375\n",
      "Trained batch 2407 batch loss 0.583443046 batch mAP 0.54486084 batch PCKh 0.75\n",
      "Trained batch 2408 batch loss 0.607065618 batch mAP 0.519866943 batch PCKh 0.5\n",
      "Trained batch 2409 batch loss 0.618935466 batch mAP 0.493988037 batch PCKh 0.5625\n",
      "Trained batch 2410 batch loss 0.574041605 batch mAP 0.504089355 batch PCKh 0.5625\n",
      "Trained batch 2411 batch loss 0.713233769 batch mAP 0.417602539 batch PCKh 0.4375\n",
      "Trained batch 2412 batch loss 0.599378765 batch mAP 0.495849609 batch PCKh 0.5\n",
      "Trained batch 2413 batch loss 0.581675649 batch mAP 0.4921875 batch PCKh 0.3125\n",
      "Trained batch 2414 batch loss 0.62683624 batch mAP 0.453186035 batch PCKh 0.75\n",
      "Trained batch 2415 batch loss 0.489308536 batch mAP 0.489044189 batch PCKh 0.4375\n",
      "Trained batch 2416 batch loss 0.568436384 batch mAP 0.493347168 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2417 batch loss 0.518088281 batch mAP 0.430084229 batch PCKh 0.5\n",
      "Trained batch 2418 batch loss 0.495494872 batch mAP 0.416564941 batch PCKh 0.5625\n",
      "Trained batch 2419 batch loss 0.54670316 batch mAP 0.443511963 batch PCKh 0.625\n",
      "Trained batch 2420 batch loss 0.552975774 batch mAP 0.435241699 batch PCKh 0.625\n",
      "Trained batch 2421 batch loss 0.533851206 batch mAP 0.404174805 batch PCKh 0.75\n",
      "Trained batch 2422 batch loss 0.543094933 batch mAP 0.493743896 batch PCKh 0.5625\n",
      "Trained batch 2423 batch loss 0.620309055 batch mAP 0.487792969 batch PCKh 0.6875\n",
      "Trained batch 2424 batch loss 0.581660569 batch mAP 0.482879639 batch PCKh 0.75\n",
      "Trained batch 2425 batch loss 0.585896254 batch mAP 0.4871521 batch PCKh 0.0625\n",
      "Trained batch 2426 batch loss 0.520557046 batch mAP 0.536071777 batch PCKh 0.625\n",
      "Trained batch 2427 batch loss 0.485826015 batch mAP 0.5 batch PCKh 0.25\n",
      "Trained batch 2428 batch loss 0.517505705 batch mAP 0.544525146 batch PCKh 0.75\n",
      "Trained batch 2429 batch loss 0.58551538 batch mAP 0.51574707 batch PCKh 0.6875\n",
      "Trained batch 2430 batch loss 0.65632689 batch mAP 0.494140625 batch PCKh 0.25\n",
      "Trained batch 2431 batch loss 0.560514152 batch mAP 0.538024902 batch PCKh 0.75\n",
      "Trained batch 2432 batch loss 0.729570746 batch mAP 0.40914917 batch PCKh 0.125\n",
      "Trained batch 2433 batch loss 0.584414482 batch mAP 0.543273926 batch PCKh 0.6875\n",
      "Trained batch 2434 batch loss 0.588975608 batch mAP 0.546478271 batch PCKh 0.8125\n",
      "Trained batch 2435 batch loss 0.58342886 batch mAP 0.514465332 batch PCKh 0.625\n",
      "Trained batch 2436 batch loss 0.573880494 batch mAP 0.49432373 batch PCKh 0.625\n",
      "Trained batch 2437 batch loss 0.575064063 batch mAP 0.553131104 batch PCKh 0.25\n",
      "Trained batch 2438 batch loss 0.570158601 batch mAP 0.588775635 batch PCKh 0.3125\n",
      "Trained batch 2439 batch loss 0.513141394 batch mAP 0.520599365 batch PCKh 0.875\n",
      "Trained batch 2440 batch loss 0.507605851 batch mAP 0.575683594 batch PCKh 0.5\n",
      "Trained batch 2441 batch loss 0.523958266 batch mAP 0.563385 batch PCKh 0.75\n",
      "Trained batch 2442 batch loss 0.563277364 batch mAP 0.569244385 batch PCKh 0.5625\n",
      "Trained batch 2443 batch loss 0.603586257 batch mAP 0.567138672 batch PCKh 0.75\n",
      "Trained batch 2444 batch loss 0.535659552 batch mAP 0.546844482 batch PCKh 0.5\n",
      "Trained batch 2445 batch loss 0.537378907 batch mAP 0.523681641 batch PCKh 0.75\n",
      "Trained batch 2446 batch loss 0.552414536 batch mAP 0.518249512 batch PCKh 0.625\n",
      "Trained batch 2447 batch loss 0.559009671 batch mAP 0.588287354 batch PCKh 0.125\n",
      "Trained batch 2448 batch loss 0.53722465 batch mAP 0.573120117 batch PCKh 0.375\n",
      "Trained batch 2449 batch loss 0.509458184 batch mAP 0.542694092 batch PCKh 0.5625\n",
      "Trained batch 2450 batch loss 0.526209354 batch mAP 0.591644287 batch PCKh 0.5625\n",
      "Trained batch 2451 batch loss 0.60025084 batch mAP 0.557769775 batch PCKh 0.3125\n",
      "Trained batch 2452 batch loss 0.50562185 batch mAP 0.582885742 batch PCKh 0.5625\n",
      "Trained batch 2453 batch loss 0.505342841 batch mAP 0.601196289 batch PCKh 0.25\n",
      "Trained batch 2454 batch loss 0.590880573 batch mAP 0.543029785 batch PCKh 0.4375\n",
      "Trained batch 2455 batch loss 0.678225338 batch mAP 0.505401611 batch PCKh 0.6875\n",
      "Trained batch 2456 batch loss 0.573693514 batch mAP 0.527740479 batch PCKh 0.125\n",
      "Trained batch 2457 batch loss 0.650721 batch mAP 0.439453125 batch PCKh 0\n",
      "Trained batch 2458 batch loss 0.697498083 batch mAP 0.4972229 batch PCKh 0.5\n",
      "Trained batch 2459 batch loss 0.532804728 batch mAP 0.514251709 batch PCKh 0.1875\n",
      "Trained batch 2460 batch loss 0.562195957 batch mAP 0.530944824 batch PCKh 0.375\n",
      "Trained batch 2461 batch loss 0.613531768 batch mAP 0.488647461 batch PCKh 0\n",
      "Trained batch 2462 batch loss 0.598197281 batch mAP 0.456359863 batch PCKh 0\n",
      "Trained batch 2463 batch loss 0.620129585 batch mAP 0.464630127 batch PCKh 0.4375\n",
      "Trained batch 2464 batch loss 0.637110591 batch mAP 0.407836914 batch PCKh 0.3125\n",
      "Trained batch 2465 batch loss 0.57346642 batch mAP 0.460113525 batch PCKh 0\n",
      "Trained batch 2466 batch loss 0.600784361 batch mAP 0.363800049 batch PCKh 0.5625\n",
      "Trained batch 2467 batch loss 0.547531724 batch mAP 0.436462402 batch PCKh 0.25\n",
      "Trained batch 2468 batch loss 0.579417825 batch mAP 0.309631348 batch PCKh 0.3125\n",
      "Trained batch 2469 batch loss 0.583516836 batch mAP 0.384368896 batch PCKh 0.125\n",
      "Trained batch 2470 batch loss 0.664166093 batch mAP 0.422576904 batch PCKh 0\n",
      "Trained batch 2471 batch loss 0.668672144 batch mAP 0.393310547 batch PCKh 0.4375\n",
      "Trained batch 2472 batch loss 0.507344 batch mAP 0.23840332 batch PCKh 0.9375\n",
      "Trained batch 2473 batch loss 0.635609269 batch mAP 0.472442627 batch PCKh 0.0625\n",
      "Trained batch 2474 batch loss 0.601595342 batch mAP 0.360778809 batch PCKh 0.5625\n",
      "Trained batch 2475 batch loss 0.603749514 batch mAP 0.482330322 batch PCKh 0.5\n",
      "Trained batch 2476 batch loss 0.577049732 batch mAP 0.505645752 batch PCKh 0.4375\n",
      "Trained batch 2477 batch loss 0.538217425 batch mAP 0.524841309 batch PCKh 0.1875\n",
      "Trained batch 2478 batch loss 0.512197673 batch mAP 0.553649902 batch PCKh 0.5625\n",
      "Trained batch 2479 batch loss 0.555205882 batch mAP 0.545257568 batch PCKh 0.25\n",
      "Trained batch 2480 batch loss 0.497605354 batch mAP 0.543609619 batch PCKh 0.1875\n",
      "Trained batch 2481 batch loss 0.56767714 batch mAP 0.559997559 batch PCKh 0.3125\n",
      "Trained batch 2482 batch loss 0.56775111 batch mAP 0.593017578 batch PCKh 0.25\n",
      "Trained batch 2483 batch loss 0.566550136 batch mAP 0.601257324 batch PCKh 0.3125\n",
      "Trained batch 2484 batch loss 0.467763662 batch mAP 0.588012695 batch PCKh 0.375\n",
      "Trained batch 2485 batch loss 0.49593696 batch mAP 0.620056152 batch PCKh 0.3125\n",
      "Trained batch 2486 batch loss 0.6213696 batch mAP 0.473571777 batch PCKh 0.1875\n",
      "Trained batch 2487 batch loss 0.550717 batch mAP 0.5206604 batch PCKh 0.4375\n",
      "Trained batch 2488 batch loss 0.588300824 batch mAP 0.535705566 batch PCKh 0.375\n",
      "Trained batch 2489 batch loss 0.566187739 batch mAP 0.548461914 batch PCKh 0.625\n",
      "Trained batch 2490 batch loss 0.579964519 batch mAP 0.535949707 batch PCKh 0.8125\n",
      "Trained batch 2491 batch loss 0.518394589 batch mAP 0.51260376 batch PCKh 0.1875\n",
      "Trained batch 2492 batch loss 0.529013872 batch mAP 0.568237305 batch PCKh 0.375\n",
      "Trained batch 2493 batch loss 0.56699717 batch mAP 0.579284668 batch PCKh 0.375\n",
      "Trained batch 2494 batch loss 0.550021529 batch mAP 0.527709961 batch PCKh 0.25\n",
      "Trained batch 2495 batch loss 0.54736352 batch mAP 0.561126709 batch PCKh 0.1875\n",
      "Trained batch 2496 batch loss 0.620321631 batch mAP 0.534545898 batch PCKh 0.375\n",
      "Trained batch 2497 batch loss 0.530160546 batch mAP 0.590942383 batch PCKh 0.25\n",
      "Trained batch 2498 batch loss 0.537524104 batch mAP 0.48236084 batch PCKh 0.5\n",
      "Trained batch 2499 batch loss 0.573410392 batch mAP 0.37322998 batch PCKh 0.6875\n",
      "Trained batch 2500 batch loss 0.755704165 batch mAP 0.453552246 batch PCKh 0.125\n",
      "Trained batch 2501 batch loss 0.748140574 batch mAP 0.299072266 batch PCKh 0.25\n",
      "Trained batch 2502 batch loss 0.715475678 batch mAP 0.469451904 batch PCKh 0\n",
      "Trained batch 2503 batch loss 0.693555593 batch mAP 0.466796875 batch PCKh 0.125\n",
      "Trained batch 2504 batch loss 0.58355397 batch mAP 0.344390869 batch PCKh 0.5625\n",
      "Trained batch 2505 batch loss 0.593382359 batch mAP 0.398498535 batch PCKh 0.1875\n",
      "Trained batch 2506 batch loss 0.669622421 batch mAP 0.388946533 batch PCKh 0.1875\n",
      "Trained batch 2507 batch loss 0.694718122 batch mAP 0.397705078 batch PCKh 0\n",
      "Trained batch 2508 batch loss 0.674687207 batch mAP 0.264038086 batch PCKh 0.1875\n",
      "Trained batch 2509 batch loss 0.592598379 batch mAP 0.068359375 batch PCKh 0.4375\n",
      "Trained batch 2510 batch loss 0.648462534 batch mAP 0.292358398 batch PCKh 0.0625\n",
      "Trained batch 2511 batch loss 0.646917701 batch mAP 0.2734375 batch PCKh 0.125\n",
      "Trained batch 2512 batch loss 0.658627152 batch mAP 0.288391113 batch PCKh 0\n",
      "Trained batch 2513 batch loss 0.592073917 batch mAP 0.281585693 batch PCKh 0.1875\n",
      "Trained batch 2514 batch loss 0.595616043 batch mAP 0.320587158 batch PCKh 0.5\n",
      "Trained batch 2515 batch loss 0.561964273 batch mAP 0.343414307 batch PCKh 0.875\n",
      "Trained batch 2516 batch loss 0.613634825 batch mAP 0.337493896 batch PCKh 0.5625\n",
      "Trained batch 2517 batch loss 0.589334607 batch mAP 0.305511475 batch PCKh 0.75\n",
      "Trained batch 2518 batch loss 0.573534966 batch mAP 0.288452148 batch PCKh 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2519 batch loss 0.553185225 batch mAP 0.385925293 batch PCKh 0.75\n",
      "Trained batch 2520 batch loss 0.534509659 batch mAP 0.357055664 batch PCKh 0.1875\n",
      "Trained batch 2521 batch loss 0.583066225 batch mAP 0.542999268 batch PCKh 0.6875\n",
      "Trained batch 2522 batch loss 0.608267903 batch mAP 0.514373779 batch PCKh 0.6875\n",
      "Trained batch 2523 batch loss 0.631454468 batch mAP 0.517944336 batch PCKh 0.0625\n",
      "Trained batch 2524 batch loss 0.619074762 batch mAP 0.336853027 batch PCKh 0.25\n",
      "Trained batch 2525 batch loss 0.606177688 batch mAP 0.329162598 batch PCKh 0.1875\n",
      "Trained batch 2526 batch loss 0.603360891 batch mAP 0.310119629 batch PCKh 0.3125\n",
      "Trained batch 2527 batch loss 0.653608561 batch mAP 0.125061035 batch PCKh 0.125\n",
      "Trained batch 2528 batch loss 0.627410293 batch mAP 0.359863281 batch PCKh 0.25\n",
      "Trained batch 2529 batch loss 0.633139908 batch mAP 0.350952148 batch PCKh 0.5\n",
      "Trained batch 2530 batch loss 0.66862607 batch mAP 0.380432129 batch PCKh 0.5625\n",
      "Trained batch 2531 batch loss 0.708050847 batch mAP 0.380737305 batch PCKh 0.0625\n",
      "Trained batch 2532 batch loss 0.721874535 batch mAP 0.398620605 batch PCKh 0\n",
      "Trained batch 2533 batch loss 0.678344429 batch mAP 0.353851318 batch PCKh 0\n",
      "Trained batch 2534 batch loss 0.717729807 batch mAP 0.420349121 batch PCKh 0.0625\n",
      "Trained batch 2535 batch loss 0.612789869 batch mAP 0.362731934 batch PCKh 0\n",
      "Trained batch 2536 batch loss 0.699933231 batch mAP 0.393615723 batch PCKh 0\n",
      "Trained batch 2537 batch loss 0.628036678 batch mAP 0.2918396 batch PCKh 0.0625\n",
      "Trained batch 2538 batch loss 0.610203385 batch mAP 0.26763916 batch PCKh 0.125\n",
      "Trained batch 2539 batch loss 0.610122144 batch mAP 0.247192383 batch PCKh 0.3125\n",
      "Trained batch 2540 batch loss 0.535424411 batch mAP 0.145690918 batch PCKh 0.3125\n",
      "Trained batch 2541 batch loss 0.624600708 batch mAP 0.398193359 batch PCKh 0.625\n",
      "Trained batch 2542 batch loss 0.679161131 batch mAP 0.413421631 batch PCKh 0.5625\n",
      "Trained batch 2543 batch loss 0.628563 batch mAP 0.420898438 batch PCKh 0.4375\n",
      "Trained batch 2544 batch loss 0.702313781 batch mAP 0.440094 batch PCKh 0\n",
      "Trained batch 2545 batch loss 0.571822047 batch mAP 0.524383545 batch PCKh 0.25\n",
      "Trained batch 2546 batch loss 0.601404 batch mAP 0.515838623 batch PCKh 0.3125\n",
      "Trained batch 2547 batch loss 0.566452324 batch mAP 0.541259766 batch PCKh 0.125\n",
      "Trained batch 2548 batch loss 0.598771691 batch mAP 0.575195312 batch PCKh 0.5\n",
      "Trained batch 2549 batch loss 0.556886196 batch mAP 0.523620605 batch PCKh 0.3125\n",
      "Trained batch 2550 batch loss 0.589329243 batch mAP 0.546844482 batch PCKh 0.625\n",
      "Trained batch 2551 batch loss 0.580377221 batch mAP 0.554229736 batch PCKh 0.5625\n",
      "Trained batch 2552 batch loss 0.588040233 batch mAP 0.534484863 batch PCKh 0.3125\n",
      "Trained batch 2553 batch loss 0.54553014 batch mAP 0.539215088 batch PCKh 0.375\n",
      "Trained batch 2554 batch loss 0.579464436 batch mAP 0.585327148 batch PCKh 0.4375\n",
      "Trained batch 2555 batch loss 0.576386213 batch mAP 0.595977783 batch PCKh 0.4375\n",
      "Trained batch 2556 batch loss 0.591482043 batch mAP 0.586456299 batch PCKh 0.0625\n",
      "Trained batch 2557 batch loss 0.576720834 batch mAP 0.562683105 batch PCKh 0.1875\n",
      "Trained batch 2558 batch loss 0.594488621 batch mAP 0.563476562 batch PCKh 0.4375\n",
      "Trained batch 2559 batch loss 0.584249 batch mAP 0.571655273 batch PCKh 0.25\n",
      "Trained batch 2560 batch loss 0.640835762 batch mAP 0.504882812 batch PCKh 0.125\n",
      "Trained batch 2561 batch loss 0.661344409 batch mAP 0.477783203 batch PCKh 0.1875\n",
      "Trained batch 2562 batch loss 0.676863849 batch mAP 0.497344971 batch PCKh 0.375\n",
      "Trained batch 2563 batch loss 0.603588283 batch mAP 0.508880615 batch PCKh 0.25\n",
      "Trained batch 2564 batch loss 0.601339459 batch mAP 0.54309082 batch PCKh 0.375\n",
      "Trained batch 2565 batch loss 0.554861248 batch mAP 0.532043457 batch PCKh 0.0625\n",
      "Trained batch 2566 batch loss 0.53145808 batch mAP 0.500305176 batch PCKh 0.3125\n",
      "Trained batch 2567 batch loss 0.458663076 batch mAP 0.582458496 batch PCKh 0.3125\n",
      "Trained batch 2568 batch loss 0.544395924 batch mAP 0.563324 batch PCKh 0.5\n",
      "Trained batch 2569 batch loss 0.613328576 batch mAP 0.519317627 batch PCKh 0.125\n",
      "Trained batch 2570 batch loss 0.628234267 batch mAP 0.611358643 batch PCKh 0.8125\n",
      "Trained batch 2571 batch loss 0.521672189 batch mAP 0.575683594 batch PCKh 0.375\n",
      "Trained batch 2572 batch loss 0.603745222 batch mAP 0.58581543 batch PCKh 0.875\n",
      "Trained batch 2573 batch loss 0.595491529 batch mAP 0.576141357 batch PCKh 0.25\n",
      "Trained batch 2574 batch loss 0.554637611 batch mAP 0.474395752 batch PCKh 0.1875\n",
      "Trained batch 2575 batch loss 0.550296664 batch mAP 0.544433594 batch PCKh 0.0625\n",
      "Trained batch 2576 batch loss 0.628377557 batch mAP 0.537475586 batch PCKh 0.5625\n",
      "Trained batch 2577 batch loss 0.691328645 batch mAP 0.486602783 batch PCKh 0\n",
      "Trained batch 2578 batch loss 0.550124764 batch mAP 0.516052246 batch PCKh 0\n",
      "Trained batch 2579 batch loss 0.679929 batch mAP 0.539123535 batch PCKh 0.375\n",
      "Trained batch 2580 batch loss 0.514379859 batch mAP 0.532501221 batch PCKh 0.1875\n",
      "Trained batch 2581 batch loss 0.56917727 batch mAP 0.44342041 batch PCKh 0.625\n",
      "Trained batch 2582 batch loss 0.660254657 batch mAP 0.460418701 batch PCKh 0.3125\n",
      "Trained batch 2583 batch loss 0.619997 batch mAP 0.4168396 batch PCKh 0.3125\n",
      "Trained batch 2584 batch loss 0.654287517 batch mAP 0.368591309 batch PCKh 0.375\n",
      "Trained batch 2585 batch loss 0.68318212 batch mAP 0.438903809 batch PCKh 0.3125\n",
      "Trained batch 2586 batch loss 0.71571 batch mAP 0.449035645 batch PCKh 0.4375\n",
      "Trained batch 2587 batch loss 0.760121465 batch mAP 0.390869141 batch PCKh 0.0625\n",
      "Trained batch 2588 batch loss 0.805583715 batch mAP 0.349731445 batch PCKh 0\n",
      "Trained batch 2589 batch loss 0.687160373 batch mAP 0.273925781 batch PCKh 0\n",
      "Trained batch 2590 batch loss 0.73313421 batch mAP 0.248626709 batch PCKh 0.0625\n",
      "Trained batch 2591 batch loss 0.681631446 batch mAP 0.198822021 batch PCKh 0.5\n",
      "Trained batch 2592 batch loss 0.592837214 batch mAP 0.138061523 batch PCKh 0.1875\n",
      "Trained batch 2593 batch loss 0.546046853 batch mAP 0.171081543 batch PCKh 0.125\n",
      "Trained batch 2594 batch loss 0.593172729 batch mAP 0.134979248 batch PCKh 0.625\n",
      "Trained batch 2595 batch loss 0.544380307 batch mAP 0.182006836 batch PCKh 0.125\n",
      "Trained batch 2596 batch loss 0.608284831 batch mAP 0.229187012 batch PCKh 0.125\n",
      "Trained batch 2597 batch loss 0.709619641 batch mAP 0.279632568 batch PCKh 0.5625\n",
      "Trained batch 2598 batch loss 0.675302267 batch mAP 0.33807373 batch PCKh 0.125\n",
      "Trained batch 2599 batch loss 0.78408587 batch mAP 0.283691406 batch PCKh 0.1875\n",
      "Trained batch 2600 batch loss 0.61337769 batch mAP 0.373016357 batch PCKh 0\n",
      "Trained batch 2601 batch loss 0.725684047 batch mAP 0.404876709 batch PCKh 0.5\n",
      "Trained batch 2602 batch loss 0.604121268 batch mAP 0.361877441 batch PCKh 0.25\n",
      "Trained batch 2603 batch loss 0.611994505 batch mAP 0.367218018 batch PCKh 0.4375\n",
      "Trained batch 2604 batch loss 0.659962416 batch mAP 0.377502441 batch PCKh 0.1875\n",
      "Trained batch 2605 batch loss 0.587390244 batch mAP 0.37612915 batch PCKh 0.5625\n",
      "Trained batch 2606 batch loss 0.647567749 batch mAP 0.372528076 batch PCKh 0.6875\n",
      "Trained batch 2607 batch loss 0.655860364 batch mAP 0.43572998 batch PCKh 0.5625\n",
      "Trained batch 2608 batch loss 0.637384772 batch mAP 0.438659668 batch PCKh 0.25\n",
      "Trained batch 2609 batch loss 0.662941098 batch mAP 0.445129395 batch PCKh 0.8125\n",
      "Trained batch 2610 batch loss 0.678918958 batch mAP 0.387786865 batch PCKh 0.8125\n",
      "Trained batch 2611 batch loss 0.636744916 batch mAP 0.375976562 batch PCKh 0.875\n",
      "Trained batch 2612 batch loss 0.565195739 batch mAP 0.443756104 batch PCKh 0.5625\n",
      "Trained batch 2613 batch loss 0.646560431 batch mAP 0.355102539 batch PCKh 0.5625\n",
      "Trained batch 2614 batch loss 0.637757957 batch mAP 0.331665039 batch PCKh 0.4375\n",
      "Trained batch 2615 batch loss 0.663042784 batch mAP 0.377288818 batch PCKh 0.6875\n",
      "Trained batch 2616 batch loss 0.680438638 batch mAP 0.425872803 batch PCKh 0.8125\n",
      "Trained batch 2617 batch loss 0.59528029 batch mAP 0.506195068 batch PCKh 0.6875\n",
      "Trained batch 2618 batch loss 0.581097543 batch mAP 0.487854 batch PCKh 0.625\n",
      "Trained batch 2619 batch loss 0.614650965 batch mAP 0.522186279 batch PCKh 0.75\n",
      "Trained batch 2620 batch loss 0.653068662 batch mAP 0.469482422 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2621 batch loss 0.618364394 batch mAP 0.48538208 batch PCKh 0.625\n",
      "Trained batch 2622 batch loss 0.632941961 batch mAP 0.506988525 batch PCKh 0.6875\n",
      "Trained batch 2623 batch loss 0.660089374 batch mAP 0.492767334 batch PCKh 0.125\n",
      "Trained batch 2624 batch loss 0.574427187 batch mAP 0.481292725 batch PCKh 0.25\n",
      "Trained batch 2625 batch loss 0.587560058 batch mAP 0.496429443 batch PCKh 0.4375\n",
      "Trained batch 2626 batch loss 0.715587735 batch mAP 0.490203857 batch PCKh 0.5\n",
      "Trained batch 2627 batch loss 0.625047266 batch mAP 0.506622314 batch PCKh 0.5625\n",
      "Trained batch 2628 batch loss 0.703125358 batch mAP 0.500732422 batch PCKh 0.5\n",
      "Trained batch 2629 batch loss 0.643514633 batch mAP 0.46585083 batch PCKh 0.125\n",
      "Trained batch 2630 batch loss 0.605060816 batch mAP 0.458099365 batch PCKh 0.5625\n",
      "Trained batch 2631 batch loss 0.658097148 batch mAP 0.524505615 batch PCKh 0.625\n",
      "Trained batch 2632 batch loss 0.659383476 batch mAP 0.53225708 batch PCKh 0.625\n",
      "Trained batch 2633 batch loss 0.612996817 batch mAP 0.509643555 batch PCKh 0.625\n",
      "Trained batch 2634 batch loss 0.434634179 batch mAP 0.499084473 batch PCKh 0.375\n",
      "Trained batch 2635 batch loss 0.546000481 batch mAP 0.530212402 batch PCKh 0.25\n",
      "Trained batch 2636 batch loss 0.519702554 batch mAP 0.553283691 batch PCKh 0.1875\n",
      "Trained batch 2637 batch loss 0.524930418 batch mAP 0.570800781 batch PCKh 0.3125\n",
      "Trained batch 2638 batch loss 0.580500543 batch mAP 0.577423096 batch PCKh 0.3125\n",
      "Trained batch 2639 batch loss 0.572624624 batch mAP 0.612884521 batch PCKh 0.1875\n",
      "Trained batch 2640 batch loss 0.592073739 batch mAP 0.496704102 batch PCKh 0.3125\n",
      "Trained batch 2641 batch loss 0.571733713 batch mAP 0.281158447 batch PCKh 0.25\n",
      "Trained batch 2642 batch loss 0.636837482 batch mAP 0.326904297 batch PCKh 0.5\n",
      "Trained batch 2643 batch loss 0.734951735 batch mAP 0.153778076 batch PCKh 0\n",
      "Trained batch 2644 batch loss 0.606195 batch mAP 0.453674316 batch PCKh 0.3125\n",
      "Trained batch 2645 batch loss 0.66815865 batch mAP 0.461273193 batch PCKh 0.125\n",
      "Trained batch 2646 batch loss 0.647096515 batch mAP 0.493469238 batch PCKh 0\n",
      "Trained batch 2647 batch loss 0.559088886 batch mAP 0.561523438 batch PCKh 0.875\n",
      "Trained batch 2648 batch loss 0.613022745 batch mAP 0.51071167 batch PCKh 0.6875\n",
      "Trained batch 2649 batch loss 0.601983428 batch mAP 0.510101318 batch PCKh 0.75\n",
      "Trained batch 2650 batch loss 0.731400251 batch mAP 0.477386475 batch PCKh 0.6875\n",
      "Trained batch 2651 batch loss 0.67620194 batch mAP 0.494506836 batch PCKh 0.6875\n",
      "Trained batch 2652 batch loss 0.564094663 batch mAP 0.507782 batch PCKh 0.625\n",
      "Trained batch 2653 batch loss 0.607817411 batch mAP 0.352966309 batch PCKh 0.3125\n",
      "Trained batch 2654 batch loss 0.702287197 batch mAP 0.407165527 batch PCKh 0.3125\n",
      "Trained batch 2655 batch loss 0.7365973 batch mAP 0.417114258 batch PCKh 0\n",
      "Trained batch 2656 batch loss 0.688457608 batch mAP 0.43170166 batch PCKh 0.4375\n",
      "Trained batch 2657 batch loss 0.679778457 batch mAP 0.318267822 batch PCKh 0.0625\n",
      "Trained batch 2658 batch loss 0.704559684 batch mAP 0.4190979 batch PCKh 0.0625\n",
      "Trained batch 2659 batch loss 0.800496697 batch mAP 0.456359863 batch PCKh 0.3125\n",
      "Trained batch 2660 batch loss 0.735569119 batch mAP 0.390380859 batch PCKh 0.5\n",
      "Trained batch 2661 batch loss 0.672500551 batch mAP 0.397338867 batch PCKh 0\n",
      "Trained batch 2662 batch loss 0.590180874 batch mAP 0.325775146 batch PCKh 0.4375\n",
      "Trained batch 2663 batch loss 0.527368426 batch mAP 0.385803223 batch PCKh 0\n",
      "Trained batch 2664 batch loss 0.533082366 batch mAP 0.141204834 batch PCKh 0.125\n",
      "Trained batch 2665 batch loss 0.5680058 batch mAP 0.297393799 batch PCKh 0.3125\n",
      "Trained batch 2666 batch loss 0.660486937 batch mAP 0.349609375 batch PCKh 0.125\n",
      "Trained batch 2667 batch loss 0.619175494 batch mAP 0.374145508 batch PCKh 0.8125\n",
      "Trained batch 2668 batch loss 0.559811592 batch mAP 0.385101318 batch PCKh 0.25\n",
      "Trained batch 2669 batch loss 0.583618581 batch mAP 0.44229126 batch PCKh 0.25\n",
      "Trained batch 2670 batch loss 0.521452427 batch mAP 0.481933594 batch PCKh 0\n",
      "Trained batch 2671 batch loss 0.537267208 batch mAP 0.526977539 batch PCKh 0.375\n",
      "Trained batch 2672 batch loss 0.521791458 batch mAP 0.556976318 batch PCKh 0.625\n",
      "Trained batch 2673 batch loss 0.551107049 batch mAP 0.611816406 batch PCKh 0.3125\n",
      "Trained batch 2674 batch loss 0.505881846 batch mAP 0.563598633 batch PCKh 0.625\n",
      "Trained batch 2675 batch loss 0.523561597 batch mAP 0.557067871 batch PCKh 0.3125\n",
      "Trained batch 2676 batch loss 0.592368722 batch mAP 0.388366699 batch PCKh 0.1875\n",
      "Trained batch 2677 batch loss 0.510794044 batch mAP 0.410095215 batch PCKh 0.1875\n",
      "Trained batch 2678 batch loss 0.460911751 batch mAP 0.540496826 batch PCKh 0.1875\n",
      "Trained batch 2679 batch loss 0.488784671 batch mAP 0.445770264 batch PCKh 0.3125\n",
      "Trained batch 2680 batch loss 0.531997621 batch mAP 0.333648682 batch PCKh 0.5\n",
      "Trained batch 2681 batch loss 0.532415748 batch mAP 0.419647217 batch PCKh 0.375\n",
      "Trained batch 2682 batch loss 0.610037565 batch mAP 0.242523193 batch PCKh 0.25\n",
      "Trained batch 2683 batch loss 0.629354954 batch mAP 0.219268799 batch PCKh 0.375\n",
      "Trained batch 2684 batch loss 0.643708944 batch mAP 0.21484375 batch PCKh 0.3125\n",
      "Trained batch 2685 batch loss 0.626934648 batch mAP 0.435943604 batch PCKh 0.375\n",
      "Trained batch 2686 batch loss 0.479689419 batch mAP 0.557525635 batch PCKh 0.125\n",
      "Trained batch 2687 batch loss 0.529603958 batch mAP 0.537445068 batch PCKh 0.3125\n",
      "Trained batch 2688 batch loss 0.525165737 batch mAP 0.548492432 batch PCKh 0.3125\n",
      "Trained batch 2689 batch loss 0.61232388 batch mAP 0.5496521 batch PCKh 0\n",
      "Trained batch 2690 batch loss 0.8481884 batch mAP 0.449310303 batch PCKh 0\n",
      "Trained batch 2691 batch loss 0.767648458 batch mAP 0.38760376 batch PCKh 0.375\n",
      "Trained batch 2692 batch loss 0.608668387 batch mAP 0.478881836 batch PCKh 0.5\n",
      "Trained batch 2693 batch loss 0.726846576 batch mAP 0.391143799 batch PCKh 0.25\n",
      "Trained batch 2694 batch loss 0.619112492 batch mAP 0.19821167 batch PCKh 0.75\n",
      "Trained batch 2695 batch loss 0.607660651 batch mAP 0.11819458 batch PCKh 0.625\n",
      "Trained batch 2696 batch loss 0.67162323 batch mAP 0.0127563477 batch PCKh 0.6875\n",
      "Trained batch 2697 batch loss 0.752127171 batch mAP 0.135467529 batch PCKh 0.25\n",
      "Trained batch 2698 batch loss 0.678465128 batch mAP 0.0940246582 batch PCKh 0.5625\n",
      "Trained batch 2699 batch loss 0.644264162 batch mAP 0.00189208984 batch PCKh 0.3125\n",
      "Trained batch 2700 batch loss 0.725345254 batch mAP 0.0187988281 batch PCKh 0.625\n",
      "Trained batch 2701 batch loss 0.606224895 batch mAP 0.0104064941 batch PCKh 0.1875\n",
      "Trained batch 2702 batch loss 0.727645755 batch mAP 0.12612915 batch PCKh 0.625\n",
      "Trained batch 2703 batch loss 0.515307188 batch mAP 0.0115356445 batch PCKh 0.0625\n",
      "Trained batch 2704 batch loss 0.629787087 batch mAP 0.0497131348 batch PCKh 0.25\n",
      "Trained batch 2705 batch loss 0.539142609 batch mAP 0.0110778809 batch PCKh 0.3125\n",
      "Trained batch 2706 batch loss 0.577854693 batch mAP 0.335388184 batch PCKh 0.0625\n",
      "Trained batch 2707 batch loss 0.582736492 batch mAP 0.435028076 batch PCKh 0.4375\n",
      "Trained batch 2708 batch loss 0.578934968 batch mAP 0.376434326 batch PCKh 0.75\n",
      "Trained batch 2709 batch loss 0.591179967 batch mAP 0.442047119 batch PCKh 0.125\n",
      "Trained batch 2710 batch loss 0.71515131 batch mAP 0.433410645 batch PCKh 0.5625\n",
      "Trained batch 2711 batch loss 0.645217299 batch mAP 0.446960449 batch PCKh 0.6875\n",
      "Trained batch 2712 batch loss 0.611646473 batch mAP 0.523529053 batch PCKh 0.875\n",
      "Trained batch 2713 batch loss 0.695992112 batch mAP 0.368377686 batch PCKh 0.4375\n",
      "Trained batch 2714 batch loss 0.693534 batch mAP 0.38180542 batch PCKh 0.0625\n",
      "Trained batch 2715 batch loss 0.679054 batch mAP 0.379333496 batch PCKh 0.375\n",
      "Trained batch 2716 batch loss 0.636648238 batch mAP 0.3230896 batch PCKh 0.5625\n",
      "Trained batch 2717 batch loss 0.591816723 batch mAP 0.451446533 batch PCKh 0.5625\n",
      "Trained batch 2718 batch loss 0.639986217 batch mAP 0.417114258 batch PCKh 0.5625\n",
      "Trained batch 2719 batch loss 0.692825079 batch mAP 0.356140137 batch PCKh 0.1875\n",
      "Trained batch 2720 batch loss 0.683391869 batch mAP 0.387298584 batch PCKh 0.0625\n",
      "Trained batch 2721 batch loss 0.619649529 batch mAP 0.449493408 batch PCKh 0\n",
      "Trained batch 2722 batch loss 0.568444312 batch mAP 0.544769287 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2723 batch loss 0.495041311 batch mAP 0.472930908 batch PCKh 0.3125\n",
      "Trained batch 2724 batch loss 0.512546599 batch mAP 0.496826172 batch PCKh 0.6875\n",
      "Trained batch 2725 batch loss 0.555762231 batch mAP 0.556488037 batch PCKh 0.5625\n",
      "Trained batch 2726 batch loss 0.552543044 batch mAP 0.453155518 batch PCKh 0.125\n",
      "Trained batch 2727 batch loss 0.638972878 batch mAP 0.470977783 batch PCKh 0.125\n",
      "Trained batch 2728 batch loss 0.60370189 batch mAP 0.511352539 batch PCKh 0.75\n",
      "Trained batch 2729 batch loss 0.725303411 batch mAP 0.467163086 batch PCKh 0\n",
      "Trained batch 2730 batch loss 0.627108812 batch mAP 0.479919434 batch PCKh 0\n",
      "Trained batch 2731 batch loss 0.596119165 batch mAP 0.549224854 batch PCKh 0.1875\n",
      "Trained batch 2732 batch loss 0.604758799 batch mAP 0.534454346 batch PCKh 0.3125\n",
      "Trained batch 2733 batch loss 0.541933239 batch mAP 0.453338623 batch PCKh 0.25\n",
      "Trained batch 2734 batch loss 0.536158919 batch mAP 0.485717773 batch PCKh 0.25\n",
      "Trained batch 2735 batch loss 0.565143347 batch mAP 0.57244873 batch PCKh 0.25\n",
      "Trained batch 2736 batch loss 0.547976851 batch mAP 0.523925781 batch PCKh 0.25\n",
      "Trained batch 2737 batch loss 0.519818902 batch mAP 0.55368042 batch PCKh 0.25\n",
      "Trained batch 2738 batch loss 0.568680763 batch mAP 0.602478 batch PCKh 0.625\n",
      "Trained batch 2739 batch loss 0.548948765 batch mAP 0.554840088 batch PCKh 0.6875\n",
      "Trained batch 2740 batch loss 0.574650586 batch mAP 0.53024292 batch PCKh 0.5625\n",
      "Trained batch 2741 batch loss 0.606468081 batch mAP 0.44317627 batch PCKh 0.1875\n",
      "Trained batch 2742 batch loss 0.627066493 batch mAP 0.378509521 batch PCKh 0.25\n",
      "Trained batch 2743 batch loss 0.624807715 batch mAP 0.449859619 batch PCKh 0.375\n",
      "Trained batch 2744 batch loss 0.6352005 batch mAP 0.507507324 batch PCKh 0.3125\n",
      "Trained batch 2745 batch loss 0.627046943 batch mAP 0.447357178 batch PCKh 0.4375\n",
      "Trained batch 2746 batch loss 0.684961259 batch mAP 0.467865 batch PCKh 0.5625\n",
      "Trained batch 2747 batch loss 0.660604358 batch mAP 0.445159912 batch PCKh 0.8125\n",
      "Trained batch 2748 batch loss 0.597129941 batch mAP 0.508392334 batch PCKh 0.75\n",
      "Trained batch 2749 batch loss 0.66098994 batch mAP 0.472045898 batch PCKh 0.5\n",
      "Trained batch 2750 batch loss 0.704801083 batch mAP 0.445617676 batch PCKh 0\n",
      "Trained batch 2751 batch loss 0.513604164 batch mAP 0.213165283 batch PCKh 0.375\n",
      "Trained batch 2752 batch loss 0.670130789 batch mAP 0.449646 batch PCKh 0.75\n",
      "Trained batch 2753 batch loss 0.671034 batch mAP 0.393280029 batch PCKh 0.25\n",
      "Trained batch 2754 batch loss 0.612802684 batch mAP 0.374237061 batch PCKh 0.125\n",
      "Trained batch 2755 batch loss 0.573471665 batch mAP 0.17590332 batch PCKh 0.1875\n",
      "Trained batch 2756 batch loss 0.579583 batch mAP 0.304748535 batch PCKh 0.3125\n",
      "Trained batch 2757 batch loss 0.628878295 batch mAP 0.313323975 batch PCKh 0.3125\n",
      "Trained batch 2758 batch loss 0.625023484 batch mAP 0.41394043 batch PCKh 0.25\n",
      "Trained batch 2759 batch loss 0.594964266 batch mAP 0.43057251 batch PCKh 0.5\n",
      "Trained batch 2760 batch loss 0.659154832 batch mAP 0.458374023 batch PCKh 0.25\n",
      "Trained batch 2761 batch loss 0.681977034 batch mAP 0.414245605 batch PCKh 0.375\n",
      "Trained batch 2762 batch loss 0.654865503 batch mAP 0.445800781 batch PCKh 0.1875\n",
      "Trained batch 2763 batch loss 0.699980915 batch mAP 0.328643799 batch PCKh 0.3125\n",
      "Trained batch 2764 batch loss 0.598685503 batch mAP 0.3644104 batch PCKh 0.375\n",
      "Trained batch 2765 batch loss 0.558760107 batch mAP 0.426300049 batch PCKh 0.25\n",
      "Trained batch 2766 batch loss 0.582889676 batch mAP 0.265960693 batch PCKh 0.5\n",
      "Trained batch 2767 batch loss 0.667504549 batch mAP 0.428710938 batch PCKh 0.1875\n",
      "Trained batch 2768 batch loss 0.692924559 batch mAP 0.308410645 batch PCKh 0.1875\n",
      "Trained batch 2769 batch loss 0.715398073 batch mAP 0.384979248 batch PCKh 0.5625\n",
      "Trained batch 2770 batch loss 0.68871367 batch mAP 0.363098145 batch PCKh 0.75\n",
      "Trained batch 2771 batch loss 0.717003584 batch mAP 0.321990967 batch PCKh 0\n",
      "Trained batch 2772 batch loss 0.627362907 batch mAP 0.471740723 batch PCKh 0.4375\n",
      "Trained batch 2773 batch loss 0.636324763 batch mAP 0.488647461 batch PCKh 0.5625\n",
      "Trained batch 2774 batch loss 0.561746061 batch mAP 0.469116211 batch PCKh 0.375\n",
      "Trained batch 2775 batch loss 0.559697866 batch mAP 0.493988037 batch PCKh 0.8125\n",
      "Trained batch 2776 batch loss 0.597130537 batch mAP 0.481048584 batch PCKh 0.5\n",
      "Epoch 2 train loss 0.6196727156639099 train mAP 0.42622625827789307 train PCKh\n",
      "Validated batch 1 batch loss 0.656898 batch mAP 0.468139648 batch PCKh 0.75\n",
      "Validated batch 2 batch loss 0.694208 batch mAP 0.355987549 batch PCKh 0.0625\n",
      "Validated batch 3 batch loss 0.588437557 batch mAP 0.480102539 batch PCKh 0.6875\n",
      "Validated batch 4 batch loss 0.671556473 batch mAP 0.368743896 batch PCKh 0.5\n",
      "Validated batch 5 batch loss 0.60734129 batch mAP 0.445648193 batch PCKh 0.625\n",
      "Validated batch 6 batch loss 0.620219707 batch mAP 0.4581604 batch PCKh 0.75\n",
      "Validated batch 7 batch loss 0.579297185 batch mAP 0.499206543 batch PCKh 0.1875\n",
      "Validated batch 8 batch loss 0.615272105 batch mAP 0.421813965 batch PCKh 0.25\n",
      "Validated batch 9 batch loss 0.642944872 batch mAP 0.40133667 batch PCKh 0.25\n",
      "Validated batch 10 batch loss 0.669419289 batch mAP 0.311157227 batch PCKh 0.6875\n",
      "Validated batch 11 batch loss 0.797498822 batch mAP 0.358673096 batch PCKh 0.4375\n",
      "Validated batch 12 batch loss 0.617773473 batch mAP 0.398529053 batch PCKh 0\n",
      "Validated batch 13 batch loss 0.564607859 batch mAP 0.45022583 batch PCKh 0.5\n",
      "Validated batch 14 batch loss 0.615066528 batch mAP 0.483337402 batch PCKh 0.4375\n",
      "Validated batch 15 batch loss 0.643415093 batch mAP 0.383270264 batch PCKh 0.75\n",
      "Validated batch 16 batch loss 0.656326532 batch mAP 0.471710205 batch PCKh 0.25\n",
      "Validated batch 17 batch loss 0.615963578 batch mAP 0.499023438 batch PCKh 0.25\n",
      "Validated batch 18 batch loss 0.662757277 batch mAP 0.47088623 batch PCKh 0.5\n",
      "Validated batch 19 batch loss 0.60194838 batch mAP 0.46307373 batch PCKh 0.125\n",
      "Validated batch 20 batch loss 0.640851259 batch mAP 0.500427246 batch PCKh 0.75\n",
      "Validated batch 21 batch loss 0.607729077 batch mAP 0.51272583 batch PCKh 0.8125\n",
      "Validated batch 22 batch loss 0.643303633 batch mAP 0.455871582 batch PCKh 0.75\n",
      "Validated batch 23 batch loss 0.579844117 batch mAP 0.475311279 batch PCKh 0.625\n",
      "Validated batch 24 batch loss 0.546344459 batch mAP 0.500579834 batch PCKh 0.625\n",
      "Validated batch 25 batch loss 0.646323204 batch mAP 0.466461182 batch PCKh 0.625\n",
      "Validated batch 26 batch loss 0.749546 batch mAP 0.360961914 batch PCKh 0\n",
      "Validated batch 27 batch loss 0.621866584 batch mAP 0.416107178 batch PCKh 0.5625\n",
      "Validated batch 28 batch loss 0.567818761 batch mAP 0.431091309 batch PCKh 0.4375\n",
      "Validated batch 29 batch loss 0.6790663 batch mAP 0.366088867 batch PCKh 0.0625\n",
      "Validated batch 30 batch loss 0.610144317 batch mAP 0.459075928 batch PCKh 0\n",
      "Validated batch 31 batch loss 0.649967551 batch mAP 0.548522949 batch PCKh 0.3125\n",
      "Validated batch 32 batch loss 0.638171434 batch mAP 0.428894043 batch PCKh 0.5625\n",
      "Validated batch 33 batch loss 0.651442528 batch mAP 0.40524292 batch PCKh 0.4375\n",
      "Validated batch 34 batch loss 0.687324882 batch mAP 0.373809814 batch PCKh 0.6875\n",
      "Validated batch 35 batch loss 0.578254282 batch mAP 0.462249756 batch PCKh 0.625\n",
      "Validated batch 36 batch loss 0.526026249 batch mAP 0.50869751 batch PCKh 0.4375\n",
      "Validated batch 37 batch loss 0.646663666 batch mAP 0.347961426 batch PCKh 0.6875\n",
      "Validated batch 38 batch loss 0.625038505 batch mAP 0.402130127 batch PCKh 0.625\n",
      "Validated batch 39 batch loss 0.617209315 batch mAP 0.472564697 batch PCKh 0.25\n",
      "Validated batch 40 batch loss 0.549976 batch mAP 0.499115 batch PCKh 0.5\n",
      "Validated batch 41 batch loss 0.621531665 batch mAP 0.440460205 batch PCKh 0.625\n",
      "Validated batch 42 batch loss 0.63493222 batch mAP 0.451416016 batch PCKh 0.4375\n",
      "Validated batch 43 batch loss 0.580028772 batch mAP 0.470214844 batch PCKh 0.5\n",
      "Validated batch 44 batch loss 0.652930677 batch mAP 0.442230225 batch PCKh 0.5\n",
      "Validated batch 45 batch loss 0.611482561 batch mAP 0.466522217 batch PCKh 0\n",
      "Validated batch 46 batch loss 0.66934818 batch mAP 0.388031 batch PCKh 0.375\n",
      "Validated batch 47 batch loss 0.621434093 batch mAP 0.481201172 batch PCKh 0.5\n",
      "Validated batch 48 batch loss 0.608303666 batch mAP 0.454071045 batch PCKh 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 49 batch loss 0.632344186 batch mAP 0.333465576 batch PCKh 0.8125\n",
      "Validated batch 50 batch loss 0.620008647 batch mAP 0.431518555 batch PCKh 0.6875\n",
      "Validated batch 51 batch loss 0.62492913 batch mAP 0.429656982 batch PCKh 0.5\n",
      "Validated batch 52 batch loss 0.545095265 batch mAP 0.431091309 batch PCKh 0.125\n",
      "Validated batch 53 batch loss 0.62115252 batch mAP 0.426849365 batch PCKh 0.4375\n",
      "Validated batch 54 batch loss 0.636546612 batch mAP 0.50692749 batch PCKh 0.375\n",
      "Validated batch 55 batch loss 0.632972062 batch mAP 0.432952881 batch PCKh 0.3125\n",
      "Validated batch 56 batch loss 0.686313689 batch mAP 0.474273682 batch PCKh 0.1875\n",
      "Validated batch 57 batch loss 0.598127365 batch mAP 0.459014893 batch PCKh 0.875\n",
      "Validated batch 58 batch loss 0.669745266 batch mAP 0.529693604 batch PCKh 0.375\n",
      "Validated batch 59 batch loss 0.672440946 batch mAP 0.332061768 batch PCKh 0.5625\n",
      "Validated batch 60 batch loss 0.674646437 batch mAP 0.432800293 batch PCKh 0.125\n",
      "Validated batch 61 batch loss 0.6558007 batch mAP 0.386138916 batch PCKh 0.1875\n",
      "Validated batch 62 batch loss 0.610425 batch mAP 0.428253174 batch PCKh 0.625\n",
      "Validated batch 63 batch loss 0.681120932 batch mAP 0.337615967 batch PCKh 0\n",
      "Validated batch 64 batch loss 0.605428219 batch mAP 0.387664795 batch PCKh 0.3125\n",
      "Validated batch 65 batch loss 0.589856923 batch mAP 0.480102539 batch PCKh 0.75\n",
      "Validated batch 66 batch loss 0.628776491 batch mAP 0.521270752 batch PCKh 0.5\n",
      "Validated batch 67 batch loss 0.588319361 batch mAP 0.48840332 batch PCKh 0.4375\n",
      "Validated batch 68 batch loss 0.699700594 batch mAP 0.431396484 batch PCKh 0.25\n",
      "Validated batch 69 batch loss 0.629057884 batch mAP 0.459716797 batch PCKh 0.0625\n",
      "Validated batch 70 batch loss 0.65166 batch mAP 0.326263428 batch PCKh 0.3125\n",
      "Validated batch 71 batch loss 0.58684659 batch mAP 0.313995361 batch PCKh 0\n",
      "Validated batch 72 batch loss 0.67100656 batch mAP 0.35736084 batch PCKh 0.5625\n",
      "Validated batch 73 batch loss 0.653984129 batch mAP 0.454193115 batch PCKh 0.4375\n",
      "Validated batch 74 batch loss 0.676016271 batch mAP 0.373260498 batch PCKh 0.125\n",
      "Validated batch 75 batch loss 0.703666329 batch mAP 0.328460693 batch PCKh 0\n",
      "Validated batch 76 batch loss 0.635007143 batch mAP 0.378875732 batch PCKh 0.375\n",
      "Validated batch 77 batch loss 0.611434877 batch mAP 0.457946777 batch PCKh 0.5\n",
      "Validated batch 78 batch loss 0.61228615 batch mAP 0.4737854 batch PCKh 0.4375\n",
      "Validated batch 79 batch loss 0.659822822 batch mAP 0.370117188 batch PCKh 0.375\n",
      "Validated batch 80 batch loss 0.742789388 batch mAP 0.323791504 batch PCKh 0\n",
      "Validated batch 81 batch loss 0.545000255 batch mAP 0.411682129 batch PCKh 0.4375\n",
      "Validated batch 82 batch loss 0.565215349 batch mAP 0.474823 batch PCKh 0.125\n",
      "Validated batch 83 batch loss 0.624095201 batch mAP 0.372406 batch PCKh 0.125\n",
      "Validated batch 84 batch loss 0.699261785 batch mAP 0.410797119 batch PCKh 0.5\n",
      "Validated batch 85 batch loss 0.527314663 batch mAP 0.540313721 batch PCKh 0.4375\n",
      "Validated batch 86 batch loss 0.540110111 batch mAP 0.528900146 batch PCKh 0.375\n",
      "Validated batch 87 batch loss 0.53840661 batch mAP 0.4324646 batch PCKh 0.625\n",
      "Validated batch 88 batch loss 0.680607915 batch mAP 0.378295898 batch PCKh 0.5\n",
      "Validated batch 89 batch loss 0.655587792 batch mAP 0.491333 batch PCKh 0.1875\n",
      "Validated batch 90 batch loss 0.677072048 batch mAP 0.416503906 batch PCKh 0.6875\n",
      "Validated batch 91 batch loss 0.590433478 batch mAP 0.447723389 batch PCKh 0.5\n",
      "Validated batch 92 batch loss 0.607609749 batch mAP 0.470916748 batch PCKh 0.625\n",
      "Validated batch 93 batch loss 0.62022388 batch mAP 0.494232178 batch PCKh 0.375\n",
      "Validated batch 94 batch loss 0.642863274 batch mAP 0.312591553 batch PCKh 0.3125\n",
      "Validated batch 95 batch loss 0.593892 batch mAP 0.466888428 batch PCKh 0.375\n",
      "Validated batch 96 batch loss 0.565369487 batch mAP 0.463195801 batch PCKh 0.375\n",
      "Validated batch 97 batch loss 0.603888154 batch mAP 0.354125977 batch PCKh 0.5625\n",
      "Validated batch 98 batch loss 0.612466276 batch mAP 0.2684021 batch PCKh 0\n",
      "Validated batch 99 batch loss 0.601999462 batch mAP 0.411621094 batch PCKh 0.625\n",
      "Validated batch 100 batch loss 0.544059217 batch mAP 0.420806885 batch PCKh 0.5625\n",
      "Validated batch 101 batch loss 0.621006131 batch mAP 0.401947021 batch PCKh 0.8125\n",
      "Validated batch 102 batch loss 0.623504758 batch mAP 0.407531738 batch PCKh 0.5625\n",
      "Validated batch 103 batch loss 0.608784616 batch mAP 0.344024658 batch PCKh 0.3125\n",
      "Validated batch 104 batch loss 0.622096181 batch mAP 0.440643311 batch PCKh 0.5\n",
      "Validated batch 105 batch loss 0.616756797 batch mAP 0.414428711 batch PCKh 0.625\n",
      "Validated batch 106 batch loss 0.657776594 batch mAP 0.31350708 batch PCKh 0.0625\n",
      "Validated batch 107 batch loss 0.609635 batch mAP 0.390319824 batch PCKh 0.3125\n",
      "Validated batch 108 batch loss 0.629947543 batch mAP 0.466217041 batch PCKh 0.4375\n",
      "Validated batch 109 batch loss 0.678470731 batch mAP 0.33694458 batch PCKh 0\n",
      "Validated batch 110 batch loss 0.531419218 batch mAP 0.502197266 batch PCKh 0.4375\n",
      "Validated batch 111 batch loss 0.579225242 batch mAP 0.604858398 batch PCKh 0.6875\n",
      "Validated batch 112 batch loss 0.613272071 batch mAP 0.458374023 batch PCKh 0.0625\n",
      "Validated batch 113 batch loss 0.689847827 batch mAP 0.321472168 batch PCKh 0.375\n",
      "Validated batch 114 batch loss 0.517981529 batch mAP 0.425842285 batch PCKh 0.375\n",
      "Validated batch 115 batch loss 0.714591861 batch mAP 0.334838867 batch PCKh 0.125\n",
      "Validated batch 116 batch loss 0.572086453 batch mAP 0.421356201 batch PCKh 0.25\n",
      "Validated batch 117 batch loss 0.539209 batch mAP 0.489868164 batch PCKh 0.1875\n",
      "Validated batch 118 batch loss 0.638034165 batch mAP 0.39163208 batch PCKh 0.4375\n",
      "Validated batch 119 batch loss 0.575574517 batch mAP 0.416259766 batch PCKh 0.5\n",
      "Validated batch 120 batch loss 0.604962111 batch mAP 0.430999756 batch PCKh 0\n",
      "Validated batch 121 batch loss 0.65549463 batch mAP 0.432281494 batch PCKh 0.5625\n",
      "Validated batch 122 batch loss 0.615594864 batch mAP 0.365264893 batch PCKh 0.25\n",
      "Validated batch 123 batch loss 0.540135145 batch mAP 0.473175049 batch PCKh 0.5625\n",
      "Validated batch 124 batch loss 0.628090501 batch mAP 0.444519043 batch PCKh 0.25\n",
      "Validated batch 125 batch loss 0.704288 batch mAP 0.263427734 batch PCKh 0.125\n",
      "Validated batch 126 batch loss 0.697745562 batch mAP 0.365020752 batch PCKh 0.3125\n",
      "Validated batch 127 batch loss 0.550225258 batch mAP 0.460723877 batch PCKh 0.1875\n",
      "Validated batch 128 batch loss 0.692696 batch mAP 0.378417969 batch PCKh 0.375\n",
      "Validated batch 129 batch loss 0.597704649 batch mAP 0.442260742 batch PCKh 0.625\n",
      "Validated batch 130 batch loss 0.499416292 batch mAP 0.525146484 batch PCKh 0.1875\n",
      "Validated batch 131 batch loss 0.652358174 batch mAP 0.361785889 batch PCKh 0.75\n",
      "Validated batch 132 batch loss 0.676962733 batch mAP 0.285461426 batch PCKh 0\n",
      "Validated batch 133 batch loss 0.633792162 batch mAP 0.384979248 batch PCKh 0.375\n",
      "Validated batch 134 batch loss 0.607207775 batch mAP 0.418823242 batch PCKh 0.25\n",
      "Validated batch 135 batch loss 0.642739117 batch mAP 0.357788086 batch PCKh 0.0625\n",
      "Validated batch 136 batch loss 0.482977241 batch mAP 0.460906982 batch PCKh 0.625\n",
      "Validated batch 137 batch loss 0.659636199 batch mAP 0.359863281 batch PCKh 0.1875\n",
      "Validated batch 138 batch loss 0.62591958 batch mAP 0.432983398 batch PCKh 0.4375\n",
      "Validated batch 139 batch loss 0.66491282 batch mAP 0.437835693 batch PCKh 0.3125\n",
      "Validated batch 140 batch loss 0.594964921 batch mAP 0.356781 batch PCKh 0\n",
      "Validated batch 141 batch loss 0.576349854 batch mAP 0.427398682 batch PCKh 0.125\n",
      "Validated batch 142 batch loss 0.596090555 batch mAP 0.408844 batch PCKh 0.1875\n",
      "Validated batch 143 batch loss 0.565601408 batch mAP 0.455780029 batch PCKh 0.75\n",
      "Validated batch 144 batch loss 0.644154549 batch mAP 0.390197754 batch PCKh 0.5\n",
      "Validated batch 145 batch loss 0.570450068 batch mAP 0.554107666 batch PCKh 0.25\n",
      "Validated batch 146 batch loss 0.570586324 batch mAP 0.506408691 batch PCKh 0.5625\n",
      "Validated batch 147 batch loss 0.622666359 batch mAP 0.389373779 batch PCKh 0.625\n",
      "Validated batch 148 batch loss 0.57805407 batch mAP 0.512512207 batch PCKh 0.6875\n",
      "Validated batch 149 batch loss 0.596290231 batch mAP 0.48614502 batch PCKh 0.125\n",
      "Validated batch 150 batch loss 0.683666945 batch mAP 0.397247314 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 151 batch loss 0.564666927 batch mAP 0.447357178 batch PCKh 0.5\n",
      "Validated batch 152 batch loss 0.673264623 batch mAP 0.351074219 batch PCKh 0.1875\n",
      "Validated batch 153 batch loss 0.60597831 batch mAP 0.414764404 batch PCKh 0.0625\n",
      "Validated batch 154 batch loss 0.710617065 batch mAP 0.367553711 batch PCKh 0.25\n",
      "Validated batch 155 batch loss 0.651117384 batch mAP 0.359771729 batch PCKh 0.6875\n",
      "Validated batch 156 batch loss 0.625693262 batch mAP 0.49017334 batch PCKh 0.5625\n",
      "Validated batch 157 batch loss 0.672698 batch mAP 0.411682129 batch PCKh 0.25\n",
      "Validated batch 158 batch loss 0.592818201 batch mAP 0.377746582 batch PCKh 0.5\n",
      "Validated batch 159 batch loss 0.608545721 batch mAP 0.461212158 batch PCKh 0.375\n",
      "Validated batch 160 batch loss 0.615912735 batch mAP 0.420471191 batch PCKh 0.6875\n",
      "Validated batch 161 batch loss 0.697756171 batch mAP 0.324462891 batch PCKh 0.6875\n",
      "Validated batch 162 batch loss 0.69861114 batch mAP 0.426483154 batch PCKh 0.375\n",
      "Validated batch 163 batch loss 0.667794168 batch mAP 0.40447998 batch PCKh 0.5\n",
      "Validated batch 164 batch loss 0.64102304 batch mAP 0.26171875 batch PCKh 0.4375\n",
      "Validated batch 165 batch loss 0.640593767 batch mAP 0.502838135 batch PCKh 0.5625\n",
      "Validated batch 166 batch loss 0.647777796 batch mAP 0.444976807 batch PCKh 0.625\n",
      "Validated batch 167 batch loss 0.655242205 batch mAP 0.479675293 batch PCKh 0.5\n",
      "Validated batch 168 batch loss 0.687300563 batch mAP 0.37677002 batch PCKh 0.625\n",
      "Validated batch 169 batch loss 0.697888613 batch mAP 0.409454346 batch PCKh 0.6875\n",
      "Validated batch 170 batch loss 0.643497527 batch mAP 0.4815979 batch PCKh 0.5\n",
      "Validated batch 171 batch loss 0.704733253 batch mAP 0.393280029 batch PCKh 0.4375\n",
      "Validated batch 172 batch loss 0.592701197 batch mAP 0.495574951 batch PCKh 0.1875\n",
      "Validated batch 173 batch loss 0.647868514 batch mAP 0.48638916 batch PCKh 0.6875\n",
      "Validated batch 174 batch loss 0.532814741 batch mAP 0.415344238 batch PCKh 0.1875\n",
      "Validated batch 175 batch loss 0.590451479 batch mAP 0.443450928 batch PCKh 0.625\n",
      "Validated batch 176 batch loss 0.601532042 batch mAP 0.474334717 batch PCKh 0.375\n",
      "Validated batch 177 batch loss 0.669171572 batch mAP 0.295013428 batch PCKh 0.125\n",
      "Validated batch 178 batch loss 0.640040815 batch mAP 0.529418945 batch PCKh 0.3125\n",
      "Validated batch 179 batch loss 0.676513851 batch mAP 0.409576416 batch PCKh 0.5625\n",
      "Validated batch 180 batch loss 0.598160267 batch mAP 0.437103271 batch PCKh 0.5\n",
      "Validated batch 181 batch loss 0.608850837 batch mAP 0.482025146 batch PCKh 0.3125\n",
      "Validated batch 182 batch loss 0.635220528 batch mAP 0.409606934 batch PCKh 0.25\n",
      "Validated batch 183 batch loss 0.676042557 batch mAP 0.353912354 batch PCKh 0.25\n",
      "Validated batch 184 batch loss 0.589077771 batch mAP 0.449920654 batch PCKh 0.5\n",
      "Validated batch 185 batch loss 0.650359571 batch mAP 0.407531738 batch PCKh 0.1875\n",
      "Validated batch 186 batch loss 0.643268585 batch mAP 0.429748535 batch PCKh 0.5625\n",
      "Validated batch 187 batch loss 0.691893101 batch mAP 0.495361328 batch PCKh 0.4375\n",
      "Validated batch 188 batch loss 0.620028377 batch mAP 0.489471436 batch PCKh 0.5\n",
      "Validated batch 189 batch loss 0.497668624 batch mAP 0.535217285 batch PCKh 0.125\n",
      "Validated batch 190 batch loss 0.655256391 batch mAP 0.397583 batch PCKh 0\n",
      "Validated batch 191 batch loss 0.653957486 batch mAP 0.406921387 batch PCKh 0.3125\n",
      "Validated batch 192 batch loss 0.586603403 batch mAP 0.362213135 batch PCKh 0.5\n",
      "Validated batch 193 batch loss 0.579701245 batch mAP 0.406097412 batch PCKh 0.375\n",
      "Validated batch 194 batch loss 0.659292758 batch mAP 0.481719971 batch PCKh 0.375\n",
      "Validated batch 195 batch loss 0.56636095 batch mAP 0.384887695 batch PCKh 0.3125\n",
      "Validated batch 196 batch loss 0.655638099 batch mAP 0.560089111 batch PCKh 0.75\n",
      "Validated batch 197 batch loss 0.597853601 batch mAP 0.474884033 batch PCKh 0.3125\n",
      "Validated batch 198 batch loss 0.611721158 batch mAP 0.442565918 batch PCKh 0.25\n",
      "Validated batch 199 batch loss 0.58547461 batch mAP 0.465820312 batch PCKh 0.625\n",
      "Validated batch 200 batch loss 0.683320522 batch mAP 0.353240967 batch PCKh 0.1875\n",
      "Validated batch 201 batch loss 0.470008045 batch mAP 0.481994629 batch PCKh 0.375\n",
      "Validated batch 202 batch loss 0.718726 batch mAP 0.443908691 batch PCKh 0.75\n",
      "Validated batch 203 batch loss 0.702970743 batch mAP 0.375488281 batch PCKh 0\n",
      "Validated batch 204 batch loss 0.584361553 batch mAP 0.534820557 batch PCKh 0.375\n",
      "Validated batch 205 batch loss 0.628414631 batch mAP 0.433685303 batch PCKh 0.125\n",
      "Validated batch 206 batch loss 0.563285768 batch mAP 0.404571533 batch PCKh 0.25\n",
      "Validated batch 207 batch loss 0.558986068 batch mAP 0.460418701 batch PCKh 0.75\n",
      "Validated batch 208 batch loss 0.588281631 batch mAP 0.517547607 batch PCKh 0.6875\n",
      "Validated batch 209 batch loss 0.606530905 batch mAP 0.273254395 batch PCKh 0.25\n",
      "Validated batch 210 batch loss 0.583943844 batch mAP 0.446777344 batch PCKh 0.3125\n",
      "Validated batch 211 batch loss 0.692461789 batch mAP 0.40045166 batch PCKh 0.625\n",
      "Validated batch 212 batch loss 0.693818033 batch mAP 0.353729248 batch PCKh 0.125\n",
      "Validated batch 213 batch loss 0.649221957 batch mAP 0.289306641 batch PCKh 0\n",
      "Validated batch 214 batch loss 0.722132266 batch mAP 0.295318604 batch PCKh 0.125\n",
      "Validated batch 215 batch loss 0.70571965 batch mAP 0.242797852 batch PCKh 0.25\n",
      "Validated batch 216 batch loss 0.693037868 batch mAP 0.390716553 batch PCKh 0.5\n",
      "Validated batch 217 batch loss 0.596109629 batch mAP 0.461334229 batch PCKh 0.5625\n",
      "Validated batch 218 batch loss 0.628025 batch mAP 0.44229126 batch PCKh 0.5625\n",
      "Validated batch 219 batch loss 0.720291674 batch mAP 0.401550293 batch PCKh 0\n",
      "Validated batch 220 batch loss 0.651765227 batch mAP 0.401947021 batch PCKh 0.4375\n",
      "Validated batch 221 batch loss 0.638203382 batch mAP 0.302246094 batch PCKh 0.3125\n",
      "Validated batch 222 batch loss 0.631222785 batch mAP 0.460144043 batch PCKh 0.625\n",
      "Validated batch 223 batch loss 0.743059456 batch mAP 0.344116211 batch PCKh 0\n",
      "Validated batch 224 batch loss 0.557271361 batch mAP 0.365325928 batch PCKh 0.5\n",
      "Validated batch 225 batch loss 0.60332644 batch mAP 0.392852783 batch PCKh 0.3125\n",
      "Validated batch 226 batch loss 0.682065547 batch mAP 0.385437 batch PCKh 0.75\n",
      "Validated batch 227 batch loss 0.514599085 batch mAP 0.524932861 batch PCKh 0.1875\n",
      "Validated batch 228 batch loss 0.493078977 batch mAP 0.453491211 batch PCKh 0.25\n",
      "Validated batch 229 batch loss 0.583011746 batch mAP 0.506011963 batch PCKh 0.5625\n",
      "Validated batch 230 batch loss 0.664396107 batch mAP 0.439422607 batch PCKh 0.1875\n",
      "Validated batch 231 batch loss 0.637207747 batch mAP 0.444854736 batch PCKh 0.8125\n",
      "Validated batch 232 batch loss 0.523359597 batch mAP 0.404052734 batch PCKh 0.1875\n",
      "Validated batch 233 batch loss 0.570633411 batch mAP 0.381774902 batch PCKh 0.75\n",
      "Validated batch 234 batch loss 0.615863562 batch mAP 0.440216064 batch PCKh 0.3125\n",
      "Validated batch 235 batch loss 0.666833758 batch mAP 0.46206665 batch PCKh 0.625\n",
      "Validated batch 236 batch loss 0.620369434 batch mAP 0.403045654 batch PCKh 0.6875\n",
      "Validated batch 237 batch loss 0.597476 batch mAP 0.468688965 batch PCKh 0.125\n",
      "Validated batch 238 batch loss 0.568942428 batch mAP 0.570953369 batch PCKh 0.375\n",
      "Validated batch 239 batch loss 0.616988242 batch mAP 0.445678711 batch PCKh 0.5625\n",
      "Validated batch 240 batch loss 0.635831177 batch mAP 0.47744751 batch PCKh 0.5\n",
      "Validated batch 241 batch loss 0.71566838 batch mAP 0.338592529 batch PCKh 0.3125\n",
      "Validated batch 242 batch loss 0.682042241 batch mAP 0.455993652 batch PCKh 0.4375\n",
      "Validated batch 243 batch loss 0.593895197 batch mAP 0.523468 batch PCKh 0.75\n",
      "Validated batch 244 batch loss 0.513112962 batch mAP 0.452972412 batch PCKh 0.875\n",
      "Validated batch 245 batch loss 0.612288952 batch mAP 0.530517578 batch PCKh 0\n",
      "Validated batch 246 batch loss 0.60172224 batch mAP 0.474945068 batch PCKh 0.375\n",
      "Validated batch 247 batch loss 0.64789 batch mAP 0.376983643 batch PCKh 0.3125\n",
      "Validated batch 248 batch loss 0.56561625 batch mAP 0.475769043 batch PCKh 0.1875\n",
      "Validated batch 249 batch loss 0.621402919 batch mAP 0.408111572 batch PCKh 0.625\n",
      "Validated batch 250 batch loss 0.663187385 batch mAP 0.440979 batch PCKh 0.5\n",
      "Validated batch 251 batch loss 0.655358315 batch mAP 0.381896973 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 252 batch loss 0.61322546 batch mAP 0.417480469 batch PCKh 0.5\n",
      "Validated batch 253 batch loss 0.561295807 batch mAP 0.474609375 batch PCKh 0.5\n",
      "Validated batch 254 batch loss 0.580396652 batch mAP 0.381622314 batch PCKh 0.75\n",
      "Validated batch 255 batch loss 0.442990422 batch mAP 0.468536377 batch PCKh 0.25\n",
      "Validated batch 256 batch loss 0.57096827 batch mAP 0.504486084 batch PCKh 0.4375\n",
      "Validated batch 257 batch loss 0.612573 batch mAP 0.478729248 batch PCKh 0.8125\n",
      "Validated batch 258 batch loss 0.579772115 batch mAP 0.517395 batch PCKh 0.5625\n",
      "Validated batch 259 batch loss 0.616543174 batch mAP 0.425415039 batch PCKh 0.375\n",
      "Validated batch 260 batch loss 0.528557539 batch mAP 0.558227539 batch PCKh 0.5\n",
      "Validated batch 261 batch loss 0.625952244 batch mAP 0.404052734 batch PCKh 0.6875\n",
      "Validated batch 262 batch loss 0.612020731 batch mAP 0.336700439 batch PCKh 0.5625\n",
      "Validated batch 263 batch loss 0.652207792 batch mAP 0.331207275 batch PCKh 0.125\n",
      "Validated batch 264 batch loss 0.613239467 batch mAP 0.497619629 batch PCKh 0.75\n",
      "Validated batch 265 batch loss 0.583990455 batch mAP 0.489105225 batch PCKh 0.375\n",
      "Validated batch 266 batch loss 0.510103405 batch mAP 0.483795166 batch PCKh 0.4375\n",
      "Validated batch 267 batch loss 0.62073487 batch mAP 0.433105469 batch PCKh 0\n",
      "Validated batch 268 batch loss 0.592152894 batch mAP 0.446075439 batch PCKh 0.375\n",
      "Validated batch 269 batch loss 0.660923958 batch mAP 0.394348145 batch PCKh 0.0625\n",
      "Validated batch 270 batch loss 0.703837037 batch mAP 0.451782227 batch PCKh 0.25\n",
      "Validated batch 271 batch loss 0.648817956 batch mAP 0.391906738 batch PCKh 0.1875\n",
      "Validated batch 272 batch loss 0.617295504 batch mAP 0.518920898 batch PCKh 0.5625\n",
      "Validated batch 273 batch loss 0.637864828 batch mAP 0.471374512 batch PCKh 0.3125\n",
      "Validated batch 274 batch loss 0.650070727 batch mAP 0.455749512 batch PCKh 0.5625\n",
      "Validated batch 275 batch loss 0.536807418 batch mAP 0.485321045 batch PCKh 0.6875\n",
      "Validated batch 276 batch loss 0.546592951 batch mAP 0.412872314 batch PCKh 0.125\n",
      "Validated batch 277 batch loss 0.587288499 batch mAP 0.535705566 batch PCKh 0.6875\n",
      "Validated batch 278 batch loss 0.614561558 batch mAP 0.40524292 batch PCKh 0.5\n",
      "Validated batch 279 batch loss 0.640370667 batch mAP 0.516845703 batch PCKh 0.0625\n",
      "Validated batch 280 batch loss 0.641039252 batch mAP 0.382049561 batch PCKh 0.5\n",
      "Validated batch 281 batch loss 0.600534439 batch mAP 0.399810791 batch PCKh 0.3125\n",
      "Validated batch 282 batch loss 0.600201607 batch mAP 0.415435791 batch PCKh 0.125\n",
      "Validated batch 283 batch loss 0.520974278 batch mAP 0.531799316 batch PCKh 0.1875\n",
      "Validated batch 284 batch loss 0.570360839 batch mAP 0.425689697 batch PCKh 0\n",
      "Validated batch 285 batch loss 0.617188394 batch mAP 0.507263184 batch PCKh 0.875\n",
      "Validated batch 286 batch loss 0.604909897 batch mAP 0.503295898 batch PCKh 0.1875\n",
      "Validated batch 287 batch loss 0.629328251 batch mAP 0.463592529 batch PCKh 0.3125\n",
      "Validated batch 288 batch loss 0.749125481 batch mAP 0.408966064 batch PCKh 0\n",
      "Validated batch 289 batch loss 0.570616841 batch mAP 0.453399658 batch PCKh 0.5625\n",
      "Validated batch 290 batch loss 0.549187481 batch mAP 0.44909668 batch PCKh 0.5625\n",
      "Validated batch 291 batch loss 0.679848909 batch mAP 0.422485352 batch PCKh 0.375\n",
      "Validated batch 292 batch loss 0.500037193 batch mAP 0.451263428 batch PCKh 0.125\n",
      "Validated batch 293 batch loss 0.549589634 batch mAP 0.495941162 batch PCKh 0.1875\n",
      "Validated batch 294 batch loss 0.616057217 batch mAP 0.490875244 batch PCKh 0.4375\n",
      "Validated batch 295 batch loss 0.594451427 batch mAP 0.490814209 batch PCKh 0.1875\n",
      "Validated batch 296 batch loss 0.634079933 batch mAP 0.422363281 batch PCKh 0.1875\n",
      "Validated batch 297 batch loss 0.523372 batch mAP 0.549346924 batch PCKh 0.4375\n",
      "Validated batch 298 batch loss 0.551046729 batch mAP 0.503448486 batch PCKh 0.5625\n",
      "Validated batch 299 batch loss 0.605282664 batch mAP 0.444824219 batch PCKh 0.125\n",
      "Validated batch 300 batch loss 0.651236653 batch mAP 0.468170166 batch PCKh 0\n",
      "Validated batch 301 batch loss 0.571321487 batch mAP 0.399383545 batch PCKh 0.0625\n",
      "Validated batch 302 batch loss 0.54315269 batch mAP 0.448181152 batch PCKh 0.4375\n",
      "Validated batch 303 batch loss 0.664595604 batch mAP 0.414703369 batch PCKh 0.5\n",
      "Validated batch 304 batch loss 0.576181829 batch mAP 0.476867676 batch PCKh 0.25\n",
      "Validated batch 305 batch loss 0.64493829 batch mAP 0.517303467 batch PCKh 0.875\n",
      "Validated batch 306 batch loss 0.66054225 batch mAP 0.422698975 batch PCKh 0.6875\n",
      "Validated batch 307 batch loss 0.689468 batch mAP 0.330200195 batch PCKh 0\n",
      "Validated batch 308 batch loss 0.678158 batch mAP 0.466918945 batch PCKh 0.1875\n",
      "Validated batch 309 batch loss 0.570249319 batch mAP 0.487823486 batch PCKh 0.5625\n",
      "Validated batch 310 batch loss 0.583209097 batch mAP 0.43536377 batch PCKh 0.75\n",
      "Validated batch 311 batch loss 0.683493137 batch mAP 0.330505371 batch PCKh 0.1875\n",
      "Validated batch 312 batch loss 0.663493 batch mAP 0.354675293 batch PCKh 0\n",
      "Validated batch 313 batch loss 0.567057192 batch mAP 0.508422852 batch PCKh 0.1875\n",
      "Validated batch 314 batch loss 0.498156101 batch mAP 0.551269531 batch PCKh 0.25\n",
      "Validated batch 315 batch loss 0.565822423 batch mAP 0.364318848 batch PCKh 0\n",
      "Validated batch 316 batch loss 0.537235856 batch mAP 0.472320557 batch PCKh 0.3125\n",
      "Validated batch 317 batch loss 0.663314462 batch mAP 0.319763184 batch PCKh 0.375\n",
      "Validated batch 318 batch loss 0.51911211 batch mAP 0.566589355 batch PCKh 0.75\n",
      "Validated batch 319 batch loss 0.542871058 batch mAP 0.512695312 batch PCKh 0.5\n",
      "Validated batch 320 batch loss 0.666347623 batch mAP 0.435394287 batch PCKh 0.3125\n",
      "Validated batch 321 batch loss 0.677615523 batch mAP 0.385467529 batch PCKh 0.375\n",
      "Validated batch 322 batch loss 0.69974035 batch mAP 0.394470215 batch PCKh 0.5625\n",
      "Validated batch 323 batch loss 0.686859667 batch mAP 0.362670898 batch PCKh 0.3125\n",
      "Validated batch 324 batch loss 0.676942289 batch mAP 0.379394531 batch PCKh 0.125\n",
      "Validated batch 325 batch loss 0.633221865 batch mAP 0.462371826 batch PCKh 0.1875\n",
      "Validated batch 326 batch loss 0.572938442 batch mAP 0.485351562 batch PCKh 0.4375\n",
      "Validated batch 327 batch loss 0.699875772 batch mAP 0.538024902 batch PCKh 0.4375\n",
      "Validated batch 328 batch loss 0.55084753 batch mAP 0.429321289 batch PCKh 0.5625\n",
      "Validated batch 329 batch loss 0.621581614 batch mAP 0.505188 batch PCKh 0.3125\n",
      "Validated batch 330 batch loss 0.581582 batch mAP 0.492584229 batch PCKh 0.5625\n",
      "Validated batch 331 batch loss 0.614485204 batch mAP 0.485443115 batch PCKh 0.8125\n",
      "Validated batch 332 batch loss 0.51595068 batch mAP 0.541992188 batch PCKh 0.5625\n",
      "Validated batch 333 batch loss 0.619720399 batch mAP 0.422821045 batch PCKh 0.25\n",
      "Validated batch 334 batch loss 0.673199892 batch mAP 0.470306396 batch PCKh 0.5\n",
      "Validated batch 335 batch loss 0.608454 batch mAP 0.46862793 batch PCKh 0.4375\n",
      "Validated batch 336 batch loss 0.65702951 batch mAP 0.384613037 batch PCKh 0.5625\n",
      "Validated batch 337 batch loss 0.614621878 batch mAP 0.473693848 batch PCKh 0.0625\n",
      "Validated batch 338 batch loss 0.576755464 batch mAP 0.471466064 batch PCKh 0.4375\n",
      "Validated batch 339 batch loss 0.560887337 batch mAP 0.476409912 batch PCKh 0.5625\n",
      "Validated batch 340 batch loss 0.575832129 batch mAP 0.571899414 batch PCKh 0.3125\n",
      "Validated batch 341 batch loss 0.660660744 batch mAP 0.447235107 batch PCKh 0.5\n",
      "Validated batch 342 batch loss 0.599268854 batch mAP 0.468475342 batch PCKh 0.375\n",
      "Validated batch 343 batch loss 0.598590493 batch mAP 0.488952637 batch PCKh 0.25\n",
      "Validated batch 344 batch loss 0.638100624 batch mAP 0.449707031 batch PCKh 0.25\n",
      "Validated batch 345 batch loss 0.651962757 batch mAP 0.380218506 batch PCKh 0.25\n",
      "Validated batch 346 batch loss 0.651807666 batch mAP 0.372253418 batch PCKh 0.5625\n",
      "Validated batch 347 batch loss 0.580894113 batch mAP 0.448516846 batch PCKh 0.375\n",
      "Validated batch 348 batch loss 0.658913791 batch mAP 0.343841553 batch PCKh 0.375\n",
      "Validated batch 349 batch loss 0.5635221 batch mAP 0.424316406 batch PCKh 0.4375\n",
      "Validated batch 350 batch loss 0.517024875 batch mAP 0.381286621 batch PCKh 0.5\n",
      "Validated batch 351 batch loss 0.664727 batch mAP 0.383270264 batch PCKh 0.375\n",
      "Validated batch 352 batch loss 0.563505173 batch mAP 0.531402588 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 353 batch loss 0.595390677 batch mAP 0.438293457 batch PCKh 0.6875\n",
      "Validated batch 354 batch loss 0.640854418 batch mAP 0.387298584 batch PCKh 0.375\n",
      "Validated batch 355 batch loss 0.595574319 batch mAP 0.52822876 batch PCKh 0.125\n",
      "Validated batch 356 batch loss 0.616058 batch mAP 0.500244141 batch PCKh 0.3125\n",
      "Validated batch 357 batch loss 0.570961237 batch mAP 0.382568359 batch PCKh 0.1875\n",
      "Validated batch 358 batch loss 0.701608181 batch mAP 0.333404541 batch PCKh 0.1875\n",
      "Validated batch 359 batch loss 0.619539857 batch mAP 0.507629395 batch PCKh 0.375\n",
      "Validated batch 360 batch loss 0.630501211 batch mAP 0.401641846 batch PCKh 0.1875\n",
      "Validated batch 361 batch loss 0.687378943 batch mAP 0.46685791 batch PCKh 0.4375\n",
      "Validated batch 362 batch loss 0.728640795 batch mAP 0.393035889 batch PCKh 0\n",
      "Validated batch 363 batch loss 0.749062836 batch mAP 0.317596436 batch PCKh 0.0625\n",
      "Validated batch 364 batch loss 0.656466782 batch mAP 0.47076416 batch PCKh 0.5\n",
      "Validated batch 365 batch loss 0.598260462 batch mAP 0.414001465 batch PCKh 0.625\n",
      "Validated batch 366 batch loss 0.634275436 batch mAP 0.477325439 batch PCKh 0.3125\n",
      "Validated batch 367 batch loss 0.583606362 batch mAP 0.332427979 batch PCKh 0.375\n",
      "Validated batch 368 batch loss 0.540879428 batch mAP 0.494812 batch PCKh 0.4375\n",
      "Validated batch 369 batch loss 0.531287551 batch mAP 0.470550537 batch PCKh 0.4375\n",
      "Epoch 2 val loss 0.6199129819869995 val mAP 0.4327426552772522 val PCKh\n",
      "Epoch 2 completed in 787.85 seconds\n",
      "Model /aiffel/aiffel/model_weight/GD08/y_model-epoch-2-loss-0.6199.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.62392056 batch mAP 0.413269043 batch PCKh 0.875\n",
      "Trained batch 2 batch loss 0.593057394 batch mAP 0.451416016 batch PCKh 0.4375\n",
      "Trained batch 3 batch loss 0.604834318 batch mAP 0.474639893 batch PCKh 0.5\n",
      "Trained batch 4 batch loss 0.612029791 batch mAP 0.50479126 batch PCKh 0.875\n",
      "Trained batch 5 batch loss 0.696604848 batch mAP 0.472320557 batch PCKh 0.75\n",
      "Trained batch 6 batch loss 0.66551137 batch mAP 0.498382568 batch PCKh 0.75\n",
      "Trained batch 7 batch loss 0.583984375 batch mAP 0.514770508 batch PCKh 0.3125\n",
      "Trained batch 8 batch loss 0.698269069 batch mAP 0.477783203 batch PCKh 0.1875\n",
      "Trained batch 9 batch loss 0.656727433 batch mAP 0.474609375 batch PCKh 0.1875\n",
      "Trained batch 10 batch loss 0.628704667 batch mAP 0.509552 batch PCKh 0.3125\n",
      "Trained batch 11 batch loss 0.564950526 batch mAP 0.52734375 batch PCKh 0.125\n",
      "Trained batch 12 batch loss 0.691661119 batch mAP 0.510009766 batch PCKh 0.3125\n",
      "Trained batch 13 batch loss 0.618917227 batch mAP 0.515258789 batch PCKh 0.625\n",
      "Trained batch 14 batch loss 0.594275 batch mAP 0.510192871 batch PCKh 0.75\n",
      "Trained batch 15 batch loss 0.599984407 batch mAP 0.451385498 batch PCKh 0.75\n",
      "Trained batch 16 batch loss 0.586196303 batch mAP 0.462921143 batch PCKh 0.5\n",
      "Trained batch 17 batch loss 0.548234224 batch mAP 0.45300293 batch PCKh 0.25\n",
      "Trained batch 18 batch loss 0.652883947 batch mAP 0.480529785 batch PCKh 0.625\n",
      "Trained batch 19 batch loss 0.676192343 batch mAP 0.464202881 batch PCKh 0.3125\n",
      "Trained batch 20 batch loss 0.662513196 batch mAP 0.494842529 batch PCKh 0.125\n",
      "Trained batch 21 batch loss 0.675847411 batch mAP 0.450012207 batch PCKh 0.25\n",
      "Trained batch 22 batch loss 0.643451393 batch mAP 0.480438232 batch PCKh 0.125\n",
      "Trained batch 23 batch loss 0.627824605 batch mAP 0.492156982 batch PCKh 0.3125\n",
      "Trained batch 24 batch loss 0.598551631 batch mAP 0.514068604 batch PCKh 0.3125\n",
      "Trained batch 25 batch loss 0.533756852 batch mAP 0.463867188 batch PCKh 0.25\n",
      "Trained batch 26 batch loss 0.636916518 batch mAP 0.478210449 batch PCKh 0.5\n",
      "Trained batch 27 batch loss 0.574297369 batch mAP 0.491424561 batch PCKh 0.5\n",
      "Trained batch 28 batch loss 0.616739511 batch mAP 0.532104492 batch PCKh 0.375\n",
      "Trained batch 29 batch loss 0.63473779 batch mAP 0.523223877 batch PCKh 0.1875\n",
      "Trained batch 30 batch loss 0.626036763 batch mAP 0.563934326 batch PCKh 0.3125\n",
      "Trained batch 31 batch loss 0.561993241 batch mAP 0.508911133 batch PCKh 0.25\n",
      "Trained batch 32 batch loss 0.614213228 batch mAP 0.49432373 batch PCKh 0.5625\n",
      "Trained batch 33 batch loss 0.706609428 batch mAP 0.472198486 batch PCKh 0.8125\n",
      "Trained batch 34 batch loss 0.680927098 batch mAP 0.435180664 batch PCKh 0.75\n",
      "Trained batch 35 batch loss 0.66214633 batch mAP 0.395141602 batch PCKh 0.75\n",
      "Trained batch 36 batch loss 0.729842782 batch mAP 0.37512207 batch PCKh 0.0625\n",
      "Trained batch 37 batch loss 0.628748059 batch mAP 0.459533691 batch PCKh 0.1875\n",
      "Trained batch 38 batch loss 0.63818717 batch mAP 0.433898926 batch PCKh 0.375\n",
      "Trained batch 39 batch loss 0.644027352 batch mAP 0.440826416 batch PCKh 0.25\n",
      "Trained batch 40 batch loss 0.547566533 batch mAP 0.402679443 batch PCKh 0.1875\n",
      "Trained batch 41 batch loss 0.501282215 batch mAP 0.391021729 batch PCKh 0.5\n",
      "Trained batch 42 batch loss 0.501929581 batch mAP 0.304321289 batch PCKh 0.1875\n",
      "Trained batch 43 batch loss 0.524444818 batch mAP 0.403564453 batch PCKh 0.1875\n",
      "Trained batch 44 batch loss 0.534773 batch mAP 0.17199707 batch PCKh 0.5625\n",
      "Trained batch 45 batch loss 0.580108762 batch mAP 0.276519775 batch PCKh 0.1875\n",
      "Trained batch 46 batch loss 0.571552515 batch mAP 0.29864502 batch PCKh 0.5625\n",
      "Trained batch 47 batch loss 0.572992146 batch mAP 0.389221191 batch PCKh 0.375\n",
      "Trained batch 48 batch loss 0.623794436 batch mAP 0.541992188 batch PCKh 0.375\n",
      "Trained batch 49 batch loss 0.488899022 batch mAP 0.537750244 batch PCKh 0.0625\n",
      "Trained batch 50 batch loss 0.47704491 batch mAP 0.531982422 batch PCKh 0.4375\n",
      "Trained batch 51 batch loss 0.521075547 batch mAP 0.524505615 batch PCKh 0.0625\n",
      "Trained batch 52 batch loss 0.553186238 batch mAP 0.484436035 batch PCKh 0.125\n",
      "Trained batch 53 batch loss 0.530938685 batch mAP 0.493286133 batch PCKh 0.3125\n",
      "Trained batch 54 batch loss 0.651067257 batch mAP 0.365600586 batch PCKh 0.25\n",
      "Trained batch 55 batch loss 0.610743761 batch mAP 0.420135498 batch PCKh 0.25\n",
      "Trained batch 56 batch loss 0.684081137 batch mAP 0.381439209 batch PCKh 0.1875\n",
      "Trained batch 57 batch loss 0.546898067 batch mAP 0.464813232 batch PCKh 0.375\n",
      "Trained batch 58 batch loss 0.650716722 batch mAP 0.537475586 batch PCKh 0.8125\n",
      "Trained batch 59 batch loss 0.608394623 batch mAP 0.539672852 batch PCKh 0.1875\n",
      "Trained batch 60 batch loss 0.645358086 batch mAP 0.457885742 batch PCKh 0\n",
      "Trained batch 61 batch loss 0.670677185 batch mAP 0.415924072 batch PCKh 0.5\n",
      "Trained batch 62 batch loss 0.607252479 batch mAP 0.447174072 batch PCKh 0.125\n",
      "Trained batch 63 batch loss 0.680727363 batch mAP 0.465789795 batch PCKh 0.125\n",
      "Trained batch 64 batch loss 0.575916529 batch mAP 0.424530029 batch PCKh 0.375\n",
      "Trained batch 65 batch loss 0.609181345 batch mAP 0.387512207 batch PCKh 0.3125\n",
      "Trained batch 66 batch loss 0.642989576 batch mAP 0.347625732 batch PCKh 0.3125\n",
      "Trained batch 67 batch loss 0.532235 batch mAP 0.218292236 batch PCKh 0.125\n",
      "Trained batch 68 batch loss 0.616048038 batch mAP 0.241546631 batch PCKh 0.25\n",
      "Trained batch 69 batch loss 0.599522114 batch mAP 0.326812744 batch PCKh 0.25\n",
      "Trained batch 70 batch loss 0.596665919 batch mAP 0.40246582 batch PCKh 0.625\n",
      "Trained batch 71 batch loss 0.646903515 batch mAP 0.433105469 batch PCKh 0.125\n",
      "Trained batch 72 batch loss 0.647730291 batch mAP 0.475616455 batch PCKh 0.5\n",
      "Trained batch 73 batch loss 0.496371984 batch mAP 0.493621826 batch PCKh 0.1875\n",
      "Trained batch 74 batch loss 0.525177 batch mAP 0.527557373 batch PCKh 0.125\n",
      "Trained batch 75 batch loss 0.412249029 batch mAP 0.531616211 batch PCKh 0\n",
      "Trained batch 76 batch loss 0.408610135 batch mAP 0.477996826 batch PCKh 0\n",
      "Trained batch 77 batch loss 0.480390906 batch mAP 0.441009521 batch PCKh 0.1875\n",
      "Trained batch 78 batch loss 0.54480511 batch mAP 0.594451904 batch PCKh 0.5625\n",
      "Trained batch 79 batch loss 0.540389776 batch mAP 0.516784668 batch PCKh 0.3125\n",
      "Trained batch 80 batch loss 0.661692381 batch mAP 0.543731689 batch PCKh 0.25\n",
      "Trained batch 81 batch loss 0.691632926 batch mAP 0.580993652 batch PCKh 0.0625\n",
      "Trained batch 82 batch loss 0.698764443 batch mAP 0.544555664 batch PCKh 0.1875\n",
      "Trained batch 83 batch loss 0.714209 batch mAP 0.322357178 batch PCKh 0\n",
      "Trained batch 84 batch loss 0.709237516 batch mAP 0.19342041 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 85 batch loss 0.692601502 batch mAP 0.309265137 batch PCKh 0.4375\n",
      "Trained batch 86 batch loss 0.650182843 batch mAP 0.453094482 batch PCKh 0.1875\n",
      "Trained batch 87 batch loss 0.673086464 batch mAP 0.493560791 batch PCKh 0.25\n",
      "Trained batch 88 batch loss 0.6144225 batch mAP 0.430450439 batch PCKh 0.25\n",
      "Trained batch 89 batch loss 0.679406881 batch mAP 0.493042 batch PCKh 0\n",
      "Trained batch 90 batch loss 0.550751388 batch mAP 0.493469238 batch PCKh 0.125\n",
      "Trained batch 91 batch loss 0.628294 batch mAP 0.518341064 batch PCKh 0.375\n",
      "Trained batch 92 batch loss 0.571463287 batch mAP 0.508148193 batch PCKh 0.125\n",
      "Trained batch 93 batch loss 0.591773272 batch mAP 0.459320068 batch PCKh 0.5\n",
      "Trained batch 94 batch loss 0.598452449 batch mAP 0.513519287 batch PCKh 0.5\n",
      "Trained batch 95 batch loss 0.569052875 batch mAP 0.534942627 batch PCKh 0.375\n",
      "Trained batch 96 batch loss 0.624400139 batch mAP 0.477417 batch PCKh 0.4375\n",
      "Trained batch 97 batch loss 0.580201447 batch mAP 0.547058105 batch PCKh 0.75\n",
      "Trained batch 98 batch loss 0.588503122 batch mAP 0.499908447 batch PCKh 0.4375\n",
      "Trained batch 99 batch loss 0.734416902 batch mAP 0.381225586 batch PCKh 0.5625\n",
      "Trained batch 100 batch loss 0.623915553 batch mAP 0.448974609 batch PCKh 0.3125\n",
      "Trained batch 101 batch loss 0.654617548 batch mAP 0.477966309 batch PCKh 0.1875\n",
      "Trained batch 102 batch loss 0.679480195 batch mAP 0.471221924 batch PCKh 0.4375\n",
      "Trained batch 103 batch loss 0.575689793 batch mAP 0.535186768 batch PCKh 0.375\n",
      "Trained batch 104 batch loss 0.547367 batch mAP 0.512390137 batch PCKh 0.25\n",
      "Trained batch 105 batch loss 0.58797437 batch mAP 0.533477783 batch PCKh 0.25\n",
      "Trained batch 106 batch loss 0.669292927 batch mAP 0.497711182 batch PCKh 0.125\n",
      "Trained batch 107 batch loss 0.543128 batch mAP 0.484893799 batch PCKh 0.0625\n",
      "Trained batch 108 batch loss 0.634679198 batch mAP 0.509552 batch PCKh 0.8125\n",
      "Trained batch 109 batch loss 0.618393064 batch mAP 0.472930908 batch PCKh 0.375\n",
      "Trained batch 110 batch loss 0.565837741 batch mAP 0.464599609 batch PCKh 0.5625\n",
      "Trained batch 111 batch loss 0.592681706 batch mAP 0.468078613 batch PCKh 0.625\n",
      "Trained batch 112 batch loss 0.577723861 batch mAP 0.479858398 batch PCKh 0.4375\n",
      "Trained batch 113 batch loss 0.610920906 batch mAP 0.471099854 batch PCKh 0.75\n",
      "Trained batch 114 batch loss 0.584041178 batch mAP 0.500183105 batch PCKh 0.5\n",
      "Trained batch 115 batch loss 0.60657084 batch mAP 0.496826172 batch PCKh 0.5625\n",
      "Trained batch 116 batch loss 0.596197 batch mAP 0.499084473 batch PCKh 0.5625\n",
      "Trained batch 117 batch loss 0.638183117 batch mAP 0.466644287 batch PCKh 0.4375\n",
      "Trained batch 118 batch loss 0.641901791 batch mAP 0.460418701 batch PCKh 0.1875\n",
      "Trained batch 119 batch loss 0.600529552 batch mAP 0.485473633 batch PCKh 0.5625\n",
      "Trained batch 120 batch loss 0.671201587 batch mAP 0.480957031 batch PCKh 0.1875\n",
      "Trained batch 121 batch loss 0.657244 batch mAP 0.488830566 batch PCKh 0.8125\n",
      "Trained batch 122 batch loss 0.644695401 batch mAP 0.471191406 batch PCKh 0.6875\n",
      "Trained batch 123 batch loss 0.609692216 batch mAP 0.47277832 batch PCKh 0.625\n",
      "Trained batch 124 batch loss 0.658196211 batch mAP 0.472839355 batch PCKh 0.1875\n",
      "Trained batch 125 batch loss 0.649350345 batch mAP 0.466217041 batch PCKh 0.375\n",
      "Trained batch 126 batch loss 0.597577572 batch mAP 0.455535889 batch PCKh 0.1875\n",
      "Trained batch 127 batch loss 0.611010194 batch mAP 0.440948486 batch PCKh 0.4375\n",
      "Trained batch 128 batch loss 0.682392716 batch mAP 0.451202393 batch PCKh 0.75\n",
      "Trained batch 129 batch loss 0.573490858 batch mAP 0.419647217 batch PCKh 0.6875\n",
      "Trained batch 130 batch loss 0.561807454 batch mAP 0.422912598 batch PCKh 0.6875\n",
      "Trained batch 131 batch loss 0.526129842 batch mAP 0.420623779 batch PCKh 0.25\n",
      "Trained batch 132 batch loss 0.592135429 batch mAP 0.429321289 batch PCKh 0.5\n",
      "Trained batch 133 batch loss 0.581314147 batch mAP 0.404968262 batch PCKh 0.5\n",
      "Trained batch 134 batch loss 0.591847062 batch mAP 0.492919922 batch PCKh 0.3125\n",
      "Trained batch 135 batch loss 0.551045418 batch mAP 0.484985352 batch PCKh 0.6875\n",
      "Trained batch 136 batch loss 0.655045927 batch mAP 0.499816895 batch PCKh 0.25\n",
      "Trained batch 137 batch loss 0.602631807 batch mAP 0.538452148 batch PCKh 0.3125\n",
      "Trained batch 138 batch loss 0.580084443 batch mAP 0.545257568 batch PCKh 0.4375\n",
      "Trained batch 139 batch loss 0.596096694 batch mAP 0.517425537 batch PCKh 0.375\n",
      "Trained batch 140 batch loss 0.601923287 batch mAP 0.552337646 batch PCKh 0.3125\n",
      "Trained batch 141 batch loss 0.620104 batch mAP 0.535888672 batch PCKh 0.3125\n",
      "Trained batch 142 batch loss 0.586421251 batch mAP 0.562561035 batch PCKh 0.25\n",
      "Trained batch 143 batch loss 0.563103378 batch mAP 0.582275391 batch PCKh 0.5\n",
      "Trained batch 144 batch loss 0.521667 batch mAP 0.588012695 batch PCKh 0.875\n",
      "Trained batch 145 batch loss 0.559375226 batch mAP 0.598297119 batch PCKh 0.375\n",
      "Trained batch 146 batch loss 0.632059336 batch mAP 0.560974121 batch PCKh 0.4375\n",
      "Trained batch 147 batch loss 0.681423485 batch mAP 0.454528809 batch PCKh 0.25\n",
      "Trained batch 148 batch loss 0.601047337 batch mAP 0.52331543 batch PCKh 0.75\n",
      "Trained batch 149 batch loss 0.62285018 batch mAP 0.545105 batch PCKh 0.75\n",
      "Trained batch 150 batch loss 0.599307 batch mAP 0.551879883 batch PCKh 0.625\n",
      "Trained batch 151 batch loss 0.678389847 batch mAP 0.50958252 batch PCKh 0.4375\n",
      "Trained batch 152 batch loss 0.578430593 batch mAP 0.577423096 batch PCKh 0.75\n",
      "Trained batch 153 batch loss 0.610866785 batch mAP 0.535675049 batch PCKh 0.25\n",
      "Trained batch 154 batch loss 0.528119385 batch mAP 0.539794922 batch PCKh 0.4375\n",
      "Trained batch 155 batch loss 0.577688634 batch mAP 0.540252686 batch PCKh 0.5\n",
      "Trained batch 156 batch loss 0.518016636 batch mAP 0.507354736 batch PCKh 0.3125\n",
      "Trained batch 157 batch loss 0.598995626 batch mAP 0.531341553 batch PCKh 0.0625\n",
      "Trained batch 158 batch loss 0.580803871 batch mAP 0.518157959 batch PCKh 0.625\n",
      "Trained batch 159 batch loss 0.644454956 batch mAP 0.53894043 batch PCKh 0.375\n",
      "Trained batch 160 batch loss 0.6517272 batch mAP 0.491485596 batch PCKh 0.5\n",
      "Trained batch 161 batch loss 0.668197513 batch mAP 0.461456299 batch PCKh 0.125\n",
      "Trained batch 162 batch loss 0.591889501 batch mAP 0.450500488 batch PCKh 0.625\n",
      "Trained batch 163 batch loss 0.667988896 batch mAP 0.475219727 batch PCKh 0.1875\n",
      "Trained batch 164 batch loss 0.642077863 batch mAP 0.472045898 batch PCKh 0.4375\n",
      "Trained batch 165 batch loss 0.663634717 batch mAP 0.459381104 batch PCKh 0.1875\n",
      "Trained batch 166 batch loss 0.664206684 batch mAP 0.438598633 batch PCKh 0.625\n",
      "Trained batch 167 batch loss 0.705377 batch mAP 0.42300415 batch PCKh 0.375\n",
      "Trained batch 168 batch loss 0.637159824 batch mAP 0.403076172 batch PCKh 0.625\n",
      "Trained batch 169 batch loss 0.628371119 batch mAP 0.420013428 batch PCKh 0.5\n",
      "Trained batch 170 batch loss 0.659441471 batch mAP 0.457885742 batch PCKh 0.5\n",
      "Trained batch 171 batch loss 0.585778415 batch mAP 0.318664551 batch PCKh 0.25\n",
      "Trained batch 172 batch loss 0.631610751 batch mAP 0.407165527 batch PCKh 0.4375\n",
      "Trained batch 173 batch loss 0.564023077 batch mAP 0.361083984 batch PCKh 0.4375\n",
      "Trained batch 174 batch loss 0.515235364 batch mAP 0.443817139 batch PCKh 0.5625\n",
      "Trained batch 175 batch loss 0.535758853 batch mAP 0.473846436 batch PCKh 0.75\n",
      "Trained batch 176 batch loss 0.523920119 batch mAP 0.468292236 batch PCKh 0.8125\n",
      "Trained batch 177 batch loss 0.516150057 batch mAP 0.468170166 batch PCKh 0.5625\n",
      "Trained batch 178 batch loss 0.547377706 batch mAP 0.463623047 batch PCKh 0.5625\n",
      "Trained batch 179 batch loss 0.549292684 batch mAP 0.481628418 batch PCKh 0.375\n",
      "Trained batch 180 batch loss 0.579211593 batch mAP 0.488342285 batch PCKh 0.5\n",
      "Trained batch 181 batch loss 0.574019909 batch mAP 0.496368408 batch PCKh 0.1875\n",
      "Trained batch 182 batch loss 0.616829 batch mAP 0.506073 batch PCKh 0.5\n",
      "Trained batch 183 batch loss 0.596177578 batch mAP 0.461761475 batch PCKh 0.0625\n",
      "Trained batch 184 batch loss 0.569384396 batch mAP 0.574432373 batch PCKh 0.375\n",
      "Trained batch 185 batch loss 0.615146756 batch mAP 0.498596191 batch PCKh 0.625\n",
      "Trained batch 186 batch loss 0.594795048 batch mAP 0.531402588 batch PCKh 0.0625\n",
      "Trained batch 187 batch loss 0.575844169 batch mAP 0.47064209 batch PCKh 0.5625\n",
      "Trained batch 188 batch loss 0.644977212 batch mAP 0.399963379 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 189 batch loss 0.69112432 batch mAP 0.319244385 batch PCKh 0\n",
      "Trained batch 190 batch loss 0.689484715 batch mAP 0.365325928 batch PCKh 0\n",
      "Trained batch 191 batch loss 0.687991738 batch mAP 0.463592529 batch PCKh 0\n",
      "Trained batch 192 batch loss 0.689571083 batch mAP 0.478240967 batch PCKh 0.125\n",
      "Trained batch 193 batch loss 0.636395574 batch mAP 0.345458984 batch PCKh 0\n",
      "Trained batch 194 batch loss 0.691180229 batch mAP 0.406097412 batch PCKh 0.0625\n",
      "Trained batch 195 batch loss 0.563086808 batch mAP 0.452758789 batch PCKh 0.25\n",
      "Trained batch 196 batch loss 0.606789708 batch mAP 0.342224121 batch PCKh 0.3125\n",
      "Trained batch 197 batch loss 0.533518136 batch mAP 0.408752441 batch PCKh 0.3125\n",
      "Trained batch 198 batch loss 0.571147323 batch mAP 0.40612793 batch PCKh 0.625\n",
      "Trained batch 199 batch loss 0.612800598 batch mAP 0.479766846 batch PCKh 0.3125\n",
      "Trained batch 200 batch loss 0.6355533 batch mAP 0.448852539 batch PCKh 0.5625\n",
      "Trained batch 201 batch loss 0.653219819 batch mAP 0.468383789 batch PCKh 0\n",
      "Trained batch 202 batch loss 0.63131541 batch mAP 0.513946533 batch PCKh 0.3125\n",
      "Trained batch 203 batch loss 0.575781465 batch mAP 0.589752197 batch PCKh 0.125\n",
      "Trained batch 204 batch loss 0.606552124 batch mAP 0.589019775 batch PCKh 0.1875\n",
      "Trained batch 205 batch loss 0.528719902 batch mAP 0.507598877 batch PCKh 0.4375\n",
      "Trained batch 206 batch loss 0.54008162 batch mAP 0.552001953 batch PCKh 0.5\n",
      "Trained batch 207 batch loss 0.497379 batch mAP 0.558349609 batch PCKh 0.75\n",
      "Trained batch 208 batch loss 0.590487123 batch mAP 0.525146484 batch PCKh 0.25\n",
      "Trained batch 209 batch loss 0.520148098 batch mAP 0.578918457 batch PCKh 0.4375\n",
      "Trained batch 210 batch loss 0.571976423 batch mAP 0.551757812 batch PCKh 0.375\n",
      "Trained batch 211 batch loss 0.468737453 batch mAP 0.563110352 batch PCKh 0.375\n",
      "Trained batch 212 batch loss 0.566813469 batch mAP 0.575561523 batch PCKh 0.4375\n",
      "Trained batch 213 batch loss 0.567569494 batch mAP 0.567932129 batch PCKh 0.3125\n",
      "Trained batch 214 batch loss 0.628950715 batch mAP 0.586761475 batch PCKh 0.0625\n",
      "Trained batch 215 batch loss 0.541795611 batch mAP 0.580169678 batch PCKh 0.5625\n",
      "Trained batch 216 batch loss 0.529942274 batch mAP 0.609008789 batch PCKh 0.375\n",
      "Trained batch 217 batch loss 0.566246033 batch mAP 0.576599121 batch PCKh 0.5\n",
      "Trained batch 218 batch loss 0.57006228 batch mAP 0.514556885 batch PCKh 0.8125\n",
      "Trained batch 219 batch loss 0.552323878 batch mAP 0.563324 batch PCKh 0.6875\n",
      "Trained batch 220 batch loss 0.548410833 batch mAP 0.515228271 batch PCKh 0.125\n",
      "Trained batch 221 batch loss 0.540280223 batch mAP 0.571044922 batch PCKh 0.4375\n",
      "Trained batch 222 batch loss 0.565189421 batch mAP 0.530822754 batch PCKh 0.3125\n",
      "Trained batch 223 batch loss 0.565796 batch mAP 0.534942627 batch PCKh 0.4375\n",
      "Trained batch 224 batch loss 0.543341577 batch mAP 0.593231201 batch PCKh 0.875\n",
      "Trained batch 225 batch loss 0.629356742 batch mAP 0.542144775 batch PCKh 0.625\n",
      "Trained batch 226 batch loss 0.641816 batch mAP 0.484802246 batch PCKh 0.625\n",
      "Trained batch 227 batch loss 0.604152 batch mAP 0.43927002 batch PCKh 0.375\n",
      "Trained batch 228 batch loss 0.614868045 batch mAP 0.538299561 batch PCKh 0.1875\n",
      "Trained batch 229 batch loss 0.661007106 batch mAP 0.496002197 batch PCKh 0.8125\n",
      "Trained batch 230 batch loss 0.592162788 batch mAP 0.516113281 batch PCKh 0.6875\n",
      "Trained batch 231 batch loss 0.585190594 batch mAP 0.5262146 batch PCKh 0.375\n",
      "Trained batch 232 batch loss 0.616093397 batch mAP 0.499145508 batch PCKh 0.25\n",
      "Trained batch 233 batch loss 0.55324614 batch mAP 0.508148193 batch PCKh 0.6875\n",
      "Trained batch 234 batch loss 0.465102553 batch mAP 0.477722168 batch PCKh 0\n",
      "Trained batch 235 batch loss 0.519567132 batch mAP 0.473449707 batch PCKh 0.3125\n",
      "Trained batch 236 batch loss 0.591803 batch mAP 0.476409912 batch PCKh 0.6875\n",
      "Trained batch 237 batch loss 0.551789 batch mAP 0.439910889 batch PCKh 0.125\n",
      "Trained batch 238 batch loss 0.606659532 batch mAP 0.476654053 batch PCKh 0.75\n",
      "Trained batch 239 batch loss 0.536446393 batch mAP 0.479675293 batch PCKh 0.5625\n",
      "Trained batch 240 batch loss 0.60182631 batch mAP 0.512939453 batch PCKh 0.3125\n",
      "Trained batch 241 batch loss 0.56911248 batch mAP 0.568878174 batch PCKh 0.625\n",
      "Trained batch 242 batch loss 0.596486807 batch mAP 0.479248047 batch PCKh 0\n",
      "Trained batch 243 batch loss 0.704371631 batch mAP 0.485717773 batch PCKh 0.25\n",
      "Trained batch 244 batch loss 0.655199349 batch mAP 0.494384766 batch PCKh 0.1875\n",
      "Trained batch 245 batch loss 0.610834658 batch mAP 0.524017334 batch PCKh 0.4375\n",
      "Trained batch 246 batch loss 0.641218483 batch mAP 0.534942627 batch PCKh 0.625\n",
      "Trained batch 247 batch loss 0.576141477 batch mAP 0.479766846 batch PCKh 0.6875\n",
      "Trained batch 248 batch loss 0.64266032 batch mAP 0.50881958 batch PCKh 0.4375\n",
      "Trained batch 249 batch loss 0.571563542 batch mAP 0.545288086 batch PCKh 0.625\n",
      "Trained batch 250 batch loss 0.614689 batch mAP 0.502868652 batch PCKh 0.4375\n",
      "Trained batch 251 batch loss 0.587921679 batch mAP 0.484405518 batch PCKh 0.4375\n",
      "Trained batch 252 batch loss 0.567435265 batch mAP 0.508270264 batch PCKh 0.0625\n",
      "Trained batch 253 batch loss 0.606612265 batch mAP 0.520202637 batch PCKh 0.4375\n",
      "Trained batch 254 batch loss 0.606381714 batch mAP 0.483215332 batch PCKh 0.6875\n",
      "Trained batch 255 batch loss 0.591062188 batch mAP 0.491943359 batch PCKh 0.5\n",
      "Trained batch 256 batch loss 0.547700644 batch mAP 0.484039307 batch PCKh 0.375\n",
      "Trained batch 257 batch loss 0.641847491 batch mAP 0.531707764 batch PCKh 0.3125\n",
      "Trained batch 258 batch loss 0.571683109 batch mAP 0.535705566 batch PCKh 0.8125\n",
      "Trained batch 259 batch loss 0.641250074 batch mAP 0.514465332 batch PCKh 0.875\n",
      "Trained batch 260 batch loss 0.607947707 batch mAP 0.492675781 batch PCKh 0.625\n",
      "Trained batch 261 batch loss 0.65838182 batch mAP 0.483337402 batch PCKh 0.25\n",
      "Trained batch 262 batch loss 0.659171939 batch mAP 0.492279053 batch PCKh 0.25\n",
      "Trained batch 263 batch loss 0.577189 batch mAP 0.49105835 batch PCKh 0.375\n",
      "Trained batch 264 batch loss 0.539329648 batch mAP 0.47668457 batch PCKh 0.1875\n",
      "Trained batch 265 batch loss 0.596261382 batch mAP 0.458374023 batch PCKh 0.8125\n",
      "Trained batch 266 batch loss 0.596107185 batch mAP 0.504058838 batch PCKh 0.6875\n",
      "Trained batch 267 batch loss 0.582250416 batch mAP 0.516540527 batch PCKh 0.3125\n",
      "Trained batch 268 batch loss 0.65414995 batch mAP 0.437072754 batch PCKh 0.125\n",
      "Trained batch 269 batch loss 0.574064374 batch mAP 0.540100098 batch PCKh 0.5625\n",
      "Trained batch 270 batch loss 0.650620759 batch mAP 0.510620117 batch PCKh 0.125\n",
      "Trained batch 271 batch loss 0.64409554 batch mAP 0.513916 batch PCKh 0.875\n",
      "Trained batch 272 batch loss 0.598290622 batch mAP 0.490631104 batch PCKh 0.6875\n",
      "Trained batch 273 batch loss 0.593435645 batch mAP 0.528595 batch PCKh 0.5625\n",
      "Trained batch 274 batch loss 0.584543467 batch mAP 0.537414551 batch PCKh 0.3125\n",
      "Trained batch 275 batch loss 0.564599872 batch mAP 0.489837646 batch PCKh 0.875\n",
      "Trained batch 276 batch loss 0.563049078 batch mAP 0.460083 batch PCKh 0.1875\n",
      "Trained batch 277 batch loss 0.573738098 batch mAP 0.540863037 batch PCKh 0.5625\n",
      "Trained batch 278 batch loss 0.553836823 batch mAP 0.577697754 batch PCKh 0.375\n",
      "Trained batch 279 batch loss 0.51009351 batch mAP 0.611907959 batch PCKh 0.375\n",
      "Trained batch 280 batch loss 0.606496692 batch mAP 0.62789917 batch PCKh 0.25\n",
      "Trained batch 281 batch loss 0.613130271 batch mAP 0.566711426 batch PCKh 0.4375\n",
      "Trained batch 282 batch loss 0.610750616 batch mAP 0.612854 batch PCKh 0.375\n",
      "Trained batch 283 batch loss 0.688052416 batch mAP 0.557617188 batch PCKh 0.25\n",
      "Trained batch 284 batch loss 0.619088709 batch mAP 0.566833496 batch PCKh 0.4375\n",
      "Trained batch 285 batch loss 0.62509048 batch mAP 0.553161621 batch PCKh 0.1875\n",
      "Trained batch 286 batch loss 0.682253957 batch mAP 0.555847168 batch PCKh 0.125\n",
      "Trained batch 287 batch loss 0.561367691 batch mAP 0.581634521 batch PCKh 0.3125\n",
      "Trained batch 288 batch loss 0.598421276 batch mAP 0.550842285 batch PCKh 0.25\n",
      "Trained batch 289 batch loss 0.602107406 batch mAP 0.562866211 batch PCKh 0.8125\n",
      "Trained batch 290 batch loss 0.639773488 batch mAP 0.557769775 batch PCKh 0.375\n",
      "Trained batch 291 batch loss 0.6504848 batch mAP 0.521392822 batch PCKh 0.25\n",
      "Trained batch 292 batch loss 0.656949401 batch mAP 0.498352051 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 293 batch loss 0.598284721 batch mAP 0.513549805 batch PCKh 0.6875\n",
      "Trained batch 294 batch loss 0.614636064 batch mAP 0.482513428 batch PCKh 0.3125\n",
      "Trained batch 295 batch loss 0.611902773 batch mAP 0.511322 batch PCKh 0.3125\n",
      "Trained batch 296 batch loss 0.53508544 batch mAP 0.547576904 batch PCKh 0.4375\n",
      "Trained batch 297 batch loss 0.592556059 batch mAP 0.546600342 batch PCKh 0.125\n",
      "Trained batch 298 batch loss 0.603043 batch mAP 0.541809082 batch PCKh 0.6875\n",
      "Trained batch 299 batch loss 0.605442524 batch mAP 0.53112793 batch PCKh 0.1875\n",
      "Trained batch 300 batch loss 0.536945283 batch mAP 0.584350586 batch PCKh 0.4375\n",
      "Trained batch 301 batch loss 0.588568747 batch mAP 0.580841064 batch PCKh 0.8125\n",
      "Trained batch 302 batch loss 0.522442698 batch mAP 0.576629639 batch PCKh 0.5625\n",
      "Trained batch 303 batch loss 0.60611546 batch mAP 0.516540527 batch PCKh 0.875\n",
      "Trained batch 304 batch loss 0.648796201 batch mAP 0.517364502 batch PCKh 0.4375\n",
      "Trained batch 305 batch loss 0.63088 batch mAP 0.503631592 batch PCKh 0.75\n",
      "Trained batch 306 batch loss 0.576210856 batch mAP 0.544464111 batch PCKh 0.8125\n",
      "Trained batch 307 batch loss 0.632630765 batch mAP 0.52166748 batch PCKh 0.6875\n",
      "Trained batch 308 batch loss 0.585304797 batch mAP 0.51272583 batch PCKh 0.5\n",
      "Trained batch 309 batch loss 0.63175118 batch mAP 0.467315674 batch PCKh 0.8125\n",
      "Trained batch 310 batch loss 0.543492 batch mAP 0.52142334 batch PCKh 0.5625\n",
      "Trained batch 311 batch loss 0.547988772 batch mAP 0.526306152 batch PCKh 0.625\n",
      "Trained batch 312 batch loss 0.569357395 batch mAP 0.558136 batch PCKh 0.5\n",
      "Trained batch 313 batch loss 0.574819863 batch mAP 0.602172852 batch PCKh 0.375\n",
      "Trained batch 314 batch loss 0.520179 batch mAP 0.539825439 batch PCKh 0.6875\n",
      "Trained batch 315 batch loss 0.485108912 batch mAP 0.611419678 batch PCKh 0.75\n",
      "Trained batch 316 batch loss 0.540819108 batch mAP 0.592285156 batch PCKh 0.75\n",
      "Trained batch 317 batch loss 0.578542471 batch mAP 0.531585693 batch PCKh 0.4375\n",
      "Trained batch 318 batch loss 0.525998473 batch mAP 0.572876 batch PCKh 0.5625\n",
      "Trained batch 319 batch loss 0.512491107 batch mAP 0.592224121 batch PCKh 0.5\n",
      "Trained batch 320 batch loss 0.593738556 batch mAP 0.569915771 batch PCKh 0.4375\n",
      "Trained batch 321 batch loss 0.51970917 batch mAP 0.573577881 batch PCKh 0.3125\n",
      "Trained batch 322 batch loss 0.562337458 batch mAP 0.611663818 batch PCKh 0.75\n",
      "Trained batch 323 batch loss 0.492643327 batch mAP 0.57901 batch PCKh 0.1875\n",
      "Trained batch 324 batch loss 0.553269684 batch mAP 0.595397949 batch PCKh 0.5\n",
      "Trained batch 325 batch loss 0.519072831 batch mAP 0.568054199 batch PCKh 0.625\n",
      "Trained batch 326 batch loss 0.506817 batch mAP 0.583129883 batch PCKh 0.0625\n",
      "Trained batch 327 batch loss 0.540212631 batch mAP 0.583648682 batch PCKh 0.5625\n",
      "Trained batch 328 batch loss 0.474849373 batch mAP 0.612365723 batch PCKh 0.3125\n",
      "Trained batch 329 batch loss 0.582179308 batch mAP 0.546813965 batch PCKh 0.375\n",
      "Trained batch 330 batch loss 0.507027566 batch mAP 0.562561035 batch PCKh 0.375\n",
      "Trained batch 331 batch loss 0.734141946 batch mAP 0.43862915 batch PCKh 0.0625\n",
      "Trained batch 332 batch loss 0.672795 batch mAP 0.532104492 batch PCKh 0.375\n",
      "Trained batch 333 batch loss 0.653885305 batch mAP 0.542999268 batch PCKh 0.625\n",
      "Trained batch 334 batch loss 0.529951751 batch mAP 0.487579346 batch PCKh 0.0625\n",
      "Trained batch 335 batch loss 0.545218647 batch mAP 0.487609863 batch PCKh 0.6875\n",
      "Trained batch 336 batch loss 0.521881342 batch mAP 0.463134766 batch PCKh 0\n",
      "Trained batch 337 batch loss 0.642827928 batch mAP 0.479736328 batch PCKh 0.125\n",
      "Trained batch 338 batch loss 0.578717649 batch mAP 0.460998535 batch PCKh 0.25\n",
      "Trained batch 339 batch loss 0.604275823 batch mAP 0.442596436 batch PCKh 0.375\n",
      "Trained batch 340 batch loss 0.562790155 batch mAP 0.43862915 batch PCKh 0.125\n",
      "Trained batch 341 batch loss 0.502485275 batch mAP 0.444854736 batch PCKh 0.5\n",
      "Trained batch 342 batch loss 0.576446235 batch mAP 0.423095703 batch PCKh 0.1875\n",
      "Trained batch 343 batch loss 0.5979774 batch mAP 0.43548584 batch PCKh 0.4375\n",
      "Trained batch 344 batch loss 0.624400735 batch mAP 0.484741211 batch PCKh 0.0625\n",
      "Trained batch 345 batch loss 0.579764962 batch mAP 0.48739624 batch PCKh 0.625\n",
      "Trained batch 346 batch loss 0.548488855 batch mAP 0.478729248 batch PCKh 0.625\n",
      "Trained batch 347 batch loss 0.692361116 batch mAP 0.428741455 batch PCKh 0.125\n",
      "Trained batch 348 batch loss 0.64198184 batch mAP 0.431274414 batch PCKh 0.5625\n",
      "Trained batch 349 batch loss 0.594454944 batch mAP 0.432830811 batch PCKh 0\n",
      "Trained batch 350 batch loss 0.643137634 batch mAP 0.435760498 batch PCKh 0.75\n",
      "Trained batch 351 batch loss 0.5044505 batch mAP 0.412139893 batch PCKh 0.625\n",
      "Trained batch 352 batch loss 0.659916043 batch mAP 0.447174072 batch PCKh 0\n",
      "Trained batch 353 batch loss 0.636706948 batch mAP 0.44631958 batch PCKh 0.3125\n",
      "Trained batch 354 batch loss 0.609990239 batch mAP 0.426940918 batch PCKh 0.4375\n",
      "Trained batch 355 batch loss 0.603168905 batch mAP 0.476531982 batch PCKh 0.125\n",
      "Trained batch 356 batch loss 0.651051879 batch mAP 0.436737061 batch PCKh 0.3125\n",
      "Trained batch 357 batch loss 0.646212399 batch mAP 0.430847168 batch PCKh 0.25\n",
      "Trained batch 358 batch loss 0.657387614 batch mAP 0.459136963 batch PCKh 0.5625\n",
      "Trained batch 359 batch loss 0.598853827 batch mAP 0.390380859 batch PCKh 0.625\n",
      "Trained batch 360 batch loss 0.640852332 batch mAP 0.385620117 batch PCKh 0.6875\n",
      "Trained batch 361 batch loss 0.499368697 batch mAP 0.494018555 batch PCKh 0.1875\n",
      "Trained batch 362 batch loss 0.531279624 batch mAP 0.483734131 batch PCKh 0\n",
      "Trained batch 363 batch loss 0.560897291 batch mAP 0.441345215 batch PCKh 0.625\n",
      "Trained batch 364 batch loss 0.533494473 batch mAP 0.470275879 batch PCKh 0.5625\n",
      "Trained batch 365 batch loss 0.567897916 batch mAP 0.466796875 batch PCKh 0.375\n",
      "Trained batch 366 batch loss 0.515365779 batch mAP 0.507324219 batch PCKh 0.625\n",
      "Trained batch 367 batch loss 0.496216685 batch mAP 0.490600586 batch PCKh 0.6875\n",
      "Trained batch 368 batch loss 0.5754776 batch mAP 0.501190186 batch PCKh 0.75\n",
      "Trained batch 369 batch loss 0.577284217 batch mAP 0.51953125 batch PCKh 0.6875\n",
      "Trained batch 370 batch loss 0.578018785 batch mAP 0.487670898 batch PCKh 0.75\n",
      "Trained batch 371 batch loss 0.582866073 batch mAP 0.496582031 batch PCKh 0.6875\n",
      "Trained batch 372 batch loss 0.538940847 batch mAP 0.498352051 batch PCKh 0.75\n",
      "Trained batch 373 batch loss 0.596350193 batch mAP 0.542053223 batch PCKh 0.5625\n",
      "Trained batch 374 batch loss 0.625695825 batch mAP 0.55380249 batch PCKh 0.75\n",
      "Trained batch 375 batch loss 0.513142765 batch mAP 0.524475098 batch PCKh 0.25\n",
      "Trained batch 376 batch loss 0.557987034 batch mAP 0.570953369 batch PCKh 0.5625\n",
      "Trained batch 377 batch loss 0.581958115 batch mAP 0.532959 batch PCKh 0.75\n",
      "Trained batch 378 batch loss 0.635089815 batch mAP 0.47756958 batch PCKh 0.4375\n",
      "Trained batch 379 batch loss 0.545950294 batch mAP 0.57522583 batch PCKh 0.75\n",
      "Trained batch 380 batch loss 0.533401966 batch mAP 0.526977539 batch PCKh 0.8125\n",
      "Trained batch 381 batch loss 0.528702915 batch mAP 0.578033447 batch PCKh 0.4375\n",
      "Trained batch 382 batch loss 0.54293257 batch mAP 0.512908936 batch PCKh 0.25\n",
      "Trained batch 383 batch loss 0.605197072 batch mAP 0.513977051 batch PCKh 0.3125\n",
      "Trained batch 384 batch loss 0.62023133 batch mAP 0.529266357 batch PCKh 0.4375\n",
      "Trained batch 385 batch loss 0.62973243 batch mAP 0.521820068 batch PCKh 0.625\n",
      "Trained batch 386 batch loss 0.64259696 batch mAP 0.476623535 batch PCKh 0.375\n",
      "Trained batch 387 batch loss 0.617408931 batch mAP 0.506225586 batch PCKh 0.3125\n",
      "Trained batch 388 batch loss 0.605876207 batch mAP 0.515014648 batch PCKh 0.5625\n",
      "Trained batch 389 batch loss 0.573497474 batch mAP 0.510467529 batch PCKh 0.5625\n",
      "Trained batch 390 batch loss 0.592235923 batch mAP 0.479675293 batch PCKh 0.375\n",
      "Trained batch 391 batch loss 0.605308533 batch mAP 0.525390625 batch PCKh 0.4375\n",
      "Trained batch 392 batch loss 0.562866569 batch mAP 0.536651611 batch PCKh 0.25\n",
      "Trained batch 393 batch loss 0.569500208 batch mAP 0.548461914 batch PCKh 0.0625\n",
      "Trained batch 394 batch loss 0.591484725 batch mAP 0.542999268 batch PCKh 0.625\n",
      "Trained batch 395 batch loss 0.60129714 batch mAP 0.546661377 batch PCKh 0.625\n",
      "Trained batch 396 batch loss 0.670364738 batch mAP 0.549743652 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 397 batch loss 0.709643602 batch mAP 0.498809814 batch PCKh 0.8125\n",
      "Trained batch 398 batch loss 0.616540074 batch mAP 0.5390625 batch PCKh 0\n",
      "Trained batch 399 batch loss 0.576680899 batch mAP 0.501831055 batch PCKh 0.875\n",
      "Trained batch 400 batch loss 0.589506626 batch mAP 0.539703369 batch PCKh 0.6875\n",
      "Trained batch 401 batch loss 0.5279724 batch mAP 0.440917969 batch PCKh 0.3125\n",
      "Trained batch 402 batch loss 0.657023668 batch mAP 0.501068115 batch PCKh 0.5625\n",
      "Trained batch 403 batch loss 0.644359112 batch mAP 0.460418701 batch PCKh 0.8125\n",
      "Trained batch 404 batch loss 0.595677614 batch mAP 0.538391113 batch PCKh 0.4375\n",
      "Trained batch 405 batch loss 0.656903923 batch mAP 0.500793457 batch PCKh 0.875\n",
      "Trained batch 406 batch loss 0.637077928 batch mAP 0.494689941 batch PCKh 0.6875\n",
      "Trained batch 407 batch loss 0.604646087 batch mAP 0.505493164 batch PCKh 0.625\n",
      "Trained batch 408 batch loss 0.627885878 batch mAP 0.512634277 batch PCKh 0.75\n",
      "Trained batch 409 batch loss 0.584040761 batch mAP 0.482940674 batch PCKh 0.25\n",
      "Trained batch 410 batch loss 0.619232059 batch mAP 0.475219727 batch PCKh 0.5\n",
      "Trained batch 411 batch loss 0.566004 batch mAP 0.521240234 batch PCKh 0.3125\n",
      "Trained batch 412 batch loss 0.632482827 batch mAP 0.510986328 batch PCKh 0.25\n",
      "Trained batch 413 batch loss 0.645631194 batch mAP 0.495758057 batch PCKh 0.8125\n",
      "Trained batch 414 batch loss 0.658576906 batch mAP 0.509307861 batch PCKh 0.25\n",
      "Trained batch 415 batch loss 0.663298488 batch mAP 0.533447266 batch PCKh 0.6875\n",
      "Trained batch 416 batch loss 0.596304119 batch mAP 0.532012939 batch PCKh 0.625\n",
      "Trained batch 417 batch loss 0.683646798 batch mAP 0.507659912 batch PCKh 0.125\n",
      "Trained batch 418 batch loss 0.585165799 batch mAP 0.507598877 batch PCKh 0.625\n",
      "Trained batch 419 batch loss 0.589338124 batch mAP 0.533905 batch PCKh 0.75\n",
      "Trained batch 420 batch loss 0.620615542 batch mAP 0.526977539 batch PCKh 0.25\n",
      "Trained batch 421 batch loss 0.597379208 batch mAP 0.553955078 batch PCKh 0.5625\n",
      "Trained batch 422 batch loss 0.468440682 batch mAP 0.585968 batch PCKh 0.1875\n",
      "Trained batch 423 batch loss 0.465873599 batch mAP 0.58984375 batch PCKh 0.1875\n",
      "Trained batch 424 batch loss 0.476681 batch mAP 0.582428 batch PCKh 0.25\n",
      "Trained batch 425 batch loss 0.566657245 batch mAP 0.585174561 batch PCKh 0.25\n",
      "Trained batch 426 batch loss 0.570342898 batch mAP 0.612640381 batch PCKh 0.3125\n",
      "Trained batch 427 batch loss 0.5337798 batch mAP 0.634521484 batch PCKh 0.1875\n",
      "Trained batch 428 batch loss 0.577389657 batch mAP 0.54385376 batch PCKh 0.4375\n",
      "Trained batch 429 batch loss 0.56221652 batch mAP 0.54385376 batch PCKh 0.5625\n",
      "Trained batch 430 batch loss 0.616885304 batch mAP 0.350036621 batch PCKh 0.0625\n",
      "Trained batch 431 batch loss 0.676518917 batch mAP 0.362030029 batch PCKh 0.1875\n",
      "Trained batch 432 batch loss 0.661812 batch mAP 0.28805542 batch PCKh 0\n",
      "Trained batch 433 batch loss 0.627639294 batch mAP 0.49899292 batch PCKh 0.4375\n",
      "Trained batch 434 batch loss 0.671600699 batch mAP 0.545684814 batch PCKh 0.125\n",
      "Trained batch 435 batch loss 0.60489428 batch mAP 0.524719238 batch PCKh 0.6875\n",
      "Trained batch 436 batch loss 0.54946512 batch mAP 0.567504883 batch PCKh 0.6875\n",
      "Trained batch 437 batch loss 0.593203604 batch mAP 0.489501953 batch PCKh 0.5\n",
      "Trained batch 438 batch loss 0.563874602 batch mAP 0.512359619 batch PCKh 0.5\n",
      "Trained batch 439 batch loss 0.644705415 batch mAP 0.444396973 batch PCKh 0.25\n",
      "Trained batch 440 batch loss 0.698636532 batch mAP 0.442382812 batch PCKh 0.0625\n",
      "Trained batch 441 batch loss 0.599007666 batch mAP 0.410064697 batch PCKh 0.375\n",
      "Trained batch 442 batch loss 0.544651508 batch mAP 0.403167725 batch PCKh 0.3125\n",
      "Trained batch 443 batch loss 0.538726568 batch mAP 0.362518311 batch PCKh 0.3125\n",
      "Trained batch 444 batch loss 0.587399244 batch mAP 0.473052979 batch PCKh 0.25\n",
      "Trained batch 445 batch loss 0.567562163 batch mAP 0.516143799 batch PCKh 0.1875\n",
      "Trained batch 446 batch loss 0.602142632 batch mAP 0.550476074 batch PCKh 0.5625\n",
      "Trained batch 447 batch loss 0.610750794 batch mAP 0.567962646 batch PCKh 0.5\n",
      "Trained batch 448 batch loss 0.58275 batch mAP 0.507720947 batch PCKh 0.25\n",
      "Trained batch 449 batch loss 0.637244523 batch mAP 0.517547607 batch PCKh 0.6875\n",
      "Trained batch 450 batch loss 0.600891113 batch mAP 0.534179688 batch PCKh 0.75\n",
      "Trained batch 451 batch loss 0.546719074 batch mAP 0.539855957 batch PCKh 0.625\n",
      "Trained batch 452 batch loss 0.653285 batch mAP 0.483184814 batch PCKh 0.5625\n",
      "Trained batch 453 batch loss 0.554225862 batch mAP 0.534790039 batch PCKh 0.125\n",
      "Trained batch 454 batch loss 0.610281885 batch mAP 0.514221191 batch PCKh 0.625\n",
      "Trained batch 455 batch loss 0.58371675 batch mAP 0.545776367 batch PCKh 0.625\n",
      "Trained batch 456 batch loss 0.658735 batch mAP 0.509796143 batch PCKh 0\n",
      "Trained batch 457 batch loss 0.538384795 batch mAP 0.584899902 batch PCKh 0.375\n",
      "Trained batch 458 batch loss 0.4946374 batch mAP 0.513549805 batch PCKh 0.125\n",
      "Trained batch 459 batch loss 0.669963121 batch mAP 0.426269531 batch PCKh 0.3125\n",
      "Trained batch 460 batch loss 0.57780695 batch mAP 0.470672607 batch PCKh 0.25\n",
      "Trained batch 461 batch loss 0.63034904 batch mAP 0.494018555 batch PCKh 0.625\n",
      "Trained batch 462 batch loss 0.557534397 batch mAP 0.485198975 batch PCKh 0.4375\n",
      "Trained batch 463 batch loss 0.532783628 batch mAP 0.488586426 batch PCKh 0.625\n",
      "Trained batch 464 batch loss 0.572190166 batch mAP 0.506317139 batch PCKh 0.375\n",
      "Trained batch 465 batch loss 0.633376777 batch mAP 0.476531982 batch PCKh 0.25\n",
      "Trained batch 466 batch loss 0.622212768 batch mAP 0.53012085 batch PCKh 0.0625\n",
      "Trained batch 467 batch loss 0.707498908 batch mAP 0.513641357 batch PCKh 0.8125\n",
      "Trained batch 468 batch loss 0.654847 batch mAP 0.485351562 batch PCKh 0.625\n",
      "Trained batch 469 batch loss 0.538283169 batch mAP 0.530700684 batch PCKh 0.1875\n",
      "Trained batch 470 batch loss 0.520344138 batch mAP 0.487457275 batch PCKh 0.1875\n",
      "Trained batch 471 batch loss 0.617351294 batch mAP 0.487487793 batch PCKh 0.5\n",
      "Trained batch 472 batch loss 0.660508394 batch mAP 0.43637085 batch PCKh 0.25\n",
      "Trained batch 473 batch loss 0.59747 batch mAP 0.472717285 batch PCKh 0.1875\n",
      "Trained batch 474 batch loss 0.545599699 batch mAP 0.409942627 batch PCKh 0.1875\n",
      "Trained batch 475 batch loss 0.593815744 batch mAP 0.46762085 batch PCKh 0.625\n",
      "Trained batch 476 batch loss 0.624657869 batch mAP 0.503936768 batch PCKh 0.6875\n",
      "Trained batch 477 batch loss 0.620831 batch mAP 0.528961182 batch PCKh 0.125\n",
      "Trained batch 478 batch loss 0.590241551 batch mAP 0.511322 batch PCKh 0.5625\n",
      "Trained batch 479 batch loss 0.569848716 batch mAP 0.505218506 batch PCKh 0.4375\n",
      "Trained batch 480 batch loss 0.572374046 batch mAP 0.48336792 batch PCKh 0.1875\n",
      "Trained batch 481 batch loss 0.602326095 batch mAP 0.504089355 batch PCKh 0.6875\n",
      "Trained batch 482 batch loss 0.516680121 batch mAP 0.532470703 batch PCKh 0.75\n",
      "Trained batch 483 batch loss 0.569963276 batch mAP 0.542053223 batch PCKh 0.3125\n",
      "Trained batch 484 batch loss 0.579674482 batch mAP 0.531311035 batch PCKh 0.125\n",
      "Trained batch 485 batch loss 0.655560195 batch mAP 0.5078125 batch PCKh 0.375\n",
      "Trained batch 486 batch loss 0.584579468 batch mAP 0.502685547 batch PCKh 0.5625\n",
      "Trained batch 487 batch loss 0.666522205 batch mAP 0.453552246 batch PCKh 0.6875\n",
      "Trained batch 488 batch loss 0.565506 batch mAP 0.478820801 batch PCKh 0.625\n",
      "Trained batch 489 batch loss 0.590425372 batch mAP 0.483520508 batch PCKh 0.4375\n",
      "Trained batch 490 batch loss 0.486749113 batch mAP 0.493347168 batch PCKh 0\n",
      "Trained batch 491 batch loss 0.541481 batch mAP 0.492340088 batch PCKh 0.75\n",
      "Trained batch 492 batch loss 0.528782189 batch mAP 0.465820312 batch PCKh 0.6875\n",
      "Trained batch 493 batch loss 0.637745082 batch mAP 0.521240234 batch PCKh 0.3125\n",
      "Trained batch 494 batch loss 0.609518468 batch mAP 0.535522461 batch PCKh 0\n",
      "Trained batch 495 batch loss 0.568845034 batch mAP 0.534301758 batch PCKh 0.625\n",
      "Trained batch 496 batch loss 0.596049368 batch mAP 0.489501953 batch PCKh 0.625\n",
      "Trained batch 497 batch loss 0.670706749 batch mAP 0.448242188 batch PCKh 0.625\n",
      "Trained batch 498 batch loss 0.659509897 batch mAP 0.501983643 batch PCKh 0.875\n",
      "Trained batch 499 batch loss 0.643443882 batch mAP 0.42074585 batch PCKh 0.5\n",
      "Trained batch 500 batch loss 0.55383569 batch mAP 0.560668945 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 501 batch loss 0.515839577 batch mAP 0.544555664 batch PCKh 0.5625\n",
      "Trained batch 502 batch loss 0.503484845 batch mAP 0.501525879 batch PCKh 0.5\n",
      "Trained batch 503 batch loss 0.571670473 batch mAP 0.432403564 batch PCKh 0.4375\n",
      "Trained batch 504 batch loss 0.559280753 batch mAP 0.512176514 batch PCKh 0.1875\n",
      "Trained batch 505 batch loss 0.566988349 batch mAP 0.507049561 batch PCKh 0.3125\n",
      "Trained batch 506 batch loss 0.615074813 batch mAP 0.515319824 batch PCKh 0.1875\n",
      "Trained batch 507 batch loss 0.663367033 batch mAP 0.492462158 batch PCKh 0.5625\n",
      "Trained batch 508 batch loss 0.580183387 batch mAP 0.47454834 batch PCKh 0.375\n",
      "Trained batch 509 batch loss 0.575735927 batch mAP 0.516449 batch PCKh 0.375\n",
      "Trained batch 510 batch loss 0.603690267 batch mAP 0.489746094 batch PCKh 0.625\n",
      "Trained batch 511 batch loss 0.63005209 batch mAP 0.475952148 batch PCKh 0.125\n",
      "Trained batch 512 batch loss 0.623028696 batch mAP 0.505340576 batch PCKh 0.25\n",
      "Trained batch 513 batch loss 0.511074424 batch mAP 0.531341553 batch PCKh 0.5625\n",
      "Trained batch 514 batch loss 0.508477092 batch mAP 0.523834229 batch PCKh 0.5\n",
      "Trained batch 515 batch loss 0.552006662 batch mAP 0.452087402 batch PCKh 0.6875\n",
      "Trained batch 516 batch loss 0.498456061 batch mAP 0.444824219 batch PCKh 0.6875\n",
      "Trained batch 517 batch loss 0.581319 batch mAP 0.496154785 batch PCKh 0.3125\n",
      "Trained batch 518 batch loss 0.605829358 batch mAP 0.561004639 batch PCKh 0.4375\n",
      "Trained batch 519 batch loss 0.551512897 batch mAP 0.560791 batch PCKh 0.4375\n",
      "Trained batch 520 batch loss 0.474256724 batch mAP 0.50894165 batch PCKh 0.125\n",
      "Trained batch 521 batch loss 0.561155796 batch mAP 0.516723633 batch PCKh 0.4375\n",
      "Trained batch 522 batch loss 0.520103335 batch mAP 0.52532959 batch PCKh 0.5625\n",
      "Trained batch 523 batch loss 0.581514478 batch mAP 0.503448486 batch PCKh 0.75\n",
      "Trained batch 524 batch loss 0.531305254 batch mAP 0.537597656 batch PCKh 0.625\n",
      "Trained batch 525 batch loss 0.609861553 batch mAP 0.495880127 batch PCKh 0.5\n",
      "Trained batch 526 batch loss 0.626918793 batch mAP 0.445983887 batch PCKh 0.25\n",
      "Trained batch 527 batch loss 0.592368722 batch mAP 0.400543213 batch PCKh 0.1875\n",
      "Trained batch 528 batch loss 0.568828762 batch mAP 0.42175293 batch PCKh 0.3125\n",
      "Trained batch 529 batch loss 0.635489583 batch mAP 0.466918945 batch PCKh 0.25\n",
      "Trained batch 530 batch loss 0.491487235 batch mAP 0.412323 batch PCKh 0.1875\n",
      "Trained batch 531 batch loss 0.620494604 batch mAP 0.448791504 batch PCKh 0.375\n",
      "Trained batch 532 batch loss 0.637440264 batch mAP 0.417144775 batch PCKh 0.75\n",
      "Trained batch 533 batch loss 0.574891388 batch mAP 0.466522217 batch PCKh 0.5\n",
      "Trained batch 534 batch loss 0.546571434 batch mAP 0.39831543 batch PCKh 0.625\n",
      "Trained batch 535 batch loss 0.542045891 batch mAP 0.384277344 batch PCKh 0.5\n",
      "Trained batch 536 batch loss 0.618781447 batch mAP 0.357452393 batch PCKh 0.375\n",
      "Trained batch 537 batch loss 0.556376159 batch mAP 0.289123535 batch PCKh 0.375\n",
      "Trained batch 538 batch loss 0.62563169 batch mAP 0.288879395 batch PCKh 0.5625\n",
      "Trained batch 539 batch loss 0.642011166 batch mAP 0.245697021 batch PCKh 0.625\n",
      "Trained batch 540 batch loss 0.623699248 batch mAP 0.304534912 batch PCKh 0.375\n",
      "Trained batch 541 batch loss 0.587123275 batch mAP 0.178070068 batch PCKh 0.1875\n",
      "Trained batch 542 batch loss 0.526837826 batch mAP 0.258911133 batch PCKh 0.625\n",
      "Trained batch 543 batch loss 0.556221902 batch mAP 0.222564697 batch PCKh 0.5625\n",
      "Trained batch 544 batch loss 0.522679925 batch mAP 0.326507568 batch PCKh 0.625\n",
      "Trained batch 545 batch loss 0.523085892 batch mAP 0.428771973 batch PCKh 0.5\n",
      "Trained batch 546 batch loss 0.557499051 batch mAP 0.453796387 batch PCKh 0.5\n",
      "Trained batch 547 batch loss 0.576848686 batch mAP 0.522399902 batch PCKh 0.1875\n",
      "Trained batch 548 batch loss 0.596956074 batch mAP 0.477783203 batch PCKh 0.4375\n",
      "Trained batch 549 batch loss 0.67789048 batch mAP 0.395935059 batch PCKh 0.3125\n",
      "Trained batch 550 batch loss 0.59079951 batch mAP 0.411071777 batch PCKh 0.3125\n",
      "Trained batch 551 batch loss 0.629102707 batch mAP 0.472473145 batch PCKh 0.6875\n",
      "Trained batch 552 batch loss 0.607922792 batch mAP 0.331329346 batch PCKh 0.6875\n",
      "Trained batch 553 batch loss 0.558799148 batch mAP 0.270629883 batch PCKh 0.6875\n",
      "Trained batch 554 batch loss 0.58317548 batch mAP 0.44708252 batch PCKh 0.875\n",
      "Trained batch 555 batch loss 0.568715 batch mAP 0.450439453 batch PCKh 0.8125\n",
      "Trained batch 556 batch loss 0.47214967 batch mAP 0.566741943 batch PCKh 0.75\n",
      "Trained batch 557 batch loss 0.504096925 batch mAP 0.556915283 batch PCKh 0.375\n",
      "Trained batch 558 batch loss 0.498280406 batch mAP 0.509552 batch PCKh 0.6875\n",
      "Trained batch 559 batch loss 0.465430051 batch mAP 0.529968262 batch PCKh 0.875\n",
      "Trained batch 560 batch loss 0.461787343 batch mAP 0.55480957 batch PCKh 0.6875\n",
      "Trained batch 561 batch loss 0.457168072 batch mAP 0.584747314 batch PCKh 0.75\n",
      "Trained batch 562 batch loss 0.493876278 batch mAP 0.546783447 batch PCKh 0.5625\n",
      "Trained batch 563 batch loss 0.565241933 batch mAP 0.523590088 batch PCKh 0.75\n",
      "Trained batch 564 batch loss 0.623209953 batch mAP 0.497741699 batch PCKh 0.375\n",
      "Trained batch 565 batch loss 0.635948062 batch mAP 0.480804443 batch PCKh 0.5625\n",
      "Trained batch 566 batch loss 0.697283864 batch mAP 0.456085205 batch PCKh 0.3125\n",
      "Trained batch 567 batch loss 0.682495654 batch mAP 0.460632324 batch PCKh 0.375\n",
      "Trained batch 568 batch loss 0.583745718 batch mAP 0.493652344 batch PCKh 0.5625\n",
      "Trained batch 569 batch loss 0.667033 batch mAP 0.443267822 batch PCKh 0.875\n",
      "Trained batch 570 batch loss 0.665711045 batch mAP 0.377838135 batch PCKh 0.4375\n",
      "Trained batch 571 batch loss 0.57670784 batch mAP 0.473876953 batch PCKh 0.8125\n",
      "Trained batch 572 batch loss 0.633699298 batch mAP 0.439147949 batch PCKh 0.625\n",
      "Trained batch 573 batch loss 0.591867566 batch mAP 0.454406738 batch PCKh 0.5\n",
      "Trained batch 574 batch loss 0.589979827 batch mAP 0.421875 batch PCKh 0.8125\n",
      "Trained batch 575 batch loss 0.707963943 batch mAP 0.45224 batch PCKh 0.125\n",
      "Trained batch 576 batch loss 0.609681726 batch mAP 0.478607178 batch PCKh 0.6875\n",
      "Trained batch 577 batch loss 0.603080511 batch mAP 0.492156982 batch PCKh 0.375\n",
      "Trained batch 578 batch loss 0.507765174 batch mAP 0.488220215 batch PCKh 0.5\n",
      "Trained batch 579 batch loss 0.563385785 batch mAP 0.507049561 batch PCKh 0.25\n",
      "Trained batch 580 batch loss 0.614525199 batch mAP 0.486450195 batch PCKh 0.25\n",
      "Trained batch 581 batch loss 0.50647378 batch mAP 0.475402832 batch PCKh 0.1875\n",
      "Trained batch 582 batch loss 0.529526949 batch mAP 0.468536377 batch PCKh 0\n",
      "Trained batch 583 batch loss 0.472560883 batch mAP 0.489013672 batch PCKh 0.5625\n",
      "Trained batch 584 batch loss 0.535228729 batch mAP 0.471893311 batch PCKh 0.5625\n",
      "Trained batch 585 batch loss 0.528692305 batch mAP 0.503295898 batch PCKh 0.75\n",
      "Trained batch 586 batch loss 0.497601926 batch mAP 0.507843 batch PCKh 0.6875\n",
      "Trained batch 587 batch loss 0.57027638 batch mAP 0.483551025 batch PCKh 0.5\n",
      "Trained batch 588 batch loss 0.557987 batch mAP 0.481048584 batch PCKh 0.875\n",
      "Trained batch 589 batch loss 0.531942248 batch mAP 0.490570068 batch PCKh 0.5625\n",
      "Trained batch 590 batch loss 0.503044128 batch mAP 0.484985352 batch PCKh 0.5\n",
      "Trained batch 591 batch loss 0.415493101 batch mAP 0.538909912 batch PCKh 0.5625\n",
      "Trained batch 592 batch loss 0.390719682 batch mAP 0.557128906 batch PCKh 0.5\n",
      "Trained batch 593 batch loss 0.45152092 batch mAP 0.532012939 batch PCKh 0.5\n",
      "Trained batch 594 batch loss 0.454089761 batch mAP 0.491607666 batch PCKh 0\n",
      "Trained batch 595 batch loss 0.423992097 batch mAP 0.561462402 batch PCKh 0\n",
      "Trained batch 596 batch loss 0.394114226 batch mAP 0.581176758 batch PCKh 0.3125\n",
      "Trained batch 597 batch loss 0.487024695 batch mAP 0.547729492 batch PCKh 0.1875\n",
      "Trained batch 598 batch loss 0.564020097 batch mAP 0.514678955 batch PCKh 0.625\n",
      "Trained batch 599 batch loss 0.619339585 batch mAP 0.50604248 batch PCKh 0.625\n",
      "Trained batch 600 batch loss 0.648884535 batch mAP 0.52532959 batch PCKh 0.3125\n",
      "Trained batch 601 batch loss 0.535862 batch mAP 0.621887207 batch PCKh 0.5\n",
      "Trained batch 602 batch loss 0.621831596 batch mAP 0.569000244 batch PCKh 0.3125\n",
      "Trained batch 603 batch loss 0.566426039 batch mAP 0.552856445 batch PCKh 0.125\n",
      "Trained batch 604 batch loss 0.572137773 batch mAP 0.508880615 batch PCKh 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 605 batch loss 0.536949396 batch mAP 0.545135498 batch PCKh 0.4375\n",
      "Trained batch 606 batch loss 0.594347537 batch mAP 0.582305908 batch PCKh 0.375\n",
      "Trained batch 607 batch loss 0.546902478 batch mAP 0.55758667 batch PCKh 0.5\n",
      "Trained batch 608 batch loss 0.634809494 batch mAP 0.521728516 batch PCKh 0.25\n",
      "Trained batch 609 batch loss 0.582934558 batch mAP 0.512115479 batch PCKh 0.3125\n",
      "Trained batch 610 batch loss 0.600002885 batch mAP 0.529724121 batch PCKh 0.5625\n",
      "Trained batch 611 batch loss 0.599695921 batch mAP 0.541229248 batch PCKh 0.75\n",
      "Trained batch 612 batch loss 0.61096555 batch mAP 0.541595459 batch PCKh 0.5625\n",
      "Trained batch 613 batch loss 0.581142724 batch mAP 0.500640869 batch PCKh 0.5\n",
      "Trained batch 614 batch loss 0.498769224 batch mAP 0.523193359 batch PCKh 0.3125\n",
      "Trained batch 615 batch loss 0.587330401 batch mAP 0.532714844 batch PCKh 0.3125\n",
      "Trained batch 616 batch loss 0.464300126 batch mAP 0.565490723 batch PCKh 0.25\n",
      "Trained batch 617 batch loss 0.462823629 batch mAP 0.601043701 batch PCKh 0.75\n",
      "Trained batch 618 batch loss 0.570982337 batch mAP 0.560943604 batch PCKh 0.75\n",
      "Trained batch 619 batch loss 0.659474611 batch mAP 0.543487549 batch PCKh 0.625\n",
      "Trained batch 620 batch loss 0.555256665 batch mAP 0.574371338 batch PCKh 0.625\n",
      "Trained batch 621 batch loss 0.613580108 batch mAP 0.487091064 batch PCKh 0.375\n",
      "Trained batch 622 batch loss 0.620131373 batch mAP 0.516357422 batch PCKh 0.375\n",
      "Trained batch 623 batch loss 0.648144722 batch mAP 0.568786621 batch PCKh 0.75\n",
      "Trained batch 624 batch loss 0.60224247 batch mAP 0.51940918 batch PCKh 0.8125\n",
      "Trained batch 625 batch loss 0.568317354 batch mAP 0.553710938 batch PCKh 0.375\n",
      "Trained batch 626 batch loss 0.625405669 batch mAP 0.543670654 batch PCKh 0.25\n",
      "Trained batch 627 batch loss 0.597102046 batch mAP 0.531982422 batch PCKh 0.625\n",
      "Trained batch 628 batch loss 0.520523071 batch mAP 0.490875244 batch PCKh 0.5625\n",
      "Trained batch 629 batch loss 0.588641405 batch mAP 0.520904541 batch PCKh 0.6875\n",
      "Trained batch 630 batch loss 0.616985679 batch mAP 0.515136719 batch PCKh 0.1875\n",
      "Trained batch 631 batch loss 0.685648263 batch mAP 0.457214355 batch PCKh 0.625\n",
      "Trained batch 632 batch loss 0.625850737 batch mAP 0.513000488 batch PCKh 0.625\n",
      "Trained batch 633 batch loss 0.62500304 batch mAP 0.524841309 batch PCKh 0.5625\n",
      "Trained batch 634 batch loss 0.641088426 batch mAP 0.522827148 batch PCKh 0.5\n",
      "Trained batch 635 batch loss 0.578485966 batch mAP 0.522918701 batch PCKh 0.5\n",
      "Trained batch 636 batch loss 0.597757399 batch mAP 0.546722412 batch PCKh 0.5625\n",
      "Trained batch 637 batch loss 0.600170255 batch mAP 0.486694336 batch PCKh 0.4375\n",
      "Trained batch 638 batch loss 0.673564374 batch mAP 0.467865 batch PCKh 0.25\n",
      "Trained batch 639 batch loss 0.580187678 batch mAP 0.501281738 batch PCKh 0.5625\n",
      "Trained batch 640 batch loss 0.542872906 batch mAP 0.489471436 batch PCKh 0.3125\n",
      "Trained batch 641 batch loss 0.583197713 batch mAP 0.484527588 batch PCKh 0.0625\n",
      "Trained batch 642 batch loss 0.529502034 batch mAP 0.495025635 batch PCKh 0.875\n",
      "Trained batch 643 batch loss 0.546963 batch mAP 0.474853516 batch PCKh 0.25\n",
      "Trained batch 644 batch loss 0.542448521 batch mAP 0.520477295 batch PCKh 0.5625\n",
      "Trained batch 645 batch loss 0.653486133 batch mAP 0.496826172 batch PCKh 0.6875\n",
      "Trained batch 646 batch loss 0.618813038 batch mAP 0.506256104 batch PCKh 0.75\n",
      "Trained batch 647 batch loss 0.637287319 batch mAP 0.42565918 batch PCKh 0.1875\n",
      "Trained batch 648 batch loss 0.584516346 batch mAP 0.499786377 batch PCKh 0.5625\n",
      "Trained batch 649 batch loss 0.556435347 batch mAP 0.523468 batch PCKh 0.4375\n",
      "Trained batch 650 batch loss 0.574454784 batch mAP 0.490722656 batch PCKh 0.5625\n",
      "Trained batch 651 batch loss 0.561760068 batch mAP 0.507476807 batch PCKh 0.5\n",
      "Trained batch 652 batch loss 0.539722204 batch mAP 0.458007812 batch PCKh 0.1875\n",
      "Trained batch 653 batch loss 0.585382223 batch mAP 0.515533447 batch PCKh 0.5625\n",
      "Trained batch 654 batch loss 0.653186202 batch mAP 0.504699707 batch PCKh 0.75\n",
      "Trained batch 655 batch loss 0.633702397 batch mAP 0.553344727 batch PCKh 0.5625\n",
      "Trained batch 656 batch loss 0.687875152 batch mAP 0.536102295 batch PCKh 0\n",
      "Trained batch 657 batch loss 0.604238927 batch mAP 0.61529541 batch PCKh 0.5\n",
      "Trained batch 658 batch loss 0.551967084 batch mAP 0.561645508 batch PCKh 0.625\n",
      "Trained batch 659 batch loss 0.600899637 batch mAP 0.59942627 batch PCKh 0.75\n",
      "Trained batch 660 batch loss 0.631041765 batch mAP 0.501586914 batch PCKh 0.875\n",
      "Trained batch 661 batch loss 0.647473574 batch mAP 0.368225098 batch PCKh 0.625\n",
      "Trained batch 662 batch loss 0.604182541 batch mAP 0.207611084 batch PCKh 0.75\n",
      "Trained batch 663 batch loss 0.604716897 batch mAP 0.197967529 batch PCKh 0.5625\n",
      "Trained batch 664 batch loss 0.592257738 batch mAP 0.185089111 batch PCKh 0.625\n",
      "Trained batch 665 batch loss 0.590909779 batch mAP 0.115234375 batch PCKh 0.3125\n",
      "Trained batch 666 batch loss 0.670801044 batch mAP 0.120819092 batch PCKh 0.625\n",
      "Trained batch 667 batch loss 0.600476 batch mAP 0.193450928 batch PCKh 0.4375\n",
      "Trained batch 668 batch loss 0.654698312 batch mAP 0.408782959 batch PCKh 0.3125\n",
      "Trained batch 669 batch loss 0.667920232 batch mAP 0.484680176 batch PCKh 0.3125\n",
      "Trained batch 670 batch loss 0.696162 batch mAP 0.499572754 batch PCKh 0.6875\n",
      "Trained batch 671 batch loss 0.626422882 batch mAP 0.532074 batch PCKh 0.3125\n",
      "Trained batch 672 batch loss 0.543753862 batch mAP 0.552001953 batch PCKh 0.4375\n",
      "Trained batch 673 batch loss 0.583087862 batch mAP 0.582183838 batch PCKh 0.5\n",
      "Trained batch 674 batch loss 0.571942627 batch mAP 0.50982666 batch PCKh 0.375\n",
      "Trained batch 675 batch loss 0.670709431 batch mAP 0.46472168 batch PCKh 0.375\n",
      "Trained batch 676 batch loss 0.642961442 batch mAP 0.561187744 batch PCKh 0.5625\n",
      "Trained batch 677 batch loss 0.574931 batch mAP 0.573547363 batch PCKh 0.1875\n",
      "Trained batch 678 batch loss 0.587512553 batch mAP 0.54586792 batch PCKh 0.3125\n",
      "Trained batch 679 batch loss 0.596069157 batch mAP 0.514648438 batch PCKh 0.375\n",
      "Trained batch 680 batch loss 0.664987803 batch mAP 0.475189209 batch PCKh 0\n",
      "Trained batch 681 batch loss 0.609083593 batch mAP 0.498016357 batch PCKh 0.5625\n",
      "Trained batch 682 batch loss 0.604417562 batch mAP 0.454772949 batch PCKh 0.5\n",
      "Trained batch 683 batch loss 0.602085412 batch mAP 0.450561523 batch PCKh 0.5\n",
      "Trained batch 684 batch loss 0.604782164 batch mAP 0.39855957 batch PCKh 0.8125\n",
      "Trained batch 685 batch loss 0.549264669 batch mAP 0.483551025 batch PCKh 0.625\n",
      "Trained batch 686 batch loss 0.641483068 batch mAP 0.521026611 batch PCKh 0.625\n",
      "Trained batch 687 batch loss 0.61209619 batch mAP 0.515350342 batch PCKh 0.6875\n",
      "Trained batch 688 batch loss 0.499654442 batch mAP 0.500061035 batch PCKh 0.375\n",
      "Trained batch 689 batch loss 0.630499363 batch mAP 0.500946045 batch PCKh 0.4375\n",
      "Trained batch 690 batch loss 0.683172524 batch mAP 0.454559326 batch PCKh 0.25\n",
      "Trained batch 691 batch loss 0.543351233 batch mAP 0.555053711 batch PCKh 0.25\n",
      "Trained batch 692 batch loss 0.662189126 batch mAP 0.522583 batch PCKh 0.625\n",
      "Trained batch 693 batch loss 0.557023585 batch mAP 0.543914795 batch PCKh 0.25\n",
      "Trained batch 694 batch loss 0.524325 batch mAP 0.570770264 batch PCKh 0.5625\n",
      "Trained batch 695 batch loss 0.549295425 batch mAP 0.58215332 batch PCKh 0.3125\n",
      "Trained batch 696 batch loss 0.509512663 batch mAP 0.620117188 batch PCKh 0.25\n",
      "Trained batch 697 batch loss 0.470751822 batch mAP 0.586364746 batch PCKh 0.25\n",
      "Trained batch 698 batch loss 0.555228472 batch mAP 0.605377197 batch PCKh 0.1875\n",
      "Trained batch 699 batch loss 0.499121845 batch mAP 0.620452881 batch PCKh 0.5625\n",
      "Trained batch 700 batch loss 0.626037955 batch mAP 0.538696289 batch PCKh 0.8125\n",
      "Trained batch 701 batch loss 0.517432094 batch mAP 0.563873291 batch PCKh 0.625\n",
      "Trained batch 702 batch loss 0.588271499 batch mAP 0.528625488 batch PCKh 0.625\n",
      "Trained batch 703 batch loss 0.672466457 batch mAP 0.481719971 batch PCKh 0.625\n",
      "Trained batch 704 batch loss 0.572219133 batch mAP 0.591217041 batch PCKh 0.5\n",
      "Trained batch 705 batch loss 0.543830335 batch mAP 0.570770264 batch PCKh 0.1875\n",
      "Trained batch 706 batch loss 0.58935082 batch mAP 0.573669434 batch PCKh 0.1875\n",
      "Trained batch 707 batch loss 0.659425855 batch mAP 0.486541748 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 708 batch loss 0.689378738 batch mAP 0.480651855 batch PCKh 0.875\n",
      "Trained batch 709 batch loss 0.582947254 batch mAP 0.516296387 batch PCKh 0.625\n",
      "Trained batch 710 batch loss 0.566342473 batch mAP 0.529602051 batch PCKh 0.75\n",
      "Trained batch 711 batch loss 0.710044801 batch mAP 0.496917725 batch PCKh 0.375\n",
      "Trained batch 712 batch loss 0.495045483 batch mAP 0.571014404 batch PCKh 0.125\n",
      "Trained batch 713 batch loss 0.624074459 batch mAP 0.552185059 batch PCKh 0.1875\n",
      "Trained batch 714 batch loss 0.641472042 batch mAP 0.530914307 batch PCKh 0.3125\n",
      "Trained batch 715 batch loss 0.593769789 batch mAP 0.490234375 batch PCKh 0.25\n",
      "Trained batch 716 batch loss 0.635929883 batch mAP 0.474761963 batch PCKh 0.1875\n",
      "Trained batch 717 batch loss 0.566900313 batch mAP 0.448608398 batch PCKh 0.5\n",
      "Trained batch 718 batch loss 0.564775169 batch mAP 0.370697021 batch PCKh 0.375\n",
      "Trained batch 719 batch loss 0.587409139 batch mAP 0.371002197 batch PCKh 0.375\n",
      "Trained batch 720 batch loss 0.628733575 batch mAP 0.444793701 batch PCKh 0.25\n",
      "Trained batch 721 batch loss 0.591869712 batch mAP 0.474914551 batch PCKh 0.6875\n",
      "Trained batch 722 batch loss 0.611790121 batch mAP 0.433074951 batch PCKh 0.1875\n",
      "Trained batch 723 batch loss 0.650315821 batch mAP 0.467803955 batch PCKh 0.25\n",
      "Trained batch 724 batch loss 0.638540268 batch mAP 0.467102051 batch PCKh 0.25\n",
      "Trained batch 725 batch loss 0.647774 batch mAP 0.448730469 batch PCKh 0.3125\n",
      "Trained batch 726 batch loss 0.545071244 batch mAP 0.388000488 batch PCKh 0.6875\n",
      "Trained batch 727 batch loss 0.596428514 batch mAP 0.382324219 batch PCKh 0.1875\n",
      "Trained batch 728 batch loss 0.574077785 batch mAP 0.405548096 batch PCKh 0.25\n",
      "Trained batch 729 batch loss 0.577433109 batch mAP 0.44442749 batch PCKh 0.5625\n",
      "Trained batch 730 batch loss 0.66739738 batch mAP 0.499328613 batch PCKh 0.0625\n",
      "Trained batch 731 batch loss 0.679271519 batch mAP 0.457824707 batch PCKh 0.5\n",
      "Trained batch 732 batch loss 0.683358788 batch mAP 0.461792 batch PCKh 0.625\n",
      "Trained batch 733 batch loss 0.669291377 batch mAP 0.496887207 batch PCKh 0.3125\n",
      "Trained batch 734 batch loss 0.56499666 batch mAP 0.543457031 batch PCKh 0.375\n",
      "Trained batch 735 batch loss 0.565224767 batch mAP 0.577178955 batch PCKh 0.6875\n",
      "Trained batch 736 batch loss 0.558268428 batch mAP 0.526001 batch PCKh 0.6875\n",
      "Trained batch 737 batch loss 0.61985743 batch mAP 0.564025879 batch PCKh 0.5\n",
      "Trained batch 738 batch loss 0.721497238 batch mAP 0.616455078 batch PCKh 0.0625\n",
      "Trained batch 739 batch loss 0.747710645 batch mAP 0.563690186 batch PCKh 0\n",
      "Trained batch 740 batch loss 0.749760747 batch mAP 0.514709473 batch PCKh 0.0625\n",
      "Trained batch 741 batch loss 0.723754 batch mAP 0.545837402 batch PCKh 0.0625\n",
      "Trained batch 742 batch loss 0.648825765 batch mAP 0.546722412 batch PCKh 0.25\n",
      "Trained batch 743 batch loss 0.629627347 batch mAP 0.505981445 batch PCKh 0.5\n",
      "Trained batch 744 batch loss 0.493423402 batch mAP 0.425231934 batch PCKh 0.6875\n",
      "Trained batch 745 batch loss 0.594948351 batch mAP 0.442077637 batch PCKh 0.4375\n",
      "Trained batch 746 batch loss 0.585673332 batch mAP 0.380249023 batch PCKh 0.3125\n",
      "Trained batch 747 batch loss 0.592886448 batch mAP 0.412414551 batch PCKh 0.5\n",
      "Trained batch 748 batch loss 0.489568293 batch mAP 0.378326416 batch PCKh 0.4375\n",
      "Trained batch 749 batch loss 0.644957 batch mAP 0.474334717 batch PCKh 0.5625\n",
      "Trained batch 750 batch loss 0.610414863 batch mAP 0.488647461 batch PCKh 0.625\n",
      "Trained batch 751 batch loss 0.659583807 batch mAP 0.417511 batch PCKh 0.6875\n",
      "Trained batch 752 batch loss 0.568422675 batch mAP 0.484313965 batch PCKh 0.25\n",
      "Trained batch 753 batch loss 0.629356563 batch mAP 0.433074951 batch PCKh 0\n",
      "Trained batch 754 batch loss 0.650593102 batch mAP 0.488952637 batch PCKh 0.8125\n",
      "Trained batch 755 batch loss 0.583664775 batch mAP 0.454284668 batch PCKh 0.1875\n",
      "Trained batch 756 batch loss 0.661373377 batch mAP 0.475006104 batch PCKh 0.1875\n",
      "Trained batch 757 batch loss 0.661955774 batch mAP 0.473693848 batch PCKh 0.25\n",
      "Trained batch 758 batch loss 0.702661157 batch mAP 0.475769043 batch PCKh 0.3125\n",
      "Trained batch 759 batch loss 0.663904071 batch mAP 0.456848145 batch PCKh 0.1875\n",
      "Trained batch 760 batch loss 0.688954234 batch mAP 0.469024658 batch PCKh 0.4375\n",
      "Trained batch 761 batch loss 0.65822053 batch mAP 0.43560791 batch PCKh 0.625\n",
      "Trained batch 762 batch loss 0.633993268 batch mAP 0.470031738 batch PCKh 0.625\n",
      "Trained batch 763 batch loss 0.589728892 batch mAP 0.367767334 batch PCKh 0.4375\n",
      "Trained batch 764 batch loss 0.676512599 batch mAP 0.385986328 batch PCKh 0.5\n",
      "Trained batch 765 batch loss 0.669519961 batch mAP 0.362487793 batch PCKh 0.5625\n",
      "Trained batch 766 batch loss 0.580388427 batch mAP 0.36932373 batch PCKh 0.3125\n",
      "Trained batch 767 batch loss 0.547513068 batch mAP 0.444519043 batch PCKh 0.25\n",
      "Trained batch 768 batch loss 0.5595752 batch mAP 0.505859375 batch PCKh 0.3125\n",
      "Trained batch 769 batch loss 0.628057718 batch mAP 0.511749268 batch PCKh 0.1875\n",
      "Trained batch 770 batch loss 0.608320713 batch mAP 0.557525635 batch PCKh 0.3125\n",
      "Trained batch 771 batch loss 0.582268119 batch mAP 0.509460449 batch PCKh 0.5\n",
      "Trained batch 772 batch loss 0.628561378 batch mAP 0.536651611 batch PCKh 0.8125\n",
      "Trained batch 773 batch loss 0.58832866 batch mAP 0.5184021 batch PCKh 0.125\n",
      "Trained batch 774 batch loss 0.563335657 batch mAP 0.540679932 batch PCKh 0.1875\n",
      "Trained batch 775 batch loss 0.580020308 batch mAP 0.597045898 batch PCKh 0.3125\n",
      "Trained batch 776 batch loss 0.578575492 batch mAP 0.576629639 batch PCKh 0.8125\n",
      "Trained batch 777 batch loss 0.593899965 batch mAP 0.580841064 batch PCKh 0.5\n",
      "Trained batch 778 batch loss 0.531363428 batch mAP 0.575592041 batch PCKh 0.6875\n",
      "Trained batch 779 batch loss 0.551679492 batch mAP 0.550994873 batch PCKh 0.5625\n",
      "Trained batch 780 batch loss 0.581448436 batch mAP 0.523376465 batch PCKh 0.75\n",
      "Trained batch 781 batch loss 0.490612864 batch mAP 0.548095703 batch PCKh 0.5\n",
      "Trained batch 782 batch loss 0.49579522 batch mAP 0.519744873 batch PCKh 0.0625\n",
      "Trained batch 783 batch loss 0.467518091 batch mAP 0.527893066 batch PCKh 0\n",
      "Trained batch 784 batch loss 0.486974567 batch mAP 0.480407715 batch PCKh 0\n",
      "Trained batch 785 batch loss 0.431358248 batch mAP 0.518127441 batch PCKh 0\n",
      "Trained batch 786 batch loss 0.516036034 batch mAP 0.507720947 batch PCKh 0.5625\n",
      "Trained batch 787 batch loss 0.587043405 batch mAP 0.448822021 batch PCKh 0.5\n",
      "Trained batch 788 batch loss 0.62098968 batch mAP 0.457641602 batch PCKh 0.5\n",
      "Trained batch 789 batch loss 0.683276057 batch mAP 0.453704834 batch PCKh 0.5\n",
      "Trained batch 790 batch loss 0.692476809 batch mAP 0.45614624 batch PCKh 0.0625\n",
      "Trained batch 791 batch loss 0.818552673 batch mAP 0.43258667 batch PCKh 0\n",
      "Trained batch 792 batch loss 0.765286624 batch mAP 0.50012207 batch PCKh 0.0625\n",
      "Trained batch 793 batch loss 0.748689175 batch mAP 0.464996338 batch PCKh 0.0625\n",
      "Trained batch 794 batch loss 0.481987089 batch mAP 0.383270264 batch PCKh 0.375\n",
      "Trained batch 795 batch loss 0.529754937 batch mAP 0.316619873 batch PCKh 0.375\n",
      "Trained batch 796 batch loss 0.541070402 batch mAP 0.189575195 batch PCKh 0.75\n",
      "Trained batch 797 batch loss 0.547557533 batch mAP 0.109863281 batch PCKh 0.25\n",
      "Trained batch 798 batch loss 0.620968521 batch mAP 0.130828857 batch PCKh 0.4375\n",
      "Trained batch 799 batch loss 0.66403532 batch mAP 0.0828552246 batch PCKh 0.25\n",
      "Trained batch 800 batch loss 0.624220967 batch mAP 0.0406799316 batch PCKh 0.375\n",
      "Trained batch 801 batch loss 0.583274484 batch mAP 0.0419006348 batch PCKh 0.25\n",
      "Trained batch 802 batch loss 0.638080955 batch mAP 0.136077881 batch PCKh 0.1875\n",
      "Trained batch 803 batch loss 0.603902936 batch mAP 0.210205078 batch PCKh 0.625\n",
      "Trained batch 804 batch loss 0.621112466 batch mAP 0.258758545 batch PCKh 0.375\n",
      "Trained batch 805 batch loss 0.572463 batch mAP 0.348358154 batch PCKh 0.6875\n",
      "Trained batch 806 batch loss 0.605488241 batch mAP 0.403961182 batch PCKh 0.4375\n",
      "Trained batch 807 batch loss 0.634968638 batch mAP 0.318878174 batch PCKh 0.5625\n",
      "Trained batch 808 batch loss 0.683183908 batch mAP 0.373779297 batch PCKh 0.5\n",
      "Trained batch 809 batch loss 0.607095063 batch mAP 0.393951416 batch PCKh 0.4375\n",
      "Trained batch 810 batch loss 0.615388632 batch mAP 0.34197998 batch PCKh 0.5\n",
      "Trained batch 811 batch loss 0.583946466 batch mAP 0.575897217 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 812 batch loss 0.581200838 batch mAP 0.582672119 batch PCKh 0.375\n",
      "Trained batch 813 batch loss 0.547239542 batch mAP 0.557037354 batch PCKh 0.375\n",
      "Trained batch 814 batch loss 0.500099123 batch mAP 0.581634521 batch PCKh 0.1875\n",
      "Trained batch 815 batch loss 0.570349514 batch mAP 0.378540039 batch PCKh 0.375\n",
      "Trained batch 816 batch loss 0.587496877 batch mAP 0.561645508 batch PCKh 0.5\n",
      "Trained batch 817 batch loss 0.601775408 batch mAP 0.569763184 batch PCKh 0.25\n",
      "Trained batch 818 batch loss 0.570988 batch mAP 0.579223633 batch PCKh 0.5625\n",
      "Trained batch 819 batch loss 0.659059823 batch mAP 0.537567139 batch PCKh 0.5\n",
      "Trained batch 820 batch loss 0.633664608 batch mAP 0.507232666 batch PCKh 0\n",
      "Trained batch 821 batch loss 0.551836967 batch mAP 0.528961182 batch PCKh 0.6875\n",
      "Trained batch 822 batch loss 0.555762768 batch mAP 0.514373779 batch PCKh 0.6875\n",
      "Trained batch 823 batch loss 0.458896101 batch mAP 0.47567749 batch PCKh 0.125\n",
      "Trained batch 824 batch loss 0.550482 batch mAP 0.461517334 batch PCKh 0\n",
      "Trained batch 825 batch loss 0.486849904 batch mAP 0.514953613 batch PCKh 0\n",
      "Trained batch 826 batch loss 0.522727489 batch mAP 0.469573975 batch PCKh 0\n",
      "Trained batch 827 batch loss 0.593057752 batch mAP 0.479370117 batch PCKh 0.0625\n",
      "Trained batch 828 batch loss 0.688431382 batch mAP 0.444824219 batch PCKh 0.5625\n",
      "Trained batch 829 batch loss 0.679427445 batch mAP 0.429473877 batch PCKh 0.375\n",
      "Trained batch 830 batch loss 0.782565176 batch mAP 0.44354248 batch PCKh 0.0625\n",
      "Trained batch 831 batch loss 0.574797153 batch mAP 0.450866699 batch PCKh 0.5625\n",
      "Trained batch 832 batch loss 0.55788976 batch mAP 0.244293213 batch PCKh 0.25\n",
      "Trained batch 833 batch loss 0.562125206 batch mAP 0.123809814 batch PCKh 0.1875\n",
      "Trained batch 834 batch loss 0.503883779 batch mAP 0.0660705566 batch PCKh 0.25\n",
      "Trained batch 835 batch loss 0.44764 batch mAP 0.046875 batch PCKh 0.1875\n",
      "Trained batch 836 batch loss 0.58437413 batch mAP 0.0975952148 batch PCKh 0.1875\n",
      "Trained batch 837 batch loss 0.508479953 batch mAP 0.153717041 batch PCKh 0.625\n",
      "Trained batch 838 batch loss 0.568061888 batch mAP 0.24118042 batch PCKh 0.0625\n",
      "Trained batch 839 batch loss 0.657479286 batch mAP 0.465362549 batch PCKh 0.0625\n",
      "Trained batch 840 batch loss 0.560088634 batch mAP 0.434570312 batch PCKh 0.375\n",
      "Trained batch 841 batch loss 0.660506129 batch mAP 0.530975342 batch PCKh 0.375\n",
      "Trained batch 842 batch loss 0.682979584 batch mAP 0.501464844 batch PCKh 0.4375\n",
      "Trained batch 843 batch loss 0.623714387 batch mAP 0.524871826 batch PCKh 0.1875\n",
      "Trained batch 844 batch loss 0.614786208 batch mAP 0.512268066 batch PCKh 0.1875\n",
      "Trained batch 845 batch loss 0.599487543 batch mAP 0.486297607 batch PCKh 0.1875\n",
      "Trained batch 846 batch loss 0.569230855 batch mAP 0.622955322 batch PCKh 0.3125\n",
      "Trained batch 847 batch loss 0.598698914 batch mAP 0.559265137 batch PCKh 0.5\n",
      "Trained batch 848 batch loss 0.623971 batch mAP 0.513336182 batch PCKh 0.0625\n",
      "Trained batch 849 batch loss 0.561854601 batch mAP 0.595001221 batch PCKh 0.375\n",
      "Trained batch 850 batch loss 0.606976867 batch mAP 0.502258301 batch PCKh 0\n",
      "Trained batch 851 batch loss 0.550292909 batch mAP 0.378814697 batch PCKh 0\n",
      "Trained batch 852 batch loss 0.503358662 batch mAP 0.328521729 batch PCKh 0.3125\n",
      "Trained batch 853 batch loss 0.622285128 batch mAP 0.476409912 batch PCKh 0.8125\n",
      "Trained batch 854 batch loss 0.588175774 batch mAP 0.41583252 batch PCKh 0.5625\n",
      "Trained batch 855 batch loss 0.705678165 batch mAP 0.336975098 batch PCKh 0.5625\n",
      "Trained batch 856 batch loss 0.644662857 batch mAP 0.522918701 batch PCKh 0.0625\n",
      "Trained batch 857 batch loss 0.717638791 batch mAP 0.443603516 batch PCKh 0\n",
      "Trained batch 858 batch loss 0.661715031 batch mAP 0.500488281 batch PCKh 0\n",
      "Trained batch 859 batch loss 0.639679432 batch mAP 0.474273682 batch PCKh 0.6875\n",
      "Trained batch 860 batch loss 0.53667587 batch mAP 0.422851562 batch PCKh 0.875\n",
      "Trained batch 861 batch loss 0.480197728 batch mAP 0.414855957 batch PCKh 0.5625\n",
      "Trained batch 862 batch loss 0.544597208 batch mAP 0.368042 batch PCKh 0.5\n",
      "Trained batch 863 batch loss 0.515232325 batch mAP 0.328186035 batch PCKh 0.25\n",
      "Trained batch 864 batch loss 0.519448936 batch mAP 0.294799805 batch PCKh 0.3125\n",
      "Trained batch 865 batch loss 0.526342034 batch mAP 0.333862305 batch PCKh 0.625\n",
      "Trained batch 866 batch loss 0.549327791 batch mAP 0.413879395 batch PCKh 0.3125\n",
      "Trained batch 867 batch loss 0.710430741 batch mAP 0.466796875 batch PCKh 0.4375\n",
      "Trained batch 868 batch loss 0.679696321 batch mAP 0.440582275 batch PCKh 0.875\n",
      "Trained batch 869 batch loss 0.603274226 batch mAP 0.498260498 batch PCKh 0\n",
      "Trained batch 870 batch loss 0.752304792 batch mAP 0.437011719 batch PCKh 0.25\n",
      "Trained batch 871 batch loss 0.594911635 batch mAP 0.497436523 batch PCKh 0.1875\n",
      "Trained batch 872 batch loss 0.624297261 batch mAP 0.484802246 batch PCKh 0.4375\n",
      "Trained batch 873 batch loss 0.654208302 batch mAP 0.473510742 batch PCKh 0.625\n",
      "Trained batch 874 batch loss 0.540677786 batch mAP 0.399932861 batch PCKh 0.75\n",
      "Trained batch 875 batch loss 0.571913 batch mAP 0.469146729 batch PCKh 0.4375\n",
      "Trained batch 876 batch loss 0.559152722 batch mAP 0.436157227 batch PCKh 0.4375\n",
      "Trained batch 877 batch loss 0.518949151 batch mAP 0.440307617 batch PCKh 0.25\n",
      "Trained batch 878 batch loss 0.613740206 batch mAP 0.419799805 batch PCKh 0.625\n",
      "Trained batch 879 batch loss 0.584402442 batch mAP 0.413574219 batch PCKh 0.3125\n",
      "Trained batch 880 batch loss 0.601887643 batch mAP 0.347930908 batch PCKh 0.6875\n",
      "Trained batch 881 batch loss 0.501406074 batch mAP 0.417816162 batch PCKh 0.5\n",
      "Trained batch 882 batch loss 0.524212122 batch mAP 0.458221436 batch PCKh 0.25\n",
      "Trained batch 883 batch loss 0.717005491 batch mAP 0.410125732 batch PCKh 0.5\n",
      "Trained batch 884 batch loss 0.667300403 batch mAP 0.505004883 batch PCKh 0.75\n",
      "Trained batch 885 batch loss 0.590250254 batch mAP 0.500213623 batch PCKh 0.1875\n",
      "Trained batch 886 batch loss 0.539357066 batch mAP 0.478851318 batch PCKh 0.375\n",
      "Trained batch 887 batch loss 0.629535198 batch mAP 0.450073242 batch PCKh 0.125\n",
      "Trained batch 888 batch loss 0.613003671 batch mAP 0.481262207 batch PCKh 0.1875\n",
      "Trained batch 889 batch loss 0.601037085 batch mAP 0.442169189 batch PCKh 0.6875\n",
      "Trained batch 890 batch loss 0.540646493 batch mAP 0.438934326 batch PCKh 0.0625\n",
      "Trained batch 891 batch loss 0.641849279 batch mAP 0.416992188 batch PCKh 0.4375\n",
      "Trained batch 892 batch loss 0.59629935 batch mAP 0.393127441 batch PCKh 0.25\n",
      "Trained batch 893 batch loss 0.58478868 batch mAP 0.390045166 batch PCKh 0.5\n",
      "Trained batch 894 batch loss 0.590861917 batch mAP 0.457122803 batch PCKh 0.6875\n",
      "Trained batch 895 batch loss 0.580634713 batch mAP 0.436767578 batch PCKh 0.5\n",
      "Trained batch 896 batch loss 0.606889307 batch mAP 0.470581055 batch PCKh 0.75\n",
      "Trained batch 897 batch loss 0.567672133 batch mAP 0.383392334 batch PCKh 0.5625\n",
      "Trained batch 898 batch loss 0.529033065 batch mAP 0.491729736 batch PCKh 0.75\n",
      "Trained batch 899 batch loss 0.562465429 batch mAP 0.260131836 batch PCKh 0.625\n",
      "Trained batch 900 batch loss 0.606290698 batch mAP 0.311645508 batch PCKh 0.5625\n",
      "Trained batch 901 batch loss 0.578180552 batch mAP 0.497344971 batch PCKh 0.625\n",
      "Trained batch 902 batch loss 0.617626786 batch mAP 0.52935791 batch PCKh 0.6875\n",
      "Trained batch 903 batch loss 0.546473503 batch mAP 0.568603516 batch PCKh 0.25\n",
      "Trained batch 904 batch loss 0.579839647 batch mAP 0.561553955 batch PCKh 0.625\n",
      "Trained batch 905 batch loss 0.60835886 batch mAP 0.563018799 batch PCKh 0.4375\n",
      "Trained batch 906 batch loss 0.594848216 batch mAP 0.559539795 batch PCKh 0.0625\n",
      "Trained batch 907 batch loss 0.606028 batch mAP 0.603210449 batch PCKh 0.25\n",
      "Trained batch 908 batch loss 0.516464412 batch mAP 0.590789795 batch PCKh 0.3125\n",
      "Trained batch 909 batch loss 0.52185607 batch mAP 0.599090576 batch PCKh 0.3125\n",
      "Trained batch 910 batch loss 0.54715991 batch mAP 0.580078125 batch PCKh 0.3125\n",
      "Trained batch 911 batch loss 0.525188565 batch mAP 0.5703125 batch PCKh 0.25\n",
      "Trained batch 912 batch loss 0.555759966 batch mAP 0.535553 batch PCKh 0.25\n",
      "Trained batch 913 batch loss 0.57169205 batch mAP 0.511474609 batch PCKh 0.375\n",
      "Trained batch 914 batch loss 0.628047049 batch mAP 0.464660645 batch PCKh 0.25\n",
      "Trained batch 915 batch loss 0.567009151 batch mAP 0.46572876 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 916 batch loss 0.530305922 batch mAP 0.500824 batch PCKh 0.1875\n",
      "Trained batch 917 batch loss 0.514278352 batch mAP 0.506225586 batch PCKh 0.375\n",
      "Trained batch 918 batch loss 0.577095211 batch mAP 0.49420166 batch PCKh 0.5\n",
      "Trained batch 919 batch loss 0.526026487 batch mAP 0.512023926 batch PCKh 0.5625\n",
      "Trained batch 920 batch loss 0.517657101 batch mAP 0.499084473 batch PCKh 0\n",
      "Trained batch 921 batch loss 0.712668419 batch mAP 0.45980835 batch PCKh 0\n",
      "Trained batch 922 batch loss 0.62138027 batch mAP 0.554290771 batch PCKh 0.25\n",
      "Trained batch 923 batch loss 0.576225877 batch mAP 0.586212158 batch PCKh 0.125\n",
      "Trained batch 924 batch loss 0.792038679 batch mAP 0.389709473 batch PCKh 0\n",
      "Trained batch 925 batch loss 0.761022627 batch mAP 0.454833984 batch PCKh 0.1875\n",
      "Trained batch 926 batch loss 0.537129819 batch mAP 0.49697876 batch PCKh 0.625\n",
      "Trained batch 927 batch loss 0.666465521 batch mAP 0.492950439 batch PCKh 0.125\n",
      "Trained batch 928 batch loss 0.746482432 batch mAP 0.452514648 batch PCKh 0\n",
      "Trained batch 929 batch loss 0.66021 batch mAP 0.440643311 batch PCKh 0.625\n",
      "Trained batch 930 batch loss 0.587055266 batch mAP 0.376373291 batch PCKh 0.75\n",
      "Trained batch 931 batch loss 0.617872119 batch mAP 0.355438232 batch PCKh 0.75\n",
      "Trained batch 932 batch loss 0.668588281 batch mAP 0.141052246 batch PCKh 0.8125\n",
      "Trained batch 933 batch loss 0.680607319 batch mAP 0.103515625 batch PCKh 0.3125\n",
      "Trained batch 934 batch loss 0.583555 batch mAP 0.0424804688 batch PCKh 0.75\n",
      "Trained batch 935 batch loss 0.589149535 batch mAP 0.171112061 batch PCKh 0.4375\n",
      "Trained batch 936 batch loss 0.583823442 batch mAP 0.0602417 batch PCKh 0.625\n",
      "Trained batch 937 batch loss 0.603975177 batch mAP 0.354827881 batch PCKh 0.75\n",
      "Trained batch 938 batch loss 0.594020247 batch mAP 0.41217041 batch PCKh 0.5625\n",
      "Trained batch 939 batch loss 0.626645505 batch mAP 0.428710938 batch PCKh 0.8125\n",
      "Trained batch 940 batch loss 0.518012822 batch mAP 0.459564209 batch PCKh 0.625\n",
      "Trained batch 941 batch loss 0.594147801 batch mAP 0.519470215 batch PCKh 0.3125\n",
      "Trained batch 942 batch loss 0.588516653 batch mAP 0.540679932 batch PCKh 0.375\n",
      "Trained batch 943 batch loss 0.577096939 batch mAP 0.587738037 batch PCKh 0.375\n",
      "Trained batch 944 batch loss 0.627141178 batch mAP 0.547851562 batch PCKh 0.375\n",
      "Trained batch 945 batch loss 0.640538394 batch mAP 0.373626709 batch PCKh 0\n",
      "Trained batch 946 batch loss 0.628282666 batch mAP 0.554992676 batch PCKh 0.5\n",
      "Trained batch 947 batch loss 0.616955638 batch mAP 0.486206055 batch PCKh 0.5\n",
      "Trained batch 948 batch loss 0.56858772 batch mAP 0.442016602 batch PCKh 0.5\n",
      "Trained batch 949 batch loss 0.574544072 batch mAP 0.304138184 batch PCKh 0.75\n",
      "Trained batch 950 batch loss 0.530629694 batch mAP 0.344390869 batch PCKh 0.625\n",
      "Trained batch 951 batch loss 0.560247838 batch mAP 0.446716309 batch PCKh 0.6875\n",
      "Trained batch 952 batch loss 0.615884244 batch mAP 0.451019287 batch PCKh 0.5625\n",
      "Trained batch 953 batch loss 0.611042142 batch mAP 0.450286865 batch PCKh 0.8125\n",
      "Trained batch 954 batch loss 0.61446172 batch mAP 0.487365723 batch PCKh 0.75\n",
      "Trained batch 955 batch loss 0.62080586 batch mAP 0.482727051 batch PCKh 0.4375\n",
      "Trained batch 956 batch loss 0.661676526 batch mAP 0.462982178 batch PCKh 0.4375\n",
      "Trained batch 957 batch loss 0.648596346 batch mAP 0.470947266 batch PCKh 0.625\n",
      "Trained batch 958 batch loss 0.711812735 batch mAP 0.398590088 batch PCKh 0.125\n",
      "Trained batch 959 batch loss 0.645862758 batch mAP 0.458831787 batch PCKh 0.3125\n",
      "Trained batch 960 batch loss 0.633640945 batch mAP 0.478240967 batch PCKh 0.5\n",
      "Trained batch 961 batch loss 0.677122176 batch mAP 0.391204834 batch PCKh 0.6875\n",
      "Trained batch 962 batch loss 0.613382 batch mAP 0.360015869 batch PCKh 0.3125\n",
      "Trained batch 963 batch loss 0.509833455 batch mAP 0.442169189 batch PCKh 0.1875\n",
      "Trained batch 964 batch loss 0.623377502 batch mAP 0.396606445 batch PCKh 0.1875\n",
      "Trained batch 965 batch loss 0.622923374 batch mAP 0.32043457 batch PCKh 0.6875\n",
      "Trained batch 966 batch loss 0.621067226 batch mAP 0.409729 batch PCKh 0.75\n",
      "Trained batch 967 batch loss 0.566232502 batch mAP 0.362579346 batch PCKh 0.5625\n",
      "Trained batch 968 batch loss 0.602737069 batch mAP 0.437469482 batch PCKh 0.4375\n",
      "Trained batch 969 batch loss 0.647078156 batch mAP 0.423553467 batch PCKh 0.125\n",
      "Trained batch 970 batch loss 0.581313372 batch mAP 0.407318115 batch PCKh 0.6875\n",
      "Trained batch 971 batch loss 0.49283731 batch mAP 0.345214844 batch PCKh 0.5\n",
      "Trained batch 972 batch loss 0.612750411 batch mAP 0.397827148 batch PCKh 0.1875\n",
      "Trained batch 973 batch loss 0.613747716 batch mAP 0.3855896 batch PCKh 0.5\n",
      "Trained batch 974 batch loss 0.708351374 batch mAP 0.291168213 batch PCKh 0.3125\n",
      "Trained batch 975 batch loss 0.63218677 batch mAP 0.325286865 batch PCKh 0.1875\n",
      "Trained batch 976 batch loss 0.532494187 batch mAP 0.476135254 batch PCKh 0.125\n",
      "Trained batch 977 batch loss 0.568944931 batch mAP 0.408691406 batch PCKh 0\n",
      "Trained batch 978 batch loss 0.66208154 batch mAP 0.492736816 batch PCKh 0.75\n",
      "Trained batch 979 batch loss 0.618923545 batch mAP 0.527160645 batch PCKh 0.0625\n",
      "Trained batch 980 batch loss 0.60736835 batch mAP 0.510162354 batch PCKh 0.5\n",
      "Trained batch 981 batch loss 0.617015898 batch mAP 0.513061523 batch PCKh 0.4375\n",
      "Trained batch 982 batch loss 0.562061 batch mAP 0.536895752 batch PCKh 0.4375\n",
      "Trained batch 983 batch loss 0.473431319 batch mAP 0.521850586 batch PCKh 0.3125\n",
      "Trained batch 984 batch loss 0.551375091 batch mAP 0.527954102 batch PCKh 0.375\n",
      "Trained batch 985 batch loss 0.571226358 batch mAP 0.527526855 batch PCKh 0.3125\n",
      "Trained batch 986 batch loss 0.632002652 batch mAP 0.515716553 batch PCKh 0.4375\n",
      "Trained batch 987 batch loss 0.572347224 batch mAP 0.556854248 batch PCKh 0.3125\n",
      "Trained batch 988 batch loss 0.568142295 batch mAP 0.547821045 batch PCKh 0.6875\n",
      "Trained batch 989 batch loss 0.545643151 batch mAP 0.57711792 batch PCKh 0.3125\n",
      "Trained batch 990 batch loss 0.527227402 batch mAP 0.57333374 batch PCKh 0.375\n",
      "Trained batch 991 batch loss 0.528602481 batch mAP 0.618255615 batch PCKh 0.25\n",
      "Trained batch 992 batch loss 0.553083599 batch mAP 0.586608887 batch PCKh 0.3125\n",
      "Trained batch 993 batch loss 0.545814514 batch mAP 0.572784424 batch PCKh 0.4375\n",
      "Trained batch 994 batch loss 0.543444276 batch mAP 0.607116699 batch PCKh 0.6875\n",
      "Trained batch 995 batch loss 0.563916862 batch mAP 0.577362061 batch PCKh 0.3125\n",
      "Trained batch 996 batch loss 0.591581583 batch mAP 0.507965088 batch PCKh 0.875\n",
      "Trained batch 997 batch loss 0.653042197 batch mAP 0.358154297 batch PCKh 0.625\n",
      "Trained batch 998 batch loss 0.578094482 batch mAP 0.33795166 batch PCKh 0.5\n",
      "Trained batch 999 batch loss 0.65986079 batch mAP 0.352264404 batch PCKh 0.5\n",
      "Trained batch 1000 batch loss 0.577255368 batch mAP 0.532043457 batch PCKh 0.6875\n",
      "Trained batch 1001 batch loss 0.588954568 batch mAP 0.527740479 batch PCKh 0.6875\n",
      "Trained batch 1002 batch loss 0.496610314 batch mAP 0.517303467 batch PCKh 0.375\n",
      "Trained batch 1003 batch loss 0.519400597 batch mAP 0.529846191 batch PCKh 0.5\n",
      "Trained batch 1004 batch loss 0.517444491 batch mAP 0.528411865 batch PCKh 0.375\n",
      "Trained batch 1005 batch loss 0.57018429 batch mAP 0.551452637 batch PCKh 0.375\n",
      "Trained batch 1006 batch loss 0.64214617 batch mAP 0.513244629 batch PCKh 0.5\n",
      "Trained batch 1007 batch loss 0.623667181 batch mAP 0.516723633 batch PCKh 0.375\n",
      "Trained batch 1008 batch loss 0.59429419 batch mAP 0.507232666 batch PCKh 0.375\n",
      "Trained batch 1009 batch loss 0.644533813 batch mAP 0.502655 batch PCKh 0.375\n",
      "Trained batch 1010 batch loss 0.670680344 batch mAP 0.488708496 batch PCKh 0.3125\n",
      "Trained batch 1011 batch loss 0.637571514 batch mAP 0.490997314 batch PCKh 0.25\n",
      "Trained batch 1012 batch loss 0.589048624 batch mAP 0.504486084 batch PCKh 0.8125\n",
      "Trained batch 1013 batch loss 0.59722805 batch mAP 0.499328613 batch PCKh 0.5\n",
      "Trained batch 1014 batch loss 0.621617198 batch mAP 0.507782 batch PCKh 0.5\n",
      "Trained batch 1015 batch loss 0.603196263 batch mAP 0.525878906 batch PCKh 0.4375\n",
      "Trained batch 1016 batch loss 0.656213641 batch mAP 0.472442627 batch PCKh 0.5\n",
      "Trained batch 1017 batch loss 0.647297263 batch mAP 0.49822998 batch PCKh 0.3125\n",
      "Trained batch 1018 batch loss 0.608958125 batch mAP 0.428131104 batch PCKh 0.25\n",
      "Trained batch 1019 batch loss 0.651852131 batch mAP 0.44909668 batch PCKh 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1020 batch loss 0.600806057 batch mAP 0.50289917 batch PCKh 0.5625\n",
      "Trained batch 1021 batch loss 0.554779887 batch mAP 0.474090576 batch PCKh 0.375\n",
      "Trained batch 1022 batch loss 0.560067713 batch mAP 0.49005127 batch PCKh 0.0625\n",
      "Trained batch 1023 batch loss 0.587513208 batch mAP 0.456604 batch PCKh 0.0625\n",
      "Trained batch 1024 batch loss 0.579848051 batch mAP 0.501464844 batch PCKh 0.5625\n",
      "Trained batch 1025 batch loss 0.536607862 batch mAP 0.555908203 batch PCKh 0.25\n",
      "Trained batch 1026 batch loss 0.593981743 batch mAP 0.38482666 batch PCKh 0.5625\n",
      "Trained batch 1027 batch loss 0.592700899 batch mAP 0.340515137 batch PCKh 0.375\n",
      "Trained batch 1028 batch loss 0.52606982 batch mAP 0.383361816 batch PCKh 0.5625\n",
      "Trained batch 1029 batch loss 0.51219672 batch mAP 0.415588379 batch PCKh 0.4375\n",
      "Trained batch 1030 batch loss 0.509139478 batch mAP 0.443084717 batch PCKh 0.0625\n",
      "Trained batch 1031 batch loss 0.649047494 batch mAP 0.395782471 batch PCKh 0.375\n",
      "Trained batch 1032 batch loss 0.619283199 batch mAP 0.460357666 batch PCKh 0.6875\n",
      "Trained batch 1033 batch loss 0.523574233 batch mAP 0.491546631 batch PCKh 0.4375\n",
      "Trained batch 1034 batch loss 0.605374753 batch mAP 0.459442139 batch PCKh 0.25\n",
      "Trained batch 1035 batch loss 0.572184145 batch mAP 0.490509033 batch PCKh 0.4375\n",
      "Trained batch 1036 batch loss 0.587622464 batch mAP 0.469848633 batch PCKh 0.1875\n",
      "Trained batch 1037 batch loss 0.513691664 batch mAP 0.507446289 batch PCKh 0.625\n",
      "Trained batch 1038 batch loss 0.498293221 batch mAP 0.423980713 batch PCKh 0.6875\n",
      "Trained batch 1039 batch loss 0.553787649 batch mAP 0.417327881 batch PCKh 0.75\n",
      "Trained batch 1040 batch loss 0.572271287 batch mAP 0.49395752 batch PCKh 0.6875\n",
      "Trained batch 1041 batch loss 0.567043662 batch mAP 0.39440918 batch PCKh 0.4375\n",
      "Trained batch 1042 batch loss 0.600359917 batch mAP 0.453948975 batch PCKh 0.375\n",
      "Trained batch 1043 batch loss 0.642546535 batch mAP 0.479492188 batch PCKh 0.5625\n",
      "Trained batch 1044 batch loss 0.621442437 batch mAP 0.474243164 batch PCKh 0.75\n",
      "Trained batch 1045 batch loss 0.618195534 batch mAP 0.539672852 batch PCKh 0.25\n",
      "Trained batch 1046 batch loss 0.559717417 batch mAP 0.546691895 batch PCKh 0.75\n",
      "Trained batch 1047 batch loss 0.592764199 batch mAP 0.512451172 batch PCKh 0.4375\n",
      "Trained batch 1048 batch loss 0.582935095 batch mAP 0.573455811 batch PCKh 0.1875\n",
      "Trained batch 1049 batch loss 0.633920312 batch mAP 0.504638672 batch PCKh 0.4375\n",
      "Trained batch 1050 batch loss 0.673015594 batch mAP 0.493255615 batch PCKh 0.3125\n",
      "Trained batch 1051 batch loss 0.702545166 batch mAP 0.457763672 batch PCKh 0.3125\n",
      "Trained batch 1052 batch loss 0.702759147 batch mAP 0.463745117 batch PCKh 0.5\n",
      "Trained batch 1053 batch loss 0.668518543 batch mAP 0.49331665 batch PCKh 0.5625\n",
      "Trained batch 1054 batch loss 0.678491652 batch mAP 0.470275879 batch PCKh 0.4375\n",
      "Trained batch 1055 batch loss 0.652875304 batch mAP 0.433197021 batch PCKh 0.5625\n",
      "Trained batch 1056 batch loss 0.594924033 batch mAP 0.449859619 batch PCKh 0.75\n",
      "Trained batch 1057 batch loss 0.57139194 batch mAP 0.374786377 batch PCKh 0.875\n",
      "Trained batch 1058 batch loss 0.566592276 batch mAP 0.415252686 batch PCKh 0.8125\n",
      "Trained batch 1059 batch loss 0.600582302 batch mAP 0.375366211 batch PCKh 0.875\n",
      "Trained batch 1060 batch loss 0.568662226 batch mAP 0.349334717 batch PCKh 0.0625\n",
      "Trained batch 1061 batch loss 0.555037498 batch mAP 0.364898682 batch PCKh 0.375\n",
      "Trained batch 1062 batch loss 0.603947759 batch mAP 0.497375488 batch PCKh 0.375\n",
      "Trained batch 1063 batch loss 0.677457 batch mAP 0.460083 batch PCKh 0.625\n",
      "Trained batch 1064 batch loss 0.634510756 batch mAP 0.492523193 batch PCKh 0\n",
      "Trained batch 1065 batch loss 0.666342437 batch mAP 0.489379883 batch PCKh 0.1875\n",
      "Trained batch 1066 batch loss 0.709600568 batch mAP 0.466461182 batch PCKh 0.125\n",
      "Trained batch 1067 batch loss 0.808960259 batch mAP 0.291503906 batch PCKh 0.125\n",
      "Trained batch 1068 batch loss 0.704030514 batch mAP 0.323272705 batch PCKh 0.5\n",
      "Trained batch 1069 batch loss 0.727546334 batch mAP 0.371032715 batch PCKh 0.3125\n",
      "Trained batch 1070 batch loss 0.607908607 batch mAP 0.474304199 batch PCKh 0.1875\n",
      "Trained batch 1071 batch loss 0.608098865 batch mAP 0.444396973 batch PCKh 0\n",
      "Trained batch 1072 batch loss 0.551508 batch mAP 0.386047363 batch PCKh 0.625\n",
      "Trained batch 1073 batch loss 0.597078204 batch mAP 0.386535645 batch PCKh 0.1875\n",
      "Trained batch 1074 batch loss 0.696254492 batch mAP 0.397705078 batch PCKh 0.0625\n",
      "Trained batch 1075 batch loss 0.627096891 batch mAP 0.376800537 batch PCKh 0.125\n",
      "Trained batch 1076 batch loss 0.601168394 batch mAP 0.370452881 batch PCKh 0.375\n",
      "Trained batch 1077 batch loss 0.668841362 batch mAP 0.380462646 batch PCKh 0.3125\n",
      "Trained batch 1078 batch loss 0.693145275 batch mAP 0.40625 batch PCKh 0.25\n",
      "Trained batch 1079 batch loss 0.757983267 batch mAP 0.389892578 batch PCKh 0.0625\n",
      "Trained batch 1080 batch loss 0.669720471 batch mAP 0.411682129 batch PCKh 0.1875\n",
      "Trained batch 1081 batch loss 0.674350619 batch mAP 0.397186279 batch PCKh 0.1875\n",
      "Trained batch 1082 batch loss 0.532653451 batch mAP 0.39944458 batch PCKh 0.3125\n",
      "Trained batch 1083 batch loss 0.513276935 batch mAP 0.368408203 batch PCKh 0.3125\n",
      "Trained batch 1084 batch loss 0.56122452 batch mAP 0.447540283 batch PCKh 0.0625\n",
      "Trained batch 1085 batch loss 0.642711461 batch mAP 0.453033447 batch PCKh 0.5\n",
      "Trained batch 1086 batch loss 0.548134148 batch mAP 0.4609375 batch PCKh 0.375\n",
      "Trained batch 1087 batch loss 0.660949111 batch mAP 0.484710693 batch PCKh 0\n",
      "Trained batch 1088 batch loss 0.631622553 batch mAP 0.526245117 batch PCKh 0.6875\n",
      "Trained batch 1089 batch loss 0.624862432 batch mAP 0.511383057 batch PCKh 0.1875\n",
      "Trained batch 1090 batch loss 0.645209074 batch mAP 0.473693848 batch PCKh 0.375\n",
      "Trained batch 1091 batch loss 0.634317338 batch mAP 0.434082031 batch PCKh 0.375\n",
      "Trained batch 1092 batch loss 0.662316203 batch mAP 0.389648438 batch PCKh 0.0625\n",
      "Trained batch 1093 batch loss 0.631837904 batch mAP 0.465698242 batch PCKh 0.5625\n",
      "Trained batch 1094 batch loss 0.662579417 batch mAP 0.521759033 batch PCKh 0.3125\n",
      "Trained batch 1095 batch loss 0.645821095 batch mAP 0.563446045 batch PCKh 0.25\n",
      "Trained batch 1096 batch loss 0.644341707 batch mAP 0.403900146 batch PCKh 0.5625\n",
      "Trained batch 1097 batch loss 0.577262163 batch mAP 0.570373535 batch PCKh 0.25\n",
      "Trained batch 1098 batch loss 0.602025867 batch mAP 0.484039307 batch PCKh 0.1875\n",
      "Trained batch 1099 batch loss 0.614135742 batch mAP 0.495605469 batch PCKh 0.375\n",
      "Trained batch 1100 batch loss 0.625940561 batch mAP 0.485717773 batch PCKh 0.3125\n",
      "Trained batch 1101 batch loss 0.599892 batch mAP 0.428436279 batch PCKh 0.5625\n",
      "Trained batch 1102 batch loss 0.571261942 batch mAP 0.477417 batch PCKh 0.6875\n",
      "Trained batch 1103 batch loss 0.558250904 batch mAP 0.503143311 batch PCKh 0.3125\n",
      "Trained batch 1104 batch loss 0.603847802 batch mAP 0.534210205 batch PCKh 0.5625\n",
      "Trained batch 1105 batch loss 0.570967317 batch mAP 0.549285889 batch PCKh 0.6875\n",
      "Trained batch 1106 batch loss 0.539246559 batch mAP 0.546936035 batch PCKh 0.4375\n",
      "Trained batch 1107 batch loss 0.550064087 batch mAP 0.5753479 batch PCKh 0.4375\n",
      "Trained batch 1108 batch loss 0.625051 batch mAP 0.568206787 batch PCKh 0.5625\n",
      "Trained batch 1109 batch loss 0.626628339 batch mAP 0.543884277 batch PCKh 0.5625\n",
      "Trained batch 1110 batch loss 0.537086 batch mAP 0.567810059 batch PCKh 0.375\n",
      "Trained batch 1111 batch loss 0.583655953 batch mAP 0.557678223 batch PCKh 0.3125\n",
      "Trained batch 1112 batch loss 0.550778687 batch mAP 0.548217773 batch PCKh 0.5\n",
      "Trained batch 1113 batch loss 0.564560115 batch mAP 0.59753418 batch PCKh 0.1875\n",
      "Trained batch 1114 batch loss 0.55068481 batch mAP 0.556274414 batch PCKh 0.125\n",
      "Trained batch 1115 batch loss 0.506202281 batch mAP 0.624115 batch PCKh 0.5625\n",
      "Trained batch 1116 batch loss 0.538118839 batch mAP 0.583099365 batch PCKh 0\n",
      "Trained batch 1117 batch loss 0.532481551 batch mAP 0.524261475 batch PCKh 0.5625\n",
      "Trained batch 1118 batch loss 0.555899382 batch mAP 0.54296875 batch PCKh 0.6875\n",
      "Trained batch 1119 batch loss 0.568788171 batch mAP 0.523345947 batch PCKh 0.1875\n",
      "Trained batch 1120 batch loss 0.590329468 batch mAP 0.487640381 batch PCKh 0.5625\n",
      "Trained batch 1121 batch loss 0.59141773 batch mAP 0.488311768 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1122 batch loss 0.658859253 batch mAP 0.50012207 batch PCKh 0.375\n",
      "Trained batch 1123 batch loss 0.561363697 batch mAP 0.533355713 batch PCKh 0.4375\n",
      "Trained batch 1124 batch loss 0.546151757 batch mAP 0.487213135 batch PCKh 0.3125\n",
      "Trained batch 1125 batch loss 0.735968888 batch mAP 0.381439209 batch PCKh 0.8125\n",
      "Trained batch 1126 batch loss 0.574078 batch mAP 0.461456299 batch PCKh 0.6875\n",
      "Trained batch 1127 batch loss 0.681809 batch mAP 0.44744873 batch PCKh 0.875\n",
      "Trained batch 1128 batch loss 0.562442541 batch mAP 0.471191406 batch PCKh 0.5625\n",
      "Trained batch 1129 batch loss 0.605424881 batch mAP 0.48059082 batch PCKh 0.4375\n",
      "Trained batch 1130 batch loss 0.617816389 batch mAP 0.484466553 batch PCKh 0.875\n",
      "Trained batch 1131 batch loss 0.678312898 batch mAP 0.464874268 batch PCKh 0.875\n",
      "Trained batch 1132 batch loss 0.55525434 batch mAP 0.446411133 batch PCKh 0.375\n",
      "Trained batch 1133 batch loss 0.576985717 batch mAP 0.356658936 batch PCKh 0.3125\n",
      "Trained batch 1134 batch loss 0.492051482 batch mAP 0.321899414 batch PCKh 0.25\n",
      "Trained batch 1135 batch loss 0.493963242 batch mAP 0.264068604 batch PCKh 0.75\n",
      "Trained batch 1136 batch loss 0.453290462 batch mAP 0.230438232 batch PCKh 0.3125\n",
      "Trained batch 1137 batch loss 0.521905422 batch mAP 0.337554932 batch PCKh 0.3125\n",
      "Trained batch 1138 batch loss 0.574082732 batch mAP 0.350280762 batch PCKh 0.25\n",
      "Trained batch 1139 batch loss 0.534092486 batch mAP 0.451873779 batch PCKh 0.25\n",
      "Trained batch 1140 batch loss 0.547208548 batch mAP 0.488922119 batch PCKh 0.25\n",
      "Trained batch 1141 batch loss 0.600630403 batch mAP 0.564209 batch PCKh 0.25\n",
      "Trained batch 1142 batch loss 0.556726217 batch mAP 0.535522461 batch PCKh 0.4375\n",
      "Trained batch 1143 batch loss 0.554086328 batch mAP 0.555603 batch PCKh 0.25\n",
      "Trained batch 1144 batch loss 0.544315696 batch mAP 0.536529541 batch PCKh 0.0625\n",
      "Trained batch 1145 batch loss 0.626252 batch mAP 0.402801514 batch PCKh 0.5625\n",
      "Trained batch 1146 batch loss 0.559696257 batch mAP 0.567993164 batch PCKh 0.25\n",
      "Trained batch 1147 batch loss 0.644066691 batch mAP 0.581115723 batch PCKh 0.3125\n",
      "Trained batch 1148 batch loss 0.609956086 batch mAP 0.581481934 batch PCKh 0.625\n",
      "Trained batch 1149 batch loss 0.67572248 batch mAP 0.495697021 batch PCKh 0.125\n",
      "Trained batch 1150 batch loss 0.646420479 batch mAP 0.571655273 batch PCKh 0.3125\n",
      "Trained batch 1151 batch loss 0.588949919 batch mAP 0.453125 batch PCKh 0.125\n",
      "Trained batch 1152 batch loss 0.68256104 batch mAP 0.369018555 batch PCKh 0.75\n",
      "Trained batch 1153 batch loss 0.677403092 batch mAP 0.43057251 batch PCKh 0.5625\n",
      "Trained batch 1154 batch loss 0.605867326 batch mAP 0.451568604 batch PCKh 0.0625\n",
      "Trained batch 1155 batch loss 0.50458622 batch mAP 0.457611084 batch PCKh 0.125\n",
      "Trained batch 1156 batch loss 0.547989726 batch mAP 0.49597168 batch PCKh 0\n",
      "Trained batch 1157 batch loss 0.591751814 batch mAP 0.518035889 batch PCKh 0.25\n",
      "Trained batch 1158 batch loss 0.519447505 batch mAP 0.493774414 batch PCKh 0.625\n",
      "Trained batch 1159 batch loss 0.59267664 batch mAP 0.452941895 batch PCKh 0.625\n",
      "Trained batch 1160 batch loss 0.606153488 batch mAP 0.467559814 batch PCKh 0.625\n",
      "Trained batch 1161 batch loss 0.61574775 batch mAP 0.477783203 batch PCKh 0.625\n",
      "Trained batch 1162 batch loss 0.642034411 batch mAP 0.514343262 batch PCKh 0.4375\n",
      "Trained batch 1163 batch loss 0.667237103 batch mAP 0.474304199 batch PCKh 0.1875\n",
      "Trained batch 1164 batch loss 0.745297551 batch mAP 0.465393066 batch PCKh 0.125\n",
      "Trained batch 1165 batch loss 0.615583301 batch mAP 0.516693115 batch PCKh 0.375\n",
      "Trained batch 1166 batch loss 0.645552337 batch mAP 0.554046631 batch PCKh 0.0625\n",
      "Trained batch 1167 batch loss 0.657171845 batch mAP 0.583313 batch PCKh 0.3125\n",
      "Trained batch 1168 batch loss 0.632241964 batch mAP 0.520904541 batch PCKh 0.5\n",
      "Trained batch 1169 batch loss 0.536685228 batch mAP 0.518005371 batch PCKh 0.4375\n",
      "Trained batch 1170 batch loss 0.507394969 batch mAP 0.499298096 batch PCKh 0.0625\n",
      "Trained batch 1171 batch loss 0.597780347 batch mAP 0.410675049 batch PCKh 0.1875\n",
      "Trained batch 1172 batch loss 0.613705814 batch mAP 0.440490723 batch PCKh 0\n",
      "Trained batch 1173 batch loss 0.574807227 batch mAP 0.474884033 batch PCKh 0.5\n",
      "Trained batch 1174 batch loss 0.592101395 batch mAP 0.445007324 batch PCKh 0.4375\n",
      "Trained batch 1175 batch loss 0.601360381 batch mAP 0.528595 batch PCKh 0.125\n",
      "Trained batch 1176 batch loss 0.580177486 batch mAP 0.522003174 batch PCKh 0.4375\n",
      "Trained batch 1177 batch loss 0.640174031 batch mAP 0.47454834 batch PCKh 0.75\n",
      "Trained batch 1178 batch loss 0.660440207 batch mAP 0.518585205 batch PCKh 0.375\n",
      "Trained batch 1179 batch loss 0.687130094 batch mAP 0.427642822 batch PCKh 0.1875\n",
      "Trained batch 1180 batch loss 0.542327702 batch mAP 0.494934082 batch PCKh 0.125\n",
      "Trained batch 1181 batch loss 0.563243508 batch mAP 0.413330078 batch PCKh 0.25\n",
      "Trained batch 1182 batch loss 0.588377953 batch mAP 0.440856934 batch PCKh 0.625\n",
      "Trained batch 1183 batch loss 0.545105159 batch mAP 0.321411133 batch PCKh 0.3125\n",
      "Trained batch 1184 batch loss 0.575187087 batch mAP 0.358642578 batch PCKh 0.4375\n",
      "Trained batch 1185 batch loss 0.646017313 batch mAP 0.323486328 batch PCKh 0.0625\n",
      "Trained batch 1186 batch loss 0.521184504 batch mAP 0.398071289 batch PCKh 0.5\n",
      "Trained batch 1187 batch loss 0.607055902 batch mAP 0.397705078 batch PCKh 0.4375\n",
      "Trained batch 1188 batch loss 0.507796526 batch mAP 0.442199707 batch PCKh 0.5625\n",
      "Trained batch 1189 batch loss 0.518025875 batch mAP 0.511535645 batch PCKh 0.375\n",
      "Trained batch 1190 batch loss 0.500532687 batch mAP 0.524047852 batch PCKh 0.5\n",
      "Trained batch 1191 batch loss 0.478627712 batch mAP 0.565887451 batch PCKh 0.5625\n",
      "Trained batch 1192 batch loss 0.480129361 batch mAP 0.589660645 batch PCKh 0.5\n",
      "Trained batch 1193 batch loss 0.551394105 batch mAP 0.564941406 batch PCKh 0.375\n",
      "Trained batch 1194 batch loss 0.54915458 batch mAP 0.577972412 batch PCKh 0.75\n",
      "Trained batch 1195 batch loss 0.586225 batch mAP 0.525848389 batch PCKh 0.75\n",
      "Trained batch 1196 batch loss 0.572239578 batch mAP 0.532440186 batch PCKh 0.25\n",
      "Trained batch 1197 batch loss 0.530398 batch mAP 0.55255127 batch PCKh 0.6875\n",
      "Trained batch 1198 batch loss 0.549398661 batch mAP 0.507415771 batch PCKh 0.375\n",
      "Trained batch 1199 batch loss 0.501906574 batch mAP 0.561096191 batch PCKh 0.3125\n",
      "Trained batch 1200 batch loss 0.525993586 batch mAP 0.4972229 batch PCKh 0.1875\n",
      "Trained batch 1201 batch loss 0.532994032 batch mAP 0.554595947 batch PCKh 0.6875\n",
      "Trained batch 1202 batch loss 0.559274316 batch mAP 0.519134521 batch PCKh 0.875\n",
      "Trained batch 1203 batch loss 0.568040431 batch mAP 0.55267334 batch PCKh 0.3125\n",
      "Trained batch 1204 batch loss 0.509480059 batch mAP 0.539215088 batch PCKh 0.3125\n",
      "Trained batch 1205 batch loss 0.644779146 batch mAP 0.534423828 batch PCKh 0.1875\n",
      "Trained batch 1206 batch loss 0.598870754 batch mAP 0.562469482 batch PCKh 0.125\n",
      "Trained batch 1207 batch loss 0.667433858 batch mAP 0.531738281 batch PCKh 0.4375\n",
      "Trained batch 1208 batch loss 0.598047316 batch mAP 0.547668457 batch PCKh 0.4375\n",
      "Trained batch 1209 batch loss 0.73374331 batch mAP 0.526123047 batch PCKh 0\n",
      "Trained batch 1210 batch loss 0.641824126 batch mAP 0.550720215 batch PCKh 0.1875\n",
      "Trained batch 1211 batch loss 0.571201265 batch mAP 0.51260376 batch PCKh 0.1875\n",
      "Trained batch 1212 batch loss 0.671097 batch mAP 0.408752441 batch PCKh 0.3125\n",
      "Trained batch 1213 batch loss 0.605928659 batch mAP 0.371795654 batch PCKh 0.3125\n",
      "Trained batch 1214 batch loss 0.556854069 batch mAP 0.388946533 batch PCKh 0.0625\n",
      "Trained batch 1215 batch loss 0.636613369 batch mAP 0.470428467 batch PCKh 0.5\n",
      "Trained batch 1216 batch loss 0.646714509 batch mAP 0.455688477 batch PCKh 0.5625\n",
      "Trained batch 1217 batch loss 0.621267676 batch mAP 0.465454102 batch PCKh 0.375\n",
      "Trained batch 1218 batch loss 0.580444098 batch mAP 0.482879639 batch PCKh 0.4375\n",
      "Trained batch 1219 batch loss 0.582198381 batch mAP 0.414367676 batch PCKh 0.25\n",
      "Trained batch 1220 batch loss 0.712406635 batch mAP 0.483337402 batch PCKh 0.25\n",
      "Trained batch 1221 batch loss 0.621123 batch mAP 0.524292 batch PCKh 0.25\n",
      "Trained batch 1222 batch loss 0.587207496 batch mAP 0.535705566 batch PCKh 0.1875\n",
      "Trained batch 1223 batch loss 0.605911255 batch mAP 0.582672119 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1224 batch loss 0.657715738 batch mAP 0.538330078 batch PCKh 0.125\n",
      "Trained batch 1225 batch loss 0.785936356 batch mAP 0.45880127 batch PCKh 0\n",
      "Trained batch 1226 batch loss 0.634625793 batch mAP 0.548309326 batch PCKh 0.4375\n",
      "Trained batch 1227 batch loss 0.629903316 batch mAP 0.545471191 batch PCKh 0.5\n",
      "Trained batch 1228 batch loss 0.558340371 batch mAP 0.519928 batch PCKh 0.25\n",
      "Trained batch 1229 batch loss 0.56076026 batch mAP 0.492156982 batch PCKh 0.25\n",
      "Trained batch 1230 batch loss 0.637976 batch mAP 0.475494385 batch PCKh 0.3125\n",
      "Trained batch 1231 batch loss 0.692815661 batch mAP 0.455474854 batch PCKh 0.125\n",
      "Trained batch 1232 batch loss 0.723872125 batch mAP 0.475036621 batch PCKh 0.25\n",
      "Trained batch 1233 batch loss 0.651196182 batch mAP 0.474609375 batch PCKh 0.625\n",
      "Trained batch 1234 batch loss 0.6113922 batch mAP 0.489227295 batch PCKh 0.6875\n",
      "Trained batch 1235 batch loss 0.648684561 batch mAP 0.50177 batch PCKh 0.6875\n",
      "Trained batch 1236 batch loss 0.564616323 batch mAP 0.491516113 batch PCKh 0.4375\n",
      "Trained batch 1237 batch loss 0.632112503 batch mAP 0.483978271 batch PCKh 0.25\n",
      "Trained batch 1238 batch loss 0.584691703 batch mAP 0.490448 batch PCKh 0.125\n",
      "Trained batch 1239 batch loss 0.588413239 batch mAP 0.493682861 batch PCKh 0.625\n",
      "Trained batch 1240 batch loss 0.583031893 batch mAP 0.530792236 batch PCKh 0.625\n",
      "Trained batch 1241 batch loss 0.502349198 batch mAP 0.556671143 batch PCKh 0.5\n",
      "Trained batch 1242 batch loss 0.572276115 batch mAP 0.538970947 batch PCKh 0.25\n",
      "Trained batch 1243 batch loss 0.564586937 batch mAP 0.504303 batch PCKh 0.0625\n",
      "Trained batch 1244 batch loss 0.631734431 batch mAP 0.529327393 batch PCKh 0.5\n",
      "Trained batch 1245 batch loss 0.627768397 batch mAP 0.566345215 batch PCKh 0.6875\n",
      "Trained batch 1246 batch loss 0.575337291 batch mAP 0.573944092 batch PCKh 0.4375\n",
      "Trained batch 1247 batch loss 0.636144161 batch mAP 0.501037598 batch PCKh 0.5625\n",
      "Trained batch 1248 batch loss 0.593279123 batch mAP 0.506225586 batch PCKh 0.125\n",
      "Trained batch 1249 batch loss 0.560099 batch mAP 0.593841553 batch PCKh 0.4375\n",
      "Trained batch 1250 batch loss 0.528600276 batch mAP 0.566772461 batch PCKh 0.1875\n",
      "Trained batch 1251 batch loss 0.629021049 batch mAP 0.594451904 batch PCKh 0.75\n",
      "Trained batch 1252 batch loss 0.55397594 batch mAP 0.596954346 batch PCKh 0.25\n",
      "Trained batch 1253 batch loss 0.634288907 batch mAP 0.645294189 batch PCKh 0.25\n",
      "Trained batch 1254 batch loss 0.551613927 batch mAP 0.645507812 batch PCKh 0.5\n",
      "Trained batch 1255 batch loss 0.613458693 batch mAP 0.594421387 batch PCKh 0.25\n",
      "Trained batch 1256 batch loss 0.600472689 batch mAP 0.549499512 batch PCKh 0.375\n",
      "Trained batch 1257 batch loss 0.560371816 batch mAP 0.525817871 batch PCKh 0.1875\n",
      "Trained batch 1258 batch loss 0.548596144 batch mAP 0.551544189 batch PCKh 0.5625\n",
      "Trained batch 1259 batch loss 0.558336735 batch mAP 0.566650391 batch PCKh 0.8125\n",
      "Trained batch 1260 batch loss 0.624139071 batch mAP 0.560943604 batch PCKh 0.1875\n",
      "Trained batch 1261 batch loss 0.623781204 batch mAP 0.547699 batch PCKh 0.375\n",
      "Trained batch 1262 batch loss 0.671324492 batch mAP 0.555206299 batch PCKh 0.75\n",
      "Trained batch 1263 batch loss 0.623827636 batch mAP 0.482086182 batch PCKh 0.25\n",
      "Trained batch 1264 batch loss 0.579269052 batch mAP 0.52444458 batch PCKh 0.5\n",
      "Trained batch 1265 batch loss 0.677485168 batch mAP 0.557098389 batch PCKh 0.875\n",
      "Trained batch 1266 batch loss 0.578558147 batch mAP 0.602081299 batch PCKh 0.5\n",
      "Trained batch 1267 batch loss 0.719806612 batch mAP 0.537811279 batch PCKh 0.125\n",
      "Trained batch 1268 batch loss 0.716915607 batch mAP 0.530181885 batch PCKh 0.125\n",
      "Trained batch 1269 batch loss 0.669312477 batch mAP 0.514404297 batch PCKh 0.125\n",
      "Trained batch 1270 batch loss 0.698651075 batch mAP 0.470184326 batch PCKh 0.25\n",
      "Trained batch 1271 batch loss 0.731818318 batch mAP 0.447296143 batch PCKh 0.4375\n",
      "Trained batch 1272 batch loss 0.527533293 batch mAP 0.402709961 batch PCKh 0.5\n",
      "Trained batch 1273 batch loss 0.586971521 batch mAP 0.427856445 batch PCKh 0.875\n",
      "Trained batch 1274 batch loss 0.597874522 batch mAP 0.475494385 batch PCKh 0.6875\n",
      "Trained batch 1275 batch loss 0.585983157 batch mAP 0.346313477 batch PCKh 0.625\n",
      "Trained batch 1276 batch loss 0.471955687 batch mAP 0.382232666 batch PCKh 0.5\n",
      "Trained batch 1277 batch loss 0.453330636 batch mAP 0.375335693 batch PCKh 0\n",
      "Trained batch 1278 batch loss 0.438109577 batch mAP 0.382659912 batch PCKh 0\n",
      "Trained batch 1279 batch loss 0.516557 batch mAP 0.490142822 batch PCKh 0.5\n",
      "Trained batch 1280 batch loss 0.534589 batch mAP 0.524475098 batch PCKh 0.4375\n",
      "Trained batch 1281 batch loss 0.640006244 batch mAP 0.522094727 batch PCKh 0.6875\n",
      "Trained batch 1282 batch loss 0.585646152 batch mAP 0.535553 batch PCKh 0.375\n",
      "Trained batch 1283 batch loss 0.480720758 batch mAP 0.490203857 batch PCKh 0\n",
      "Trained batch 1284 batch loss 0.431955397 batch mAP 0.5390625 batch PCKh 0.125\n",
      "Trained batch 1285 batch loss 0.43183893 batch mAP 0.559936523 batch PCKh 0\n",
      "Trained batch 1286 batch loss 0.415190518 batch mAP 0.533569336 batch PCKh 0\n",
      "Trained batch 1287 batch loss 0.435675383 batch mAP 0.49810791 batch PCKh 0\n",
      "Trained batch 1288 batch loss 0.449683785 batch mAP 0.480438232 batch PCKh 0.5\n",
      "Trained batch 1289 batch loss 0.539881527 batch mAP 0.469177246 batch PCKh 0.0625\n",
      "Trained batch 1290 batch loss 0.5327 batch mAP 0.471801758 batch PCKh 0.625\n",
      "Trained batch 1291 batch loss 0.579498351 batch mAP 0.361999512 batch PCKh 0.5625\n",
      "Trained batch 1292 batch loss 0.587288439 batch mAP 0.401031494 batch PCKh 0.6875\n",
      "Trained batch 1293 batch loss 0.545048118 batch mAP 0.414489746 batch PCKh 0.6875\n",
      "Trained batch 1294 batch loss 0.559232414 batch mAP 0.474975586 batch PCKh 0.375\n",
      "Trained batch 1295 batch loss 0.630798817 batch mAP 0.469604492 batch PCKh 0.125\n",
      "Trained batch 1296 batch loss 0.65884012 batch mAP 0.456085205 batch PCKh 0.1875\n",
      "Trained batch 1297 batch loss 0.566243112 batch mAP 0.472747803 batch PCKh 0.375\n",
      "Trained batch 1298 batch loss 0.606360197 batch mAP 0.499786377 batch PCKh 0.25\n",
      "Trained batch 1299 batch loss 0.704635 batch mAP 0.465057373 batch PCKh 0.3125\n",
      "Trained batch 1300 batch loss 0.566047788 batch mAP 0.439697266 batch PCKh 0.375\n",
      "Trained batch 1301 batch loss 0.592264712 batch mAP 0.388580322 batch PCKh 0.3125\n",
      "Trained batch 1302 batch loss 0.583149731 batch mAP 0.25579834 batch PCKh 0.3125\n",
      "Trained batch 1303 batch loss 0.650194407 batch mAP 0.377655029 batch PCKh 0.1875\n",
      "Trained batch 1304 batch loss 0.547119319 batch mAP 0.332244873 batch PCKh 0.25\n",
      "Trained batch 1305 batch loss 0.544511199 batch mAP 0.370117188 batch PCKh 0.4375\n",
      "Trained batch 1306 batch loss 0.551892 batch mAP 0.421691895 batch PCKh 0.5\n",
      "Trained batch 1307 batch loss 0.504942954 batch mAP 0.497039795 batch PCKh 0.3125\n",
      "Trained batch 1308 batch loss 0.512606323 batch mAP 0.515167236 batch PCKh 0.3125\n",
      "Trained batch 1309 batch loss 0.569516957 batch mAP 0.493103027 batch PCKh 0.25\n",
      "Trained batch 1310 batch loss 0.503579736 batch mAP 0.594360352 batch PCKh 0.4375\n",
      "Trained batch 1311 batch loss 0.567965269 batch mAP 0.572601318 batch PCKh 0.25\n",
      "Trained batch 1312 batch loss 0.560804 batch mAP 0.627838135 batch PCKh 0.3125\n",
      "Trained batch 1313 batch loss 0.428456366 batch mAP 0.591766357 batch PCKh 0.3125\n",
      "Trained batch 1314 batch loss 0.51208204 batch mAP 0.612457275 batch PCKh 0.25\n",
      "Trained batch 1315 batch loss 0.515309572 batch mAP 0.571868896 batch PCKh 0.25\n",
      "Trained batch 1316 batch loss 0.585452259 batch mAP 0.482849121 batch PCKh 0.3125\n",
      "Trained batch 1317 batch loss 0.529812634 batch mAP 0.498657227 batch PCKh 0.875\n",
      "Trained batch 1318 batch loss 0.589950562 batch mAP 0.554962158 batch PCKh 0.5625\n",
      "Trained batch 1319 batch loss 0.484606415 batch mAP 0.553314209 batch PCKh 0.6875\n",
      "Trained batch 1320 batch loss 0.524944067 batch mAP 0.580413818 batch PCKh 0.1875\n",
      "Trained batch 1321 batch loss 0.574485183 batch mAP 0.585907 batch PCKh 0.5625\n",
      "Trained batch 1322 batch loss 0.517897367 batch mAP 0.605834961 batch PCKh 0.4375\n",
      "Trained batch 1323 batch loss 0.512815535 batch mAP 0.578765869 batch PCKh 0\n",
      "Trained batch 1324 batch loss 0.563273549 batch mAP 0.551208496 batch PCKh 0.125\n",
      "Trained batch 1325 batch loss 0.571981132 batch mAP 0.579406738 batch PCKh 0.5625\n",
      "Trained batch 1326 batch loss 0.48677218 batch mAP 0.569213867 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1327 batch loss 0.565137267 batch mAP 0.512542725 batch PCKh 0.5625\n",
      "Trained batch 1328 batch loss 0.665470243 batch mAP 0.45980835 batch PCKh 0.6875\n",
      "Trained batch 1329 batch loss 0.767135561 batch mAP 0.309753418 batch PCKh 0.1875\n",
      "Trained batch 1330 batch loss 0.688550651 batch mAP 0.467773438 batch PCKh 0.8125\n",
      "Trained batch 1331 batch loss 0.569430172 batch mAP 0.543884277 batch PCKh 0\n",
      "Trained batch 1332 batch loss 0.60906446 batch mAP 0.560180664 batch PCKh 0.25\n",
      "Trained batch 1333 batch loss 0.560164571 batch mAP 0.526763916 batch PCKh 0.375\n",
      "Trained batch 1334 batch loss 0.609989107 batch mAP 0.51751709 batch PCKh 0.25\n",
      "Trained batch 1335 batch loss 0.653994083 batch mAP 0.464691162 batch PCKh 0\n",
      "Trained batch 1336 batch loss 0.654878199 batch mAP 0.443634033 batch PCKh 0.125\n",
      "Trained batch 1337 batch loss 0.672828674 batch mAP 0.336730957 batch PCKh 0.1875\n",
      "Trained batch 1338 batch loss 0.631668329 batch mAP 0.304199219 batch PCKh 0.1875\n",
      "Trained batch 1339 batch loss 0.545502841 batch mAP 0.127105713 batch PCKh 0.5625\n",
      "Trained batch 1340 batch loss 0.717049241 batch mAP 0.429107666 batch PCKh 0.0625\n",
      "Trained batch 1341 batch loss 0.634577096 batch mAP 0.403045654 batch PCKh 0.25\n",
      "Trained batch 1342 batch loss 0.528763771 batch mAP 0.410888672 batch PCKh 0.4375\n",
      "Trained batch 1343 batch loss 0.547279 batch mAP 0.473510742 batch PCKh 0.75\n",
      "Trained batch 1344 batch loss 0.541210473 batch mAP 0.444152832 batch PCKh 0.75\n",
      "Trained batch 1345 batch loss 0.637294412 batch mAP 0.456207275 batch PCKh 0.6875\n",
      "Trained batch 1346 batch loss 0.575440526 batch mAP 0.507476807 batch PCKh 0.6875\n",
      "Trained batch 1347 batch loss 0.626468301 batch mAP 0.550842285 batch PCKh 0.375\n",
      "Trained batch 1348 batch loss 0.593644 batch mAP 0.412139893 batch PCKh 0.625\n",
      "Trained batch 1349 batch loss 0.526717246 batch mAP 0.344909668 batch PCKh 0.3125\n",
      "Trained batch 1350 batch loss 0.533761919 batch mAP 0.461700439 batch PCKh 0.5\n",
      "Trained batch 1351 batch loss 0.554700732 batch mAP 0.353790283 batch PCKh 0.6875\n",
      "Trained batch 1352 batch loss 0.520787358 batch mAP 0.445098877 batch PCKh 0.1875\n",
      "Trained batch 1353 batch loss 0.480573922 batch mAP 0.551879883 batch PCKh 0.5\n",
      "Trained batch 1354 batch loss 0.528222084 batch mAP 0.45224 batch PCKh 0.6875\n",
      "Trained batch 1355 batch loss 0.488964915 batch mAP 0.558502197 batch PCKh 0.5625\n",
      "Trained batch 1356 batch loss 0.565144062 batch mAP 0.575408936 batch PCKh 0.0625\n",
      "Trained batch 1357 batch loss 0.741394043 batch mAP 0.479248047 batch PCKh 0\n",
      "Trained batch 1358 batch loss 0.760464787 batch mAP 0.444732666 batch PCKh 0.3125\n",
      "Trained batch 1359 batch loss 0.668224216 batch mAP 0.561126709 batch PCKh 0.125\n",
      "Trained batch 1360 batch loss 0.570986 batch mAP 0.514678955 batch PCKh 0.5\n",
      "Trained batch 1361 batch loss 0.577777863 batch mAP 0.431976318 batch PCKh 0.4375\n",
      "Trained batch 1362 batch loss 0.537475 batch mAP 0.306976318 batch PCKh 0.4375\n",
      "Trained batch 1363 batch loss 0.625105858 batch mAP 0.322509766 batch PCKh 0.375\n",
      "Trained batch 1364 batch loss 0.503808439 batch mAP 0.131774902 batch PCKh 0.375\n",
      "Trained batch 1365 batch loss 0.595929921 batch mAP 0.359191895 batch PCKh 0.375\n",
      "Trained batch 1366 batch loss 0.650288463 batch mAP 0.451202393 batch PCKh 0.125\n",
      "Trained batch 1367 batch loss 0.587577045 batch mAP 0.479400635 batch PCKh 0.125\n",
      "Trained batch 1368 batch loss 0.513149798 batch mAP 0.451568604 batch PCKh 0.625\n",
      "Trained batch 1369 batch loss 0.60083431 batch mAP 0.498138428 batch PCKh 0.5625\n",
      "Trained batch 1370 batch loss 0.542978764 batch mAP 0.510162354 batch PCKh 0.375\n",
      "Trained batch 1371 batch loss 0.573645771 batch mAP 0.553649902 batch PCKh 0.6875\n",
      "Trained batch 1372 batch loss 0.504905403 batch mAP 0.540130615 batch PCKh 0.5\n",
      "Trained batch 1373 batch loss 0.653558493 batch mAP 0.591369629 batch PCKh 0.25\n",
      "Trained batch 1374 batch loss 0.57762152 batch mAP 0.56060791 batch PCKh 0.6875\n",
      "Trained batch 1375 batch loss 0.598781109 batch mAP 0.535736084 batch PCKh 0\n",
      "Trained batch 1376 batch loss 0.59329474 batch mAP 0.594116211 batch PCKh 0.875\n",
      "Trained batch 1377 batch loss 0.640264571 batch mAP 0.579772949 batch PCKh 0.3125\n",
      "Trained batch 1378 batch loss 0.569511652 batch mAP 0.528961182 batch PCKh 0.625\n",
      "Trained batch 1379 batch loss 0.568726897 batch mAP 0.602264404 batch PCKh 0.3125\n",
      "Trained batch 1380 batch loss 0.678302467 batch mAP 0.526977539 batch PCKh 0.25\n",
      "Trained batch 1381 batch loss 0.653857231 batch mAP 0.530303955 batch PCKh 0.5\n",
      "Trained batch 1382 batch loss 0.512889385 batch mAP 0.598266602 batch PCKh 0.1875\n",
      "Trained batch 1383 batch loss 0.559317946 batch mAP 0.560882568 batch PCKh 0.5\n",
      "Trained batch 1384 batch loss 0.492964923 batch mAP 0.646514893 batch PCKh 0.25\n",
      "Trained batch 1385 batch loss 0.58866781 batch mAP 0.632202148 batch PCKh 0.25\n",
      "Trained batch 1386 batch loss 0.595641732 batch mAP 0.568908691 batch PCKh 0.0625\n",
      "Trained batch 1387 batch loss 0.543431163 batch mAP 0.636047363 batch PCKh 0.4375\n",
      "Trained batch 1388 batch loss 0.616467237 batch mAP 0.607055664 batch PCKh 0.4375\n",
      "Trained batch 1389 batch loss 0.777381182 batch mAP 0.507446289 batch PCKh 0.3125\n",
      "Trained batch 1390 batch loss 0.757517219 batch mAP 0.53112793 batch PCKh 0\n",
      "Trained batch 1391 batch loss 0.634048104 batch mAP 0.565979 batch PCKh 0.25\n",
      "Trained batch 1392 batch loss 0.554019392 batch mAP 0.533325195 batch PCKh 0.6875\n",
      "Trained batch 1393 batch loss 0.659076571 batch mAP 0.491821289 batch PCKh 0.8125\n",
      "Trained batch 1394 batch loss 0.606822252 batch mAP 0.5128479 batch PCKh 0.1875\n",
      "Trained batch 1395 batch loss 0.575427711 batch mAP 0.475830078 batch PCKh 0.75\n",
      "Trained batch 1396 batch loss 0.763544 batch mAP 0.441009521 batch PCKh 0.125\n",
      "Trained batch 1397 batch loss 0.646971226 batch mAP 0.41696167 batch PCKh 0.1875\n",
      "Trained batch 1398 batch loss 0.660271645 batch mAP 0.372894287 batch PCKh 0.1875\n",
      "Trained batch 1399 batch loss 0.654501677 batch mAP 0.415466309 batch PCKh 0.3125\n",
      "Trained batch 1400 batch loss 0.586105347 batch mAP 0.349731445 batch PCKh 0.75\n",
      "Trained batch 1401 batch loss 0.549671412 batch mAP 0.253753662 batch PCKh 0\n",
      "Trained batch 1402 batch loss 0.574202538 batch mAP 0.221954346 batch PCKh 0.375\n",
      "Trained batch 1403 batch loss 0.570434451 batch mAP 0.289978027 batch PCKh 0.25\n",
      "Trained batch 1404 batch loss 0.580328941 batch mAP 0.386566162 batch PCKh 0.25\n",
      "Trained batch 1405 batch loss 0.582576692 batch mAP 0.48349 batch PCKh 0.25\n",
      "Trained batch 1406 batch loss 0.567884684 batch mAP 0.494445801 batch PCKh 0.625\n",
      "Trained batch 1407 batch loss 0.57384 batch mAP 0.476287842 batch PCKh 0\n",
      "Trained batch 1408 batch loss 0.578176916 batch mAP 0.502288818 batch PCKh 0.625\n",
      "Trained batch 1409 batch loss 0.693699241 batch mAP 0.480621338 batch PCKh 0.75\n",
      "Trained batch 1410 batch loss 0.637733161 batch mAP 0.471618652 batch PCKh 0.625\n",
      "Trained batch 1411 batch loss 0.584259093 batch mAP 0.552093506 batch PCKh 0.75\n",
      "Trained batch 1412 batch loss 0.637608767 batch mAP 0.520507812 batch PCKh 0.75\n",
      "Trained batch 1413 batch loss 0.678231359 batch mAP 0.520050049 batch PCKh 0.625\n",
      "Trained batch 1414 batch loss 0.648504674 batch mAP 0.537475586 batch PCKh 0.375\n",
      "Trained batch 1415 batch loss 0.656488121 batch mAP 0.416717529 batch PCKh 0.3125\n",
      "Trained batch 1416 batch loss 0.588794947 batch mAP 0.501251221 batch PCKh 0.625\n",
      "Trained batch 1417 batch loss 0.551507235 batch mAP 0.519439697 batch PCKh 0.3125\n",
      "Trained batch 1418 batch loss 0.761707902 batch mAP 0.317718506 batch PCKh 0.0625\n",
      "Trained batch 1419 batch loss 0.652102828 batch mAP 0.475067139 batch PCKh 0.625\n",
      "Trained batch 1420 batch loss 0.560617924 batch mAP 0.526947 batch PCKh 0.0625\n",
      "Trained batch 1421 batch loss 0.490824878 batch mAP 0.533874512 batch PCKh 0.5\n",
      "Trained batch 1422 batch loss 0.542487204 batch mAP 0.502807617 batch PCKh 0.375\n",
      "Trained batch 1423 batch loss 0.519895 batch mAP 0.543823242 batch PCKh 0.375\n",
      "Trained batch 1424 batch loss 0.519971609 batch mAP 0.50769043 batch PCKh 0.625\n",
      "Trained batch 1425 batch loss 0.541204333 batch mAP 0.510681152 batch PCKh 0.3125\n",
      "Trained batch 1426 batch loss 0.631586075 batch mAP 0.496795654 batch PCKh 0.4375\n",
      "Trained batch 1427 batch loss 0.626842439 batch mAP 0.506164551 batch PCKh 0.625\n",
      "Trained batch 1428 batch loss 0.700133085 batch mAP 0.401611328 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1429 batch loss 0.535892248 batch mAP 0.540496826 batch PCKh 0.125\n",
      "Trained batch 1430 batch loss 0.596382737 batch mAP 0.547607422 batch PCKh 0.25\n",
      "Trained batch 1431 batch loss 0.583993673 batch mAP 0.5050354 batch PCKh 0.1875\n",
      "Trained batch 1432 batch loss 0.616225362 batch mAP 0.515197754 batch PCKh 0\n",
      "Trained batch 1433 batch loss 0.678820252 batch mAP 0.502288818 batch PCKh 0.8125\n",
      "Trained batch 1434 batch loss 0.595460892 batch mAP 0.441040039 batch PCKh 0.3125\n",
      "Trained batch 1435 batch loss 0.560402095 batch mAP 0.493621826 batch PCKh 0.6875\n",
      "Trained batch 1436 batch loss 0.608853936 batch mAP 0.386810303 batch PCKh 0.3125\n",
      "Trained batch 1437 batch loss 0.61579895 batch mAP 0.449676514 batch PCKh 0.625\n",
      "Trained batch 1438 batch loss 0.574197769 batch mAP 0.401275635 batch PCKh 0.375\n",
      "Trained batch 1439 batch loss 0.709746063 batch mAP 0.347747803 batch PCKh 0.0625\n",
      "Trained batch 1440 batch loss 0.546288967 batch mAP 0.311309814 batch PCKh 0.1875\n",
      "Trained batch 1441 batch loss 0.558785319 batch mAP 0.251098633 batch PCKh 0.5\n",
      "Trained batch 1442 batch loss 0.499959528 batch mAP 0.38092041 batch PCKh 0.75\n",
      "Trained batch 1443 batch loss 0.605571628 batch mAP 0.398101807 batch PCKh 0.375\n",
      "Trained batch 1444 batch loss 0.607163 batch mAP 0.374511719 batch PCKh 0.3125\n",
      "Trained batch 1445 batch loss 0.538801551 batch mAP 0.460693359 batch PCKh 0.25\n",
      "Trained batch 1446 batch loss 0.561015368 batch mAP 0.49105835 batch PCKh 0.5\n",
      "Trained batch 1447 batch loss 0.643191397 batch mAP 0.403259277 batch PCKh 0.625\n",
      "Trained batch 1448 batch loss 0.584783673 batch mAP 0.481231689 batch PCKh 0.1875\n",
      "Trained batch 1449 batch loss 0.634537101 batch mAP 0.459198 batch PCKh 0.125\n",
      "Trained batch 1450 batch loss 0.590752125 batch mAP 0.449615479 batch PCKh 0\n",
      "Trained batch 1451 batch loss 0.531958342 batch mAP 0.473876953 batch PCKh 0.625\n",
      "Trained batch 1452 batch loss 0.586233556 batch mAP 0.476989746 batch PCKh 0.8125\n",
      "Trained batch 1453 batch loss 0.627426744 batch mAP 0.466705322 batch PCKh 0.6875\n",
      "Trained batch 1454 batch loss 0.632945597 batch mAP 0.483123779 batch PCKh 0.625\n",
      "Trained batch 1455 batch loss 0.667234242 batch mAP 0.443145752 batch PCKh 0.25\n",
      "Trained batch 1456 batch loss 0.576231658 batch mAP 0.404571533 batch PCKh 0.5625\n",
      "Trained batch 1457 batch loss 0.530767679 batch mAP 0.399780273 batch PCKh 0.5625\n",
      "Trained batch 1458 batch loss 0.508112192 batch mAP 0.419158936 batch PCKh 0.875\n",
      "Trained batch 1459 batch loss 0.570672452 batch mAP 0.410705566 batch PCKh 0.375\n",
      "Trained batch 1460 batch loss 0.580453336 batch mAP 0.414245605 batch PCKh 0.625\n",
      "Trained batch 1461 batch loss 0.598117888 batch mAP 0.414855957 batch PCKh 0.5\n",
      "Trained batch 1462 batch loss 0.654046059 batch mAP 0.378723145 batch PCKh 0.4375\n",
      "Trained batch 1463 batch loss 0.601864696 batch mAP 0.453216553 batch PCKh 0.1875\n",
      "Trained batch 1464 batch loss 0.542023301 batch mAP 0.500518799 batch PCKh 0.125\n",
      "Trained batch 1465 batch loss 0.623671353 batch mAP 0.488128662 batch PCKh 0.125\n",
      "Trained batch 1466 batch loss 0.583709538 batch mAP 0.503265381 batch PCKh 0.25\n",
      "Trained batch 1467 batch loss 0.663310528 batch mAP 0.491210938 batch PCKh 0.5625\n",
      "Trained batch 1468 batch loss 0.542644739 batch mAP 0.535217285 batch PCKh 0.25\n",
      "Trained batch 1469 batch loss 0.529332876 batch mAP 0.534210205 batch PCKh 0.8125\n",
      "Trained batch 1470 batch loss 0.491065025 batch mAP 0.521972656 batch PCKh 0.625\n",
      "Trained batch 1471 batch loss 0.477772504 batch mAP 0.490386963 batch PCKh 0.375\n",
      "Trained batch 1472 batch loss 0.603259385 batch mAP 0.49798584 batch PCKh 0\n",
      "Trained batch 1473 batch loss 0.659799337 batch mAP 0.4034729 batch PCKh 0.1875\n",
      "Trained batch 1474 batch loss 0.602995157 batch mAP 0.476409912 batch PCKh 0.125\n",
      "Trained batch 1475 batch loss 0.618683338 batch mAP 0.514038086 batch PCKh 0.4375\n",
      "Trained batch 1476 batch loss 0.653196 batch mAP 0.463562 batch PCKh 0.1875\n",
      "Trained batch 1477 batch loss 0.574691415 batch mAP 0.524871826 batch PCKh 0.1875\n",
      "Trained batch 1478 batch loss 0.619520545 batch mAP 0.418273926 batch PCKh 0.4375\n",
      "Trained batch 1479 batch loss 0.716216683 batch mAP 0.415161133 batch PCKh 0.3125\n",
      "Trained batch 1480 batch loss 0.553456604 batch mAP 0.49029541 batch PCKh 0.25\n",
      "Trained batch 1481 batch loss 0.457452893 batch mAP 0.543243408 batch PCKh 0.3125\n",
      "Trained batch 1482 batch loss 0.591235876 batch mAP 0.525115967 batch PCKh 0.5\n",
      "Trained batch 1483 batch loss 0.516933918 batch mAP 0.529815674 batch PCKh 0.3125\n",
      "Trained batch 1484 batch loss 0.549779177 batch mAP 0.53692627 batch PCKh 0.8125\n",
      "Trained batch 1485 batch loss 0.537279 batch mAP 0.451171875 batch PCKh 0.25\n",
      "Trained batch 1486 batch loss 0.480420709 batch mAP 0.475158691 batch PCKh 0.375\n",
      "Trained batch 1487 batch loss 0.557539344 batch mAP 0.557495117 batch PCKh 0.3125\n",
      "Trained batch 1488 batch loss 0.558699 batch mAP 0.573730469 batch PCKh 0.6875\n",
      "Trained batch 1489 batch loss 0.504258752 batch mAP 0.581176758 batch PCKh 0.4375\n",
      "Trained batch 1490 batch loss 0.471773088 batch mAP 0.620910645 batch PCKh 0.75\n",
      "Trained batch 1491 batch loss 0.596873164 batch mAP 0.602264404 batch PCKh 0.8125\n",
      "Trained batch 1492 batch loss 0.527510345 batch mAP 0.579772949 batch PCKh 0.25\n",
      "Trained batch 1493 batch loss 0.648208082 batch mAP 0.541412354 batch PCKh 0.1875\n",
      "Trained batch 1494 batch loss 0.512902141 batch mAP 0.551391602 batch PCKh 0.375\n",
      "Trained batch 1495 batch loss 0.613388598 batch mAP 0.540435791 batch PCKh 0\n",
      "Trained batch 1496 batch loss 0.620583177 batch mAP 0.553070068 batch PCKh 0.6875\n",
      "Trained batch 1497 batch loss 0.481333405 batch mAP 0.542572 batch PCKh 0.25\n",
      "Trained batch 1498 batch loss 0.542905271 batch mAP 0.556396484 batch PCKh 0\n",
      "Trained batch 1499 batch loss 0.488069624 batch mAP 0.50567627 batch PCKh 0.0625\n",
      "Trained batch 1500 batch loss 0.579575479 batch mAP 0.514434814 batch PCKh 0\n",
      "Trained batch 1501 batch loss 0.665986061 batch mAP 0.481781 batch PCKh 0.625\n",
      "Trained batch 1502 batch loss 0.566909075 batch mAP 0.526824951 batch PCKh 0\n",
      "Trained batch 1503 batch loss 0.634636819 batch mAP 0.554595947 batch PCKh 0.5\n",
      "Trained batch 1504 batch loss 0.629915893 batch mAP 0.531860352 batch PCKh 0.4375\n",
      "Trained batch 1505 batch loss 0.566414475 batch mAP 0.532928467 batch PCKh 0.1875\n",
      "Trained batch 1506 batch loss 0.618081748 batch mAP 0.527435303 batch PCKh 0\n",
      "Trained batch 1507 batch loss 0.621933818 batch mAP 0.527587891 batch PCKh 0.3125\n",
      "Trained batch 1508 batch loss 0.663246274 batch mAP 0.494171143 batch PCKh 0\n",
      "Trained batch 1509 batch loss 0.57691437 batch mAP 0.481506348 batch PCKh 0.25\n",
      "Trained batch 1510 batch loss 0.569353342 batch mAP 0.51159668 batch PCKh 0.3125\n",
      "Trained batch 1511 batch loss 0.632091105 batch mAP 0.407531738 batch PCKh 0\n",
      "Trained batch 1512 batch loss 0.619050503 batch mAP 0.45892334 batch PCKh 0.8125\n",
      "Trained batch 1513 batch loss 0.525595427 batch mAP 0.494567871 batch PCKh 0.5\n",
      "Trained batch 1514 batch loss 0.66924119 batch mAP 0.453430176 batch PCKh 0.375\n",
      "Trained batch 1515 batch loss 0.573139846 batch mAP 0.489532471 batch PCKh 0.625\n",
      "Trained batch 1516 batch loss 0.568924308 batch mAP 0.477630615 batch PCKh 0.5625\n",
      "Trained batch 1517 batch loss 0.588481069 batch mAP 0.507110596 batch PCKh 0.3125\n",
      "Trained batch 1518 batch loss 0.501235068 batch mAP 0.546875 batch PCKh 0.3125\n",
      "Trained batch 1519 batch loss 0.643528163 batch mAP 0.514556885 batch PCKh 0.4375\n",
      "Trained batch 1520 batch loss 0.515508294 batch mAP 0.533325195 batch PCKh 0.5625\n",
      "Trained batch 1521 batch loss 0.66864 batch mAP 0.484008789 batch PCKh 0.8125\n",
      "Trained batch 1522 batch loss 0.614169955 batch mAP 0.512420654 batch PCKh 0.5\n",
      "Trained batch 1523 batch loss 0.591432571 batch mAP 0.471221924 batch PCKh 0.5625\n",
      "Trained batch 1524 batch loss 0.614542544 batch mAP 0.465179443 batch PCKh 0.5\n",
      "Trained batch 1525 batch loss 0.613517702 batch mAP 0.422424316 batch PCKh 0.625\n",
      "Trained batch 1526 batch loss 0.571371436 batch mAP 0.417755127 batch PCKh 0.0625\n",
      "Trained batch 1527 batch loss 0.668564439 batch mAP 0.394195557 batch PCKh 0.5625\n",
      "Trained batch 1528 batch loss 0.539739311 batch mAP 0.424713135 batch PCKh 0.5625\n",
      "Trained batch 1529 batch loss 0.533072 batch mAP 0.439147949 batch PCKh 0.1875\n",
      "Trained batch 1530 batch loss 0.545856535 batch mAP 0.447998047 batch PCKh 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1531 batch loss 0.493524462 batch mAP 0.426147461 batch PCKh 0.5625\n",
      "Trained batch 1532 batch loss 0.45080924 batch mAP 0.22845459 batch PCKh 0.6875\n",
      "Trained batch 1533 batch loss 0.679556549 batch mAP 0.408813477 batch PCKh 0.375\n",
      "Trained batch 1534 batch loss 0.663128078 batch mAP 0.42199707 batch PCKh 0.375\n",
      "Trained batch 1535 batch loss 0.605962515 batch mAP 0.40222168 batch PCKh 0.25\n",
      "Trained batch 1536 batch loss 0.659182489 batch mAP 0.429016113 batch PCKh 0.6875\n",
      "Trained batch 1537 batch loss 0.701424599 batch mAP 0.405517578 batch PCKh 0\n",
      "Trained batch 1538 batch loss 0.672677696 batch mAP 0.340301514 batch PCKh 0.3125\n",
      "Trained batch 1539 batch loss 0.593904555 batch mAP 0.432067871 batch PCKh 0.5625\n",
      "Trained batch 1540 batch loss 0.58394897 batch mAP 0.493865967 batch PCKh 0.375\n",
      "Trained batch 1541 batch loss 0.733781 batch mAP 0.441497803 batch PCKh 0\n",
      "Trained batch 1542 batch loss 0.672223449 batch mAP 0.459655762 batch PCKh 0.375\n",
      "Trained batch 1543 batch loss 0.614873648 batch mAP 0.531890869 batch PCKh 0.3125\n",
      "Trained batch 1544 batch loss 0.595330119 batch mAP 0.515014648 batch PCKh 0\n",
      "Trained batch 1545 batch loss 0.653972387 batch mAP 0.514099121 batch PCKh 0.125\n",
      "Trained batch 1546 batch loss 0.625257194 batch mAP 0.492248535 batch PCKh 0.375\n",
      "Trained batch 1547 batch loss 0.588236749 batch mAP 0.507537842 batch PCKh 0.4375\n",
      "Trained batch 1548 batch loss 0.670166254 batch mAP 0.41986084 batch PCKh 0.5\n",
      "Trained batch 1549 batch loss 0.586551368 batch mAP 0.458007812 batch PCKh 0.8125\n",
      "Trained batch 1550 batch loss 0.612644553 batch mAP 0.474273682 batch PCKh 0.25\n",
      "Trained batch 1551 batch loss 0.638851225 batch mAP 0.495574951 batch PCKh 0.625\n",
      "Trained batch 1552 batch loss 0.649194598 batch mAP 0.479431152 batch PCKh 0.375\n",
      "Trained batch 1553 batch loss 0.605125487 batch mAP 0.442810059 batch PCKh 0.25\n",
      "Trained batch 1554 batch loss 0.680461049 batch mAP 0.439208984 batch PCKh 0.3125\n",
      "Trained batch 1555 batch loss 0.627948642 batch mAP 0.443756104 batch PCKh 0.375\n",
      "Trained batch 1556 batch loss 0.638545275 batch mAP 0.44921875 batch PCKh 0.875\n",
      "Trained batch 1557 batch loss 0.650111794 batch mAP 0.457214355 batch PCKh 0.4375\n",
      "Trained batch 1558 batch loss 0.613448739 batch mAP 0.420501709 batch PCKh 0.1875\n",
      "Trained batch 1559 batch loss 0.610880136 batch mAP 0.422546387 batch PCKh 0.0625\n",
      "Trained batch 1560 batch loss 0.59908551 batch mAP 0.440734863 batch PCKh 0.5625\n",
      "Trained batch 1561 batch loss 0.666172147 batch mAP 0.444030762 batch PCKh 0.6875\n",
      "Trained batch 1562 batch loss 0.576814294 batch mAP 0.401000977 batch PCKh 0.125\n",
      "Trained batch 1563 batch loss 0.544760346 batch mAP 0.468292236 batch PCKh 0.625\n",
      "Trained batch 1564 batch loss 0.550794959 batch mAP 0.456695557 batch PCKh 0.6875\n",
      "Trained batch 1565 batch loss 0.510870814 batch mAP 0.465484619 batch PCKh 0.3125\n",
      "Trained batch 1566 batch loss 0.510926545 batch mAP 0.513092041 batch PCKh 0.625\n",
      "Trained batch 1567 batch loss 0.606115937 batch mAP 0.47744751 batch PCKh 0.25\n",
      "Trained batch 1568 batch loss 0.585330367 batch mAP 0.438262939 batch PCKh 0.5\n",
      "Trained batch 1569 batch loss 0.579672873 batch mAP 0.536224365 batch PCKh 0.3125\n",
      "Trained batch 1570 batch loss 0.614135623 batch mAP 0.50012207 batch PCKh 0.125\n",
      "Trained batch 1571 batch loss 0.528995454 batch mAP 0.494628906 batch PCKh 0.5625\n",
      "Trained batch 1572 batch loss 0.570341766 batch mAP 0.504821777 batch PCKh 0.5625\n",
      "Trained batch 1573 batch loss 0.61463356 batch mAP 0.491424561 batch PCKh 0.75\n",
      "Trained batch 1574 batch loss 0.663590372 batch mAP 0.469360352 batch PCKh 0.625\n",
      "Trained batch 1575 batch loss 0.57396251 batch mAP 0.459259033 batch PCKh 0\n",
      "Trained batch 1576 batch loss 0.68022 batch mAP 0.436767578 batch PCKh 0.0625\n",
      "Trained batch 1577 batch loss 0.652209699 batch mAP 0.489990234 batch PCKh 0.8125\n",
      "Trained batch 1578 batch loss 0.624834478 batch mAP 0.524841309 batch PCKh 0.0625\n",
      "Trained batch 1579 batch loss 0.644801676 batch mAP 0.517974854 batch PCKh 0.5625\n",
      "Trained batch 1580 batch loss 0.659646153 batch mAP 0.500396729 batch PCKh 0.375\n",
      "Trained batch 1581 batch loss 0.589803278 batch mAP 0.53427124 batch PCKh 0.25\n",
      "Trained batch 1582 batch loss 0.580890298 batch mAP 0.550201416 batch PCKh 0.3125\n",
      "Trained batch 1583 batch loss 0.563322902 batch mAP 0.547393799 batch PCKh 0.625\n",
      "Trained batch 1584 batch loss 0.578773499 batch mAP 0.542205811 batch PCKh 0.1875\n",
      "Trained batch 1585 batch loss 0.563918114 batch mAP 0.482818604 batch PCKh 0.5625\n",
      "Trained batch 1586 batch loss 0.483519047 batch mAP 0.514648438 batch PCKh 0.75\n",
      "Trained batch 1587 batch loss 0.544864893 batch mAP 0.473693848 batch PCKh 0.75\n",
      "Trained batch 1588 batch loss 0.521555483 batch mAP 0.506500244 batch PCKh 0.6875\n",
      "Trained batch 1589 batch loss 0.607828856 batch mAP 0.446228027 batch PCKh 0.625\n",
      "Trained batch 1590 batch loss 0.569726 batch mAP 0.469543457 batch PCKh 0.25\n",
      "Trained batch 1591 batch loss 0.561486363 batch mAP 0.437316895 batch PCKh 0.3125\n",
      "Trained batch 1592 batch loss 0.683383226 batch mAP 0.445495605 batch PCKh 0.3125\n",
      "Trained batch 1593 batch loss 0.649609327 batch mAP 0.509674072 batch PCKh 0.8125\n",
      "Trained batch 1594 batch loss 0.645098448 batch mAP 0.475891113 batch PCKh 0.875\n",
      "Trained batch 1595 batch loss 0.615684748 batch mAP 0.498443604 batch PCKh 0.3125\n",
      "Trained batch 1596 batch loss 0.614900649 batch mAP 0.528015137 batch PCKh 0.75\n",
      "Trained batch 1597 batch loss 0.578448296 batch mAP 0.499572754 batch PCKh 0.5\n",
      "Trained batch 1598 batch loss 0.568405867 batch mAP 0.495605469 batch PCKh 0\n",
      "Trained batch 1599 batch loss 0.561252177 batch mAP 0.466705322 batch PCKh 0.0625\n",
      "Trained batch 1600 batch loss 0.548145831 batch mAP 0.507080078 batch PCKh 0.1875\n",
      "Trained batch 1601 batch loss 0.642717719 batch mAP 0.492431641 batch PCKh 0.4375\n",
      "Trained batch 1602 batch loss 0.674687624 batch mAP 0.491149902 batch PCKh 0.8125\n",
      "Trained batch 1603 batch loss 0.518874168 batch mAP 0.465301514 batch PCKh 0.5\n",
      "Trained batch 1604 batch loss 0.494548589 batch mAP 0.479858398 batch PCKh 0.1875\n",
      "Trained batch 1605 batch loss 0.499920756 batch mAP 0.453735352 batch PCKh 0.25\n",
      "Trained batch 1606 batch loss 0.572860837 batch mAP 0.438903809 batch PCKh 0.625\n",
      "Trained batch 1607 batch loss 0.6274544 batch mAP 0.475982666 batch PCKh 0.625\n",
      "Trained batch 1608 batch loss 0.612040818 batch mAP 0.478881836 batch PCKh 0.625\n",
      "Trained batch 1609 batch loss 0.46308893 batch mAP 0.516937256 batch PCKh 0.625\n",
      "Trained batch 1610 batch loss 0.520363927 batch mAP 0.496307373 batch PCKh 0.5625\n",
      "Trained batch 1611 batch loss 0.467604399 batch mAP 0.514404297 batch PCKh 0\n",
      "Trained batch 1612 batch loss 0.429840624 batch mAP 0.53894043 batch PCKh 0.5625\n",
      "Trained batch 1613 batch loss 0.466624796 batch mAP 0.552001953 batch PCKh 0.6875\n",
      "Trained batch 1614 batch loss 0.482023895 batch mAP 0.550842285 batch PCKh 0.5\n",
      "Trained batch 1615 batch loss 0.533580422 batch mAP 0.500640869 batch PCKh 0.75\n",
      "Trained batch 1616 batch loss 0.602257848 batch mAP 0.388061523 batch PCKh 0.5\n",
      "Trained batch 1617 batch loss 0.576809466 batch mAP 0.42666626 batch PCKh 0.5\n",
      "Trained batch 1618 batch loss 0.639043 batch mAP 0.264312744 batch PCKh 0.625\n",
      "Trained batch 1619 batch loss 0.617822647 batch mAP 0.353302 batch PCKh 0.5625\n",
      "Trained batch 1620 batch loss 0.66780591 batch mAP 0.344268799 batch PCKh 0.0625\n",
      "Trained batch 1621 batch loss 0.634632 batch mAP 0.524414062 batch PCKh 0.5625\n",
      "Trained batch 1622 batch loss 0.656287074 batch mAP 0.531433105 batch PCKh 0.125\n",
      "Trained batch 1623 batch loss 0.711468458 batch mAP 0.516479492 batch PCKh 0.1875\n",
      "Trained batch 1624 batch loss 0.590894938 batch mAP 0.529541 batch PCKh 0.25\n",
      "Trained batch 1625 batch loss 0.506784856 batch mAP 0.422485352 batch PCKh 0.25\n",
      "Trained batch 1626 batch loss 0.5198403 batch mAP 0.414337158 batch PCKh 0.625\n",
      "Trained batch 1627 batch loss 0.535602 batch mAP 0.386810303 batch PCKh 0.4375\n",
      "Trained batch 1628 batch loss 0.5525105 batch mAP 0.360839844 batch PCKh 0.4375\n",
      "Trained batch 1629 batch loss 0.578377 batch mAP 0.411895752 batch PCKh 0.4375\n",
      "Trained batch 1630 batch loss 0.689377248 batch mAP 0.428009033 batch PCKh 0.0625\n",
      "Trained batch 1631 batch loss 0.59618783 batch mAP 0.437255859 batch PCKh 0.8125\n",
      "Trained batch 1632 batch loss 0.601654053 batch mAP 0.471069336 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1633 batch loss 0.576840162 batch mAP 0.492004395 batch PCKh 0.8125\n",
      "Trained batch 1634 batch loss 0.690546751 batch mAP 0.494720459 batch PCKh 0.0625\n",
      "Trained batch 1635 batch loss 0.581079245 batch mAP 0.535461426 batch PCKh 0.5625\n",
      "Trained batch 1636 batch loss 0.528422 batch mAP 0.551940918 batch PCKh 0.6875\n",
      "Trained batch 1637 batch loss 0.693047225 batch mAP 0.506897 batch PCKh 0.3125\n",
      "Trained batch 1638 batch loss 0.693830132 batch mAP 0.511047363 batch PCKh 0.5\n",
      "Trained batch 1639 batch loss 0.706801951 batch mAP 0.500030518 batch PCKh 0.3125\n",
      "Trained batch 1640 batch loss 0.694702506 batch mAP 0.405426025 batch PCKh 0.875\n",
      "Trained batch 1641 batch loss 0.531414568 batch mAP 0.549133301 batch PCKh 0.6875\n",
      "Trained batch 1642 batch loss 0.616280317 batch mAP 0.572906494 batch PCKh 0.8125\n",
      "Trained batch 1643 batch loss 0.600141168 batch mAP 0.537994385 batch PCKh 0.875\n",
      "Trained batch 1644 batch loss 0.603806674 batch mAP 0.516235352 batch PCKh 0.75\n",
      "Trained batch 1645 batch loss 0.555875301 batch mAP 0.512207031 batch PCKh 0.1875\n",
      "Trained batch 1646 batch loss 0.51870358 batch mAP 0.519256592 batch PCKh 0.5625\n",
      "Trained batch 1647 batch loss 0.489190817 batch mAP 0.513336182 batch PCKh 0.625\n",
      "Trained batch 1648 batch loss 0.503506541 batch mAP 0.555053711 batch PCKh 0.5625\n",
      "Trained batch 1649 batch loss 0.468562186 batch mAP 0.506500244 batch PCKh 0.375\n",
      "Trained batch 1650 batch loss 0.480150253 batch mAP 0.532074 batch PCKh 0.375\n",
      "Trained batch 1651 batch loss 0.440753788 batch mAP 0.545227051 batch PCKh 0\n",
      "Trained batch 1652 batch loss 0.488038242 batch mAP 0.528778076 batch PCKh 0.5\n",
      "Trained batch 1653 batch loss 0.631613553 batch mAP 0.532684326 batch PCKh 0.75\n",
      "Trained batch 1654 batch loss 0.637066066 batch mAP 0.484588623 batch PCKh 0.25\n",
      "Trained batch 1655 batch loss 0.619031429 batch mAP 0.529876709 batch PCKh 0.625\n",
      "Trained batch 1656 batch loss 0.687322259 batch mAP 0.429412842 batch PCKh 0.8125\n",
      "Trained batch 1657 batch loss 0.58610338 batch mAP 0.543365479 batch PCKh 0.75\n",
      "Trained batch 1658 batch loss 0.645898104 batch mAP 0.504364 batch PCKh 0.0625\n",
      "Trained batch 1659 batch loss 0.598758936 batch mAP 0.548919678 batch PCKh 0.6875\n",
      "Trained batch 1660 batch loss 0.632711649 batch mAP 0.519805908 batch PCKh 0.875\n",
      "Trained batch 1661 batch loss 0.624655664 batch mAP 0.577789307 batch PCKh 0.3125\n",
      "Trained batch 1662 batch loss 0.722034812 batch mAP 0.494628906 batch PCKh 0\n",
      "Trained batch 1663 batch loss 0.677928269 batch mAP 0.54385376 batch PCKh 0.5625\n",
      "Trained batch 1664 batch loss 0.576264083 batch mAP 0.580535889 batch PCKh 0.125\n",
      "Trained batch 1665 batch loss 0.642241776 batch mAP 0.562072754 batch PCKh 0.5\n",
      "Trained batch 1666 batch loss 0.591287494 batch mAP 0.526916504 batch PCKh 0.6875\n",
      "Trained batch 1667 batch loss 0.576041579 batch mAP 0.534881592 batch PCKh 0.625\n",
      "Trained batch 1668 batch loss 0.680503726 batch mAP 0.476043701 batch PCKh 0.1875\n",
      "Trained batch 1669 batch loss 0.68645227 batch mAP 0.500213623 batch PCKh 0.375\n",
      "Trained batch 1670 batch loss 0.715222955 batch mAP 0.444641113 batch PCKh 0.3125\n",
      "Trained batch 1671 batch loss 0.603277922 batch mAP 0.551605225 batch PCKh 0.375\n",
      "Trained batch 1672 batch loss 0.601117373 batch mAP 0.472961426 batch PCKh 0.375\n",
      "Trained batch 1673 batch loss 0.549295664 batch mAP 0.50289917 batch PCKh 0.125\n",
      "Trained batch 1674 batch loss 0.518844843 batch mAP 0.546417236 batch PCKh 0.25\n",
      "Trained batch 1675 batch loss 0.503851593 batch mAP 0.547790527 batch PCKh 0.4375\n",
      "Trained batch 1676 batch loss 0.51233077 batch mAP 0.5597229 batch PCKh 0.625\n",
      "Trained batch 1677 batch loss 0.432401031 batch mAP 0.540771484 batch PCKh 0.25\n",
      "Trained batch 1678 batch loss 0.483757734 batch mAP 0.487304688 batch PCKh 0.1875\n",
      "Trained batch 1679 batch loss 0.496408731 batch mAP 0.547912598 batch PCKh 0.1875\n",
      "Trained batch 1680 batch loss 0.560542464 batch mAP 0.550109863 batch PCKh 0.1875\n",
      "Trained batch 1681 batch loss 0.541746616 batch mAP 0.509338379 batch PCKh 0.3125\n",
      "Trained batch 1682 batch loss 0.657033801 batch mAP 0.408752441 batch PCKh 0.1875\n",
      "Trained batch 1683 batch loss 0.596408 batch mAP 0.556518555 batch PCKh 0.1875\n",
      "Trained batch 1684 batch loss 0.623573303 batch mAP 0.535400391 batch PCKh 0.1875\n",
      "Trained batch 1685 batch loss 0.569808364 batch mAP 0.556671143 batch PCKh 0\n",
      "Trained batch 1686 batch loss 0.626496255 batch mAP 0.49887085 batch PCKh 0.8125\n",
      "Trained batch 1687 batch loss 0.612971902 batch mAP 0.473052979 batch PCKh 0.5625\n",
      "Trained batch 1688 batch loss 0.652483582 batch mAP 0.520599365 batch PCKh 0.8125\n",
      "Trained batch 1689 batch loss 0.653778076 batch mAP 0.534088135 batch PCKh 0.875\n",
      "Trained batch 1690 batch loss 0.729481697 batch mAP 0.479797363 batch PCKh 0.5\n",
      "Trained batch 1691 batch loss 0.556792438 batch mAP 0.495056152 batch PCKh 0.5625\n",
      "Trained batch 1692 batch loss 0.618787169 batch mAP 0.47277832 batch PCKh 0.125\n",
      "Trained batch 1693 batch loss 0.667163074 batch mAP 0.395843506 batch PCKh 0.5\n",
      "Trained batch 1694 batch loss 0.661323905 batch mAP 0.452087402 batch PCKh 0\n",
      "Trained batch 1695 batch loss 0.627647877 batch mAP 0.402526855 batch PCKh 0.375\n",
      "Trained batch 1696 batch loss 0.644158602 batch mAP 0.353759766 batch PCKh 0.4375\n",
      "Trained batch 1697 batch loss 0.558965623 batch mAP 0.187713623 batch PCKh 0.1875\n",
      "Trained batch 1698 batch loss 0.524236262 batch mAP 0.20010376 batch PCKh 0\n",
      "Trained batch 1699 batch loss 0.446864128 batch mAP 0.252288818 batch PCKh 0\n",
      "Trained batch 1700 batch loss 0.551574528 batch mAP 0.200286865 batch PCKh 0.3125\n",
      "Trained batch 1701 batch loss 0.577078938 batch mAP 0.269714355 batch PCKh 0.625\n",
      "Trained batch 1702 batch loss 0.543544292 batch mAP 0.363555908 batch PCKh 0.5625\n",
      "Trained batch 1703 batch loss 0.531681299 batch mAP 0.516143799 batch PCKh 0.375\n",
      "Trained batch 1704 batch loss 0.546393037 batch mAP 0.551269531 batch PCKh 0.75\n",
      "Trained batch 1705 batch loss 0.551745176 batch mAP 0.597381592 batch PCKh 0.5625\n",
      "Trained batch 1706 batch loss 0.564213634 batch mAP 0.50668335 batch PCKh 0\n",
      "Trained batch 1707 batch loss 0.52411288 batch mAP 0.569610596 batch PCKh 0.375\n",
      "Trained batch 1708 batch loss 0.593984663 batch mAP 0.501953125 batch PCKh 0.6875\n",
      "Trained batch 1709 batch loss 0.665421426 batch mAP 0.41696167 batch PCKh 0.0625\n",
      "Trained batch 1710 batch loss 0.647305965 batch mAP 0.446868896 batch PCKh 0.3125\n",
      "Trained batch 1711 batch loss 0.562280536 batch mAP 0.588867188 batch PCKh 0.1875\n",
      "Trained batch 1712 batch loss 0.566639662 batch mAP 0.56048584 batch PCKh 0\n",
      "Trained batch 1713 batch loss 0.563856959 batch mAP 0.565734863 batch PCKh 0.1875\n",
      "Trained batch 1714 batch loss 0.597384572 batch mAP 0.554260254 batch PCKh 0.25\n",
      "Trained batch 1715 batch loss 0.658215761 batch mAP 0.435424805 batch PCKh 0.3125\n",
      "Trained batch 1716 batch loss 0.624917388 batch mAP 0.479278564 batch PCKh 0.3125\n",
      "Trained batch 1717 batch loss 0.623563945 batch mAP 0.536987305 batch PCKh 0.375\n",
      "Trained batch 1718 batch loss 0.628838181 batch mAP 0.519104 batch PCKh 0.5625\n",
      "Trained batch 1719 batch loss 0.723250806 batch mAP 0.459198 batch PCKh 0.0625\n",
      "Trained batch 1720 batch loss 0.726206243 batch mAP 0.422973633 batch PCKh 0.4375\n",
      "Trained batch 1721 batch loss 0.745445549 batch mAP 0.448455811 batch PCKh 0\n",
      "Trained batch 1722 batch loss 0.74712652 batch mAP 0.377105713 batch PCKh 0\n",
      "Trained batch 1723 batch loss 0.619203925 batch mAP 0.367340088 batch PCKh 0.3125\n",
      "Trained batch 1724 batch loss 0.657269597 batch mAP 0.372070312 batch PCKh 0\n",
      "Trained batch 1725 batch loss 0.539700687 batch mAP 0.306427 batch PCKh 0.1875\n",
      "Trained batch 1726 batch loss 0.579525 batch mAP 0.346099854 batch PCKh 0.5625\n",
      "Trained batch 1727 batch loss 0.492108583 batch mAP 0.292419434 batch PCKh 0\n",
      "Trained batch 1728 batch loss 0.54215312 batch mAP 0.335235596 batch PCKh 0.0625\n",
      "Trained batch 1729 batch loss 0.637222409 batch mAP 0.346618652 batch PCKh 0.4375\n",
      "Trained batch 1730 batch loss 0.74239 batch mAP 0.393829346 batch PCKh 0.1875\n",
      "Trained batch 1731 batch loss 0.684067 batch mAP 0.369567871 batch PCKh 0.25\n",
      "Trained batch 1732 batch loss 0.688464403 batch mAP 0.397491455 batch PCKh 0\n",
      "Trained batch 1733 batch loss 0.649746299 batch mAP 0.372741699 batch PCKh 0.1875\n",
      "Trained batch 1734 batch loss 0.683716774 batch mAP 0.399230957 batch PCKh 0.25\n",
      "Trained batch 1735 batch loss 0.664665699 batch mAP 0.404724121 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1736 batch loss 0.664029062 batch mAP 0.435516357 batch PCKh 0.125\n",
      "Trained batch 1737 batch loss 0.602121472 batch mAP 0.464569092 batch PCKh 0.6875\n",
      "Trained batch 1738 batch loss 0.642038286 batch mAP 0.499694824 batch PCKh 0.3125\n",
      "Trained batch 1739 batch loss 0.622212887 batch mAP 0.491424561 batch PCKh 0.4375\n",
      "Trained batch 1740 batch loss 0.595418 batch mAP 0.527923584 batch PCKh 0.3125\n",
      "Trained batch 1741 batch loss 0.575214446 batch mAP 0.504211426 batch PCKh 0.375\n",
      "Trained batch 1742 batch loss 0.550822496 batch mAP 0.529052734 batch PCKh 0.625\n",
      "Trained batch 1743 batch loss 0.651090384 batch mAP 0.511016846 batch PCKh 0.4375\n",
      "Trained batch 1744 batch loss 0.688644111 batch mAP 0.50692749 batch PCKh 0.8125\n",
      "Trained batch 1745 batch loss 0.619633555 batch mAP 0.409637451 batch PCKh 0.625\n",
      "Trained batch 1746 batch loss 0.588030815 batch mAP 0.40411377 batch PCKh 0.625\n",
      "Trained batch 1747 batch loss 0.626681447 batch mAP 0.494628906 batch PCKh 0.625\n",
      "Trained batch 1748 batch loss 0.62674588 batch mAP 0.416290283 batch PCKh 0.5625\n",
      "Trained batch 1749 batch loss 0.561944246 batch mAP 0.429260254 batch PCKh 0.375\n",
      "Trained batch 1750 batch loss 0.600993037 batch mAP 0.49810791 batch PCKh 0.4375\n",
      "Trained batch 1751 batch loss 0.62187469 batch mAP 0.509857178 batch PCKh 0.625\n",
      "Trained batch 1752 batch loss 0.670630336 batch mAP 0.46774292 batch PCKh 0.375\n",
      "Trained batch 1753 batch loss 0.594661355 batch mAP 0.482513428 batch PCKh 0.5\n",
      "Trained batch 1754 batch loss 0.561304331 batch mAP 0.474914551 batch PCKh 0.4375\n",
      "Trained batch 1755 batch loss 0.580378175 batch mAP 0.450378418 batch PCKh 0.8125\n",
      "Trained batch 1756 batch loss 0.565992594 batch mAP 0.489044189 batch PCKh 0.4375\n",
      "Trained batch 1757 batch loss 0.591534138 batch mAP 0.499359131 batch PCKh 0.625\n",
      "Trained batch 1758 batch loss 0.607509136 batch mAP 0.480224609 batch PCKh 0.625\n",
      "Trained batch 1759 batch loss 0.61264354 batch mAP 0.466003418 batch PCKh 0.5625\n",
      "Trained batch 1760 batch loss 0.634417176 batch mAP 0.484039307 batch PCKh 0.875\n",
      "Trained batch 1761 batch loss 0.646236897 batch mAP 0.466339111 batch PCKh 0.5625\n",
      "Trained batch 1762 batch loss 0.538647473 batch mAP 0.499847412 batch PCKh 0.5\n",
      "Trained batch 1763 batch loss 0.587275 batch mAP 0.454803467 batch PCKh 0.25\n",
      "Trained batch 1764 batch loss 0.552811742 batch mAP 0.478057861 batch PCKh 0.875\n",
      "Trained batch 1765 batch loss 0.518454194 batch mAP 0.472198486 batch PCKh 0.6875\n",
      "Trained batch 1766 batch loss 0.516415834 batch mAP 0.535553 batch PCKh 0.5\n",
      "Trained batch 1767 batch loss 0.433909357 batch mAP 0.424072266 batch PCKh 0.5625\n",
      "Trained batch 1768 batch loss 0.54012394 batch mAP 0.487701416 batch PCKh 0.5625\n",
      "Trained batch 1769 batch loss 0.525630832 batch mAP 0.45413208 batch PCKh 0.6875\n",
      "Trained batch 1770 batch loss 0.527681828 batch mAP 0.514038086 batch PCKh 0.1875\n",
      "Trained batch 1771 batch loss 0.555508614 batch mAP 0.532806396 batch PCKh 0.5\n",
      "Trained batch 1772 batch loss 0.572122574 batch mAP 0.556030273 batch PCKh 0.6875\n",
      "Trained batch 1773 batch loss 0.578140259 batch mAP 0.550140381 batch PCKh 0.6875\n",
      "Trained batch 1774 batch loss 0.5357306 batch mAP 0.495513916 batch PCKh 0.6875\n",
      "Trained batch 1775 batch loss 0.522511 batch mAP 0.513305664 batch PCKh 0.1875\n",
      "Trained batch 1776 batch loss 0.418887198 batch mAP 0.5625 batch PCKh 0.4375\n",
      "Trained batch 1777 batch loss 0.53865248 batch mAP 0.553985596 batch PCKh 0.5625\n",
      "Trained batch 1778 batch loss 0.547000885 batch mAP 0.539886475 batch PCKh 0.75\n",
      "Trained batch 1779 batch loss 0.608053923 batch mAP 0.505859375 batch PCKh 0.75\n",
      "Trained batch 1780 batch loss 0.750810504 batch mAP 0.380371094 batch PCKh 0.125\n",
      "Trained batch 1781 batch loss 0.694663465 batch mAP 0.458892822 batch PCKh 0.1875\n",
      "Trained batch 1782 batch loss 0.663556695 batch mAP 0.528991699 batch PCKh 0.125\n",
      "Trained batch 1783 batch loss 0.59883523 batch mAP 0.568511963 batch PCKh 0.3125\n",
      "Trained batch 1784 batch loss 0.646538436 batch mAP 0.528839111 batch PCKh 0.125\n",
      "Trained batch 1785 batch loss 0.599743843 batch mAP 0.506256104 batch PCKh 0.4375\n",
      "Trained batch 1786 batch loss 0.559805393 batch mAP 0.496002197 batch PCKh 0.625\n",
      "Trained batch 1787 batch loss 0.452984333 batch mAP 0.473815918 batch PCKh 0.1875\n",
      "Trained batch 1788 batch loss 0.591134787 batch mAP 0.479095459 batch PCKh 0.5\n",
      "Trained batch 1789 batch loss 0.587083757 batch mAP 0.500183105 batch PCKh 0.3125\n",
      "Trained batch 1790 batch loss 0.588937759 batch mAP 0.510131836 batch PCKh 0.5\n",
      "Trained batch 1791 batch loss 0.613630891 batch mAP 0.515991211 batch PCKh 0.75\n",
      "Trained batch 1792 batch loss 0.579554 batch mAP 0.527130127 batch PCKh 0.625\n",
      "Trained batch 1793 batch loss 0.544885278 batch mAP 0.522216797 batch PCKh 0.5\n",
      "Trained batch 1794 batch loss 0.471869111 batch mAP 0.484588623 batch PCKh 0.5\n",
      "Trained batch 1795 batch loss 0.473547101 batch mAP 0.495391846 batch PCKh 0.5\n",
      "Trained batch 1796 batch loss 0.501495123 batch mAP 0.45880127 batch PCKh 0.375\n",
      "Trained batch 1797 batch loss 0.474521935 batch mAP 0.44140625 batch PCKh 0\n",
      "Trained batch 1798 batch loss 0.568279445 batch mAP 0.441223145 batch PCKh 0.625\n",
      "Trained batch 1799 batch loss 0.50818 batch mAP 0.4793396 batch PCKh 0.75\n",
      "Trained batch 1800 batch loss 0.583288729 batch mAP 0.52255249 batch PCKh 0.875\n",
      "Trained batch 1801 batch loss 0.564398646 batch mAP 0.554351807 batch PCKh 0.75\n",
      "Trained batch 1802 batch loss 0.55028069 batch mAP 0.522705078 batch PCKh 0.1875\n",
      "Trained batch 1803 batch loss 0.506481051 batch mAP 0.552642822 batch PCKh 0.5625\n",
      "Trained batch 1804 batch loss 0.513143599 batch mAP 0.543640137 batch PCKh 0.5\n",
      "Trained batch 1805 batch loss 0.561469197 batch mAP 0.519866943 batch PCKh 0.6875\n",
      "Trained batch 1806 batch loss 0.606596828 batch mAP 0.52911377 batch PCKh 0.3125\n",
      "Trained batch 1807 batch loss 0.568069339 batch mAP 0.531494141 batch PCKh 0.3125\n",
      "Trained batch 1808 batch loss 0.608435929 batch mAP 0.46875 batch PCKh 0.6875\n",
      "Trained batch 1809 batch loss 0.596761823 batch mAP 0.460510254 batch PCKh 0.625\n",
      "Trained batch 1810 batch loss 0.641909361 batch mAP 0.426818848 batch PCKh 0.5625\n",
      "Trained batch 1811 batch loss 0.610737801 batch mAP 0.436431885 batch PCKh 0.5625\n",
      "Trained batch 1812 batch loss 0.522613883 batch mAP 0.534484863 batch PCKh 0.625\n",
      "Trained batch 1813 batch loss 0.524413 batch mAP 0.504882812 batch PCKh 0.625\n",
      "Trained batch 1814 batch loss 0.514508128 batch mAP 0.516357422 batch PCKh 0.75\n",
      "Trained batch 1815 batch loss 0.570394754 batch mAP 0.518371582 batch PCKh 0.75\n",
      "Trained batch 1816 batch loss 0.608618498 batch mAP 0.545227051 batch PCKh 0.3125\n",
      "Trained batch 1817 batch loss 0.709714115 batch mAP 0.474578857 batch PCKh 0.1875\n",
      "Trained batch 1818 batch loss 0.613272309 batch mAP 0.522125244 batch PCKh 0.1875\n",
      "Trained batch 1819 batch loss 0.63676542 batch mAP 0.504699707 batch PCKh 0.0625\n",
      "Trained batch 1820 batch loss 0.602907777 batch mAP 0.456695557 batch PCKh 0.25\n",
      "Trained batch 1821 batch loss 0.56043762 batch mAP 0.482208252 batch PCKh 0.4375\n",
      "Trained batch 1822 batch loss 0.603279 batch mAP 0.437744141 batch PCKh 0.5625\n",
      "Trained batch 1823 batch loss 0.600437641 batch mAP 0.406311035 batch PCKh 0.8125\n",
      "Trained batch 1824 batch loss 0.600200117 batch mAP 0.460784912 batch PCKh 0.875\n",
      "Trained batch 1825 batch loss 0.591523409 batch mAP 0.502105713 batch PCKh 0.625\n",
      "Trained batch 1826 batch loss 0.575087547 batch mAP 0.525695801 batch PCKh 0.5\n",
      "Trained batch 1827 batch loss 0.611062706 batch mAP 0.504272461 batch PCKh 0.3125\n",
      "Trained batch 1828 batch loss 0.578995585 batch mAP 0.568847656 batch PCKh 0.25\n",
      "Trained batch 1829 batch loss 0.584738255 batch mAP 0.532745361 batch PCKh 0.4375\n",
      "Trained batch 1830 batch loss 0.560303569 batch mAP 0.586730957 batch PCKh 0.375\n",
      "Trained batch 1831 batch loss 0.576623142 batch mAP 0.638580322 batch PCKh 0.4375\n",
      "Trained batch 1832 batch loss 0.560916185 batch mAP 0.567993164 batch PCKh 0.375\n",
      "Trained batch 1833 batch loss 0.543332279 batch mAP 0.574676514 batch PCKh 0.3125\n",
      "Trained batch 1834 batch loss 0.645421922 batch mAP 0.571960449 batch PCKh 0.6875\n",
      "Trained batch 1835 batch loss 0.601476669 batch mAP 0.537567139 batch PCKh 0.1875\n",
      "Trained batch 1836 batch loss 0.624422431 batch mAP 0.5050354 batch PCKh 0.625\n",
      "Trained batch 1837 batch loss 0.567041278 batch mAP 0.529083252 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1838 batch loss 0.596899569 batch mAP 0.511779785 batch PCKh 0.6875\n",
      "Trained batch 1839 batch loss 0.577809393 batch mAP 0.551269531 batch PCKh 0.3125\n",
      "Trained batch 1840 batch loss 0.602987885 batch mAP 0.529785156 batch PCKh 0.4375\n",
      "Trained batch 1841 batch loss 0.65263629 batch mAP 0.521850586 batch PCKh 0.6875\n",
      "Trained batch 1842 batch loss 0.660100043 batch mAP 0.469024658 batch PCKh 0.5625\n",
      "Trained batch 1843 batch loss 0.709429562 batch mAP 0.484313965 batch PCKh 0.25\n",
      "Trained batch 1844 batch loss 0.623569608 batch mAP 0.519165039 batch PCKh 0.1875\n",
      "Trained batch 1845 batch loss 0.600313 batch mAP 0.508880615 batch PCKh 0.4375\n",
      "Trained batch 1846 batch loss 0.677288532 batch mAP 0.496856689 batch PCKh 0\n",
      "Trained batch 1847 batch loss 0.646917224 batch mAP 0.464630127 batch PCKh 0.1875\n",
      "Trained batch 1848 batch loss 0.689640403 batch mAP 0.476257324 batch PCKh 0.5625\n",
      "Trained batch 1849 batch loss 0.597740948 batch mAP 0.461181641 batch PCKh 0.4375\n",
      "Trained batch 1850 batch loss 0.659154296 batch mAP 0.425689697 batch PCKh 0.375\n",
      "Trained batch 1851 batch loss 0.617391586 batch mAP 0.424194336 batch PCKh 0.0625\n",
      "Trained batch 1852 batch loss 0.636392832 batch mAP 0.396759033 batch PCKh 0\n",
      "Trained batch 1853 batch loss 0.565865338 batch mAP 0.386322021 batch PCKh 0.25\n",
      "Trained batch 1854 batch loss 0.552630424 batch mAP 0.385437 batch PCKh 0.375\n",
      "Trained batch 1855 batch loss 0.60425061 batch mAP 0.413757324 batch PCKh 0.5\n",
      "Trained batch 1856 batch loss 0.536103487 batch mAP 0.429779053 batch PCKh 0.4375\n",
      "Trained batch 1857 batch loss 0.595310628 batch mAP 0.472076416 batch PCKh 0.3125\n",
      "Trained batch 1858 batch loss 0.549044311 batch mAP 0.447265625 batch PCKh 0.3125\n",
      "Trained batch 1859 batch loss 0.533515155 batch mAP 0.533233643 batch PCKh 0.375\n",
      "Trained batch 1860 batch loss 0.496944189 batch mAP 0.555664062 batch PCKh 0.3125\n",
      "Trained batch 1861 batch loss 0.545325756 batch mAP 0.539276123 batch PCKh 0.125\n",
      "Trained batch 1862 batch loss 0.550238 batch mAP 0.429901123 batch PCKh 0.625\n",
      "Trained batch 1863 batch loss 0.613104761 batch mAP 0.537536621 batch PCKh 0.8125\n",
      "Trained batch 1864 batch loss 0.612012744 batch mAP 0.453796387 batch PCKh 0.4375\n",
      "Trained batch 1865 batch loss 0.613261342 batch mAP 0.412658691 batch PCKh 0.4375\n",
      "Trained batch 1866 batch loss 0.560565889 batch mAP 0.616607666 batch PCKh 0.375\n",
      "Trained batch 1867 batch loss 0.551009357 batch mAP 0.658416748 batch PCKh 0.5625\n",
      "Trained batch 1868 batch loss 0.573326647 batch mAP 0.651245117 batch PCKh 0.3125\n",
      "Trained batch 1869 batch loss 0.578829527 batch mAP 0.579986572 batch PCKh 0.25\n",
      "Trained batch 1870 batch loss 0.612978 batch mAP 0.519561768 batch PCKh 0.0625\n",
      "Trained batch 1871 batch loss 0.669693172 batch mAP 0.539306641 batch PCKh 0.25\n",
      "Trained batch 1872 batch loss 0.632069826 batch mAP 0.552368164 batch PCKh 0.5\n",
      "Trained batch 1873 batch loss 0.603564382 batch mAP 0.578765869 batch PCKh 0.625\n",
      "Trained batch 1874 batch loss 0.659017503 batch mAP 0.52923584 batch PCKh 0.8125\n",
      "Trained batch 1875 batch loss 0.622454286 batch mAP 0.504730225 batch PCKh 0.1875\n",
      "Trained batch 1876 batch loss 0.56936717 batch mAP 0.506225586 batch PCKh 0.25\n",
      "Trained batch 1877 batch loss 0.661637 batch mAP 0.530487061 batch PCKh 0.5625\n",
      "Trained batch 1878 batch loss 0.602393568 batch mAP 0.525939941 batch PCKh 0.8125\n",
      "Trained batch 1879 batch loss 0.606798828 batch mAP 0.509216309 batch PCKh 0.5\n",
      "Trained batch 1880 batch loss 0.615904272 batch mAP 0.50668335 batch PCKh 0.6875\n",
      "Trained batch 1881 batch loss 0.586477757 batch mAP 0.557342529 batch PCKh 0.5\n",
      "Trained batch 1882 batch loss 0.686126828 batch mAP 0.538147 batch PCKh 0.875\n",
      "Trained batch 1883 batch loss 0.581920207 batch mAP 0.592376709 batch PCKh 0.5625\n",
      "Trained batch 1884 batch loss 0.623757899 batch mAP 0.58078 batch PCKh 0.375\n",
      "Trained batch 1885 batch loss 0.593314528 batch mAP 0.49597168 batch PCKh 0.5\n",
      "Trained batch 1886 batch loss 0.592107594 batch mAP 0.521118164 batch PCKh 0.3125\n",
      "Trained batch 1887 batch loss 0.664270282 batch mAP 0.534942627 batch PCKh 0.1875\n",
      "Trained batch 1888 batch loss 0.619780064 batch mAP 0.536071777 batch PCKh 0.3125\n",
      "Trained batch 1889 batch loss 0.551091135 batch mAP 0.572723389 batch PCKh 0.25\n",
      "Trained batch 1890 batch loss 0.572758 batch mAP 0.581604 batch PCKh 0.6875\n",
      "Trained batch 1891 batch loss 0.645688236 batch mAP 0.557830811 batch PCKh 0.625\n",
      "Trained batch 1892 batch loss 0.697004437 batch mAP 0.545379639 batch PCKh 0.4375\n",
      "Trained batch 1893 batch loss 0.592788 batch mAP 0.538665771 batch PCKh 0.625\n",
      "Trained batch 1894 batch loss 0.567593217 batch mAP 0.552581787 batch PCKh 0.4375\n",
      "Trained batch 1895 batch loss 0.549919 batch mAP 0.553192139 batch PCKh 0.8125\n",
      "Trained batch 1896 batch loss 0.573132634 batch mAP 0.569671631 batch PCKh 0.625\n",
      "Trained batch 1897 batch loss 0.580076218 batch mAP 0.51776123 batch PCKh 0.75\n",
      "Trained batch 1898 batch loss 0.645169258 batch mAP 0.492156982 batch PCKh 0.1875\n",
      "Trained batch 1899 batch loss 0.591938376 batch mAP 0.463531494 batch PCKh 0.3125\n",
      "Trained batch 1900 batch loss 0.686466515 batch mAP 0.466369629 batch PCKh 0.3125\n",
      "Trained batch 1901 batch loss 0.543723762 batch mAP 0.48034668 batch PCKh 0.375\n",
      "Trained batch 1902 batch loss 0.68022 batch mAP 0.484680176 batch PCKh 0.4375\n",
      "Trained batch 1903 batch loss 0.573246479 batch mAP 0.47668457 batch PCKh 0.125\n",
      "Trained batch 1904 batch loss 0.61429739 batch mAP 0.507720947 batch PCKh 0.5\n",
      "Trained batch 1905 batch loss 0.554181576 batch mAP 0.486297607 batch PCKh 0.8125\n",
      "Trained batch 1906 batch loss 0.595909774 batch mAP 0.539398193 batch PCKh 0\n",
      "Trained batch 1907 batch loss 0.503787816 batch mAP 0.532318115 batch PCKh 0.75\n",
      "Trained batch 1908 batch loss 0.507214665 batch mAP 0.54498291 batch PCKh 0.5625\n",
      "Trained batch 1909 batch loss 0.554452181 batch mAP 0.545440674 batch PCKh 0.0625\n",
      "Trained batch 1910 batch loss 0.68554914 batch mAP 0.48034668 batch PCKh 0.375\n",
      "Trained batch 1911 batch loss 0.664151609 batch mAP 0.517913818 batch PCKh 0.25\n",
      "Trained batch 1912 batch loss 0.574278891 batch mAP 0.522674561 batch PCKh 0.75\n",
      "Trained batch 1913 batch loss 0.510810196 batch mAP 0.551025391 batch PCKh 0\n",
      "Trained batch 1914 batch loss 0.629245281 batch mAP 0.535644531 batch PCKh 0.6875\n",
      "Trained batch 1915 batch loss 0.603353083 batch mAP 0.491210938 batch PCKh 0.4375\n",
      "Trained batch 1916 batch loss 0.568387747 batch mAP 0.498413086 batch PCKh 0.5\n",
      "Trained batch 1917 batch loss 0.597088873 batch mAP 0.474517822 batch PCKh 0.4375\n",
      "Trained batch 1918 batch loss 0.606703401 batch mAP 0.456970215 batch PCKh 0.375\n",
      "Trained batch 1919 batch loss 0.585128605 batch mAP 0.516571045 batch PCKh 0.25\n",
      "Trained batch 1920 batch loss 0.552849114 batch mAP 0.501922607 batch PCKh 0.3125\n",
      "Trained batch 1921 batch loss 0.58554256 batch mAP 0.525482178 batch PCKh 0.125\n",
      "Trained batch 1922 batch loss 0.590384364 batch mAP 0.545806885 batch PCKh 0.5\n",
      "Trained batch 1923 batch loss 0.618751407 batch mAP 0.559783936 batch PCKh 0.6875\n",
      "Trained batch 1924 batch loss 0.594265759 batch mAP 0.535949707 batch PCKh 0.75\n",
      "Trained batch 1925 batch loss 0.650573611 batch mAP 0.539978 batch PCKh 0.3125\n",
      "Trained batch 1926 batch loss 0.641464353 batch mAP 0.544708252 batch PCKh 0.5625\n",
      "Trained batch 1927 batch loss 0.702151477 batch mAP 0.486785889 batch PCKh 0.3125\n",
      "Trained batch 1928 batch loss 0.653665185 batch mAP 0.529754639 batch PCKh 0.4375\n",
      "Trained batch 1929 batch loss 0.665530145 batch mAP 0.566772461 batch PCKh 0.75\n",
      "Trained batch 1930 batch loss 0.649065614 batch mAP 0.580657959 batch PCKh 0.625\n",
      "Trained batch 1931 batch loss 0.621990442 batch mAP 0.533081055 batch PCKh 0.4375\n",
      "Trained batch 1932 batch loss 0.625453234 batch mAP 0.474823 batch PCKh 0.6875\n",
      "Trained batch 1933 batch loss 0.600473285 batch mAP 0.493103027 batch PCKh 0.5625\n",
      "Trained batch 1934 batch loss 0.577748954 batch mAP 0.51461792 batch PCKh 0.1875\n",
      "Trained batch 1935 batch loss 0.564967811 batch mAP 0.521453857 batch PCKh 0.625\n",
      "Trained batch 1936 batch loss 0.607238054 batch mAP 0.488037109 batch PCKh 0.5\n",
      "Trained batch 1937 batch loss 0.577773809 batch mAP 0.5234375 batch PCKh 0.75\n",
      "Trained batch 1938 batch loss 0.63486135 batch mAP 0.502410889 batch PCKh 0.625\n",
      "Trained batch 1939 batch loss 0.559681654 batch mAP 0.543243408 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1940 batch loss 0.616033554 batch mAP 0.519683838 batch PCKh 0.25\n",
      "Trained batch 1941 batch loss 0.619946897 batch mAP 0.506408691 batch PCKh 0.5\n",
      "Trained batch 1942 batch loss 0.602253079 batch mAP 0.524383545 batch PCKh 0.75\n",
      "Trained batch 1943 batch loss 0.561155438 batch mAP 0.574523926 batch PCKh 0.25\n",
      "Trained batch 1944 batch loss 0.59965241 batch mAP 0.588043213 batch PCKh 0.5625\n",
      "Trained batch 1945 batch loss 0.572856069 batch mAP 0.510772705 batch PCKh 0.125\n",
      "Trained batch 1946 batch loss 0.627484262 batch mAP 0.49307251 batch PCKh 0.375\n",
      "Trained batch 1947 batch loss 0.664173245 batch mAP 0.552581787 batch PCKh 0.3125\n",
      "Trained batch 1948 batch loss 0.583069623 batch mAP 0.551696777 batch PCKh 0.1875\n",
      "Trained batch 1949 batch loss 0.580707908 batch mAP 0.623260498 batch PCKh 0.625\n",
      "Trained batch 1950 batch loss 0.579560578 batch mAP 0.596374512 batch PCKh 0.375\n",
      "Trained batch 1951 batch loss 0.541910231 batch mAP 0.602294922 batch PCKh 0.5625\n",
      "Trained batch 1952 batch loss 0.638784826 batch mAP 0.554016113 batch PCKh 0.125\n",
      "Trained batch 1953 batch loss 0.60467571 batch mAP 0.541351318 batch PCKh 0.4375\n",
      "Trained batch 1954 batch loss 0.544097 batch mAP 0.498199463 batch PCKh 0.5\n",
      "Trained batch 1955 batch loss 0.60782814 batch mAP 0.567077637 batch PCKh 0.75\n",
      "Trained batch 1956 batch loss 0.606124163 batch mAP 0.520324707 batch PCKh 0.25\n",
      "Trained batch 1957 batch loss 0.692967236 batch mAP 0.496612549 batch PCKh 0.1875\n",
      "Trained batch 1958 batch loss 0.654598415 batch mAP 0.465576172 batch PCKh 0\n",
      "Trained batch 1959 batch loss 0.59555155 batch mAP 0.544586182 batch PCKh 0.6875\n",
      "Trained batch 1960 batch loss 0.7551108 batch mAP 0.471435547 batch PCKh 0.0625\n",
      "Trained batch 1961 batch loss 0.708661616 batch mAP 0.507843 batch PCKh 0.125\n",
      "Trained batch 1962 batch loss 0.69526124 batch mAP 0.512512207 batch PCKh 0.25\n",
      "Trained batch 1963 batch loss 0.625101089 batch mAP 0.547576904 batch PCKh 0.3125\n",
      "Trained batch 1964 batch loss 0.660946667 batch mAP 0.520202637 batch PCKh 0.3125\n",
      "Trained batch 1965 batch loss 0.71437341 batch mAP 0.464813232 batch PCKh 0.1875\n",
      "Trained batch 1966 batch loss 0.608842432 batch mAP 0.496917725 batch PCKh 0.625\n",
      "Trained batch 1967 batch loss 0.664526522 batch mAP 0.488647461 batch PCKh 0\n",
      "Trained batch 1968 batch loss 0.586728334 batch mAP 0.540924072 batch PCKh 0.5625\n",
      "Trained batch 1969 batch loss 0.590596557 batch mAP 0.494689941 batch PCKh 0.5\n",
      "Trained batch 1970 batch loss 0.506157 batch mAP 0.499511719 batch PCKh 0.4375\n",
      "Trained batch 1971 batch loss 0.591074407 batch mAP 0.48538208 batch PCKh 0.6875\n",
      "Trained batch 1972 batch loss 0.532962143 batch mAP 0.52255249 batch PCKh 0.3125\n",
      "Trained batch 1973 batch loss 0.577667296 batch mAP 0.547363281 batch PCKh 0.125\n",
      "Trained batch 1974 batch loss 0.565314054 batch mAP 0.573120117 batch PCKh 0.375\n",
      "Trained batch 1975 batch loss 0.591293752 batch mAP 0.509796143 batch PCKh 0.25\n",
      "Trained batch 1976 batch loss 0.677349091 batch mAP 0.373260498 batch PCKh 0\n",
      "Trained batch 1977 batch loss 0.659456491 batch mAP 0.553039551 batch PCKh 0.3125\n",
      "Trained batch 1978 batch loss 0.568814456 batch mAP 0.507019043 batch PCKh 0.4375\n",
      "Trained batch 1979 batch loss 0.628348 batch mAP 0.510864258 batch PCKh 0.625\n",
      "Trained batch 1980 batch loss 0.601404428 batch mAP 0.552032471 batch PCKh 0.125\n",
      "Trained batch 1981 batch loss 0.50414 batch mAP 0.519744873 batch PCKh 0\n",
      "Trained batch 1982 batch loss 0.495141089 batch mAP 0.536834717 batch PCKh 0.75\n",
      "Trained batch 1983 batch loss 0.583376408 batch mAP 0.513427734 batch PCKh 0.5625\n",
      "Trained batch 1984 batch loss 0.598547 batch mAP 0.564971924 batch PCKh 0.4375\n",
      "Trained batch 1985 batch loss 0.489524424 batch mAP 0.518463135 batch PCKh 0.5625\n",
      "Trained batch 1986 batch loss 0.493150175 batch mAP 0.512573242 batch PCKh 0.5625\n",
      "Trained batch 1987 batch loss 0.523013592 batch mAP 0.567352295 batch PCKh 0.1875\n",
      "Trained batch 1988 batch loss 0.532470882 batch mAP 0.577392578 batch PCKh 0.625\n",
      "Trained batch 1989 batch loss 0.547571659 batch mAP 0.558074951 batch PCKh 0.5625\n",
      "Trained batch 1990 batch loss 0.592430532 batch mAP 0.553863525 batch PCKh 0.75\n",
      "Trained batch 1991 batch loss 0.512871742 batch mAP 0.585296631 batch PCKh 0.5\n",
      "Trained batch 1992 batch loss 0.474750757 batch mAP 0.609130859 batch PCKh 0.5625\n",
      "Trained batch 1993 batch loss 0.537313759 batch mAP 0.613342285 batch PCKh 0.3125\n",
      "Trained batch 1994 batch loss 0.593823195 batch mAP 0.619598389 batch PCKh 0.375\n",
      "Trained batch 1995 batch loss 0.563514233 batch mAP 0.567962646 batch PCKh 0.3125\n",
      "Trained batch 1996 batch loss 0.506586611 batch mAP 0.542053223 batch PCKh 0.1875\n",
      "Trained batch 1997 batch loss 0.476823747 batch mAP 0.494812 batch PCKh 0.25\n",
      "Trained batch 1998 batch loss 0.554416597 batch mAP 0.494750977 batch PCKh 0.3125\n",
      "Trained batch 1999 batch loss 0.458545715 batch mAP 0.547302246 batch PCKh 0.4375\n",
      "Trained batch 2000 batch loss 0.476574 batch mAP 0.588989258 batch PCKh 0.3125\n",
      "Trained batch 2001 batch loss 0.608036399 batch mAP 0.529052734 batch PCKh 0.6875\n",
      "Trained batch 2002 batch loss 0.497971773 batch mAP 0.565734863 batch PCKh 0.3125\n",
      "Trained batch 2003 batch loss 0.52826649 batch mAP 0.525421143 batch PCKh 0.4375\n",
      "Trained batch 2004 batch loss 0.65234077 batch mAP 0.553314209 batch PCKh 0.6875\n",
      "Trained batch 2005 batch loss 0.536489904 batch mAP 0.482788086 batch PCKh 0.6875\n",
      "Trained batch 2006 batch loss 0.60171324 batch mAP 0.464996338 batch PCKh 0.375\n",
      "Trained batch 2007 batch loss 0.542203665 batch mAP 0.414886475 batch PCKh 0.3125\n",
      "Trained batch 2008 batch loss 0.45662418 batch mAP 0.294006348 batch PCKh 0.375\n",
      "Trained batch 2009 batch loss 0.505439639 batch mAP 0.351501465 batch PCKh 0.5\n",
      "Trained batch 2010 batch loss 0.64438951 batch mAP 0.340454102 batch PCKh 0.1875\n",
      "Trained batch 2011 batch loss 0.484606981 batch mAP 0.432128906 batch PCKh 0.625\n",
      "Trained batch 2012 batch loss 0.620402336 batch mAP 0.251739502 batch PCKh 0.5\n",
      "Trained batch 2013 batch loss 0.613129854 batch mAP 0.380432129 batch PCKh 0.4375\n",
      "Trained batch 2014 batch loss 0.540573 batch mAP 0.4324646 batch PCKh 0.1875\n",
      "Trained batch 2015 batch loss 0.595399916 batch mAP 0.521087646 batch PCKh 0.5\n",
      "Trained batch 2016 batch loss 0.582597 batch mAP 0.463745117 batch PCKh 0.1875\n",
      "Trained batch 2017 batch loss 0.6017887 batch mAP 0.473358154 batch PCKh 0.125\n",
      "Trained batch 2018 batch loss 0.500693858 batch mAP 0.464538574 batch PCKh 0.25\n",
      "Trained batch 2019 batch loss 0.506704271 batch mAP 0.446746826 batch PCKh 0.3125\n",
      "Trained batch 2020 batch loss 0.499784 batch mAP 0.413330078 batch PCKh 0.6875\n",
      "Trained batch 2021 batch loss 0.558221102 batch mAP 0.410308838 batch PCKh 0.375\n",
      "Trained batch 2022 batch loss 0.500962198 batch mAP 0.448730469 batch PCKh 0.5625\n",
      "Trained batch 2023 batch loss 0.577710569 batch mAP 0.401000977 batch PCKh 0.375\n",
      "Trained batch 2024 batch loss 0.557825446 batch mAP 0.468231201 batch PCKh 0.3125\n",
      "Trained batch 2025 batch loss 0.543409467 batch mAP 0.49697876 batch PCKh 0.3125\n",
      "Trained batch 2026 batch loss 0.596940756 batch mAP 0.401580811 batch PCKh 0.3125\n",
      "Trained batch 2027 batch loss 0.642033756 batch mAP 0.504974365 batch PCKh 0.6875\n",
      "Trained batch 2028 batch loss 0.679225385 batch mAP 0.537841797 batch PCKh 0.4375\n",
      "Trained batch 2029 batch loss 0.703036904 batch mAP 0.465515137 batch PCKh 0.375\n",
      "Trained batch 2030 batch loss 0.681624055 batch mAP 0.502075195 batch PCKh 0.25\n",
      "Trained batch 2031 batch loss 0.706874669 batch mAP 0.443481445 batch PCKh 0.25\n",
      "Trained batch 2032 batch loss 0.67815727 batch mAP 0.494812 batch PCKh 0.25\n",
      "Trained batch 2033 batch loss 0.632407069 batch mAP 0.408294678 batch PCKh 0.75\n",
      "Trained batch 2034 batch loss 0.531060517 batch mAP 0.365844727 batch PCKh 0.625\n",
      "Trained batch 2035 batch loss 0.581015408 batch mAP 0.230621338 batch PCKh 0.875\n",
      "Trained batch 2036 batch loss 0.557621777 batch mAP 0.306427 batch PCKh 0.25\n",
      "Trained batch 2037 batch loss 0.577954292 batch mAP 0.149932861 batch PCKh 0.5625\n",
      "Trained batch 2038 batch loss 0.576330066 batch mAP 0.140136719 batch PCKh 0.5\n",
      "Trained batch 2039 batch loss 0.647606969 batch mAP 0.0392150879 batch PCKh 0.5625\n",
      "Trained batch 2040 batch loss 0.615226507 batch mAP 0.0404968262 batch PCKh 0.0625\n",
      "Trained batch 2041 batch loss 0.536387503 batch mAP 0.0316162109 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2042 batch loss 0.570386887 batch mAP 0.0496826172 batch PCKh 0.375\n",
      "Trained batch 2043 batch loss 0.567951381 batch mAP 0.283325195 batch PCKh 0.5\n",
      "Trained batch 2044 batch loss 0.561445415 batch mAP 0.311431885 batch PCKh 0.25\n",
      "Trained batch 2045 batch loss 0.607511878 batch mAP 0.461792 batch PCKh 0.5\n",
      "Trained batch 2046 batch loss 0.554967046 batch mAP 0.4765625 batch PCKh 0.25\n",
      "Trained batch 2047 batch loss 0.541147113 batch mAP 0.5831604 batch PCKh 0.1875\n",
      "Trained batch 2048 batch loss 0.558440447 batch mAP 0.496398926 batch PCKh 0.4375\n",
      "Trained batch 2049 batch loss 0.489161283 batch mAP 0.516143799 batch PCKh 0.375\n",
      "Trained batch 2050 batch loss 0.643273234 batch mAP 0.181488037 batch PCKh 0.1875\n",
      "Trained batch 2051 batch loss 0.660165489 batch mAP 0.134368896 batch PCKh 0.5625\n",
      "Trained batch 2052 batch loss 0.522765577 batch mAP 0.511657715 batch PCKh 0.5625\n",
      "Trained batch 2053 batch loss 0.495876074 batch mAP 0.423339844 batch PCKh 0.375\n",
      "Trained batch 2054 batch loss 0.538145065 batch mAP 0.365600586 batch PCKh 0.4375\n",
      "Trained batch 2055 batch loss 0.602793932 batch mAP 0.325927734 batch PCKh 0.3125\n",
      "Trained batch 2056 batch loss 0.579230964 batch mAP 0.246521 batch PCKh 0.1875\n",
      "Trained batch 2057 batch loss 0.542331517 batch mAP 0.234527588 batch PCKh 0.125\n",
      "Trained batch 2058 batch loss 0.530153513 batch mAP 0.261138916 batch PCKh 0.1875\n",
      "Trained batch 2059 batch loss 0.569703758 batch mAP 0.280853271 batch PCKh 0.1875\n",
      "Trained batch 2060 batch loss 0.526510179 batch mAP 0.565948486 batch PCKh 0.375\n",
      "Trained batch 2061 batch loss 0.534171224 batch mAP 0.55657959 batch PCKh 0.3125\n",
      "Trained batch 2062 batch loss 0.524620533 batch mAP 0.539581299 batch PCKh 0.0625\n",
      "Trained batch 2063 batch loss 0.508685648 batch mAP 0.496459961 batch PCKh 0.125\n",
      "Trained batch 2064 batch loss 0.505664825 batch mAP 0.491149902 batch PCKh 0\n",
      "Trained batch 2065 batch loss 0.589976549 batch mAP 0.510376 batch PCKh 0.1875\n",
      "Trained batch 2066 batch loss 0.559415638 batch mAP 0.47177124 batch PCKh 0.4375\n",
      "Trained batch 2067 batch loss 0.596819878 batch mAP 0.475799561 batch PCKh 0.375\n",
      "Trained batch 2068 batch loss 0.636822462 batch mAP 0.478942871 batch PCKh 0.375\n",
      "Trained batch 2069 batch loss 0.633719444 batch mAP 0.462585449 batch PCKh 0.4375\n",
      "Trained batch 2070 batch loss 0.581429541 batch mAP 0.526062 batch PCKh 0.1875\n",
      "Trained batch 2071 batch loss 0.642863512 batch mAP 0.534637451 batch PCKh 0.3125\n",
      "Trained batch 2072 batch loss 0.536904395 batch mAP 0.577209473 batch PCKh 0.4375\n",
      "Trained batch 2073 batch loss 0.645809412 batch mAP 0.586639404 batch PCKh 0.3125\n",
      "Trained batch 2074 batch loss 0.587692916 batch mAP 0.596099854 batch PCKh 0.25\n",
      "Trained batch 2075 batch loss 0.616623282 batch mAP 0.578094482 batch PCKh 0.3125\n",
      "Trained batch 2076 batch loss 0.638383687 batch mAP 0.550415039 batch PCKh 0.125\n",
      "Trained batch 2077 batch loss 0.589114368 batch mAP 0.518859863 batch PCKh 0.25\n",
      "Trained batch 2078 batch loss 0.642456591 batch mAP 0.570587158 batch PCKh 0.25\n",
      "Trained batch 2079 batch loss 0.654129744 batch mAP 0.573669434 batch PCKh 0.25\n",
      "Trained batch 2080 batch loss 0.603860378 batch mAP 0.606872559 batch PCKh 0.375\n",
      "Trained batch 2081 batch loss 0.614025474 batch mAP 0.560913086 batch PCKh 0.625\n",
      "Trained batch 2082 batch loss 0.654243886 batch mAP 0.504180908 batch PCKh 0.75\n",
      "Trained batch 2083 batch loss 0.586442351 batch mAP 0.53427124 batch PCKh 0.25\n",
      "Trained batch 2084 batch loss 0.61168766 batch mAP 0.512023926 batch PCKh 0.1875\n",
      "Trained batch 2085 batch loss 0.538060963 batch mAP 0.411834717 batch PCKh 0.3125\n",
      "Trained batch 2086 batch loss 0.57445842 batch mAP 0.458190918 batch PCKh 0\n",
      "Trained batch 2087 batch loss 0.586858749 batch mAP 0.495391846 batch PCKh 0.125\n",
      "Trained batch 2088 batch loss 0.657518685 batch mAP 0.472167969 batch PCKh 0\n",
      "Trained batch 2089 batch loss 0.637806475 batch mAP 0.482574463 batch PCKh 0\n",
      "Trained batch 2090 batch loss 0.643852711 batch mAP 0.485137939 batch PCKh 0.6875\n",
      "Trained batch 2091 batch loss 0.572650909 batch mAP 0.493438721 batch PCKh 0.375\n",
      "Trained batch 2092 batch loss 0.655859351 batch mAP 0.492340088 batch PCKh 0.5\n",
      "Trained batch 2093 batch loss 0.555579543 batch mAP 0.517852783 batch PCKh 0.1875\n",
      "Trained batch 2094 batch loss 0.532212734 batch mAP 0.44833374 batch PCKh 0.75\n",
      "Trained batch 2095 batch loss 0.553887 batch mAP 0.504425049 batch PCKh 0.75\n",
      "Trained batch 2096 batch loss 0.520397425 batch mAP 0.464996338 batch PCKh 0.1875\n",
      "Trained batch 2097 batch loss 0.552815 batch mAP 0.496887207 batch PCKh 0.8125\n",
      "Trained batch 2098 batch loss 0.524545908 batch mAP 0.491149902 batch PCKh 0.6875\n",
      "Trained batch 2099 batch loss 0.555440366 batch mAP 0.508514404 batch PCKh 0.5\n",
      "Trained batch 2100 batch loss 0.590830803 batch mAP 0.456451416 batch PCKh 0.75\n",
      "Trained batch 2101 batch loss 0.548953772 batch mAP 0.500061035 batch PCKh 0.375\n",
      "Trained batch 2102 batch loss 0.621093 batch mAP 0.531066895 batch PCKh 0.625\n",
      "Trained batch 2103 batch loss 0.569590807 batch mAP 0.572967529 batch PCKh 0.4375\n",
      "Trained batch 2104 batch loss 0.649170399 batch mAP 0.546081543 batch PCKh 0.25\n",
      "Trained batch 2105 batch loss 0.614170551 batch mAP 0.561035156 batch PCKh 0.5\n",
      "Trained batch 2106 batch loss 0.508043885 batch mAP 0.602172852 batch PCKh 0.25\n",
      "Trained batch 2107 batch loss 0.495665073 batch mAP 0.50769043 batch PCKh 0.625\n",
      "Trained batch 2108 batch loss 0.514831424 batch mAP 0.542144775 batch PCKh 0.625\n",
      "Trained batch 2109 batch loss 0.530601621 batch mAP 0.491485596 batch PCKh 0.5\n",
      "Trained batch 2110 batch loss 0.566697955 batch mAP 0.526733398 batch PCKh 0.6875\n",
      "Trained batch 2111 batch loss 0.517709911 batch mAP 0.520629883 batch PCKh 0.625\n",
      "Trained batch 2112 batch loss 0.658405721 batch mAP 0.501861572 batch PCKh 0.625\n",
      "Trained batch 2113 batch loss 0.786980629 batch mAP 0.30871582 batch PCKh 0.5\n",
      "Trained batch 2114 batch loss 0.651544929 batch mAP 0.484832764 batch PCKh 0.625\n",
      "Trained batch 2115 batch loss 0.605062842 batch mAP 0.516723633 batch PCKh 0.4375\n",
      "Trained batch 2116 batch loss 0.605898499 batch mAP 0.475494385 batch PCKh 0.6875\n",
      "Trained batch 2117 batch loss 0.598606 batch mAP 0.517974854 batch PCKh 0.375\n",
      "Trained batch 2118 batch loss 0.530490816 batch mAP 0.432678223 batch PCKh 0.5\n",
      "Trained batch 2119 batch loss 0.482403934 batch mAP 0.557983398 batch PCKh 0.8125\n",
      "Trained batch 2120 batch loss 0.42261821 batch mAP 0.556396484 batch PCKh 0.6875\n",
      "Trained batch 2121 batch loss 0.410851657 batch mAP 0.535705566 batch PCKh 0.4375\n",
      "Trained batch 2122 batch loss 0.441367984 batch mAP 0.618621826 batch PCKh 0.3125\n",
      "Trained batch 2123 batch loss 0.415215373 batch mAP 0.578186035 batch PCKh 0.5625\n",
      "Trained batch 2124 batch loss 0.471471667 batch mAP 0.559204102 batch PCKh 0.6875\n",
      "Trained batch 2125 batch loss 0.537656 batch mAP 0.577819824 batch PCKh 0.3125\n",
      "Trained batch 2126 batch loss 0.469690621 batch mAP 0.542877197 batch PCKh 0.625\n",
      "Trained batch 2127 batch loss 0.51589787 batch mAP 0.578826904 batch PCKh 0.6875\n",
      "Trained batch 2128 batch loss 0.513530195 batch mAP 0.517944336 batch PCKh 0.375\n",
      "Trained batch 2129 batch loss 0.594817758 batch mAP 0.508880615 batch PCKh 0.75\n",
      "Trained batch 2130 batch loss 0.550454617 batch mAP 0.574554443 batch PCKh 0.625\n",
      "Trained batch 2131 batch loss 0.5794909 batch mAP 0.57043457 batch PCKh 0.125\n",
      "Trained batch 2132 batch loss 0.5570364 batch mAP 0.59362793 batch PCKh 0.5625\n",
      "Trained batch 2133 batch loss 0.501572967 batch mAP 0.495605469 batch PCKh 0.5\n",
      "Trained batch 2134 batch loss 0.64898932 batch mAP 0.565887451 batch PCKh 0.1875\n",
      "Trained batch 2135 batch loss 0.628790855 batch mAP 0.512023926 batch PCKh 0.25\n",
      "Trained batch 2136 batch loss 0.646571398 batch mAP 0.532714844 batch PCKh 0.5\n",
      "Trained batch 2137 batch loss 0.610634565 batch mAP 0.530700684 batch PCKh 0.375\n",
      "Trained batch 2138 batch loss 0.588124037 batch mAP 0.529937744 batch PCKh 0.625\n",
      "Trained batch 2139 batch loss 0.579171062 batch mAP 0.491088867 batch PCKh 0.5\n",
      "Trained batch 2140 batch loss 0.564367831 batch mAP 0.561309814 batch PCKh 0.5\n",
      "Trained batch 2141 batch loss 0.645050645 batch mAP 0.492462158 batch PCKh 0.6875\n",
      "Trained batch 2142 batch loss 0.606705487 batch mAP 0.472503662 batch PCKh 0.1875\n",
      "Trained batch 2143 batch loss 0.706493735 batch mAP 0.47265625 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2144 batch loss 0.613928735 batch mAP 0.549560547 batch PCKh 0.1875\n",
      "Trained batch 2145 batch loss 0.62807548 batch mAP 0.472442627 batch PCKh 0.5\n",
      "Trained batch 2146 batch loss 0.623383522 batch mAP 0.461914062 batch PCKh 0.625\n",
      "Trained batch 2147 batch loss 0.663350344 batch mAP 0.448364258 batch PCKh 0.5\n",
      "Trained batch 2148 batch loss 0.656644702 batch mAP 0.474823 batch PCKh 0.6875\n",
      "Trained batch 2149 batch loss 0.551956475 batch mAP 0.456665039 batch PCKh 0.4375\n",
      "Trained batch 2150 batch loss 0.596905828 batch mAP 0.461242676 batch PCKh 0.75\n",
      "Trained batch 2151 batch loss 0.56192112 batch mAP 0.466186523 batch PCKh 0.5625\n",
      "Trained batch 2152 batch loss 0.510946929 batch mAP 0.520202637 batch PCKh 0.375\n",
      "Trained batch 2153 batch loss 0.538368106 batch mAP 0.488891602 batch PCKh 0.625\n",
      "Trained batch 2154 batch loss 0.645843506 batch mAP 0.457794189 batch PCKh 0.125\n",
      "Trained batch 2155 batch loss 0.61935091 batch mAP 0.495513916 batch PCKh 0.125\n",
      "Trained batch 2156 batch loss 0.597460747 batch mAP 0.50567627 batch PCKh 0.125\n",
      "Trained batch 2157 batch loss 0.574075699 batch mAP 0.485168457 batch PCKh 0.5\n",
      "Trained batch 2158 batch loss 0.587240577 batch mAP 0.501251221 batch PCKh 0.3125\n",
      "Trained batch 2159 batch loss 0.638338685 batch mAP 0.516998291 batch PCKh 0.1875\n",
      "Trained batch 2160 batch loss 0.705842435 batch mAP 0.511413574 batch PCKh 0.25\n",
      "Trained batch 2161 batch loss 0.703379154 batch mAP 0.543518066 batch PCKh 0.125\n",
      "Trained batch 2162 batch loss 0.674850464 batch mAP 0.507507324 batch PCKh 0.875\n",
      "Trained batch 2163 batch loss 0.630326748 batch mAP 0.498565674 batch PCKh 0.125\n",
      "Trained batch 2164 batch loss 0.565303504 batch mAP 0.484832764 batch PCKh 0.25\n",
      "Trained batch 2165 batch loss 0.525437951 batch mAP 0.466430664 batch PCKh 0.4375\n",
      "Trained batch 2166 batch loss 0.529468894 batch mAP 0.504394531 batch PCKh 0.375\n",
      "Trained batch 2167 batch loss 0.587907791 batch mAP 0.503936768 batch PCKh 0.625\n",
      "Trained batch 2168 batch loss 0.631932855 batch mAP 0.507263184 batch PCKh 0.5\n",
      "Trained batch 2169 batch loss 0.614604115 batch mAP 0.547485352 batch PCKh 0.25\n",
      "Trained batch 2170 batch loss 0.496858239 batch mAP 0.473052979 batch PCKh 0.375\n",
      "Trained batch 2171 batch loss 0.56814754 batch mAP 0.438781738 batch PCKh 0\n",
      "Trained batch 2172 batch loss 0.752752185 batch mAP 0.459625244 batch PCKh 0\n",
      "Trained batch 2173 batch loss 0.736051559 batch mAP 0.520141602 batch PCKh 0.1875\n",
      "Trained batch 2174 batch loss 0.654369831 batch mAP 0.544830322 batch PCKh 0.375\n",
      "Trained batch 2175 batch loss 0.730154812 batch mAP 0.565734863 batch PCKh 0.1875\n",
      "Trained batch 2176 batch loss 0.600184441 batch mAP 0.559845 batch PCKh 0.1875\n",
      "Trained batch 2177 batch loss 0.667374611 batch mAP 0.457946777 batch PCKh 0.3125\n",
      "Trained batch 2178 batch loss 0.727627754 batch mAP 0.469085693 batch PCKh 0.5625\n",
      "Trained batch 2179 batch loss 0.652441561 batch mAP 0.419250488 batch PCKh 0.75\n",
      "Trained batch 2180 batch loss 0.585184574 batch mAP 0.42477417 batch PCKh 0.5\n",
      "Trained batch 2181 batch loss 0.599963427 batch mAP 0.430450439 batch PCKh 0.625\n",
      "Trained batch 2182 batch loss 0.59538883 batch mAP 0.490600586 batch PCKh 0.5625\n",
      "Trained batch 2183 batch loss 0.621072531 batch mAP 0.451324463 batch PCKh 0.125\n",
      "Trained batch 2184 batch loss 0.558599 batch mAP 0.490814209 batch PCKh 0.4375\n",
      "Trained batch 2185 batch loss 0.595456 batch mAP 0.470031738 batch PCKh 0.5625\n",
      "Trained batch 2186 batch loss 0.560715437 batch mAP 0.513580322 batch PCKh 0.4375\n",
      "Trained batch 2187 batch loss 0.677570403 batch mAP 0.488616943 batch PCKh 0.5625\n",
      "Trained batch 2188 batch loss 0.641114 batch mAP 0.554473877 batch PCKh 0.0625\n",
      "Trained batch 2189 batch loss 0.632846057 batch mAP 0.53503418 batch PCKh 0.5625\n",
      "Trained batch 2190 batch loss 0.587528467 batch mAP 0.545898438 batch PCKh 0.625\n",
      "Trained batch 2191 batch loss 0.595124602 batch mAP 0.484191895 batch PCKh 0\n",
      "Trained batch 2192 batch loss 0.590013146 batch mAP 0.547149658 batch PCKh 0.25\n",
      "Trained batch 2193 batch loss 0.55637 batch mAP 0.507171631 batch PCKh 0.625\n",
      "Trained batch 2194 batch loss 0.606586635 batch mAP 0.539550781 batch PCKh 0.625\n",
      "Trained batch 2195 batch loss 0.620804667 batch mAP 0.462524414 batch PCKh 0.75\n",
      "Trained batch 2196 batch loss 0.534744501 batch mAP 0.46862793 batch PCKh 0.5\n",
      "Trained batch 2197 batch loss 0.56116873 batch mAP 0.47052002 batch PCKh 0.1875\n",
      "Trained batch 2198 batch loss 0.534482479 batch mAP 0.541717529 batch PCKh 0.5\n",
      "Trained batch 2199 batch loss 0.565961 batch mAP 0.507354736 batch PCKh 0.5\n",
      "Trained batch 2200 batch loss 0.543604732 batch mAP 0.504180908 batch PCKh 0.75\n",
      "Trained batch 2201 batch loss 0.455027431 batch mAP 0.51675415 batch PCKh 0.75\n",
      "Trained batch 2202 batch loss 0.473433316 batch mAP 0.522125244 batch PCKh 0.75\n",
      "Trained batch 2203 batch loss 0.482865304 batch mAP 0.562774658 batch PCKh 0.75\n",
      "Trained batch 2204 batch loss 0.569288254 batch mAP 0.501464844 batch PCKh 0.875\n",
      "Trained batch 2205 batch loss 0.590132058 batch mAP 0.496154785 batch PCKh 0.5625\n",
      "Trained batch 2206 batch loss 0.722618461 batch mAP 0.458099365 batch PCKh 0\n",
      "Trained batch 2207 batch loss 0.590105295 batch mAP 0.487243652 batch PCKh 0.3125\n",
      "Trained batch 2208 batch loss 0.610340238 batch mAP 0.444854736 batch PCKh 0.875\n",
      "Trained batch 2209 batch loss 0.66411376 batch mAP 0.468597412 batch PCKh 0.25\n",
      "Trained batch 2210 batch loss 0.738606572 batch mAP 0.4715271 batch PCKh 0.375\n",
      "Trained batch 2211 batch loss 0.650270343 batch mAP 0.487030029 batch PCKh 0.4375\n",
      "Trained batch 2212 batch loss 0.658102751 batch mAP 0.490081787 batch PCKh 0.25\n",
      "Trained batch 2213 batch loss 0.701326609 batch mAP 0.417541504 batch PCKh 0.3125\n",
      "Trained batch 2214 batch loss 0.535773098 batch mAP 0.436615 batch PCKh 0.125\n",
      "Trained batch 2215 batch loss 0.494002163 batch mAP 0.417022705 batch PCKh 0.1875\n",
      "Trained batch 2216 batch loss 0.632233 batch mAP 0.408172607 batch PCKh 0.4375\n",
      "Trained batch 2217 batch loss 0.65803194 batch mAP 0.37097168 batch PCKh 0.1875\n",
      "Trained batch 2218 batch loss 0.683453918 batch mAP 0.362670898 batch PCKh 0\n",
      "Trained batch 2219 batch loss 0.571339846 batch mAP 0.436462402 batch PCKh 0.375\n",
      "Trained batch 2220 batch loss 0.648030818 batch mAP 0.432281494 batch PCKh 0.5\n",
      "Trained batch 2221 batch loss 0.662028849 batch mAP 0.433959961 batch PCKh 0.375\n",
      "Trained batch 2222 batch loss 0.659734249 batch mAP 0.494842529 batch PCKh 0.875\n",
      "Trained batch 2223 batch loss 0.690469861 batch mAP 0.502044678 batch PCKh 0.1875\n",
      "Trained batch 2224 batch loss 0.754939 batch mAP 0.436187744 batch PCKh 0\n",
      "Trained batch 2225 batch loss 0.643285275 batch mAP 0.483337402 batch PCKh 0.75\n",
      "Trained batch 2226 batch loss 0.592403591 batch mAP 0.5262146 batch PCKh 0.375\n",
      "Trained batch 2227 batch loss 0.610810637 batch mAP 0.525115967 batch PCKh 0.8125\n",
      "Trained batch 2228 batch loss 0.598194599 batch mAP 0.530914307 batch PCKh 0.6875\n",
      "Trained batch 2229 batch loss 0.575490713 batch mAP 0.521972656 batch PCKh 0.5\n",
      "Trained batch 2230 batch loss 0.606404662 batch mAP 0.514312744 batch PCKh 0.8125\n",
      "Trained batch 2231 batch loss 0.588177621 batch mAP 0.540130615 batch PCKh 0.6875\n",
      "Trained batch 2232 batch loss 0.563745737 batch mAP 0.507385254 batch PCKh 0.5625\n",
      "Trained batch 2233 batch loss 0.682224035 batch mAP 0.508270264 batch PCKh 0.3125\n",
      "Trained batch 2234 batch loss 0.607256174 batch mAP 0.522064209 batch PCKh 0.1875\n",
      "Trained batch 2235 batch loss 0.646689355 batch mAP 0.558807373 batch PCKh 0.1875\n",
      "Trained batch 2236 batch loss 0.575519264 batch mAP 0.516387939 batch PCKh 0.25\n",
      "Trained batch 2237 batch loss 0.631938398 batch mAP 0.47088623 batch PCKh 0.5\n",
      "Trained batch 2238 batch loss 0.6629197 batch mAP 0.511779785 batch PCKh 0.25\n",
      "Trained batch 2239 batch loss 0.70438534 batch mAP 0.460113525 batch PCKh 0.25\n",
      "Trained batch 2240 batch loss 0.648634553 batch mAP 0.571685791 batch PCKh 0.25\n",
      "Trained batch 2241 batch loss 0.672071815 batch mAP 0.586364746 batch PCKh 0.3125\n",
      "Trained batch 2242 batch loss 0.665637076 batch mAP 0.538604736 batch PCKh 0.1875\n",
      "Trained batch 2243 batch loss 0.682556331 batch mAP 0.55581665 batch PCKh 0.625\n",
      "Trained batch 2244 batch loss 0.741370082 batch mAP 0.375396729 batch PCKh 0.0625\n",
      "Trained batch 2245 batch loss 0.648922324 batch mAP 0.557189941 batch PCKh 0.3125\n",
      "Trained batch 2246 batch loss 0.65582943 batch mAP 0.524780273 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2247 batch loss 0.671760738 batch mAP 0.537994385 batch PCKh 0.5\n",
      "Trained batch 2248 batch loss 0.700087309 batch mAP 0.521759033 batch PCKh 0.8125\n",
      "Trained batch 2249 batch loss 0.699348509 batch mAP 0.537780762 batch PCKh 0.375\n",
      "Trained batch 2250 batch loss 0.67051667 batch mAP 0.485748291 batch PCKh 0.3125\n",
      "Trained batch 2251 batch loss 0.614165783 batch mAP 0.511047363 batch PCKh 0.625\n",
      "Trained batch 2252 batch loss 0.637130499 batch mAP 0.537689209 batch PCKh 0.625\n",
      "Trained batch 2253 batch loss 0.635128617 batch mAP 0.528381348 batch PCKh 0.625\n",
      "Trained batch 2254 batch loss 0.617922425 batch mAP 0.509521484 batch PCKh 0.5625\n",
      "Trained batch 2255 batch loss 0.624239922 batch mAP 0.501525879 batch PCKh 0.5625\n",
      "Trained batch 2256 batch loss 0.633592963 batch mAP 0.477752686 batch PCKh 0.1875\n",
      "Trained batch 2257 batch loss 0.659560323 batch mAP 0.513366699 batch PCKh 0.125\n",
      "Trained batch 2258 batch loss 0.683461726 batch mAP 0.499359131 batch PCKh 0.4375\n",
      "Trained batch 2259 batch loss 0.597616 batch mAP 0.53225708 batch PCKh 0.1875\n",
      "Trained batch 2260 batch loss 0.546190381 batch mAP 0.576507568 batch PCKh 0.25\n",
      "Trained batch 2261 batch loss 0.581614 batch mAP 0.572631836 batch PCKh 0.1875\n",
      "Trained batch 2262 batch loss 0.590676963 batch mAP 0.564819336 batch PCKh 0.5\n",
      "Trained batch 2263 batch loss 0.589895844 batch mAP 0.557189941 batch PCKh 0.25\n",
      "Trained batch 2264 batch loss 0.677007198 batch mAP 0.49118042 batch PCKh 0.1875\n",
      "Trained batch 2265 batch loss 0.608524919 batch mAP 0.406341553 batch PCKh 0.375\n",
      "Trained batch 2266 batch loss 0.482255191 batch mAP 0.243682861 batch PCKh 0\n",
      "Trained batch 2267 batch loss 0.452920437 batch mAP 0.24432373 batch PCKh 0\n",
      "Trained batch 2268 batch loss 0.656163335 batch mAP 0.272583 batch PCKh 0.125\n",
      "Trained batch 2269 batch loss 0.665364742 batch mAP 0.394287109 batch PCKh 0.375\n",
      "Trained batch 2270 batch loss 0.674019098 batch mAP 0.357086182 batch PCKh 0.1875\n",
      "Trained batch 2271 batch loss 0.729236841 batch mAP 0.442993164 batch PCKh 0.375\n",
      "Trained batch 2272 batch loss 0.69233048 batch mAP 0.474304199 batch PCKh 0.125\n",
      "Trained batch 2273 batch loss 0.612793207 batch mAP 0.507873535 batch PCKh 0.1875\n",
      "Trained batch 2274 batch loss 0.540935755 batch mAP 0.57220459 batch PCKh 0.5625\n",
      "Trained batch 2275 batch loss 0.492660135 batch mAP 0.597045898 batch PCKh 0.4375\n",
      "Trained batch 2276 batch loss 0.477062643 batch mAP 0.577331543 batch PCKh 0.375\n",
      "Trained batch 2277 batch loss 0.513234913 batch mAP 0.611602783 batch PCKh 0.625\n",
      "Trained batch 2278 batch loss 0.524496555 batch mAP 0.55645752 batch PCKh 0.3125\n",
      "Trained batch 2279 batch loss 0.478936315 batch mAP 0.503326416 batch PCKh 0.4375\n",
      "Trained batch 2280 batch loss 0.544050574 batch mAP 0.493652344 batch PCKh 0.5\n",
      "Trained batch 2281 batch loss 0.549723744 batch mAP 0.515594482 batch PCKh 0.3125\n",
      "Trained batch 2282 batch loss 0.484321654 batch mAP 0.579467773 batch PCKh 0.375\n",
      "Trained batch 2283 batch loss 0.528316796 batch mAP 0.59954834 batch PCKh 0.4375\n",
      "Trained batch 2284 batch loss 0.472700089 batch mAP 0.648925781 batch PCKh 0.4375\n",
      "Trained batch 2285 batch loss 0.512553275 batch mAP 0.539093 batch PCKh 0.3125\n",
      "Trained batch 2286 batch loss 0.548776567 batch mAP 0.575378418 batch PCKh 0.375\n",
      "Trained batch 2287 batch loss 0.536778331 batch mAP 0.506011963 batch PCKh 0.625\n",
      "Trained batch 2288 batch loss 0.55774045 batch mAP 0.522796631 batch PCKh 0.875\n",
      "Trained batch 2289 batch loss 0.557989776 batch mAP 0.439208984 batch PCKh 0.8125\n",
      "Trained batch 2290 batch loss 0.564013 batch mAP 0.469696045 batch PCKh 0.6875\n",
      "Trained batch 2291 batch loss 0.509531736 batch mAP 0.568511963 batch PCKh 0.5625\n",
      "Trained batch 2292 batch loss 0.620023 batch mAP 0.470977783 batch PCKh 0.75\n",
      "Trained batch 2293 batch loss 0.575720727 batch mAP 0.478546143 batch PCKh 0.875\n",
      "Trained batch 2294 batch loss 0.64062 batch mAP 0.492675781 batch PCKh 0.75\n",
      "Trained batch 2295 batch loss 0.592323482 batch mAP 0.502441406 batch PCKh 0.75\n",
      "Trained batch 2296 batch loss 0.579719543 batch mAP 0.463317871 batch PCKh 0.6875\n",
      "Trained batch 2297 batch loss 0.682450414 batch mAP 0.46685791 batch PCKh 0.5625\n",
      "Trained batch 2298 batch loss 0.594175 batch mAP 0.477905273 batch PCKh 0.25\n",
      "Trained batch 2299 batch loss 0.571151078 batch mAP 0.438079834 batch PCKh 0.125\n",
      "Trained batch 2300 batch loss 0.565794945 batch mAP 0.465789795 batch PCKh 0.75\n",
      "Trained batch 2301 batch loss 0.603739381 batch mAP 0.451690674 batch PCKh 0.5\n",
      "Trained batch 2302 batch loss 0.608059049 batch mAP 0.438354492 batch PCKh 0.1875\n",
      "Trained batch 2303 batch loss 0.657149076 batch mAP 0.418182373 batch PCKh 0.625\n",
      "Trained batch 2304 batch loss 0.550981879 batch mAP 0.401916504 batch PCKh 0.125\n",
      "Trained batch 2305 batch loss 0.653489828 batch mAP 0.402160645 batch PCKh 0.5625\n",
      "Trained batch 2306 batch loss 0.57416743 batch mAP 0.390869141 batch PCKh 0.75\n",
      "Trained batch 2307 batch loss 0.633686662 batch mAP 0.426177979 batch PCKh 0.625\n",
      "Trained batch 2308 batch loss 0.652834058 batch mAP 0.30480957 batch PCKh 0.625\n",
      "Trained batch 2309 batch loss 0.691066265 batch mAP 0.361175537 batch PCKh 0.625\n",
      "Trained batch 2310 batch loss 0.608736157 batch mAP 0.388336182 batch PCKh 0.625\n",
      "Trained batch 2311 batch loss 0.530585885 batch mAP 0.458740234 batch PCKh 0.25\n",
      "Trained batch 2312 batch loss 0.578269 batch mAP 0.482025146 batch PCKh 0.5625\n",
      "Trained batch 2313 batch loss 0.555090785 batch mAP 0.499938965 batch PCKh 0.5625\n",
      "Trained batch 2314 batch loss 0.595791101 batch mAP 0.466674805 batch PCKh 0.3125\n",
      "Trained batch 2315 batch loss 0.582522392 batch mAP 0.501312256 batch PCKh 0.125\n",
      "Trained batch 2316 batch loss 0.597195625 batch mAP 0.486175537 batch PCKh 0.6875\n",
      "Trained batch 2317 batch loss 0.535711646 batch mAP 0.522216797 batch PCKh 0.5\n",
      "Trained batch 2318 batch loss 0.562658668 batch mAP 0.492248535 batch PCKh 0.125\n",
      "Trained batch 2319 batch loss 0.586843252 batch mAP 0.545440674 batch PCKh 0.4375\n",
      "Trained batch 2320 batch loss 0.52437 batch mAP 0.577941895 batch PCKh 0.625\n",
      "Trained batch 2321 batch loss 0.590240538 batch mAP 0.585998535 batch PCKh 0.75\n",
      "Trained batch 2322 batch loss 0.535610318 batch mAP 0.606811523 batch PCKh 0.3125\n",
      "Trained batch 2323 batch loss 0.546135902 batch mAP 0.634521484 batch PCKh 0.4375\n",
      "Trained batch 2324 batch loss 0.607784867 batch mAP 0.559570312 batch PCKh 0.75\n",
      "Trained batch 2325 batch loss 0.680256188 batch mAP 0.502380371 batch PCKh 0.25\n",
      "Trained batch 2326 batch loss 0.675598741 batch mAP 0.508850098 batch PCKh 0.75\n",
      "Trained batch 2327 batch loss 0.721715 batch mAP 0.484527588 batch PCKh 0.125\n",
      "Trained batch 2328 batch loss 0.702500343 batch mAP 0.498809814 batch PCKh 0.8125\n",
      "Trained batch 2329 batch loss 0.690751374 batch mAP 0.514526367 batch PCKh 0.125\n",
      "Trained batch 2330 batch loss 0.698920429 batch mAP 0.4503479 batch PCKh 0\n",
      "Trained batch 2331 batch loss 0.591256738 batch mAP 0.521209717 batch PCKh 0.5\n",
      "Trained batch 2332 batch loss 0.640082359 batch mAP 0.503967285 batch PCKh 0.5625\n",
      "Trained batch 2333 batch loss 0.58436048 batch mAP 0.462432861 batch PCKh 0.375\n",
      "Trained batch 2334 batch loss 0.599241495 batch mAP 0.418121338 batch PCKh 0.4375\n",
      "Trained batch 2335 batch loss 0.535444915 batch mAP 0.447113037 batch PCKh 0.5\n",
      "Trained batch 2336 batch loss 0.526573539 batch mAP 0.448791504 batch PCKh 0.5\n",
      "Trained batch 2337 batch loss 0.53756237 batch mAP 0.458313 batch PCKh 0.375\n",
      "Trained batch 2338 batch loss 0.45474425 batch mAP 0.494995117 batch PCKh 0\n",
      "Trained batch 2339 batch loss 0.436707646 batch mAP 0.499603271 batch PCKh 0.5\n",
      "Trained batch 2340 batch loss 0.458082467 batch mAP 0.503723145 batch PCKh 0.6875\n",
      "Trained batch 2341 batch loss 0.444045663 batch mAP 0.54422 batch PCKh 0.6875\n",
      "Trained batch 2342 batch loss 0.475264788 batch mAP 0.553070068 batch PCKh 0.1875\n",
      "Trained batch 2343 batch loss 0.504879832 batch mAP 0.6434021 batch PCKh 0.25\n",
      "Trained batch 2344 batch loss 0.429589689 batch mAP 0.633544922 batch PCKh 0.3125\n",
      "Trained batch 2345 batch loss 0.484294802 batch mAP 0.624206543 batch PCKh 0.4375\n",
      "Trained batch 2346 batch loss 0.596851468 batch mAP 0.575408936 batch PCKh 0.3125\n",
      "Trained batch 2347 batch loss 0.613902211 batch mAP 0.550048828 batch PCKh 0.6875\n",
      "Trained batch 2348 batch loss 0.62664032 batch mAP 0.539367676 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2349 batch loss 0.636410534 batch mAP 0.529632568 batch PCKh 0.1875\n",
      "Trained batch 2350 batch loss 0.702720404 batch mAP 0.54888916 batch PCKh 0.125\n",
      "Trained batch 2351 batch loss 0.597261727 batch mAP 0.553619385 batch PCKh 0.25\n",
      "Trained batch 2352 batch loss 0.567091703 batch mAP 0.580108643 batch PCKh 0.4375\n",
      "Trained batch 2353 batch loss 0.663730145 batch mAP 0.588653564 batch PCKh 0.3125\n",
      "Trained batch 2354 batch loss 0.60525763 batch mAP 0.543396 batch PCKh 0.5\n",
      "Trained batch 2355 batch loss 0.650599718 batch mAP 0.5390625 batch PCKh 0.125\n",
      "Trained batch 2356 batch loss 0.655197382 batch mAP 0.507537842 batch PCKh 0.5625\n",
      "Trained batch 2357 batch loss 0.542018056 batch mAP 0.514404297 batch PCKh 0.625\n",
      "Trained batch 2358 batch loss 0.529697716 batch mAP 0.527862549 batch PCKh 0.8125\n",
      "Trained batch 2359 batch loss 0.516464 batch mAP 0.519226074 batch PCKh 0.75\n",
      "Trained batch 2360 batch loss 0.540276349 batch mAP 0.482513428 batch PCKh 0.875\n",
      "Trained batch 2361 batch loss 0.539731681 batch mAP 0.536315918 batch PCKh 0.625\n",
      "Trained batch 2362 batch loss 0.549274504 batch mAP 0.517547607 batch PCKh 0.8125\n",
      "Trained batch 2363 batch loss 0.615549207 batch mAP 0.563720703 batch PCKh 0.375\n",
      "Trained batch 2364 batch loss 0.578122139 batch mAP 0.552642822 batch PCKh 0.6875\n",
      "Trained batch 2365 batch loss 0.616152525 batch mAP 0.49710083 batch PCKh 0.625\n",
      "Trained batch 2366 batch loss 0.601935446 batch mAP 0.535827637 batch PCKh 0.3125\n",
      "Trained batch 2367 batch loss 0.597209752 batch mAP 0.527557373 batch PCKh 0.625\n",
      "Trained batch 2368 batch loss 0.624447942 batch mAP 0.481079102 batch PCKh 0.5\n",
      "Trained batch 2369 batch loss 0.672286749 batch mAP 0.479797363 batch PCKh 0.0625\n",
      "Trained batch 2370 batch loss 0.639833331 batch mAP 0.470672607 batch PCKh 0.5\n",
      "Trained batch 2371 batch loss 0.592701674 batch mAP 0.474487305 batch PCKh 0.5625\n",
      "Trained batch 2372 batch loss 0.585310221 batch mAP 0.509185791 batch PCKh 0.5625\n",
      "Trained batch 2373 batch loss 0.616439223 batch mAP 0.442108154 batch PCKh 0.5625\n",
      "Trained batch 2374 batch loss 0.69147712 batch mAP 0.412597656 batch PCKh 0.5625\n",
      "Trained batch 2375 batch loss 0.712367237 batch mAP 0.385498047 batch PCKh 0.625\n",
      "Trained batch 2376 batch loss 0.619551718 batch mAP 0.389099121 batch PCKh 0.25\n",
      "Trained batch 2377 batch loss 0.668354034 batch mAP 0.354675293 batch PCKh 0.4375\n",
      "Trained batch 2378 batch loss 0.695716798 batch mAP 0.395690918 batch PCKh 0.4375\n",
      "Trained batch 2379 batch loss 0.658845425 batch mAP 0.309051514 batch PCKh 0.8125\n",
      "Trained batch 2380 batch loss 0.639204443 batch mAP 0.304992676 batch PCKh 0.5\n",
      "Trained batch 2381 batch loss 0.646604657 batch mAP 0.312835693 batch PCKh 0.75\n",
      "Trained batch 2382 batch loss 0.710696638 batch mAP 0.371612549 batch PCKh 0.25\n",
      "Trained batch 2383 batch loss 0.682538092 batch mAP 0.342346191 batch PCKh 0.625\n",
      "Trained batch 2384 batch loss 0.60415554 batch mAP 0.313537598 batch PCKh 0.4375\n",
      "Trained batch 2385 batch loss 0.596909344 batch mAP 0.337646484 batch PCKh 0.3125\n",
      "Trained batch 2386 batch loss 0.648279548 batch mAP 0.388153076 batch PCKh 0.5\n",
      "Trained batch 2387 batch loss 0.617854774 batch mAP 0.404876709 batch PCKh 0.25\n",
      "Trained batch 2388 batch loss 0.575866818 batch mAP 0.440612793 batch PCKh 0.4375\n",
      "Trained batch 2389 batch loss 0.533003092 batch mAP 0.453552246 batch PCKh 0.1875\n",
      "Trained batch 2390 batch loss 0.547728837 batch mAP 0.467956543 batch PCKh 0.75\n",
      "Trained batch 2391 batch loss 0.619292319 batch mAP 0.430603027 batch PCKh 0.1875\n",
      "Trained batch 2392 batch loss 0.697302341 batch mAP 0.433563232 batch PCKh 0\n",
      "Trained batch 2393 batch loss 0.631712914 batch mAP 0.490203857 batch PCKh 0.75\n",
      "Trained batch 2394 batch loss 0.553770721 batch mAP 0.496734619 batch PCKh 0.875\n",
      "Trained batch 2395 batch loss 0.591911674 batch mAP 0.45501709 batch PCKh 0.25\n",
      "Trained batch 2396 batch loss 0.618269086 batch mAP 0.449798584 batch PCKh 0.4375\n",
      "Trained batch 2397 batch loss 0.552137196 batch mAP 0.44430542 batch PCKh 0.5\n",
      "Trained batch 2398 batch loss 0.616260707 batch mAP 0.431396484 batch PCKh 0.5625\n",
      "Trained batch 2399 batch loss 0.582516074 batch mAP 0.504699707 batch PCKh 0.625\n",
      "Trained batch 2400 batch loss 0.546502829 batch mAP 0.534881592 batch PCKh 0.3125\n",
      "Trained batch 2401 batch loss 0.65980649 batch mAP 0.530517578 batch PCKh 0.375\n",
      "Trained batch 2402 batch loss 0.684545 batch mAP 0.528656 batch PCKh 0.4375\n",
      "Trained batch 2403 batch loss 0.683286309 batch mAP 0.539276123 batch PCKh 0.375\n",
      "Trained batch 2404 batch loss 0.616337776 batch mAP 0.528076172 batch PCKh 0.1875\n",
      "Trained batch 2405 batch loss 0.410659671 batch mAP 0.565948486 batch PCKh 0.5\n",
      "Trained batch 2406 batch loss 0.457590878 batch mAP 0.532775879 batch PCKh 0.5\n",
      "Trained batch 2407 batch loss 0.488772035 batch mAP 0.528442383 batch PCKh 0\n",
      "Trained batch 2408 batch loss 0.535666466 batch mAP 0.527160645 batch PCKh 0\n",
      "Trained batch 2409 batch loss 0.587444127 batch mAP 0.561737061 batch PCKh 0.5\n",
      "Trained batch 2410 batch loss 0.598847747 batch mAP 0.512420654 batch PCKh 0.625\n",
      "Trained batch 2411 batch loss 0.565532088 batch mAP 0.544830322 batch PCKh 0.125\n",
      "Trained batch 2412 batch loss 0.531059623 batch mAP 0.556518555 batch PCKh 0.75\n",
      "Trained batch 2413 batch loss 0.572784126 batch mAP 0.589630127 batch PCKh 0.4375\n",
      "Trained batch 2414 batch loss 0.578053892 batch mAP 0.577423096 batch PCKh 0.25\n",
      "Trained batch 2415 batch loss 0.554402113 batch mAP 0.582397461 batch PCKh 0.3125\n",
      "Trained batch 2416 batch loss 0.624097466 batch mAP 0.517364502 batch PCKh 0.5625\n",
      "Trained batch 2417 batch loss 0.568337202 batch mAP 0.502868652 batch PCKh 0.375\n",
      "Trained batch 2418 batch loss 0.662606776 batch mAP 0.514434814 batch PCKh 0.75\n",
      "Trained batch 2419 batch loss 0.62841469 batch mAP 0.505493164 batch PCKh 0.375\n",
      "Trained batch 2420 batch loss 0.598384559 batch mAP 0.520172119 batch PCKh 0.6875\n",
      "Trained batch 2421 batch loss 0.675386727 batch mAP 0.467407227 batch PCKh 0.625\n",
      "Trained batch 2422 batch loss 0.612354815 batch mAP 0.482330322 batch PCKh 0.125\n",
      "Trained batch 2423 batch loss 0.602535307 batch mAP 0.43572998 batch PCKh 0.625\n",
      "Trained batch 2424 batch loss 0.590573728 batch mAP 0.482330322 batch PCKh 0.75\n",
      "Trained batch 2425 batch loss 0.685695648 batch mAP 0.46307373 batch PCKh 0.25\n",
      "Trained batch 2426 batch loss 0.625264883 batch mAP 0.50012207 batch PCKh 0.375\n",
      "Trained batch 2427 batch loss 0.680934131 batch mAP 0.473571777 batch PCKh 0.0625\n",
      "Trained batch 2428 batch loss 0.739533424 batch mAP 0.462677 batch PCKh 0.1875\n",
      "Trained batch 2429 batch loss 0.728674173 batch mAP 0.473968506 batch PCKh 0.25\n",
      "Trained batch 2430 batch loss 0.62197578 batch mAP 0.428894043 batch PCKh 0.8125\n",
      "Trained batch 2431 batch loss 0.625635684 batch mAP 0.420501709 batch PCKh 0.5\n",
      "Trained batch 2432 batch loss 0.647911191 batch mAP 0.41494751 batch PCKh 0.5\n",
      "Trained batch 2433 batch loss 0.581065416 batch mAP 0.463775635 batch PCKh 0.75\n",
      "Trained batch 2434 batch loss 0.586890578 batch mAP 0.462768555 batch PCKh 0.6875\n",
      "Trained batch 2435 batch loss 0.567399859 batch mAP 0.48223877 batch PCKh 0.5625\n",
      "Trained batch 2436 batch loss 0.626685798 batch mAP 0.510437 batch PCKh 0.3125\n",
      "Trained batch 2437 batch loss 0.609518 batch mAP 0.496063232 batch PCKh 0.3125\n",
      "Trained batch 2438 batch loss 0.579898894 batch mAP 0.518127441 batch PCKh 0.8125\n",
      "Trained batch 2439 batch loss 0.588499427 batch mAP 0.509399414 batch PCKh 0.8125\n",
      "Trained batch 2440 batch loss 0.654631 batch mAP 0.512329102 batch PCKh 0.375\n",
      "Trained batch 2441 batch loss 0.655821323 batch mAP 0.524414062 batch PCKh 0.8125\n",
      "Trained batch 2442 batch loss 0.584829211 batch mAP 0.486846924 batch PCKh 0.875\n",
      "Trained batch 2443 batch loss 0.620053291 batch mAP 0.523864746 batch PCKh 0.5625\n",
      "Trained batch 2444 batch loss 0.561482549 batch mAP 0.539123535 batch PCKh 0.75\n",
      "Trained batch 2445 batch loss 0.537311912 batch mAP 0.522888184 batch PCKh 0.375\n",
      "Trained batch 2446 batch loss 0.564061284 batch mAP 0.576721191 batch PCKh 0.5625\n",
      "Trained batch 2447 batch loss 0.547629893 batch mAP 0.50402832 batch PCKh 0.5625\n",
      "Trained batch 2448 batch loss 0.534778953 batch mAP 0.519470215 batch PCKh 0.5\n",
      "Trained batch 2449 batch loss 0.628191352 batch mAP 0.505615234 batch PCKh 0.75\n",
      "Trained batch 2450 batch loss 0.623168945 batch mAP 0.403137207 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2451 batch loss 0.573163927 batch mAP 0.499450684 batch PCKh 0.75\n",
      "Trained batch 2452 batch loss 0.63341403 batch mAP 0.504394531 batch PCKh 0.5625\n",
      "Trained batch 2453 batch loss 0.705569625 batch mAP 0.523986816 batch PCKh 0.75\n",
      "Trained batch 2454 batch loss 0.660622478 batch mAP 0.50869751 batch PCKh 0.1875\n",
      "Trained batch 2455 batch loss 0.576177418 batch mAP 0.487243652 batch PCKh 0\n",
      "Trained batch 2456 batch loss 0.604838371 batch mAP 0.570007324 batch PCKh 0.6875\n",
      "Trained batch 2457 batch loss 0.588073611 batch mAP 0.555114746 batch PCKh 0.4375\n",
      "Trained batch 2458 batch loss 0.558915615 batch mAP 0.518493652 batch PCKh 0.3125\n",
      "Trained batch 2459 batch loss 0.679638 batch mAP 0.48916626 batch PCKh 0.875\n",
      "Trained batch 2460 batch loss 0.563931227 batch mAP 0.496002197 batch PCKh 0.5\n",
      "Trained batch 2461 batch loss 0.619399071 batch mAP 0.50592041 batch PCKh 0.125\n",
      "Trained batch 2462 batch loss 0.560946882 batch mAP 0.492919922 batch PCKh 0.5625\n",
      "Trained batch 2463 batch loss 0.636504054 batch mAP 0.446685791 batch PCKh 0.5\n",
      "Trained batch 2464 batch loss 0.475769758 batch mAP 0.514190674 batch PCKh 0.625\n",
      "Trained batch 2465 batch loss 0.503118813 batch mAP 0.534088135 batch PCKh 0.5\n",
      "Trained batch 2466 batch loss 0.563100576 batch mAP 0.512634277 batch PCKh 0.625\n",
      "Trained batch 2467 batch loss 0.63251245 batch mAP 0.488952637 batch PCKh 0.625\n",
      "Trained batch 2468 batch loss 0.632285 batch mAP 0.456970215 batch PCKh 0.375\n",
      "Trained batch 2469 batch loss 0.640982747 batch mAP 0.490356445 batch PCKh 0.1875\n",
      "Trained batch 2470 batch loss 0.607054114 batch mAP 0.502807617 batch PCKh 0.125\n",
      "Trained batch 2471 batch loss 0.652340829 batch mAP 0.518463135 batch PCKh 0.375\n",
      "Trained batch 2472 batch loss 0.604956627 batch mAP 0.532684326 batch PCKh 0.1875\n",
      "Trained batch 2473 batch loss 0.60390079 batch mAP 0.531799316 batch PCKh 0.3125\n",
      "Trained batch 2474 batch loss 0.572983086 batch mAP 0.494537354 batch PCKh 0.5\n",
      "Trained batch 2475 batch loss 0.552422166 batch mAP 0.59677124 batch PCKh 0.25\n",
      "Trained batch 2476 batch loss 0.624961138 batch mAP 0.557373047 batch PCKh 0.3125\n",
      "Trained batch 2477 batch loss 0.689419687 batch mAP 0.541229248 batch PCKh 0.6875\n",
      "Trained batch 2478 batch loss 0.725894749 batch mAP 0.448516846 batch PCKh 0.0625\n",
      "Trained batch 2479 batch loss 0.718238354 batch mAP 0.529052734 batch PCKh 0.1875\n",
      "Trained batch 2480 batch loss 0.572634101 batch mAP 0.552429199 batch PCKh 0.375\n",
      "Trained batch 2481 batch loss 0.659683049 batch mAP 0.521759033 batch PCKh 0.25\n",
      "Trained batch 2482 batch loss 0.70503056 batch mAP 0.493774414 batch PCKh 0.4375\n",
      "Trained batch 2483 batch loss 0.5741 batch mAP 0.528595 batch PCKh 0.625\n",
      "Trained batch 2484 batch loss 0.63918376 batch mAP 0.436920166 batch PCKh 0.0625\n",
      "Trained batch 2485 batch loss 0.518271148 batch mAP 0.456359863 batch PCKh 0.625\n",
      "Trained batch 2486 batch loss 0.514683783 batch mAP 0.52532959 batch PCKh 0.4375\n",
      "Trained batch 2487 batch loss 0.612318277 batch mAP 0.447692871 batch PCKh 0.625\n",
      "Trained batch 2488 batch loss 0.641292214 batch mAP 0.422821045 batch PCKh 0.0625\n",
      "Trained batch 2489 batch loss 0.479402781 batch mAP 0.534729 batch PCKh 0.125\n",
      "Trained batch 2490 batch loss 0.555578113 batch mAP 0.574523926 batch PCKh 0.625\n",
      "Trained batch 2491 batch loss 0.662920356 batch mAP 0.542694092 batch PCKh 0.125\n",
      "Trained batch 2492 batch loss 0.595089912 batch mAP 0.567626953 batch PCKh 0.6875\n",
      "Trained batch 2493 batch loss 0.647122383 batch mAP 0.502410889 batch PCKh 0.375\n",
      "Trained batch 2494 batch loss 0.553523421 batch mAP 0.541107178 batch PCKh 0.625\n",
      "Trained batch 2495 batch loss 0.523088038 batch mAP 0.51828 batch PCKh 0.25\n",
      "Trained batch 2496 batch loss 0.541970432 batch mAP 0.491821289 batch PCKh 0.375\n",
      "Trained batch 2497 batch loss 0.569341838 batch mAP 0.484313965 batch PCKh 0.25\n",
      "Trained batch 2498 batch loss 0.478722095 batch mAP 0.561462402 batch PCKh 0.3125\n",
      "Trained batch 2499 batch loss 0.473494381 batch mAP 0.58291626 batch PCKh 0.25\n",
      "Trained batch 2500 batch loss 0.51651448 batch mAP 0.608398438 batch PCKh 0.375\n",
      "Trained batch 2501 batch loss 0.417180181 batch mAP 0.587463379 batch PCKh 0.1875\n",
      "Trained batch 2502 batch loss 0.495809942 batch mAP 0.620422363 batch PCKh 0.375\n",
      "Trained batch 2503 batch loss 0.564130843 batch mAP 0.567169189 batch PCKh 0.5625\n",
      "Trained batch 2504 batch loss 0.691726863 batch mAP 0.452697754 batch PCKh 0.25\n",
      "Trained batch 2505 batch loss 0.67035836 batch mAP 0.448761 batch PCKh 0.4375\n",
      "Trained batch 2506 batch loss 0.788216949 batch mAP 0.285614 batch PCKh 0\n",
      "Trained batch 2507 batch loss 0.704024434 batch mAP 0.352600098 batch PCKh 0.625\n",
      "Trained batch 2508 batch loss 0.726149917 batch mAP 0.399078369 batch PCKh 0.25\n",
      "Trained batch 2509 batch loss 0.721856117 batch mAP 0.432006836 batch PCKh 0.75\n",
      "Trained batch 2510 batch loss 0.658367634 batch mAP 0.476165771 batch PCKh 0.875\n",
      "Trained batch 2511 batch loss 0.583982468 batch mAP 0.436462402 batch PCKh 0.75\n",
      "Trained batch 2512 batch loss 0.729839563 batch mAP 0.406890869 batch PCKh 0.1875\n",
      "Trained batch 2513 batch loss 0.638868 batch mAP 0.351867676 batch PCKh 0.75\n",
      "Trained batch 2514 batch loss 0.671965718 batch mAP 0.373718262 batch PCKh 0.1875\n",
      "Trained batch 2515 batch loss 0.696207166 batch mAP 0.375335693 batch PCKh 0.3125\n",
      "Trained batch 2516 batch loss 0.60685128 batch mAP 0.374084473 batch PCKh 0.25\n",
      "Trained batch 2517 batch loss 0.65024662 batch mAP 0.374938965 batch PCKh 0.4375\n",
      "Trained batch 2518 batch loss 0.59488678 batch mAP 0.387542725 batch PCKh 0.4375\n",
      "Trained batch 2519 batch loss 0.64484 batch mAP 0.408935547 batch PCKh 0.25\n",
      "Trained batch 2520 batch loss 0.635859728 batch mAP 0.458679199 batch PCKh 0.25\n",
      "Trained batch 2521 batch loss 0.610825539 batch mAP 0.519897461 batch PCKh 0.3125\n",
      "Trained batch 2522 batch loss 0.640486121 batch mAP 0.53427124 batch PCKh 0.125\n",
      "Trained batch 2523 batch loss 0.570184946 batch mAP 0.546905518 batch PCKh 0.3125\n",
      "Trained batch 2524 batch loss 0.550524473 batch mAP 0.586517334 batch PCKh 0.25\n",
      "Trained batch 2525 batch loss 0.563889146 batch mAP 0.565368652 batch PCKh 0.4375\n",
      "Trained batch 2526 batch loss 0.573999882 batch mAP 0.560913086 batch PCKh 0.4375\n",
      "Trained batch 2527 batch loss 0.596762836 batch mAP 0.565917969 batch PCKh 0.4375\n",
      "Trained batch 2528 batch loss 0.569002509 batch mAP 0.665588379 batch PCKh 0.25\n",
      "Trained batch 2529 batch loss 0.535227954 batch mAP 0.575317383 batch PCKh 0.5\n",
      "Trained batch 2530 batch loss 0.561116397 batch mAP 0.613952637 batch PCKh 0.625\n",
      "Trained batch 2531 batch loss 0.535483 batch mAP 0.526641846 batch PCKh 0.3125\n",
      "Trained batch 2532 batch loss 0.52900511 batch mAP 0.526702881 batch PCKh 0.4375\n",
      "Trained batch 2533 batch loss 0.587187529 batch mAP 0.43258667 batch PCKh 0.625\n",
      "Trained batch 2534 batch loss 0.544401 batch mAP 0.534484863 batch PCKh 0.75\n",
      "Trained batch 2535 batch loss 0.618306458 batch mAP 0.486816406 batch PCKh 0.75\n",
      "Trained batch 2536 batch loss 0.575159311 batch mAP 0.50970459 batch PCKh 0.625\n",
      "Trained batch 2537 batch loss 0.473087102 batch mAP 0.554992676 batch PCKh 0.6875\n",
      "Trained batch 2538 batch loss 0.581500411 batch mAP 0.512420654 batch PCKh 0.3125\n",
      "Trained batch 2539 batch loss 0.580545604 batch mAP 0.541107178 batch PCKh 0.0625\n",
      "Trained batch 2540 batch loss 0.599078894 batch mAP 0.456665039 batch PCKh 0.875\n",
      "Trained batch 2541 batch loss 0.569491 batch mAP 0.478851318 batch PCKh 0.375\n",
      "Trained batch 2542 batch loss 0.522598624 batch mAP 0.497344971 batch PCKh 0.5625\n",
      "Trained batch 2543 batch loss 0.574061871 batch mAP 0.477661133 batch PCKh 0.875\n",
      "Trained batch 2544 batch loss 0.529232681 batch mAP 0.425933838 batch PCKh 0.25\n",
      "Trained batch 2545 batch loss 0.638592243 batch mAP 0.450683594 batch PCKh 0.3125\n",
      "Trained batch 2546 batch loss 0.595228 batch mAP 0.49105835 batch PCKh 0.5625\n",
      "Trained batch 2547 batch loss 0.537127852 batch mAP 0.451599121 batch PCKh 0.5625\n",
      "Trained batch 2548 batch loss 0.545134366 batch mAP 0.513031 batch PCKh 0.3125\n",
      "Trained batch 2549 batch loss 0.593389511 batch mAP 0.493255615 batch PCKh 0.0625\n",
      "Trained batch 2550 batch loss 0.545422 batch mAP 0.529785156 batch PCKh 0.25\n",
      "Trained batch 2551 batch loss 0.550609291 batch mAP 0.491668701 batch PCKh 0.25\n",
      "Trained batch 2552 batch loss 0.581052959 batch mAP 0.500091553 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2553 batch loss 0.577571 batch mAP 0.503662109 batch PCKh 0.25\n",
      "Trained batch 2554 batch loss 0.576292 batch mAP 0.542755127 batch PCKh 0.1875\n",
      "Trained batch 2555 batch loss 0.569635451 batch mAP 0.526702881 batch PCKh 0.1875\n",
      "Trained batch 2556 batch loss 0.592809618 batch mAP 0.467987061 batch PCKh 0.375\n",
      "Trained batch 2557 batch loss 0.583251894 batch mAP 0.433197021 batch PCKh 0.125\n",
      "Trained batch 2558 batch loss 0.681208074 batch mAP 0.437957764 batch PCKh 0.5\n",
      "Trained batch 2559 batch loss 0.63257885 batch mAP 0.454833984 batch PCKh 0.6875\n",
      "Trained batch 2560 batch loss 0.549827635 batch mAP 0.530487061 batch PCKh 0.6875\n",
      "Trained batch 2561 batch loss 0.552366376 batch mAP 0.518829346 batch PCKh 0.6875\n",
      "Trained batch 2562 batch loss 0.556729794 batch mAP 0.558197 batch PCKh 0.25\n",
      "Trained batch 2563 batch loss 0.57733047 batch mAP 0.559814453 batch PCKh 0.3125\n",
      "Trained batch 2564 batch loss 0.599895716 batch mAP 0.560272217 batch PCKh 0.1875\n",
      "Trained batch 2565 batch loss 0.551792741 batch mAP 0.593078613 batch PCKh 0.625\n",
      "Trained batch 2566 batch loss 0.497105837 batch mAP 0.580566406 batch PCKh 0.4375\n",
      "Trained batch 2567 batch loss 0.566312432 batch mAP 0.523345947 batch PCKh 0.5\n",
      "Trained batch 2568 batch loss 0.530633569 batch mAP 0.542480469 batch PCKh 0.4375\n",
      "Trained batch 2569 batch loss 0.51701808 batch mAP 0.557617188 batch PCKh 0.4375\n",
      "Trained batch 2570 batch loss 0.535928667 batch mAP 0.552246094 batch PCKh 0.375\n",
      "Trained batch 2571 batch loss 0.591899395 batch mAP 0.484375 batch PCKh 0\n",
      "Trained batch 2572 batch loss 0.64590466 batch mAP 0.464782715 batch PCKh 0.3125\n",
      "Trained batch 2573 batch loss 0.564494431 batch mAP 0.569549561 batch PCKh 0.5\n",
      "Trained batch 2574 batch loss 0.598208547 batch mAP 0.503814697 batch PCKh 0.375\n",
      "Trained batch 2575 batch loss 0.494768441 batch mAP 0.47668457 batch PCKh 0\n",
      "Trained batch 2576 batch loss 0.583205581 batch mAP 0.367156982 batch PCKh 0.1875\n",
      "Trained batch 2577 batch loss 0.474399924 batch mAP 0.428039551 batch PCKh 0\n",
      "Trained batch 2578 batch loss 0.455354333 batch mAP 0.449005127 batch PCKh 0\n",
      "Trained batch 2579 batch loss 0.442245334 batch mAP 0.386505127 batch PCKh 0.125\n",
      "Trained batch 2580 batch loss 0.430285215 batch mAP 0.399627686 batch PCKh 0.1875\n",
      "Trained batch 2581 batch loss 0.522491038 batch mAP 0.329071045 batch PCKh 0.125\n",
      "Trained batch 2582 batch loss 0.618914 batch mAP 0.241638184 batch PCKh 0.25\n",
      "Trained batch 2583 batch loss 0.684334397 batch mAP 0.179870605 batch PCKh 0.0625\n",
      "Trained batch 2584 batch loss 0.600363672 batch mAP 0.20690918 batch PCKh 0.5\n",
      "Trained batch 2585 batch loss 0.570917249 batch mAP 0.175354 batch PCKh 0.375\n",
      "Trained batch 2586 batch loss 0.638979912 batch mAP 0.287200928 batch PCKh 0.625\n",
      "Trained batch 2587 batch loss 0.556743503 batch mAP 0.372589111 batch PCKh 0\n",
      "Trained batch 2588 batch loss 0.571370482 batch mAP 0.400085449 batch PCKh 0.3125\n",
      "Trained batch 2589 batch loss 0.579980731 batch mAP 0.436920166 batch PCKh 0.75\n",
      "Trained batch 2590 batch loss 0.616799116 batch mAP 0.458099365 batch PCKh 0.875\n",
      "Trained batch 2591 batch loss 0.515069902 batch mAP 0.460510254 batch PCKh 0.5\n",
      "Trained batch 2592 batch loss 0.532410085 batch mAP 0.4402771 batch PCKh 0.3125\n",
      "Trained batch 2593 batch loss 0.435865045 batch mAP 0.472900391 batch PCKh 0.75\n",
      "Trained batch 2594 batch loss 0.545287192 batch mAP 0.380584717 batch PCKh 0.25\n",
      "Trained batch 2595 batch loss 0.558141232 batch mAP 0.300170898 batch PCKh 0.1875\n",
      "Trained batch 2596 batch loss 0.569069386 batch mAP 0.312103271 batch PCKh 0.1875\n",
      "Trained batch 2597 batch loss 0.556207776 batch mAP 0.317779541 batch PCKh 0.625\n",
      "Trained batch 2598 batch loss 0.535089433 batch mAP 0.3409729 batch PCKh 0.3125\n",
      "Trained batch 2599 batch loss 0.673216522 batch mAP 0.343933105 batch PCKh 0.1875\n",
      "Trained batch 2600 batch loss 0.606649578 batch mAP 0.30947876 batch PCKh 0.125\n",
      "Trained batch 2601 batch loss 0.601164699 batch mAP 0.416473389 batch PCKh 0.125\n",
      "Trained batch 2602 batch loss 0.632073 batch mAP 0.521026611 batch PCKh 0.1875\n",
      "Trained batch 2603 batch loss 0.561560035 batch mAP 0.572845459 batch PCKh 0.625\n",
      "Trained batch 2604 batch loss 0.578159273 batch mAP 0.633789062 batch PCKh 0.5\n",
      "Trained batch 2605 batch loss 0.651131272 batch mAP 0.552063 batch PCKh 0.0625\n",
      "Trained batch 2606 batch loss 0.471442401 batch mAP 0.56817627 batch PCKh 0.25\n",
      "Trained batch 2607 batch loss 0.713394165 batch mAP 0.486846924 batch PCKh 0.625\n",
      "Trained batch 2608 batch loss 0.562946498 batch mAP 0.502929688 batch PCKh 0.5625\n",
      "Trained batch 2609 batch loss 0.644946516 batch mAP 0.456481934 batch PCKh 0\n",
      "Trained batch 2610 batch loss 0.480443925 batch mAP 0.498046875 batch PCKh 0.75\n",
      "Trained batch 2611 batch loss 0.558300436 batch mAP 0.47366333 batch PCKh 0.75\n",
      "Trained batch 2612 batch loss 0.610116899 batch mAP 0.466827393 batch PCKh 0.75\n",
      "Trained batch 2613 batch loss 0.639264822 batch mAP 0.485839844 batch PCKh 0.5625\n",
      "Trained batch 2614 batch loss 0.613349 batch mAP 0.474853516 batch PCKh 0.3125\n",
      "Trained batch 2615 batch loss 0.560213208 batch mAP 0.493103027 batch PCKh 0.125\n",
      "Trained batch 2616 batch loss 0.51079452 batch mAP 0.519866943 batch PCKh 0.625\n",
      "Trained batch 2617 batch loss 0.538058877 batch mAP 0.502807617 batch PCKh 0.4375\n",
      "Trained batch 2618 batch loss 0.56444782 batch mAP 0.525024414 batch PCKh 0.4375\n",
      "Trained batch 2619 batch loss 0.60161829 batch mAP 0.505249 batch PCKh 0.625\n",
      "Trained batch 2620 batch loss 0.595411181 batch mAP 0.452392578 batch PCKh 0.25\n",
      "Trained batch 2621 batch loss 0.648442864 batch mAP 0.486450195 batch PCKh 0.5625\n",
      "Trained batch 2622 batch loss 0.684156299 batch mAP 0.466491699 batch PCKh 0.6875\n",
      "Trained batch 2623 batch loss 0.580105066 batch mAP 0.505706787 batch PCKh 0.75\n",
      "Trained batch 2624 batch loss 0.626704 batch mAP 0.444366455 batch PCKh 0.5\n",
      "Trained batch 2625 batch loss 0.646732 batch mAP 0.476470947 batch PCKh 0.5\n",
      "Trained batch 2626 batch loss 0.610555112 batch mAP 0.464141846 batch PCKh 0.6875\n",
      "Trained batch 2627 batch loss 0.700869262 batch mAP 0.467987061 batch PCKh 0.6875\n",
      "Trained batch 2628 batch loss 0.668269455 batch mAP 0.490600586 batch PCKh 0.5\n",
      "Trained batch 2629 batch loss 0.557804585 batch mAP 0.503997803 batch PCKh 0.4375\n",
      "Trained batch 2630 batch loss 0.629542291 batch mAP 0.455749512 batch PCKh 0.625\n",
      "Trained batch 2631 batch loss 0.594353139 batch mAP 0.461090088 batch PCKh 0.75\n",
      "Trained batch 2632 batch loss 0.668676853 batch mAP 0.443145752 batch PCKh 0.125\n",
      "Trained batch 2633 batch loss 0.662295759 batch mAP 0.487182617 batch PCKh 0.3125\n",
      "Trained batch 2634 batch loss 0.613377929 batch mAP 0.505584717 batch PCKh 0.5625\n",
      "Trained batch 2635 batch loss 0.57329762 batch mAP 0.522827148 batch PCKh 0.5625\n",
      "Trained batch 2636 batch loss 0.650854588 batch mAP 0.467346191 batch PCKh 0.375\n",
      "Trained batch 2637 batch loss 0.638669252 batch mAP 0.469787598 batch PCKh 0.25\n",
      "Trained batch 2638 batch loss 0.586423576 batch mAP 0.501312256 batch PCKh 0.4375\n",
      "Trained batch 2639 batch loss 0.573920488 batch mAP 0.550384521 batch PCKh 0.125\n",
      "Trained batch 2640 batch loss 0.566006482 batch mAP 0.505004883 batch PCKh 0.5\n",
      "Trained batch 2641 batch loss 0.584537 batch mAP 0.527984619 batch PCKh 0.875\n",
      "Trained batch 2642 batch loss 0.53020364 batch mAP 0.53717041 batch PCKh 0.5\n",
      "Trained batch 2643 batch loss 0.543142915 batch mAP 0.513214111 batch PCKh 0.4375\n",
      "Trained batch 2644 batch loss 0.442312956 batch mAP 0.493713379 batch PCKh 0.4375\n",
      "Trained batch 2645 batch loss 0.554652393 batch mAP 0.556976318 batch PCKh 0.75\n",
      "Trained batch 2646 batch loss 0.669148326 batch mAP 0.525024414 batch PCKh 0.875\n",
      "Trained batch 2647 batch loss 0.710417747 batch mAP 0.504699707 batch PCKh 0.0625\n",
      "Trained batch 2648 batch loss 0.653595626 batch mAP 0.546142578 batch PCKh 0\n",
      "Trained batch 2649 batch loss 0.685639918 batch mAP 0.587768555 batch PCKh 0.3125\n",
      "Trained batch 2650 batch loss 0.559960604 batch mAP 0.540924072 batch PCKh 0.5625\n",
      "Trained batch 2651 batch loss 0.528343558 batch mAP 0.552001953 batch PCKh 0.1875\n",
      "Trained batch 2652 batch loss 0.522162795 batch mAP 0.53225708 batch PCKh 0.125\n",
      "Trained batch 2653 batch loss 0.452278227 batch mAP 0.536315918 batch PCKh 0.4375\n",
      "Trained batch 2654 batch loss 0.511208415 batch mAP 0.538513184 batch PCKh 0\n",
      "Trained batch 2655 batch loss 0.626837075 batch mAP 0.500427246 batch PCKh 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2656 batch loss 0.613947034 batch mAP 0.523407 batch PCKh 0\n",
      "Trained batch 2657 batch loss 0.670822859 batch mAP 0.522796631 batch PCKh 0.1875\n",
      "Trained batch 2658 batch loss 0.671025574 batch mAP 0.482879639 batch PCKh 0.1875\n",
      "Trained batch 2659 batch loss 0.674850583 batch mAP 0.552185059 batch PCKh 0.25\n",
      "Trained batch 2660 batch loss 0.635362864 batch mAP 0.485656738 batch PCKh 0.3125\n",
      "Trained batch 2661 batch loss 0.63713038 batch mAP 0.500793457 batch PCKh 0.375\n",
      "Trained batch 2662 batch loss 0.565462649 batch mAP 0.483673096 batch PCKh 0.625\n",
      "Trained batch 2663 batch loss 0.626390696 batch mAP 0.444213867 batch PCKh 0.375\n",
      "Trained batch 2664 batch loss 0.620885491 batch mAP 0.507659912 batch PCKh 0.25\n",
      "Trained batch 2665 batch loss 0.632333636 batch mAP 0.526702881 batch PCKh 0.25\n",
      "Trained batch 2666 batch loss 0.620278895 batch mAP 0.506835938 batch PCKh 0.1875\n",
      "Trained batch 2667 batch loss 0.595692515 batch mAP 0.532714844 batch PCKh 0.625\n",
      "Trained batch 2668 batch loss 0.594841 batch mAP 0.511474609 batch PCKh 0\n",
      "Trained batch 2669 batch loss 0.635589957 batch mAP 0.481109619 batch PCKh 0\n",
      "Trained batch 2670 batch loss 0.620939672 batch mAP 0.498382568 batch PCKh 0.1875\n",
      "Trained batch 2671 batch loss 0.581407666 batch mAP 0.44744873 batch PCKh 0.1875\n",
      "Trained batch 2672 batch loss 0.590033531 batch mAP 0.404846191 batch PCKh 0.0625\n",
      "Trained batch 2673 batch loss 0.623544097 batch mAP 0.500976562 batch PCKh 0.625\n",
      "Trained batch 2674 batch loss 0.601357818 batch mAP 0.505310059 batch PCKh 0.75\n",
      "Trained batch 2675 batch loss 0.565705061 batch mAP 0.522735596 batch PCKh 0.1875\n",
      "Trained batch 2676 batch loss 0.582030416 batch mAP 0.523986816 batch PCKh 0.8125\n",
      "Trained batch 2677 batch loss 0.61596179 batch mAP 0.516601562 batch PCKh 0.8125\n",
      "Trained batch 2678 batch loss 0.558748066 batch mAP 0.533203125 batch PCKh 0.8125\n",
      "Trained batch 2679 batch loss 0.600872636 batch mAP 0.524108887 batch PCKh 0.5\n",
      "Trained batch 2680 batch loss 0.598893464 batch mAP 0.508544922 batch PCKh 0.5625\n",
      "Trained batch 2681 batch loss 0.63561058 batch mAP 0.521850586 batch PCKh 0.625\n",
      "Trained batch 2682 batch loss 0.603875637 batch mAP 0.522277832 batch PCKh 0.625\n",
      "Trained batch 2683 batch loss 0.584347486 batch mAP 0.571960449 batch PCKh 0.25\n",
      "Trained batch 2684 batch loss 0.579023 batch mAP 0.516326904 batch PCKh 0.625\n",
      "Trained batch 2685 batch loss 0.642147303 batch mAP 0.508483887 batch PCKh 0.25\n",
      "Trained batch 2686 batch loss 0.653352857 batch mAP 0.504486084 batch PCKh 0.25\n",
      "Trained batch 2687 batch loss 0.681949258 batch mAP 0.526947 batch PCKh 0.625\n",
      "Trained batch 2688 batch loss 0.580029368 batch mAP 0.567565918 batch PCKh 0.3125\n",
      "Trained batch 2689 batch loss 0.611619 batch mAP 0.545562744 batch PCKh 0.25\n",
      "Trained batch 2690 batch loss 0.637490273 batch mAP 0.514953613 batch PCKh 0.625\n",
      "Trained batch 2691 batch loss 0.605311871 batch mAP 0.58001709 batch PCKh 0.625\n",
      "Trained batch 2692 batch loss 0.624239445 batch mAP 0.552856445 batch PCKh 0.75\n",
      "Trained batch 2693 batch loss 0.610788703 batch mAP 0.537231445 batch PCKh 0.3125\n",
      "Trained batch 2694 batch loss 0.611253858 batch mAP 0.552429199 batch PCKh 0.5\n",
      "Trained batch 2695 batch loss 0.590464234 batch mAP 0.527862549 batch PCKh 0.5\n",
      "Trained batch 2696 batch loss 0.538934708 batch mAP 0.584533691 batch PCKh 0.75\n",
      "Trained batch 2697 batch loss 0.513886273 batch mAP 0.502532959 batch PCKh 0.25\n",
      "Trained batch 2698 batch loss 0.511557698 batch mAP 0.464416504 batch PCKh 0.6875\n",
      "Trained batch 2699 batch loss 0.495672882 batch mAP 0.490325928 batch PCKh 0.5\n",
      "Trained batch 2700 batch loss 0.479991555 batch mAP 0.492767334 batch PCKh 0.5\n",
      "Trained batch 2701 batch loss 0.492418 batch mAP 0.505126953 batch PCKh 0.5\n",
      "Trained batch 2702 batch loss 0.57863754 batch mAP 0.549713135 batch PCKh 0.3125\n",
      "Trained batch 2703 batch loss 0.722929716 batch mAP 0.491241455 batch PCKh 0.1875\n",
      "Trained batch 2704 batch loss 0.598120093 batch mAP 0.564575195 batch PCKh 0.125\n",
      "Trained batch 2705 batch loss 0.546323061 batch mAP 0.607727051 batch PCKh 0.5625\n",
      "Trained batch 2706 batch loss 0.624254823 batch mAP 0.5496521 batch PCKh 0.375\n",
      "Trained batch 2707 batch loss 0.670279503 batch mAP 0.47744751 batch PCKh 0.375\n",
      "Trained batch 2708 batch loss 0.740689754 batch mAP 0.431518555 batch PCKh 0\n",
      "Trained batch 2709 batch loss 0.70423305 batch mAP 0.467987061 batch PCKh 0.3125\n",
      "Trained batch 2710 batch loss 0.677048922 batch mAP 0.493652344 batch PCKh 0.3125\n",
      "Trained batch 2711 batch loss 0.648361504 batch mAP 0.440002441 batch PCKh 0.625\n",
      "Trained batch 2712 batch loss 0.611997724 batch mAP 0.387695312 batch PCKh 0.3125\n",
      "Trained batch 2713 batch loss 0.652105868 batch mAP 0.289886475 batch PCKh 0.25\n",
      "Trained batch 2714 batch loss 0.645063281 batch mAP 0.361175537 batch PCKh 0.25\n",
      "Trained batch 2715 batch loss 0.63138 batch mAP 0.394866943 batch PCKh 0.25\n",
      "Trained batch 2716 batch loss 0.564197481 batch mAP 0.395568848 batch PCKh 0.4375\n",
      "Trained batch 2717 batch loss 0.540452421 batch mAP 0.457122803 batch PCKh 0.75\n",
      "Trained batch 2718 batch loss 0.53623724 batch mAP 0.489013672 batch PCKh 0.5625\n",
      "Trained batch 2719 batch loss 0.61789757 batch mAP 0.535583496 batch PCKh 0.5\n",
      "Trained batch 2720 batch loss 0.549490631 batch mAP 0.54776 batch PCKh 0.25\n",
      "Trained batch 2721 batch loss 0.662226319 batch mAP 0.500885 batch PCKh 0.3125\n",
      "Trained batch 2722 batch loss 0.614769161 batch mAP 0.515167236 batch PCKh 0.1875\n",
      "Trained batch 2723 batch loss 0.462757289 batch mAP 0.577728271 batch PCKh 0.3125\n",
      "Trained batch 2724 batch loss 0.630077124 batch mAP 0.552307129 batch PCKh 0.625\n",
      "Trained batch 2725 batch loss 0.578538179 batch mAP 0.598205566 batch PCKh 0.4375\n",
      "Trained batch 2726 batch loss 0.608652651 batch mAP 0.61315918 batch PCKh 0.75\n",
      "Trained batch 2727 batch loss 0.605813205 batch mAP 0.57208252 batch PCKh 0.8125\n",
      "Trained batch 2728 batch loss 0.622953176 batch mAP 0.656555176 batch PCKh 0.5625\n",
      "Trained batch 2729 batch loss 0.604112744 batch mAP 0.602050781 batch PCKh 0.875\n",
      "Trained batch 2730 batch loss 0.600251794 batch mAP 0.604888916 batch PCKh 0.625\n",
      "Trained batch 2731 batch loss 0.589191675 batch mAP 0.601928711 batch PCKh 0.5625\n",
      "Trained batch 2732 batch loss 0.580273926 batch mAP 0.565063477 batch PCKh 0.75\n",
      "Trained batch 2733 batch loss 0.657160163 batch mAP 0.480377197 batch PCKh 0.6875\n",
      "Trained batch 2734 batch loss 0.559171796 batch mAP 0.612915039 batch PCKh 0.6875\n",
      "Trained batch 2735 batch loss 0.588068962 batch mAP 0.563079834 batch PCKh 0.875\n",
      "Trained batch 2736 batch loss 0.547500908 batch mAP 0.523986816 batch PCKh 0.625\n",
      "Trained batch 2737 batch loss 0.660112143 batch mAP 0.518066406 batch PCKh 0.75\n",
      "Trained batch 2738 batch loss 0.656903386 batch mAP 0.479614258 batch PCKh 0.6875\n",
      "Trained batch 2739 batch loss 0.587160349 batch mAP 0.549194336 batch PCKh 0.6875\n",
      "Trained batch 2740 batch loss 0.559873879 batch mAP 0.606658936 batch PCKh 0.375\n",
      "Trained batch 2741 batch loss 0.674529314 batch mAP 0.538330078 batch PCKh 0.125\n",
      "Trained batch 2742 batch loss 0.699244261 batch mAP 0.543548584 batch PCKh 0.0625\n",
      "Trained batch 2743 batch loss 0.680033267 batch mAP 0.492767334 batch PCKh 0.25\n",
      "Trained batch 2744 batch loss 0.66121918 batch mAP 0.520904541 batch PCKh 0.25\n",
      "Trained batch 2745 batch loss 0.731242597 batch mAP 0.467041016 batch PCKh 0\n",
      "Trained batch 2746 batch loss 0.709360898 batch mAP 0.50302124 batch PCKh 0.375\n",
      "Trained batch 2747 batch loss 0.718689084 batch mAP 0.480499268 batch PCKh 0.6875\n",
      "Trained batch 2748 batch loss 0.651172161 batch mAP 0.444030762 batch PCKh 0.3125\n",
      "Trained batch 2749 batch loss 0.482592285 batch mAP 0.412109375 batch PCKh 0.25\n",
      "Trained batch 2750 batch loss 0.513312757 batch mAP 0.406097412 batch PCKh 0\n",
      "Trained batch 2751 batch loss 0.527588248 batch mAP 0.414367676 batch PCKh 0.4375\n",
      "Trained batch 2752 batch loss 0.596696734 batch mAP 0.394226074 batch PCKh 0.25\n",
      "Trained batch 2753 batch loss 0.552429318 batch mAP 0.399688721 batch PCKh 0.4375\n",
      "Trained batch 2754 batch loss 0.613502 batch mAP 0.426635742 batch PCKh 0.25\n",
      "Trained batch 2755 batch loss 0.5607692 batch mAP 0.488525391 batch PCKh 0.3125\n",
      "Trained batch 2756 batch loss 0.576981902 batch mAP 0.50402832 batch PCKh 0.375\n",
      "Trained batch 2757 batch loss 0.486928 batch mAP 0.544616699 batch PCKh 0\n",
      "Trained batch 2758 batch loss 0.471927285 batch mAP 0.518035889 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2759 batch loss 0.496397525 batch mAP 0.564880371 batch PCKh 0.8125\n",
      "Trained batch 2760 batch loss 0.522201419 batch mAP 0.639587402 batch PCKh 0.4375\n",
      "Trained batch 2761 batch loss 0.532890797 batch mAP 0.625274658 batch PCKh 0.3125\n",
      "Trained batch 2762 batch loss 0.547918797 batch mAP 0.57119751 batch PCKh 0.125\n",
      "Trained batch 2763 batch loss 0.516023755 batch mAP 0.596862793 batch PCKh 0.25\n",
      "Trained batch 2764 batch loss 0.446196258 batch mAP 0.625640869 batch PCKh 0.25\n",
      "Trained batch 2765 batch loss 0.47717917 batch mAP 0.592956543 batch PCKh 0.25\n",
      "Trained batch 2766 batch loss 0.46157074 batch mAP 0.617248535 batch PCKh 0.5625\n",
      "Trained batch 2767 batch loss 0.536414504 batch mAP 0.566101074 batch PCKh 0\n",
      "Trained batch 2768 batch loss 0.494080901 batch mAP 0.579620361 batch PCKh 0.25\n",
      "Trained batch 2769 batch loss 0.578130424 batch mAP 0.471099854 batch PCKh 0.375\n",
      "Trained batch 2770 batch loss 0.574930429 batch mAP 0.518310547 batch PCKh 0.1875\n",
      "Trained batch 2771 batch loss 0.602803826 batch mAP 0.503875732 batch PCKh 0.375\n",
      "Trained batch 2772 batch loss 0.524716556 batch mAP 0.614074707 batch PCKh 0.25\n",
      "Trained batch 2773 batch loss 0.466976643 batch mAP 0.624267578 batch PCKh 0.5625\n",
      "Trained batch 2774 batch loss 0.493721247 batch mAP 0.558868408 batch PCKh 0.375\n",
      "Trained batch 2775 batch loss 0.555667 batch mAP 0.562835693 batch PCKh 0.5\n",
      "Trained batch 2776 batch loss 0.52893728 batch mAP 0.552124 batch PCKh 0.125\n",
      "Epoch 3 train loss 0.5932809114456177 train mAP 0.484412282705307 train PCKh\n",
      "Validated batch 1 batch loss 0.607407391 batch mAP 0.552490234 batch PCKh 0.75\n",
      "Validated batch 2 batch loss 0.655841351 batch mAP 0.455841064 batch PCKh 0.25\n",
      "Validated batch 3 batch loss 0.54165107 batch mAP 0.585723877 batch PCKh 0.6875\n",
      "Validated batch 4 batch loss 0.647791862 batch mAP 0.497772217 batch PCKh 0.5625\n",
      "Validated batch 5 batch loss 0.586319387 batch mAP 0.530975342 batch PCKh 0.6875\n",
      "Validated batch 6 batch loss 0.650760055 batch mAP 0.539428711 batch PCKh 0.75\n",
      "Validated batch 7 batch loss 0.57499367 batch mAP 0.591674805 batch PCKh 0.1875\n",
      "Validated batch 8 batch loss 0.642017484 batch mAP 0.49798584 batch PCKh 0.4375\n",
      "Validated batch 9 batch loss 0.639838398 batch mAP 0.502471924 batch PCKh 0.375\n",
      "Validated batch 10 batch loss 0.673020959 batch mAP 0.480102539 batch PCKh 0.625\n",
      "Validated batch 11 batch loss 0.776052 batch mAP 0.466033936 batch PCKh 0.4375\n",
      "Validated batch 12 batch loss 0.590440154 batch mAP 0.564971924 batch PCKh 0\n",
      "Validated batch 13 batch loss 0.558544159 batch mAP 0.56930542 batch PCKh 0.5625\n",
      "Validated batch 14 batch loss 0.637335598 batch mAP 0.608642578 batch PCKh 0.5\n",
      "Validated batch 15 batch loss 0.634457469 batch mAP 0.506195068 batch PCKh 0.8125\n",
      "Validated batch 16 batch loss 0.624355853 batch mAP 0.52230835 batch PCKh 0.375\n",
      "Validated batch 17 batch loss 0.538576126 batch mAP 0.59942627 batch PCKh 0.25\n",
      "Validated batch 18 batch loss 0.657758355 batch mAP 0.563812256 batch PCKh 0.5625\n",
      "Validated batch 19 batch loss 0.605540037 batch mAP 0.528015137 batch PCKh 0.125\n",
      "Validated batch 20 batch loss 0.638326049 batch mAP 0.567352295 batch PCKh 0.75\n",
      "Validated batch 21 batch loss 0.603220701 batch mAP 0.592468262 batch PCKh 0.875\n",
      "Validated batch 22 batch loss 0.643625379 batch mAP 0.527923584 batch PCKh 0.8125\n",
      "Validated batch 23 batch loss 0.578801394 batch mAP 0.556335449 batch PCKh 0.6875\n",
      "Validated batch 24 batch loss 0.546594 batch mAP 0.584716797 batch PCKh 0.625\n",
      "Validated batch 25 batch loss 0.686281204 batch mAP 0.555786133 batch PCKh 0.5625\n",
      "Validated batch 26 batch loss 0.749056101 batch mAP 0.558746338 batch PCKh 0.0625\n",
      "Validated batch 27 batch loss 0.596907079 batch mAP 0.554992676 batch PCKh 0.5\n",
      "Validated batch 28 batch loss 0.532456696 batch mAP 0.526306152 batch PCKh 0.4375\n",
      "Validated batch 29 batch loss 0.643682361 batch mAP 0.494842529 batch PCKh 0.1875\n",
      "Validated batch 30 batch loss 0.618153512 batch mAP 0.534790039 batch PCKh 0\n",
      "Validated batch 31 batch loss 0.635287642 batch mAP 0.624572754 batch PCKh 0.25\n",
      "Validated batch 32 batch loss 0.607250214 batch mAP 0.613342285 batch PCKh 0.8125\n",
      "Validated batch 33 batch loss 0.645677567 batch mAP 0.503662109 batch PCKh 0.4375\n",
      "Validated batch 34 batch loss 0.717995644 batch mAP 0.495788574 batch PCKh 0.75\n",
      "Validated batch 35 batch loss 0.580450714 batch mAP 0.572235107 batch PCKh 0.75\n",
      "Validated batch 36 batch loss 0.515134871 batch mAP 0.540008545 batch PCKh 0.5625\n",
      "Validated batch 37 batch loss 0.659708 batch mAP 0.462890625 batch PCKh 0.6875\n",
      "Validated batch 38 batch loss 0.629625082 batch mAP 0.500305176 batch PCKh 0.5\n",
      "Validated batch 39 batch loss 0.618490517 batch mAP 0.629211426 batch PCKh 0.25\n",
      "Validated batch 40 batch loss 0.515188336 batch mAP 0.579559326 batch PCKh 0.5625\n",
      "Validated batch 41 batch loss 0.63627857 batch mAP 0.567321777 batch PCKh 0.625\n",
      "Validated batch 42 batch loss 0.66574657 batch mAP 0.511383057 batch PCKh 0.375\n",
      "Validated batch 43 batch loss 0.552093148 batch mAP 0.568328857 batch PCKh 0.375\n",
      "Validated batch 44 batch loss 0.646537483 batch mAP 0.540222168 batch PCKh 0.625\n",
      "Validated batch 45 batch loss 0.595922232 batch mAP 0.532104492 batch PCKh 0.125\n",
      "Validated batch 46 batch loss 0.673241377 batch mAP 0.494842529 batch PCKh 0.25\n",
      "Validated batch 47 batch loss 0.552437484 batch mAP 0.571838379 batch PCKh 0.6875\n",
      "Validated batch 48 batch loss 0.642449856 batch mAP 0.562042236 batch PCKh 0.375\n",
      "Validated batch 49 batch loss 0.638736606 batch mAP 0.517791748 batch PCKh 0.5\n",
      "Validated batch 50 batch loss 0.525539637 batch mAP 0.575836182 batch PCKh 0.25\n",
      "Validated batch 51 batch loss 0.652025878 batch mAP 0.40411377 batch PCKh 0.25\n",
      "Validated batch 52 batch loss 0.510182738 batch mAP 0.624145508 batch PCKh 0.1875\n",
      "Validated batch 53 batch loss 0.558626652 batch mAP 0.530059814 batch PCKh 0\n",
      "Validated batch 54 batch loss 0.607155621 batch mAP 0.561279297 batch PCKh 0.75\n",
      "Validated batch 55 batch loss 0.64303565 batch mAP 0.616607666 batch PCKh 0.5625\n",
      "Validated batch 56 batch loss 0.550105214 batch mAP 0.634979248 batch PCKh 0.3125\n",
      "Validated batch 57 batch loss 0.757875741 batch mAP 0.470092773 batch PCKh 0.125\n",
      "Validated batch 58 batch loss 0.547363162 batch mAP 0.581329346 batch PCKh 0.5625\n",
      "Validated batch 59 batch loss 0.595318377 batch mAP 0.554870605 batch PCKh 0.3125\n",
      "Validated batch 60 batch loss 0.597301066 batch mAP 0.539855957 batch PCKh 0.75\n",
      "Validated batch 61 batch loss 0.510781527 batch mAP 0.513702393 batch PCKh 0.375\n",
      "Validated batch 62 batch loss 0.513845623 batch mAP 0.644134521 batch PCKh 0.25\n",
      "Validated batch 63 batch loss 0.514505506 batch mAP 0.632995605 batch PCKh 0.625\n",
      "Validated batch 64 batch loss 0.581534684 batch mAP 0.581695557 batch PCKh 0.4375\n",
      "Validated batch 65 batch loss 0.632847667 batch mAP 0.538696289 batch PCKh 0.375\n",
      "Validated batch 66 batch loss 0.53759104 batch mAP 0.627685547 batch PCKh 0.25\n",
      "Validated batch 67 batch loss 0.594129205 batch mAP 0.593353271 batch PCKh 0.375\n",
      "Validated batch 68 batch loss 0.567816 batch mAP 0.556243896 batch PCKh 0.5625\n",
      "Validated batch 69 batch loss 0.654536307 batch mAP 0.523803711 batch PCKh 0\n",
      "Validated batch 70 batch loss 0.561175287 batch mAP 0.490356445 batch PCKh 0\n",
      "Validated batch 71 batch loss 0.528427482 batch mAP 0.550018311 batch PCKh 0.4375\n",
      "Validated batch 72 batch loss 0.616079569 batch mAP 0.500274658 batch PCKh 0.5\n",
      "Validated batch 73 batch loss 0.597890496 batch mAP 0.57800293 batch PCKh 0.25\n",
      "Validated batch 74 batch loss 0.633033276 batch mAP 0.554229736 batch PCKh 0.8125\n",
      "Validated batch 75 batch loss 0.669879377 batch mAP 0.563598633 batch PCKh 0.75\n",
      "Validated batch 76 batch loss 0.607441187 batch mAP 0.502349854 batch PCKh 0.25\n",
      "Validated batch 77 batch loss 0.689911842 batch mAP 0.495910645 batch PCKh 0.1875\n",
      "Validated batch 78 batch loss 0.617192745 batch mAP 0.569335938 batch PCKh 0.5\n",
      "Validated batch 79 batch loss 0.565164566 batch mAP 0.572418213 batch PCKh 0.6875\n",
      "Validated batch 80 batch loss 0.623517096 batch mAP 0.487701416 batch PCKh 0.4375\n",
      "Validated batch 81 batch loss 0.667835832 batch mAP 0.476196289 batch PCKh 0.0625\n",
      "Validated batch 82 batch loss 0.62469244 batch mAP 0.534545898 batch PCKh 0.125\n",
      "Validated batch 83 batch loss 0.512751162 batch mAP 0.637176514 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 84 batch loss 0.504607081 batch mAP 0.532775879 batch PCKh 0\n",
      "Validated batch 85 batch loss 0.469204 batch mAP 0.550109863 batch PCKh 0.0625\n",
      "Validated batch 86 batch loss 0.611406803 batch mAP 0.455200195 batch PCKh 0.4375\n",
      "Validated batch 87 batch loss 0.582859397 batch mAP 0.5703125 batch PCKh 0.625\n",
      "Validated batch 88 batch loss 0.497188687 batch mAP 0.582519531 batch PCKh 0.625\n",
      "Validated batch 89 batch loss 0.571528912 batch mAP 0.576812744 batch PCKh 0.3125\n",
      "Validated batch 90 batch loss 0.6529001 batch mAP 0.497802734 batch PCKh 0.5625\n",
      "Validated batch 91 batch loss 0.75085485 batch mAP 0.472564697 batch PCKh 0.125\n",
      "Validated batch 92 batch loss 0.660320103 batch mAP 0.492980957 batch PCKh 0.5625\n",
      "Validated batch 93 batch loss 0.576064348 batch mAP 0.585632324 batch PCKh 0.4375\n",
      "Validated batch 94 batch loss 0.597924709 batch mAP 0.526062 batch PCKh 0.875\n",
      "Validated batch 95 batch loss 0.614201 batch mAP 0.462554932 batch PCKh 0.8125\n",
      "Validated batch 96 batch loss 0.618232608 batch mAP 0.542663574 batch PCKh 0.625\n",
      "Validated batch 97 batch loss 0.62042129 batch mAP 0.557922363 batch PCKh 0.0625\n",
      "Validated batch 98 batch loss 0.538246691 batch mAP 0.560882568 batch PCKh 0.5\n",
      "Validated batch 99 batch loss 0.574534297 batch mAP 0.535766602 batch PCKh 0.5625\n",
      "Validated batch 100 batch loss 0.660121679 batch mAP 0.55847168 batch PCKh 0.4375\n",
      "Validated batch 101 batch loss 0.547329724 batch mAP 0.578369141 batch PCKh 0.375\n",
      "Validated batch 102 batch loss 0.663920283 batch mAP 0.510192871 batch PCKh 0.125\n",
      "Validated batch 103 batch loss 0.567908764 batch mAP 0.610137939 batch PCKh 0.75\n",
      "Validated batch 104 batch loss 0.694475 batch mAP 0.631073 batch PCKh 0.75\n",
      "Validated batch 105 batch loss 0.614589393 batch mAP 0.492767334 batch PCKh 0.625\n",
      "Validated batch 106 batch loss 0.666138709 batch mAP 0.509979248 batch PCKh 0.125\n",
      "Validated batch 107 batch loss 0.65816 batch mAP 0.475738525 batch PCKh 0.375\n",
      "Validated batch 108 batch loss 0.638145804 batch mAP 0.523345947 batch PCKh 0.5\n",
      "Validated batch 109 batch loss 0.68439436 batch mAP 0.482574463 batch PCKh 0\n",
      "Validated batch 110 batch loss 0.618318796 batch mAP 0.516204834 batch PCKh 0.1875\n",
      "Validated batch 111 batch loss 0.557197928 batch mAP 0.587158203 batch PCKh 0.75\n",
      "Validated batch 112 batch loss 0.604846597 batch mAP 0.603057861 batch PCKh 0.5625\n",
      "Validated batch 113 batch loss 0.62865746 batch mAP 0.590454102 batch PCKh 0.375\n",
      "Validated batch 114 batch loss 0.649460316 batch mAP 0.535003662 batch PCKh 0.3125\n",
      "Validated batch 115 batch loss 0.627803683 batch mAP 0.537200928 batch PCKh 0\n",
      "Validated batch 116 batch loss 0.613503456 batch mAP 0.464569092 batch PCKh 0.375\n",
      "Validated batch 117 batch loss 0.571679592 batch mAP 0.39364624 batch PCKh 0.0625\n",
      "Validated batch 118 batch loss 0.663958073 batch mAP 0.495941162 batch PCKh 0.5625\n",
      "Validated batch 119 batch loss 0.650241792 batch mAP 0.498291016 batch PCKh 0.5625\n",
      "Validated batch 120 batch loss 0.688333511 batch mAP 0.480682373 batch PCKh 0.25\n",
      "Validated batch 121 batch loss 0.674669 batch mAP 0.46383667 batch PCKh 0\n",
      "Validated batch 122 batch loss 0.629988134 batch mAP 0.492614746 batch PCKh 0.5625\n",
      "Validated batch 123 batch loss 0.604987621 batch mAP 0.551147461 batch PCKh 0.5\n",
      "Validated batch 124 batch loss 0.634214699 batch mAP 0.55166626 batch PCKh 0.3125\n",
      "Validated batch 125 batch loss 0.647790313 batch mAP 0.546112061 batch PCKh 0.5\n",
      "Validated batch 126 batch loss 0.749559522 batch mAP 0.426940918 batch PCKh 0\n",
      "Validated batch 127 batch loss 0.532918036 batch mAP 0.497283936 batch PCKh 0.5\n",
      "Validated batch 128 batch loss 0.540251732 batch mAP 0.542419434 batch PCKh 0.125\n",
      "Validated batch 129 batch loss 0.628411531 batch mAP 0.507110596 batch PCKh 0.1875\n",
      "Validated batch 130 batch loss 0.711313426 batch mAP 0.501922607 batch PCKh 0.5625\n",
      "Validated batch 131 batch loss 0.507746518 batch mAP 0.642852783 batch PCKh 0.375\n",
      "Validated batch 132 batch loss 0.51467067 batch mAP 0.598236084 batch PCKh 0.3125\n",
      "Validated batch 133 batch loss 0.546396494 batch mAP 0.519897461 batch PCKh 0.5\n",
      "Validated batch 134 batch loss 0.684814632 batch mAP 0.514251709 batch PCKh 0.5625\n",
      "Validated batch 135 batch loss 0.646901727 batch mAP 0.630279541 batch PCKh 0.125\n",
      "Validated batch 136 batch loss 0.707012117 batch mAP 0.526672363 batch PCKh 0.6875\n",
      "Validated batch 137 batch loss 0.545872033 batch mAP 0.541931152 batch PCKh 0.5625\n",
      "Validated batch 138 batch loss 0.654905736 batch mAP 0.5652771 batch PCKh 0.8125\n",
      "Validated batch 139 batch loss 0.554486096 batch mAP 0.572357178 batch PCKh 0.375\n",
      "Validated batch 140 batch loss 0.649376452 batch mAP 0.490509033 batch PCKh 0.6875\n",
      "Validated batch 141 batch loss 0.581218243 batch mAP 0.511352539 batch PCKh 0\n",
      "Validated batch 142 batch loss 0.525484443 batch mAP 0.554321289 batch PCKh 0.5\n",
      "Validated batch 143 batch loss 0.606275141 batch mAP 0.443359375 batch PCKh 0.5\n",
      "Validated batch 144 batch loss 0.606731653 batch mAP 0.394012451 batch PCKh 0.25\n",
      "Validated batch 145 batch loss 0.628497 batch mAP 0.493560791 batch PCKh 0.5625\n",
      "Validated batch 146 batch loss 0.540770173 batch mAP 0.523590088 batch PCKh 0.5625\n",
      "Validated batch 147 batch loss 0.637454212 batch mAP 0.53817749 batch PCKh 0.75\n",
      "Validated batch 148 batch loss 0.540066719 batch mAP 0.563446045 batch PCKh 0.6875\n",
      "Validated batch 149 batch loss 0.599038363 batch mAP 0.504486084 batch PCKh 0.5625\n",
      "Validated batch 150 batch loss 0.60796082 batch mAP 0.501251221 batch PCKh 0.5625\n",
      "Validated batch 151 batch loss 0.602120936 batch mAP 0.518188477 batch PCKh 0.625\n",
      "Validated batch 152 batch loss 0.630597532 batch mAP 0.4637146 batch PCKh 0.1875\n",
      "Validated batch 153 batch loss 0.573312521 batch mAP 0.547607422 batch PCKh 0.3125\n",
      "Validated batch 154 batch loss 0.59520185 batch mAP 0.57208252 batch PCKh 0.5625\n",
      "Validated batch 155 batch loss 0.64756006 batch mAP 0.483032227 batch PCKh 0.125\n",
      "Validated batch 156 batch loss 0.576768637 batch mAP 0.51940918 batch PCKh 0.5\n",
      "Validated batch 157 batch loss 0.541030049 batch mAP 0.64175415 batch PCKh 0.625\n",
      "Validated batch 158 batch loss 0.613696635 batch mAP 0.609924316 batch PCKh 0.375\n",
      "Validated batch 159 batch loss 0.68683064 batch mAP 0.53414917 batch PCKh 0.375\n",
      "Validated batch 160 batch loss 0.535265803 batch mAP 0.547546387 batch PCKh 0.375\n",
      "Validated batch 161 batch loss 0.705767334 batch mAP 0.471923828 batch PCKh 0.0625\n",
      "Validated batch 162 batch loss 0.552501678 batch mAP 0.50100708 batch PCKh 0.25\n",
      "Validated batch 163 batch loss 0.504155219 batch mAP 0.625549316 batch PCKh 0.1875\n",
      "Validated batch 164 batch loss 0.653167248 batch mAP 0.541381836 batch PCKh 0.5625\n",
      "Validated batch 165 batch loss 0.614761949 batch mAP 0.512481689 batch PCKh 0.5\n",
      "Validated batch 166 batch loss 0.557348371 batch mAP 0.573242188 batch PCKh 0.3125\n",
      "Validated batch 167 batch loss 0.647401452 batch mAP 0.476043701 batch PCKh 0.5\n",
      "Validated batch 168 batch loss 0.590618253 batch mAP 0.487976074 batch PCKh 0.1875\n",
      "Validated batch 169 batch loss 0.576002359 batch mAP 0.558044434 batch PCKh 0.5\n",
      "Validated batch 170 batch loss 0.593491673 batch mAP 0.560913086 batch PCKh 0.1875\n",
      "Validated batch 171 batch loss 0.690781772 batch mAP 0.462158203 batch PCKh 0.25\n",
      "Validated batch 172 batch loss 0.682264566 batch mAP 0.510528564 batch PCKh 0.375\n",
      "Validated batch 173 batch loss 0.570770323 batch mAP 0.529083252 batch PCKh 0.1875\n",
      "Validated batch 174 batch loss 0.662815 batch mAP 0.493713379 batch PCKh 0.4375\n",
      "Validated batch 175 batch loss 0.63789165 batch mAP 0.541809082 batch PCKh 0.125\n",
      "Validated batch 176 batch loss 0.523543894 batch mAP 0.590454102 batch PCKh 0.625\n",
      "Validated batch 177 batch loss 0.632529914 batch mAP 0.484039307 batch PCKh 0.3125\n",
      "Validated batch 178 batch loss 0.636362314 batch mAP 0.407653809 batch PCKh 0.1875\n",
      "Validated batch 179 batch loss 0.639211297 batch mAP 0.467346191 batch PCKh 0.5625\n",
      "Validated batch 180 batch loss 0.640365243 batch mAP 0.56842041 batch PCKh 0.1875\n",
      "Validated batch 181 batch loss 0.65218389 batch mAP 0.510314941 batch PCKh 0.0625\n",
      "Validated batch 182 batch loss 0.518626928 batch mAP 0.525787354 batch PCKh 0.625\n",
      "Validated batch 183 batch loss 0.606139421 batch mAP 0.562896729 batch PCKh 0.25\n",
      "Validated batch 184 batch loss 0.63162756 batch mAP 0.505065918 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 185 batch loss 0.654701173 batch mAP 0.552032471 batch PCKh 0.5\n",
      "Validated batch 186 batch loss 0.603970408 batch mAP 0.56640625 batch PCKh 0.625\n",
      "Validated batch 187 batch loss 0.693708956 batch mAP 0.542602539 batch PCKh 0.4375\n",
      "Validated batch 188 batch loss 0.599390447 batch mAP 0.596954346 batch PCKh 0.625\n",
      "Validated batch 189 batch loss 0.496360421 batch mAP 0.611785889 batch PCKh 0.125\n",
      "Validated batch 190 batch loss 0.648479819 batch mAP 0.525543213 batch PCKh 0.1875\n",
      "Validated batch 191 batch loss 0.671093822 batch mAP 0.500213623 batch PCKh 0.3125\n",
      "Validated batch 192 batch loss 0.596589744 batch mAP 0.475860596 batch PCKh 0.5\n",
      "Validated batch 193 batch loss 0.592412949 batch mAP 0.519195557 batch PCKh 0.375\n",
      "Validated batch 194 batch loss 0.642772555 batch mAP 0.568145752 batch PCKh 0.4375\n",
      "Validated batch 195 batch loss 0.554641366 batch mAP 0.49822998 batch PCKh 0.375\n",
      "Validated batch 196 batch loss 0.658797741 batch mAP 0.592529297 batch PCKh 0.75\n",
      "Validated batch 197 batch loss 0.624691308 batch mAP 0.562438965 batch PCKh 0.25\n",
      "Validated batch 198 batch loss 0.619842529 batch mAP 0.561737061 batch PCKh 0.1875\n",
      "Validated batch 199 batch loss 0.55831939 batch mAP 0.580627441 batch PCKh 0.5625\n",
      "Validated batch 200 batch loss 0.676501155 batch mAP 0.520813 batch PCKh 0.0625\n",
      "Validated batch 201 batch loss 0.450514674 batch mAP 0.554229736 batch PCKh 0.375\n",
      "Validated batch 202 batch loss 0.711965859 batch mAP 0.588195801 batch PCKh 0.875\n",
      "Validated batch 203 batch loss 0.719964564 batch mAP 0.52130127 batch PCKh 0.0625\n",
      "Validated batch 204 batch loss 0.603375435 batch mAP 0.612976074 batch PCKh 0.375\n",
      "Validated batch 205 batch loss 0.631899 batch mAP 0.56427 batch PCKh 0.1875\n",
      "Validated batch 206 batch loss 0.571574628 batch mAP 0.566772461 batch PCKh 0.25\n",
      "Validated batch 207 batch loss 0.543211102 batch mAP 0.539367676 batch PCKh 0.6875\n",
      "Validated batch 208 batch loss 0.543805838 batch mAP 0.600158691 batch PCKh 0.875\n",
      "Validated batch 209 batch loss 0.609220147 batch mAP 0.380645752 batch PCKh 0.375\n",
      "Validated batch 210 batch loss 0.577647 batch mAP 0.573272705 batch PCKh 0.375\n",
      "Validated batch 211 batch loss 0.636188269 batch mAP 0.551605225 batch PCKh 0.6875\n",
      "Validated batch 212 batch loss 0.683363795 batch mAP 0.41885376 batch PCKh 0.25\n",
      "Validated batch 213 batch loss 0.638257623 batch mAP 0.438659668 batch PCKh 0.0625\n",
      "Validated batch 214 batch loss 0.701982319 batch mAP 0.369750977 batch PCKh 0.125\n",
      "Validated batch 215 batch loss 0.674823761 batch mAP 0.390319824 batch PCKh 0.3125\n",
      "Validated batch 216 batch loss 0.710031152 batch mAP 0.430755615 batch PCKh 0.6875\n",
      "Validated batch 217 batch loss 0.599344909 batch mAP 0.582794189 batch PCKh 0.6875\n",
      "Validated batch 218 batch loss 0.614368558 batch mAP 0.529693604 batch PCKh 0.4375\n",
      "Validated batch 219 batch loss 0.701695442 batch mAP 0.5730896 batch PCKh 0.25\n",
      "Validated batch 220 batch loss 0.658574879 batch mAP 0.439880371 batch PCKh 0.375\n",
      "Validated batch 221 batch loss 0.620544553 batch mAP 0.481567383 batch PCKh 0.1875\n",
      "Validated batch 222 batch loss 0.601039 batch mAP 0.600006104 batch PCKh 0.75\n",
      "Validated batch 223 batch loss 0.747938633 batch mAP 0.522094727 batch PCKh 0.125\n",
      "Validated batch 224 batch loss 0.547638535 batch mAP 0.497528076 batch PCKh 0.5\n",
      "Validated batch 225 batch loss 0.605590463 batch mAP 0.498291016 batch PCKh 0.1875\n",
      "Validated batch 226 batch loss 0.652285933 batch mAP 0.485015869 batch PCKh 0.75\n",
      "Validated batch 227 batch loss 0.493145525 batch mAP 0.587799072 batch PCKh 0.1875\n",
      "Validated batch 228 batch loss 0.497798979 batch mAP 0.558349609 batch PCKh 0.3125\n",
      "Validated batch 229 batch loss 0.575083137 batch mAP 0.54196167 batch PCKh 0.5625\n",
      "Validated batch 230 batch loss 0.679938793 batch mAP 0.500640869 batch PCKh 0.1875\n",
      "Validated batch 231 batch loss 0.665859044 batch mAP 0.561065674 batch PCKh 0.6875\n",
      "Validated batch 232 batch loss 0.528781593 batch mAP 0.508117676 batch PCKh 0.375\n",
      "Validated batch 233 batch loss 0.576660454 batch mAP 0.52923584 batch PCKh 0.625\n",
      "Validated batch 234 batch loss 0.592989326 batch mAP 0.548156738 batch PCKh 0.25\n",
      "Validated batch 235 batch loss 0.645735 batch mAP 0.533569336 batch PCKh 0.625\n",
      "Validated batch 236 batch loss 0.64095664 batch mAP 0.489990234 batch PCKh 0.6875\n",
      "Validated batch 237 batch loss 0.602867 batch mAP 0.558441162 batch PCKh 0.1875\n",
      "Validated batch 238 batch loss 0.542142272 batch mAP 0.613250732 batch PCKh 0.4375\n",
      "Validated batch 239 batch loss 0.63768816 batch mAP 0.590545654 batch PCKh 0.5625\n",
      "Validated batch 240 batch loss 0.654807031 batch mAP 0.567932129 batch PCKh 0.625\n",
      "Validated batch 241 batch loss 0.749697089 batch mAP 0.473175049 batch PCKh 0.3125\n",
      "Validated batch 242 batch loss 0.687209487 batch mAP 0.508178711 batch PCKh 0.375\n",
      "Validated batch 243 batch loss 0.544962883 batch mAP 0.593078613 batch PCKh 0.6875\n",
      "Validated batch 244 batch loss 0.520589888 batch mAP 0.535614 batch PCKh 0.875\n",
      "Validated batch 245 batch loss 0.623684227 batch mAP 0.623962402 batch PCKh 0.125\n",
      "Validated batch 246 batch loss 0.573216081 batch mAP 0.583953857 batch PCKh 0.3125\n",
      "Validated batch 247 batch loss 0.623924911 batch mAP 0.542266846 batch PCKh 0.3125\n",
      "Validated batch 248 batch loss 0.588172078 batch mAP 0.533569336 batch PCKh 0.0625\n",
      "Validated batch 249 batch loss 0.624341905 batch mAP 0.486022949 batch PCKh 0.6875\n",
      "Validated batch 250 batch loss 0.638370216 batch mAP 0.515930176 batch PCKh 0.5\n",
      "Validated batch 251 batch loss 0.631051183 batch mAP 0.535553 batch PCKh 0.1875\n",
      "Validated batch 252 batch loss 0.615854263 batch mAP 0.554260254 batch PCKh 0.625\n",
      "Validated batch 253 batch loss 0.579077 batch mAP 0.571807861 batch PCKh 0.5\n",
      "Validated batch 254 batch loss 0.597872138 batch mAP 0.457946777 batch PCKh 0.75\n",
      "Validated batch 255 batch loss 0.438222229 batch mAP 0.532897949 batch PCKh 0.3125\n",
      "Validated batch 256 batch loss 0.562634647 batch mAP 0.586608887 batch PCKh 0.5\n",
      "Validated batch 257 batch loss 0.60875988 batch mAP 0.584564209 batch PCKh 0.875\n",
      "Validated batch 258 batch loss 0.58843416 batch mAP 0.59954834 batch PCKh 0.625\n",
      "Validated batch 259 batch loss 0.589962125 batch mAP 0.595367432 batch PCKh 0.375\n",
      "Validated batch 260 batch loss 0.563822269 batch mAP 0.6121521 batch PCKh 0.4375\n",
      "Validated batch 261 batch loss 0.581000686 batch mAP 0.550354 batch PCKh 0.75\n",
      "Validated batch 262 batch loss 0.586431384 batch mAP 0.459228516 batch PCKh 0.5625\n",
      "Validated batch 263 batch loss 0.700011134 batch mAP 0.492095947 batch PCKh 0.0625\n",
      "Validated batch 264 batch loss 0.595857322 batch mAP 0.601043701 batch PCKh 0.8125\n",
      "Validated batch 265 batch loss 0.569691181 batch mAP 0.600067139 batch PCKh 0.375\n",
      "Validated batch 266 batch loss 0.491184324 batch mAP 0.587310791 batch PCKh 0.5\n",
      "Validated batch 267 batch loss 0.655742288 batch mAP 0.572540283 batch PCKh 0.125\n",
      "Validated batch 268 batch loss 0.597436726 batch mAP 0.603363037 batch PCKh 0.375\n",
      "Validated batch 269 batch loss 0.661157 batch mAP 0.515808105 batch PCKh 0.125\n",
      "Validated batch 270 batch loss 0.65440917 batch mAP 0.510040283 batch PCKh 0.1875\n",
      "Validated batch 271 batch loss 0.614023387 batch mAP 0.570129395 batch PCKh 0.25\n",
      "Validated batch 272 batch loss 0.628155768 batch mAP 0.595275879 batch PCKh 0.625\n",
      "Validated batch 273 batch loss 0.608121753 batch mAP 0.622009277 batch PCKh 0.375\n",
      "Validated batch 274 batch loss 0.632439 batch mAP 0.554138184 batch PCKh 0.5625\n",
      "Validated batch 275 batch loss 0.549686074 batch mAP 0.532196045 batch PCKh 0.625\n",
      "Validated batch 276 batch loss 0.524443209 batch mAP 0.531036377 batch PCKh 0.1875\n",
      "Validated batch 277 batch loss 0.604741335 batch mAP 0.547393799 batch PCKh 0.4375\n",
      "Validated batch 278 batch loss 0.555314183 batch mAP 0.478912354 batch PCKh 0.1875\n",
      "Validated batch 279 batch loss 0.641616225 batch mAP 0.492034912 batch PCKh 0\n",
      "Validated batch 280 batch loss 0.561162114 batch mAP 0.522094727 batch PCKh 0.3125\n",
      "Validated batch 281 batch loss 0.568471491 batch mAP 0.543731689 batch PCKh 0.5\n",
      "Validated batch 282 batch loss 0.609130383 batch mAP 0.513580322 batch PCKh 0.3125\n",
      "Validated batch 283 batch loss 0.605315447 batch mAP 0.5909729 batch PCKh 0.5\n",
      "Validated batch 284 batch loss 0.503677309 batch mAP 0.672607422 batch PCKh 0.3125\n",
      "Validated batch 285 batch loss 0.602042317 batch mAP 0.554626465 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 286 batch loss 0.552748084 batch mAP 0.529876709 batch PCKh 0.75\n",
      "Validated batch 287 batch loss 0.618743539 batch mAP 0.550018311 batch PCKh 0.1875\n",
      "Validated batch 288 batch loss 0.694228351 batch mAP 0.498657227 batch PCKh 0.6875\n",
      "Validated batch 289 batch loss 0.561024427 batch mAP 0.562683105 batch PCKh 0.3125\n",
      "Validated batch 290 batch loss 0.610582352 batch mAP 0.515838623 batch PCKh 0.8125\n",
      "Validated batch 291 batch loss 0.633735657 batch mAP 0.464904785 batch PCKh 0.25\n",
      "Validated batch 292 batch loss 0.694095314 batch mAP 0.484039307 batch PCKh 0.5\n",
      "Validated batch 293 batch loss 0.69192338 batch mAP 0.447967529 batch PCKh 0.1875\n",
      "Validated batch 294 batch loss 0.617830157 batch mAP 0.52166748 batch PCKh 0.5625\n",
      "Validated batch 295 batch loss 0.717784524 batch mAP 0.510955811 batch PCKh 0.125\n",
      "Validated batch 296 batch loss 0.589043796 batch mAP 0.457519531 batch PCKh 0.3125\n",
      "Validated batch 297 batch loss 0.607934237 batch mAP 0.557251 batch PCKh 0.5625\n",
      "Validated batch 298 batch loss 0.634122729 batch mAP 0.551971436 batch PCKh 0.5625\n",
      "Validated batch 299 batch loss 0.649631381 batch mAP 0.511444092 batch PCKh 0.75\n",
      "Validated batch 300 batch loss 0.714447379 batch mAP 0.437042236 batch PCKh 0.25\n",
      "Validated batch 301 batch loss 0.609557688 batch mAP 0.592346191 batch PCKh 0.625\n",
      "Validated batch 302 batch loss 0.671618581 batch mAP 0.451385498 batch PCKh 0.6875\n",
      "Validated batch 303 batch loss 0.696559191 batch mAP 0.475524902 batch PCKh 0.3125\n",
      "Validated batch 304 batch loss 0.620383382 batch mAP 0.564300537 batch PCKh 0.125\n",
      "Validated batch 305 batch loss 0.668813229 batch mAP 0.580871582 batch PCKh 0.4375\n",
      "Validated batch 306 batch loss 0.630568147 batch mAP 0.5 batch PCKh 0.8125\n",
      "Validated batch 307 batch loss 0.691614687 batch mAP 0.501525879 batch PCKh 0.75\n",
      "Validated batch 308 batch loss 0.719783127 batch mAP 0.548919678 batch PCKh 0.6875\n",
      "Validated batch 309 batch loss 0.656682312 batch mAP 0.499542236 batch PCKh 0.5\n",
      "Validated batch 310 batch loss 0.669789553 batch mAP 0.547576904 batch PCKh 0.1875\n",
      "Validated batch 311 batch loss 0.652459145 batch mAP 0.620544434 batch PCKh 0.625\n",
      "Validated batch 312 batch loss 0.511680186 batch mAP 0.500976562 batch PCKh 0.3125\n",
      "Validated batch 313 batch loss 0.568947315 batch mAP 0.580413818 batch PCKh 0.5\n",
      "Validated batch 314 batch loss 0.614402652 batch mAP 0.569976807 batch PCKh 0.375\n",
      "Validated batch 315 batch loss 0.645447195 batch mAP 0.521942139 batch PCKh 0.3125\n",
      "Validated batch 316 batch loss 0.643292308 batch mAP 0.560638428 batch PCKh 0.3125\n",
      "Validated batch 317 batch loss 0.668684244 batch mAP 0.547302246 batch PCKh 0.5625\n",
      "Validated batch 318 batch loss 0.642006457 batch mAP 0.52432251 batch PCKh 0.5\n",
      "Validated batch 319 batch loss 0.568264 batch mAP 0.580352783 batch PCKh 0.375\n",
      "Validated batch 320 batch loss 0.596991539 batch mAP 0.529754639 batch PCKh 0.375\n",
      "Validated batch 321 batch loss 0.665057302 batch mAP 0.505218506 batch PCKh 0.5625\n",
      "Validated batch 322 batch loss 0.610811055 batch mAP 0.478759766 batch PCKh 0.1875\n",
      "Validated batch 323 batch loss 0.62448442 batch mAP 0.525024414 batch PCKh 0.5625\n",
      "Validated batch 324 batch loss 0.668695 batch mAP 0.478942871 batch PCKh 0.1875\n",
      "Validated batch 325 batch loss 0.644077897 batch mAP 0.553985596 batch PCKh 0.1875\n",
      "Validated batch 326 batch loss 0.566261649 batch mAP 0.598053 batch PCKh 0.4375\n",
      "Validated batch 327 batch loss 0.737454951 batch mAP 0.564422607 batch PCKh 0.25\n",
      "Validated batch 328 batch loss 0.530203283 batch mAP 0.558959961 batch PCKh 0.5625\n",
      "Validated batch 329 batch loss 0.607315063 batch mAP 0.597869873 batch PCKh 0.4375\n",
      "Validated batch 330 batch loss 0.602156818 batch mAP 0.607666 batch PCKh 0.5625\n",
      "Validated batch 331 batch loss 0.594714761 batch mAP 0.586120605 batch PCKh 0.8125\n",
      "Validated batch 332 batch loss 0.515912473 batch mAP 0.625762939 batch PCKh 0.625\n",
      "Validated batch 333 batch loss 0.636370301 batch mAP 0.571411133 batch PCKh 0.25\n",
      "Validated batch 334 batch loss 0.637666702 batch mAP 0.607086182 batch PCKh 0.5\n",
      "Validated batch 335 batch loss 0.603631139 batch mAP 0.575286865 batch PCKh 0.5\n",
      "Validated batch 336 batch loss 0.654628217 batch mAP 0.498962402 batch PCKh 0.75\n",
      "Validated batch 337 batch loss 0.625483215 batch mAP 0.607940674 batch PCKh 0\n",
      "Validated batch 338 batch loss 0.596868753 batch mAP 0.545196533 batch PCKh 0.5625\n",
      "Validated batch 339 batch loss 0.562068403 batch mAP 0.580078125 batch PCKh 0.5625\n",
      "Validated batch 340 batch loss 0.587811172 batch mAP 0.625427246 batch PCKh 0.25\n",
      "Validated batch 341 batch loss 0.684344232 batch mAP 0.562683105 batch PCKh 0.5\n",
      "Validated batch 342 batch loss 0.602642298 batch mAP 0.561859131 batch PCKh 0.375\n",
      "Validated batch 343 batch loss 0.611357629 batch mAP 0.601165771 batch PCKh 0.25\n",
      "Validated batch 344 batch loss 0.636329532 batch mAP 0.546081543 batch PCKh 0.1875\n",
      "Validated batch 345 batch loss 0.656678081 batch mAP 0.481506348 batch PCKh 0.25\n",
      "Validated batch 346 batch loss 0.650427938 batch mAP 0.460693359 batch PCKh 0.625\n",
      "Validated batch 347 batch loss 0.575146198 batch mAP 0.522857666 batch PCKh 0.375\n",
      "Validated batch 348 batch loss 0.67123127 batch mAP 0.461486816 batch PCKh 0.4375\n",
      "Validated batch 349 batch loss 0.555356145 batch mAP 0.501190186 batch PCKh 0.375\n",
      "Validated batch 350 batch loss 0.529098094 batch mAP 0.500671387 batch PCKh 0.5625\n",
      "Validated batch 351 batch loss 0.644489646 batch mAP 0.473327637 batch PCKh 0.4375\n",
      "Validated batch 352 batch loss 0.549361229 batch mAP 0.619934082 batch PCKh 0.4375\n",
      "Validated batch 353 batch loss 0.582219362 batch mAP 0.503875732 batch PCKh 0.625\n",
      "Validated batch 354 batch loss 0.620050788 batch mAP 0.493865967 batch PCKh 0.5\n",
      "Validated batch 355 batch loss 0.582247615 batch mAP 0.551574707 batch PCKh 0.125\n",
      "Validated batch 356 batch loss 0.593298256 batch mAP 0.537689209 batch PCKh 0.5625\n",
      "Validated batch 357 batch loss 0.588423848 batch mAP 0.482849121 batch PCKh 0.0625\n",
      "Validated batch 358 batch loss 0.666573346 batch mAP 0.514648438 batch PCKh 0.375\n",
      "Validated batch 359 batch loss 0.658040166 batch mAP 0.590423584 batch PCKh 0.375\n",
      "Validated batch 360 batch loss 0.60121727 batch mAP 0.529968262 batch PCKh 0.3125\n",
      "Validated batch 361 batch loss 0.694091856 batch mAP 0.540252686 batch PCKh 0.1875\n",
      "Validated batch 362 batch loss 0.725467801 batch mAP 0.457489 batch PCKh 0\n",
      "Validated batch 363 batch loss 0.760654926 batch mAP 0.44921875 batch PCKh 0\n",
      "Validated batch 364 batch loss 0.673327684 batch mAP 0.544525146 batch PCKh 0.375\n",
      "Validated batch 365 batch loss 0.609204233 batch mAP 0.574859619 batch PCKh 0.625\n",
      "Validated batch 366 batch loss 0.660948396 batch mAP 0.535369873 batch PCKh 0.25\n",
      "Validated batch 367 batch loss 0.592765033 batch mAP 0.438110352 batch PCKh 0.4375\n",
      "Validated batch 368 batch loss 0.555496 batch mAP 0.563018799 batch PCKh 0.4375\n",
      "Validated batch 369 batch loss 0.54713136 batch mAP 0.570800781 batch PCKh 0.4375\n",
      "Epoch 3 val loss 0.6134599447250366 val mAP 0.5368416905403137 val PCKh\n",
      "Epoch 3 completed in 768.27 seconds\n",
      "Model /aiffel/aiffel/model_weight/GD08/y_model-epoch-3-loss-0.6135.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.747414529 batch mAP 0.444396973 batch PCKh 0.375\n",
      "Trained batch 2 batch loss 0.630149961 batch mAP 0.567871094 batch PCKh 0.5625\n",
      "Trained batch 3 batch loss 0.626009107 batch mAP 0.605377197 batch PCKh 0.5625\n",
      "Trained batch 4 batch loss 0.552748799 batch mAP 0.571685791 batch PCKh 0.3125\n",
      "Trained batch 5 batch loss 0.539103031 batch mAP 0.613769531 batch PCKh 0.4375\n",
      "Trained batch 6 batch loss 0.570997715 batch mAP 0.592132568 batch PCKh 0.375\n",
      "Trained batch 7 batch loss 0.576643884 batch mAP 0.522125244 batch PCKh 0.375\n",
      "Trained batch 8 batch loss 0.679935932 batch mAP 0.505310059 batch PCKh 0.125\n",
      "Trained batch 9 batch loss 0.628546357 batch mAP 0.508453369 batch PCKh 0.75\n",
      "Trained batch 10 batch loss 0.570643 batch mAP 0.469268799 batch PCKh 0.75\n",
      "Trained batch 11 batch loss 0.56611383 batch mAP 0.461334229 batch PCKh 0.75\n",
      "Trained batch 12 batch loss 0.585012138 batch mAP 0.497650146 batch PCKh 0.3125\n",
      "Trained batch 13 batch loss 0.679479241 batch mAP 0.481658936 batch PCKh 0.8125\n",
      "Trained batch 14 batch loss 0.532377183 batch mAP 0.449523926 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 15 batch loss 0.582529187 batch mAP 0.450134277 batch PCKh 0.25\n",
      "Trained batch 16 batch loss 0.57140106 batch mAP 0.430236816 batch PCKh 0.5\n",
      "Trained batch 17 batch loss 0.642864585 batch mAP 0.463104248 batch PCKh 0.75\n",
      "Trained batch 18 batch loss 0.52845484 batch mAP 0.465087891 batch PCKh 0.5\n",
      "Trained batch 19 batch loss 0.568972528 batch mAP 0.45892334 batch PCKh 0.5\n",
      "Trained batch 20 batch loss 0.612981439 batch mAP 0.486022949 batch PCKh 0.6875\n",
      "Trained batch 21 batch loss 0.515803 batch mAP 0.514190674 batch PCKh 0.75\n",
      "Trained batch 22 batch loss 0.599666476 batch mAP 0.494750977 batch PCKh 0.5625\n",
      "Trained batch 23 batch loss 0.569808483 batch mAP 0.52545166 batch PCKh 0.625\n",
      "Trained batch 24 batch loss 0.597684741 batch mAP 0.50692749 batch PCKh 0.625\n",
      "Trained batch 25 batch loss 0.627596259 batch mAP 0.510040283 batch PCKh 0.625\n",
      "Trained batch 26 batch loss 0.676288605 batch mAP 0.465087891 batch PCKh 0.5\n",
      "Trained batch 27 batch loss 0.555526257 batch mAP 0.506347656 batch PCKh 0.375\n",
      "Trained batch 28 batch loss 0.517701387 batch mAP 0.512054443 batch PCKh 0.625\n",
      "Trained batch 29 batch loss 0.498535126 batch mAP 0.528320312 batch PCKh 0.75\n",
      "Trained batch 30 batch loss 0.500598669 batch mAP 0.566223145 batch PCKh 0.625\n",
      "Trained batch 31 batch loss 0.527014 batch mAP 0.54599 batch PCKh 0.5625\n",
      "Trained batch 32 batch loss 0.442075878 batch mAP 0.579742432 batch PCKh 0.4375\n",
      "Trained batch 33 batch loss 0.446368068 batch mAP 0.54296875 batch PCKh 0.5625\n",
      "Trained batch 34 batch loss 0.474171281 batch mAP 0.517822266 batch PCKh 0.8125\n",
      "Trained batch 35 batch loss 0.514071465 batch mAP 0.581420898 batch PCKh 0.8125\n",
      "Trained batch 36 batch loss 0.446965158 batch mAP 0.617797852 batch PCKh 0.25\n",
      "Trained batch 37 batch loss 0.631602 batch mAP 0.535186768 batch PCKh 0.6875\n",
      "Trained batch 38 batch loss 0.567715764 batch mAP 0.550994873 batch PCKh 0.75\n",
      "Trained batch 39 batch loss 0.497661769 batch mAP 0.521942139 batch PCKh 0.4375\n",
      "Trained batch 40 batch loss 0.506826 batch mAP 0.517028809 batch PCKh 0.4375\n",
      "Trained batch 41 batch loss 0.426710039 batch mAP 0.573730469 batch PCKh 0.1875\n",
      "Trained batch 42 batch loss 0.490261376 batch mAP 0.550933838 batch PCKh 0.6875\n",
      "Trained batch 43 batch loss 0.541247 batch mAP 0.58984375 batch PCKh 0.625\n",
      "Trained batch 44 batch loss 0.519755 batch mAP 0.568115234 batch PCKh 0.125\n",
      "Trained batch 45 batch loss 0.5328601 batch mAP 0.584106445 batch PCKh 0.5625\n",
      "Trained batch 46 batch loss 0.619735956 batch mAP 0.599121094 batch PCKh 0.1875\n",
      "Trained batch 47 batch loss 0.488012671 batch mAP 0.641326904 batch PCKh 0.3125\n",
      "Trained batch 48 batch loss 0.532625437 batch mAP 0.620239258 batch PCKh 0.4375\n",
      "Trained batch 49 batch loss 0.530532956 batch mAP 0.589996338 batch PCKh 0.4375\n",
      "Trained batch 50 batch loss 0.57563448 batch mAP 0.559845 batch PCKh 0.3125\n",
      "Trained batch 51 batch loss 0.500044227 batch mAP 0.600494385 batch PCKh 0\n",
      "Trained batch 52 batch loss 0.536420941 batch mAP 0.633728 batch PCKh 0.1875\n",
      "Trained batch 53 batch loss 0.570410728 batch mAP 0.550445557 batch PCKh 0.625\n",
      "Trained batch 54 batch loss 0.509178042 batch mAP 0.526916504 batch PCKh 0.3125\n",
      "Trained batch 55 batch loss 0.553798556 batch mAP 0.502319336 batch PCKh 0.3125\n",
      "Trained batch 56 batch loss 0.62152797 batch mAP 0.515197754 batch PCKh 0.25\n",
      "Trained batch 57 batch loss 0.559387088 batch mAP 0.542633057 batch PCKh 0.6875\n",
      "Trained batch 58 batch loss 0.485287666 batch mAP 0.552032471 batch PCKh 0.5\n",
      "Trained batch 59 batch loss 0.513470769 batch mAP 0.550048828 batch PCKh 0.4375\n",
      "Trained batch 60 batch loss 0.522645354 batch mAP 0.557067871 batch PCKh 0.25\n",
      "Trained batch 61 batch loss 0.509780169 batch mAP 0.522735596 batch PCKh 0.25\n",
      "Trained batch 62 batch loss 0.530355752 batch mAP 0.457336426 batch PCKh 0.0625\n",
      "Trained batch 63 batch loss 0.534447312 batch mAP 0.504821777 batch PCKh 0.375\n",
      "Trained batch 64 batch loss 0.500038207 batch mAP 0.546417236 batch PCKh 0.5\n",
      "Trained batch 65 batch loss 0.554386318 batch mAP 0.540313721 batch PCKh 0.125\n",
      "Trained batch 66 batch loss 0.491352618 batch mAP 0.602539062 batch PCKh 0.1875\n",
      "Trained batch 67 batch loss 0.509324789 batch mAP 0.588165283 batch PCKh 0.0625\n",
      "Trained batch 68 batch loss 0.506921828 batch mAP 0.569549561 batch PCKh 0.3125\n",
      "Trained batch 69 batch loss 0.518186331 batch mAP 0.553192139 batch PCKh 0.0625\n",
      "Trained batch 70 batch loss 0.585509121 batch mAP 0.540618896 batch PCKh 0.25\n",
      "Trained batch 71 batch loss 0.536442041 batch mAP 0.544952393 batch PCKh 0.125\n",
      "Trained batch 72 batch loss 0.539980829 batch mAP 0.604736328 batch PCKh 0.625\n",
      "Trained batch 73 batch loss 0.591731787 batch mAP 0.589263916 batch PCKh 0.6875\n",
      "Trained batch 74 batch loss 0.583623409 batch mAP 0.586914062 batch PCKh 0.5625\n",
      "Trained batch 75 batch loss 0.600682199 batch mAP 0.595001221 batch PCKh 0.375\n",
      "Trained batch 76 batch loss 0.515815 batch mAP 0.636932373 batch PCKh 0.25\n",
      "Trained batch 77 batch loss 0.571876705 batch mAP 0.627624512 batch PCKh 0.5\n",
      "Trained batch 78 batch loss 0.574434638 batch mAP 0.645935059 batch PCKh 0.3125\n",
      "Trained batch 79 batch loss 0.580920577 batch mAP 0.613708496 batch PCKh 0.1875\n",
      "Trained batch 80 batch loss 0.544966102 batch mAP 0.624725342 batch PCKh 0.1875\n",
      "Trained batch 81 batch loss 0.613006294 batch mAP 0.587310791 batch PCKh 0.5625\n",
      "Trained batch 82 batch loss 0.608614206 batch mAP 0.526001 batch PCKh 0.375\n",
      "Trained batch 83 batch loss 0.65553093 batch mAP 0.561676 batch PCKh 0.1875\n",
      "Trained batch 84 batch loss 0.601862788 batch mAP 0.598968506 batch PCKh 0.3125\n",
      "Trained batch 85 batch loss 0.622697473 batch mAP 0.597381592 batch PCKh 0.375\n",
      "Trained batch 86 batch loss 0.565808535 batch mAP 0.586639404 batch PCKh 0.5\n",
      "Trained batch 87 batch loss 0.587094247 batch mAP 0.621398926 batch PCKh 0.5625\n",
      "Trained batch 88 batch loss 0.553425789 batch mAP 0.615997314 batch PCKh 0.4375\n",
      "Trained batch 89 batch loss 0.567568421 batch mAP 0.632141113 batch PCKh 0.3125\n",
      "Trained batch 90 batch loss 0.583353758 batch mAP 0.593261719 batch PCKh 0.4375\n",
      "Trained batch 91 batch loss 0.5194664 batch mAP 0.613342285 batch PCKh 0.5625\n",
      "Trained batch 92 batch loss 0.47949475 batch mAP 0.578857422 batch PCKh 0.1875\n",
      "Trained batch 93 batch loss 0.466934264 batch mAP 0.528045654 batch PCKh 0.1875\n",
      "Trained batch 94 batch loss 0.447750628 batch mAP 0.580505371 batch PCKh 0.4375\n",
      "Trained batch 95 batch loss 0.444598436 batch mAP 0.580871582 batch PCKh 0\n",
      "Trained batch 96 batch loss 0.560903966 batch mAP 0.55859375 batch PCKh 0.0625\n",
      "Trained batch 97 batch loss 0.64498508 batch mAP 0.575744629 batch PCKh 0.5\n",
      "Trained batch 98 batch loss 0.560343564 batch mAP 0.565887451 batch PCKh 0.3125\n",
      "Trained batch 99 batch loss 0.620777607 batch mAP 0.577911377 batch PCKh 0.25\n",
      "Trained batch 100 batch loss 0.612445951 batch mAP 0.565765381 batch PCKh 0.375\n",
      "Trained batch 101 batch loss 0.602754176 batch mAP 0.609741211 batch PCKh 0.25\n",
      "Trained batch 102 batch loss 0.559243739 batch mAP 0.604553223 batch PCKh 0.3125\n",
      "Trained batch 103 batch loss 0.625432312 batch mAP 0.545623779 batch PCKh 0.1875\n",
      "Trained batch 104 batch loss 0.571922123 batch mAP 0.497589111 batch PCKh 0.0625\n",
      "Trained batch 105 batch loss 0.746044636 batch mAP 0.455963135 batch PCKh 0.0625\n",
      "Trained batch 106 batch loss 0.589669049 batch mAP 0.487548828 batch PCKh 0.3125\n",
      "Trained batch 107 batch loss 0.544104 batch mAP 0.510467529 batch PCKh 0.1875\n",
      "Trained batch 108 batch loss 0.552457273 batch mAP 0.491271973 batch PCKh 0.5\n",
      "Trained batch 109 batch loss 0.627483964 batch mAP 0.393707275 batch PCKh 0.125\n",
      "Trained batch 110 batch loss 0.567135453 batch mAP 0.440429688 batch PCKh 0.4375\n",
      "Trained batch 111 batch loss 0.526165545 batch mAP 0.450622559 batch PCKh 0.0625\n",
      "Trained batch 112 batch loss 0.567866564 batch mAP 0.504974365 batch PCKh 0\n",
      "Trained batch 113 batch loss 0.593820095 batch mAP 0.47668457 batch PCKh 0.4375\n",
      "Trained batch 114 batch loss 0.628615618 batch mAP 0.527709961 batch PCKh 0.3125\n",
      "Trained batch 115 batch loss 0.584300101 batch mAP 0.53894043 batch PCKh 0.25\n",
      "Trained batch 116 batch loss 0.572849631 batch mAP 0.543792725 batch PCKh 0.1875\n",
      "Trained batch 117 batch loss 0.446285337 batch mAP 0.570098877 batch PCKh 0.0625\n",
      "Trained batch 118 batch loss 0.465112746 batch mAP 0.599121094 batch PCKh 0.1875\n",
      "Trained batch 119 batch loss 0.430894405 batch mAP 0.504089355 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 120 batch loss 0.384397328 batch mAP 0.507476807 batch PCKh 0\n",
      "Trained batch 121 batch loss 0.511142373 batch mAP 0.615997314 batch PCKh 0.5625\n",
      "Trained batch 122 batch loss 0.504241 batch mAP 0.653686523 batch PCKh 0.4375\n",
      "Trained batch 123 batch loss 0.63994348 batch mAP 0.627716064 batch PCKh 0.1875\n",
      "Trained batch 124 batch loss 0.621329367 batch mAP 0.633972168 batch PCKh 0.8125\n",
      "Trained batch 125 batch loss 0.681810498 batch mAP 0.611297607 batch PCKh 0.3125\n",
      "Trained batch 126 batch loss 0.676996 batch mAP 0.585388184 batch PCKh 0\n",
      "Trained batch 127 batch loss 0.661518157 batch mAP 0.523010254 batch PCKh 0.1875\n",
      "Trained batch 128 batch loss 0.685918272 batch mAP 0.538970947 batch PCKh 0.0625\n",
      "Trained batch 129 batch loss 0.637908161 batch mAP 0.51171875 batch PCKh 0.3125\n",
      "Trained batch 130 batch loss 0.651347339 batch mAP 0.528076172 batch PCKh 0.0625\n",
      "Trained batch 131 batch loss 0.602624 batch mAP 0.517272949 batch PCKh 0.125\n",
      "Trained batch 132 batch loss 0.610965312 batch mAP 0.498413086 batch PCKh 0.5625\n",
      "Trained batch 133 batch loss 0.631785095 batch mAP 0.517944336 batch PCKh 0.5\n",
      "Trained batch 134 batch loss 0.678457856 batch mAP 0.505065918 batch PCKh 0.375\n",
      "Trained batch 135 batch loss 0.531970263 batch mAP 0.499267578 batch PCKh 0.1875\n",
      "Trained batch 136 batch loss 0.586937964 batch mAP 0.462310791 batch PCKh 0.5625\n",
      "Trained batch 137 batch loss 0.546680391 batch mAP 0.533935547 batch PCKh 0.6875\n",
      "Trained batch 138 batch loss 0.61829865 batch mAP 0.51272583 batch PCKh 0.5\n",
      "Trained batch 139 batch loss 0.569762588 batch mAP 0.544921875 batch PCKh 0.8125\n",
      "Trained batch 140 batch loss 0.650718808 batch mAP 0.477935791 batch PCKh 0.75\n",
      "Trained batch 141 batch loss 0.562941134 batch mAP 0.516204834 batch PCKh 0.625\n",
      "Trained batch 142 batch loss 0.525349379 batch mAP 0.552764893 batch PCKh 0.4375\n",
      "Trained batch 143 batch loss 0.580477834 batch mAP 0.53012085 batch PCKh 0.625\n",
      "Trained batch 144 batch loss 0.607366741 batch mAP 0.466369629 batch PCKh 0.5625\n",
      "Trained batch 145 batch loss 0.561265409 batch mAP 0.503051758 batch PCKh 0.5\n",
      "Trained batch 146 batch loss 0.507456303 batch mAP 0.519378662 batch PCKh 0.5\n",
      "Trained batch 147 batch loss 0.587379575 batch mAP 0.491577148 batch PCKh 0.4375\n",
      "Trained batch 148 batch loss 0.523962259 batch mAP 0.531036377 batch PCKh 0.375\n",
      "Trained batch 149 batch loss 0.611897647 batch mAP 0.528900146 batch PCKh 0.75\n",
      "Trained batch 150 batch loss 0.511137307 batch mAP 0.556640625 batch PCKh 0.25\n",
      "Trained batch 151 batch loss 0.564525604 batch mAP 0.610412598 batch PCKh 0.3125\n",
      "Trained batch 152 batch loss 0.464286596 batch mAP 0.614013672 batch PCKh 0.4375\n",
      "Trained batch 153 batch loss 0.552793205 batch mAP 0.639587402 batch PCKh 0.6875\n",
      "Trained batch 154 batch loss 0.50467515 batch mAP 0.669921875 batch PCKh 0.5625\n",
      "Trained batch 155 batch loss 0.614768863 batch mAP 0.589263916 batch PCKh 0.625\n",
      "Trained batch 156 batch loss 0.631929755 batch mAP 0.557830811 batch PCKh 0.6875\n",
      "Trained batch 157 batch loss 0.639933884 batch mAP 0.532989502 batch PCKh 0.6875\n",
      "Trained batch 158 batch loss 0.668736339 batch mAP 0.472076416 batch PCKh 0.25\n",
      "Trained batch 159 batch loss 0.666574121 batch mAP 0.541351318 batch PCKh 0.375\n",
      "Trained batch 160 batch loss 0.713705897 batch mAP 0.504974365 batch PCKh 0.3125\n",
      "Trained batch 161 batch loss 0.620743096 batch mAP 0.512786865 batch PCKh 0.875\n",
      "Trained batch 162 batch loss 0.634042144 batch mAP 0.538574219 batch PCKh 0.25\n",
      "Trained batch 163 batch loss 0.607682824 batch mAP 0.514038086 batch PCKh 0.8125\n",
      "Trained batch 164 batch loss 0.630752 batch mAP 0.473602295 batch PCKh 0.375\n",
      "Trained batch 165 batch loss 0.585243583 batch mAP 0.382232666 batch PCKh 0.125\n",
      "Trained batch 166 batch loss 0.502923727 batch mAP 0.238494873 batch PCKh 0.4375\n",
      "Trained batch 167 batch loss 0.451988637 batch mAP 0.201416016 batch PCKh 0.4375\n",
      "Trained batch 168 batch loss 0.500005603 batch mAP 0.249084473 batch PCKh 0.5\n",
      "Trained batch 169 batch loss 0.441616 batch mAP 0.235687256 batch PCKh 0.5\n",
      "Trained batch 170 batch loss 0.408128887 batch mAP 0.2996521 batch PCKh 0\n",
      "Trained batch 171 batch loss 0.416476727 batch mAP 0.355865479 batch PCKh 0.5625\n",
      "Trained batch 172 batch loss 0.447262704 batch mAP 0.429046631 batch PCKh 0.625\n",
      "Trained batch 173 batch loss 0.506352842 batch mAP 0.500549316 batch PCKh 0.75\n",
      "Trained batch 174 batch loss 0.524212122 batch mAP 0.507782 batch PCKh 0.125\n",
      "Trained batch 175 batch loss 0.595253944 batch mAP 0.49798584 batch PCKh 0.25\n",
      "Trained batch 176 batch loss 0.60557282 batch mAP 0.543487549 batch PCKh 0.4375\n",
      "Trained batch 177 batch loss 0.579402328 batch mAP 0.484680176 batch PCKh 0.4375\n",
      "Trained batch 178 batch loss 0.549209714 batch mAP 0.4894104 batch PCKh 0.5625\n",
      "Trained batch 179 batch loss 0.577699184 batch mAP 0.541595459 batch PCKh 0.125\n",
      "Trained batch 180 batch loss 0.503377616 batch mAP 0.555633545 batch PCKh 0.125\n",
      "Trained batch 181 batch loss 0.461283237 batch mAP 0.591064453 batch PCKh 0.3125\n",
      "Trained batch 182 batch loss 0.520971417 batch mAP 0.537689209 batch PCKh 0.625\n",
      "Trained batch 183 batch loss 0.521486163 batch mAP 0.549041748 batch PCKh 0.1875\n",
      "Trained batch 184 batch loss 0.502517819 batch mAP 0.59085083 batch PCKh 0.4375\n",
      "Trained batch 185 batch loss 0.515854836 batch mAP 0.607849121 batch PCKh 0.375\n",
      "Trained batch 186 batch loss 0.531958222 batch mAP 0.606628418 batch PCKh 0.125\n",
      "Trained batch 187 batch loss 0.558995426 batch mAP 0.603729248 batch PCKh 0.6875\n",
      "Trained batch 188 batch loss 0.434855878 batch mAP 0.640167236 batch PCKh 0.625\n",
      "Trained batch 189 batch loss 0.504272521 batch mAP 0.629058838 batch PCKh 0.75\n",
      "Trained batch 190 batch loss 0.549426794 batch mAP 0.597045898 batch PCKh 0.3125\n",
      "Trained batch 191 batch loss 0.578003764 batch mAP 0.609863281 batch PCKh 0.375\n",
      "Trained batch 192 batch loss 0.499441952 batch mAP 0.588317871 batch PCKh 0.1875\n",
      "Trained batch 193 batch loss 0.535266519 batch mAP 0.587005615 batch PCKh 0\n",
      "Trained batch 194 batch loss 0.564253688 batch mAP 0.56439209 batch PCKh 0.8125\n",
      "Trained batch 195 batch loss 0.580733299 batch mAP 0.579345703 batch PCKh 0.5625\n",
      "Trained batch 196 batch loss 0.515352547 batch mAP 0.624481201 batch PCKh 0\n",
      "Trained batch 197 batch loss 0.489583313 batch mAP 0.564941406 batch PCKh 0.4375\n",
      "Trained batch 198 batch loss 0.549541533 batch mAP 0.533538818 batch PCKh 0\n",
      "Trained batch 199 batch loss 0.565502584 batch mAP 0.5284729 batch PCKh 0.4375\n",
      "Trained batch 200 batch loss 0.566295683 batch mAP 0.538513184 batch PCKh 0.0625\n",
      "Trained batch 201 batch loss 0.549043059 batch mAP 0.591918945 batch PCKh 0.875\n",
      "Trained batch 202 batch loss 0.558309436 batch mAP 0.561279297 batch PCKh 0.6875\n",
      "Trained batch 203 batch loss 0.618159413 batch mAP 0.556365967 batch PCKh 0.1875\n",
      "Trained batch 204 batch loss 0.568756163 batch mAP 0.567504883 batch PCKh 0\n",
      "Trained batch 205 batch loss 0.678195953 batch mAP 0.47946167 batch PCKh 0.625\n",
      "Trained batch 206 batch loss 0.680305302 batch mAP 0.49105835 batch PCKh 0.1875\n",
      "Trained batch 207 batch loss 0.594476283 batch mAP 0.495391846 batch PCKh 0.1875\n",
      "Trained batch 208 batch loss 0.54371351 batch mAP 0.486236572 batch PCKh 0.3125\n",
      "Trained batch 209 batch loss 0.552918732 batch mAP 0.441619873 batch PCKh 0.25\n",
      "Trained batch 210 batch loss 0.585913479 batch mAP 0.438446045 batch PCKh 0.5625\n",
      "Trained batch 211 batch loss 0.56531179 batch mAP 0.458709717 batch PCKh 0.375\n",
      "Trained batch 212 batch loss 0.550706625 batch mAP 0.450500488 batch PCKh 0.625\n",
      "Trained batch 213 batch loss 0.567355 batch mAP 0.488830566 batch PCKh 0.3125\n",
      "Trained batch 214 batch loss 0.586875677 batch mAP 0.503997803 batch PCKh 0.4375\n",
      "Trained batch 215 batch loss 0.540724516 batch mAP 0.507324219 batch PCKh 0.375\n",
      "Trained batch 216 batch loss 0.580089 batch mAP 0.5184021 batch PCKh 0.4375\n",
      "Trained batch 217 batch loss 0.489201 batch mAP 0.523925781 batch PCKh 0.5625\n",
      "Trained batch 218 batch loss 0.574668884 batch mAP 0.564453125 batch PCKh 0.3125\n",
      "Trained batch 219 batch loss 0.56289351 batch mAP 0.548034668 batch PCKh 0.375\n",
      "Trained batch 220 batch loss 0.530784607 batch mAP 0.583984375 batch PCKh 0.6875\n",
      "Trained batch 221 batch loss 0.574285507 batch mAP 0.564880371 batch PCKh 0.625\n",
      "Trained batch 222 batch loss 0.581430912 batch mAP 0.574554443 batch PCKh 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 223 batch loss 0.607185364 batch mAP 0.53793335 batch PCKh 0.75\n",
      "Trained batch 224 batch loss 0.530608475 batch mAP 0.5078125 batch PCKh 0\n",
      "Trained batch 225 batch loss 0.458400488 batch mAP 0.501281738 batch PCKh 0.375\n",
      "Trained batch 226 batch loss 0.461381733 batch mAP 0.544830322 batch PCKh 0.25\n",
      "Trained batch 227 batch loss 0.584166646 batch mAP 0.483062744 batch PCKh 0.6875\n",
      "Trained batch 228 batch loss 0.543072224 batch mAP 0.515655518 batch PCKh 0.75\n",
      "Trained batch 229 batch loss 0.595481753 batch mAP 0.395935059 batch PCKh 0.625\n",
      "Trained batch 230 batch loss 0.523425877 batch mAP 0.507385254 batch PCKh 0.1875\n",
      "Trained batch 231 batch loss 0.628366888 batch mAP 0.493255615 batch PCKh 0.5625\n",
      "Trained batch 232 batch loss 0.617189586 batch mAP 0.513671875 batch PCKh 0.6875\n",
      "Trained batch 233 batch loss 0.6285761 batch mAP 0.539703369 batch PCKh 0.6875\n",
      "Trained batch 234 batch loss 0.574175954 batch mAP 0.557647705 batch PCKh 0.5\n",
      "Trained batch 235 batch loss 0.591042876 batch mAP 0.535949707 batch PCKh 0.5\n",
      "Trained batch 236 batch loss 0.607192218 batch mAP 0.558136 batch PCKh 0.5625\n",
      "Trained batch 237 batch loss 0.487260669 batch mAP 0.563385 batch PCKh 0.8125\n",
      "Trained batch 238 batch loss 0.535362959 batch mAP 0.536224365 batch PCKh 0.375\n",
      "Trained batch 239 batch loss 0.593561053 batch mAP 0.501739502 batch PCKh 0.3125\n",
      "Trained batch 240 batch loss 0.57272613 batch mAP 0.492614746 batch PCKh 0.25\n",
      "Trained batch 241 batch loss 0.630493581 batch mAP 0.49230957 batch PCKh 0.1875\n",
      "Trained batch 242 batch loss 0.576761484 batch mAP 0.467376709 batch PCKh 0.75\n",
      "Trained batch 243 batch loss 0.610375881 batch mAP 0.478240967 batch PCKh 0.8125\n",
      "Trained batch 244 batch loss 0.593961358 batch mAP 0.505554199 batch PCKh 0.75\n",
      "Trained batch 245 batch loss 0.616940379 batch mAP 0.47744751 batch PCKh 0.25\n",
      "Trained batch 246 batch loss 0.571743727 batch mAP 0.482635498 batch PCKh 0.4375\n",
      "Trained batch 247 batch loss 0.599493384 batch mAP 0.4581604 batch PCKh 0.625\n",
      "Trained batch 248 batch loss 0.636304796 batch mAP 0.467224121 batch PCKh 0.8125\n",
      "Trained batch 249 batch loss 0.54488951 batch mAP 0.486358643 batch PCKh 0.375\n",
      "Trained batch 250 batch loss 0.565776765 batch mAP 0.478851318 batch PCKh 0\n",
      "Trained batch 251 batch loss 0.52662009 batch mAP 0.490234375 batch PCKh 0.5\n",
      "Trained batch 252 batch loss 0.601985 batch mAP 0.488372803 batch PCKh 0.4375\n",
      "Trained batch 253 batch loss 0.505420089 batch mAP 0.466491699 batch PCKh 0.3125\n",
      "Trained batch 254 batch loss 0.511657894 batch mAP 0.569671631 batch PCKh 0.4375\n",
      "Trained batch 255 batch loss 0.645222783 batch mAP 0.406036377 batch PCKh 0.125\n",
      "Trained batch 256 batch loss 0.622615278 batch mAP 0.474243164 batch PCKh 0.75\n",
      "Trained batch 257 batch loss 0.579348862 batch mAP 0.443145752 batch PCKh 0.75\n",
      "Trained batch 258 batch loss 0.570655942 batch mAP 0.452392578 batch PCKh 0.4375\n",
      "Trained batch 259 batch loss 0.57404 batch mAP 0.438903809 batch PCKh 0.5\n",
      "Trained batch 260 batch loss 0.565750837 batch mAP 0.458587646 batch PCKh 0.625\n",
      "Trained batch 261 batch loss 0.563887477 batch mAP 0.452484131 batch PCKh 0.375\n",
      "Trained batch 262 batch loss 0.548945665 batch mAP 0.512634277 batch PCKh 0.5625\n",
      "Trained batch 263 batch loss 0.599038601 batch mAP 0.568328857 batch PCKh 0.75\n",
      "Trained batch 264 batch loss 0.597453415 batch mAP 0.503540039 batch PCKh 0.4375\n",
      "Trained batch 265 batch loss 0.561840117 batch mAP 0.572906494 batch PCKh 0.3125\n",
      "Trained batch 266 batch loss 0.598597288 batch mAP 0.571105957 batch PCKh 0.4375\n",
      "Trained batch 267 batch loss 0.477199972 batch mAP 0.673492432 batch PCKh 0.5625\n",
      "Trained batch 268 batch loss 0.469259948 batch mAP 0.647247314 batch PCKh 0.25\n",
      "Trained batch 269 batch loss 0.505444169 batch mAP 0.58291626 batch PCKh 0.4375\n",
      "Trained batch 270 batch loss 0.44370842 batch mAP 0.629425049 batch PCKh 0.75\n",
      "Trained batch 271 batch loss 0.457679212 batch mAP 0.645050049 batch PCKh 0.5\n",
      "Trained batch 272 batch loss 0.488616 batch mAP 0.62677 batch PCKh 0.5625\n",
      "Trained batch 273 batch loss 0.49799332 batch mAP 0.566772461 batch PCKh 0.3125\n",
      "Trained batch 274 batch loss 0.563288033 batch mAP 0.575775146 batch PCKh 0.25\n",
      "Trained batch 275 batch loss 0.65791297 batch mAP 0.514556885 batch PCKh 0.1875\n",
      "Trained batch 276 batch loss 0.605461776 batch mAP 0.550323486 batch PCKh 0.75\n",
      "Trained batch 277 batch loss 0.596369445 batch mAP 0.533782959 batch PCKh 0.1875\n",
      "Trained batch 278 batch loss 0.642293215 batch mAP 0.51953125 batch PCKh 0.8125\n",
      "Trained batch 279 batch loss 0.656558394 batch mAP 0.431427 batch PCKh 0.0625\n",
      "Trained batch 280 batch loss 0.668853879 batch mAP 0.517242432 batch PCKh 0.125\n",
      "Trained batch 281 batch loss 0.701908767 batch mAP 0.483978271 batch PCKh 0.1875\n",
      "Trained batch 282 batch loss 0.601863801 batch mAP 0.48223877 batch PCKh 0.3125\n",
      "Trained batch 283 batch loss 0.588172317 batch mAP 0.458099365 batch PCKh 0.6875\n",
      "Trained batch 284 batch loss 0.606144369 batch mAP 0.433502197 batch PCKh 0.3125\n",
      "Trained batch 285 batch loss 0.624678254 batch mAP 0.456390381 batch PCKh 0.375\n",
      "Trained batch 286 batch loss 0.608684182 batch mAP 0.463378906 batch PCKh 0.375\n",
      "Trained batch 287 batch loss 0.536803 batch mAP 0.508117676 batch PCKh 0.4375\n",
      "Trained batch 288 batch loss 0.457997203 batch mAP 0.511108398 batch PCKh 0.625\n",
      "Trained batch 289 batch loss 0.487370372 batch mAP 0.556396484 batch PCKh 0.6875\n",
      "Trained batch 290 batch loss 0.59303242 batch mAP 0.533233643 batch PCKh 0.5\n",
      "Trained batch 291 batch loss 0.447527677 batch mAP 0.544067383 batch PCKh 0.0625\n",
      "Trained batch 292 batch loss 0.579667091 batch mAP 0.573883057 batch PCKh 0.375\n",
      "Trained batch 293 batch loss 0.652260959 batch mAP 0.513916 batch PCKh 0.0625\n",
      "Trained batch 294 batch loss 0.564370513 batch mAP 0.575378418 batch PCKh 0.5625\n",
      "Trained batch 295 batch loss 0.541091263 batch mAP 0.586792 batch PCKh 0.4375\n",
      "Trained batch 296 batch loss 0.559699357 batch mAP 0.595855713 batch PCKh 0.5625\n",
      "Trained batch 297 batch loss 0.601276577 batch mAP 0.639038086 batch PCKh 0.3125\n",
      "Trained batch 298 batch loss 0.578884423 batch mAP 0.614715576 batch PCKh 0.875\n",
      "Trained batch 299 batch loss 0.562981725 batch mAP 0.639221191 batch PCKh 0.5\n",
      "Trained batch 300 batch loss 0.588341236 batch mAP 0.625335693 batch PCKh 0.8125\n",
      "Trained batch 301 batch loss 0.549426496 batch mAP 0.598053 batch PCKh 0.875\n",
      "Trained batch 302 batch loss 0.619308352 batch mAP 0.614318848 batch PCKh 0.375\n",
      "Trained batch 303 batch loss 0.56316787 batch mAP 0.585723877 batch PCKh 0.75\n",
      "Trained batch 304 batch loss 0.574986577 batch mAP 0.614044189 batch PCKh 0.5\n",
      "Trained batch 305 batch loss 0.555264056 batch mAP 0.599731445 batch PCKh 0.4375\n",
      "Trained batch 306 batch loss 0.582619905 batch mAP 0.60269165 batch PCKh 0.25\n",
      "Trained batch 307 batch loss 0.552212596 batch mAP 0.630645752 batch PCKh 0.3125\n",
      "Trained batch 308 batch loss 0.605306089 batch mAP 0.578582764 batch PCKh 0.3125\n",
      "Trained batch 309 batch loss 0.593700886 batch mAP 0.619384766 batch PCKh 0.3125\n",
      "Trained batch 310 batch loss 0.633605778 batch mAP 0.622131348 batch PCKh 0.25\n",
      "Trained batch 311 batch loss 0.52852273 batch mAP 0.670349121 batch PCKh 0.3125\n",
      "Trained batch 312 batch loss 0.535326838 batch mAP 0.601654053 batch PCKh 0.25\n",
      "Trained batch 313 batch loss 0.496055 batch mAP 0.653625488 batch PCKh 0.4375\n",
      "Trained batch 314 batch loss 0.581994832 batch mAP 0.60446167 batch PCKh 0.3125\n",
      "Trained batch 315 batch loss 0.592000782 batch mAP 0.58215332 batch PCKh 0.25\n",
      "Trained batch 316 batch loss 0.568302453 batch mAP 0.578704834 batch PCKh 0.5625\n",
      "Trained batch 317 batch loss 0.54734087 batch mAP 0.604217529 batch PCKh 0.4375\n",
      "Trained batch 318 batch loss 0.528327584 batch mAP 0.607177734 batch PCKh 0.4375\n",
      "Trained batch 319 batch loss 0.528248429 batch mAP 0.570587158 batch PCKh 0.5625\n",
      "Trained batch 320 batch loss 0.571301222 batch mAP 0.520355225 batch PCKh 0.4375\n",
      "Trained batch 321 batch loss 0.537971735 batch mAP 0.542999268 batch PCKh 0.625\n",
      "Trained batch 322 batch loss 0.53578943 batch mAP 0.527282715 batch PCKh 0.75\n",
      "Trained batch 323 batch loss 0.574024558 batch mAP 0.522186279 batch PCKh 0.75\n",
      "Trained batch 324 batch loss 0.513552487 batch mAP 0.532226562 batch PCKh 0.75\n",
      "Trained batch 325 batch loss 0.516881 batch mAP 0.550384521 batch PCKh 0.375\n",
      "Trained batch 326 batch loss 0.544995666 batch mAP 0.568603516 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 327 batch loss 0.553046525 batch mAP 0.544830322 batch PCKh 0.5625\n",
      "Trained batch 328 batch loss 0.561761141 batch mAP 0.507537842 batch PCKh 0.625\n",
      "Trained batch 329 batch loss 0.51054579 batch mAP 0.487487793 batch PCKh 0.4375\n",
      "Trained batch 330 batch loss 0.524977505 batch mAP 0.552154541 batch PCKh 0.875\n",
      "Trained batch 331 batch loss 0.494448423 batch mAP 0.533935547 batch PCKh 0.6875\n",
      "Trained batch 332 batch loss 0.583467066 batch mAP 0.525695801 batch PCKh 0.6875\n",
      "Trained batch 333 batch loss 0.558242917 batch mAP 0.524414062 batch PCKh 0.6875\n",
      "Trained batch 334 batch loss 0.581076324 batch mAP 0.50378418 batch PCKh 0.25\n",
      "Trained batch 335 batch loss 0.491222054 batch mAP 0.527709961 batch PCKh 0.625\n",
      "Trained batch 336 batch loss 0.599410295 batch mAP 0.523407 batch PCKh 0.1875\n",
      "Trained batch 337 batch loss 0.562776625 batch mAP 0.522583 batch PCKh 0.1875\n",
      "Trained batch 338 batch loss 0.463517904 batch mAP 0.568786621 batch PCKh 0.5\n",
      "Trained batch 339 batch loss 0.550119579 batch mAP 0.584259033 batch PCKh 0.25\n",
      "Trained batch 340 batch loss 0.555713832 batch mAP 0.581329346 batch PCKh 0.3125\n",
      "Trained batch 341 batch loss 0.53975296 batch mAP 0.553131104 batch PCKh 0.3125\n",
      "Trained batch 342 batch loss 0.567417 batch mAP 0.574035645 batch PCKh 0.1875\n",
      "Trained batch 343 batch loss 0.52675724 batch mAP 0.558349609 batch PCKh 0.375\n",
      "Trained batch 344 batch loss 0.553060055 batch mAP 0.473175049 batch PCKh 0.25\n",
      "Trained batch 345 batch loss 0.636205554 batch mAP 0.44152832 batch PCKh 0.125\n",
      "Trained batch 346 batch loss 0.677782774 batch mAP 0.463134766 batch PCKh 0.375\n",
      "Trained batch 347 batch loss 0.595144451 batch mAP 0.489624023 batch PCKh 0.75\n",
      "Trained batch 348 batch loss 0.630213499 batch mAP 0.487701416 batch PCKh 0.125\n",
      "Trained batch 349 batch loss 0.463979274 batch mAP 0.529937744 batch PCKh 0.1875\n",
      "Trained batch 350 batch loss 0.63009578 batch mAP 0.524871826 batch PCKh 0.0625\n",
      "Trained batch 351 batch loss 0.624341 batch mAP 0.520751953 batch PCKh 0.3125\n",
      "Trained batch 352 batch loss 0.480078161 batch mAP 0.499023438 batch PCKh 0.1875\n",
      "Trained batch 353 batch loss 0.581466496 batch mAP 0.498779297 batch PCKh 0.5625\n",
      "Trained batch 354 batch loss 0.594810605 batch mAP 0.475006104 batch PCKh 0.5\n",
      "Trained batch 355 batch loss 0.605278492 batch mAP 0.484069824 batch PCKh 0.1875\n",
      "Trained batch 356 batch loss 0.628048658 batch mAP 0.505615234 batch PCKh 0.6875\n",
      "Trained batch 357 batch loss 0.576396823 batch mAP 0.512542725 batch PCKh 0.6875\n",
      "Trained batch 358 batch loss 0.584284306 batch mAP 0.522369385 batch PCKh 0.75\n",
      "Trained batch 359 batch loss 0.683695793 batch mAP 0.499542236 batch PCKh 0.625\n",
      "Trained batch 360 batch loss 0.663174748 batch mAP 0.473938 batch PCKh 0.3125\n",
      "Trained batch 361 batch loss 0.648015618 batch mAP 0.448974609 batch PCKh 0.125\n",
      "Trained batch 362 batch loss 0.651966 batch mAP 0.466003418 batch PCKh 0.5625\n",
      "Trained batch 363 batch loss 0.559776068 batch mAP 0.492034912 batch PCKh 0.25\n",
      "Trained batch 364 batch loss 0.558521628 batch mAP 0.582519531 batch PCKh 0.5625\n",
      "Trained batch 365 batch loss 0.531630754 batch mAP 0.530883789 batch PCKh 0.0625\n",
      "Trained batch 366 batch loss 0.554412365 batch mAP 0.529754639 batch PCKh 0.375\n",
      "Trained batch 367 batch loss 0.578209221 batch mAP 0.461700439 batch PCKh 0.1875\n",
      "Trained batch 368 batch loss 0.635439038 batch mAP 0.44442749 batch PCKh 0.6875\n",
      "Trained batch 369 batch loss 0.611569822 batch mAP 0.488983154 batch PCKh 0.125\n",
      "Trained batch 370 batch loss 0.520631194 batch mAP 0.485687256 batch PCKh 0.375\n",
      "Trained batch 371 batch loss 0.574928224 batch mAP 0.446685791 batch PCKh 0.6875\n",
      "Trained batch 372 batch loss 0.580332518 batch mAP 0.479125977 batch PCKh 0.4375\n",
      "Trained batch 373 batch loss 0.591026068 batch mAP 0.430633545 batch PCKh 0.5625\n",
      "Trained batch 374 batch loss 0.629848301 batch mAP 0.502685547 batch PCKh 0.3125\n",
      "Trained batch 375 batch loss 0.573731899 batch mAP 0.533874512 batch PCKh 0.625\n",
      "Trained batch 376 batch loss 0.554414928 batch mAP 0.503936768 batch PCKh 0.75\n",
      "Trained batch 377 batch loss 0.59000504 batch mAP 0.484130859 batch PCKh 0.6875\n",
      "Trained batch 378 batch loss 0.686026514 batch mAP 0.448181152 batch PCKh 0.375\n",
      "Trained batch 379 batch loss 0.617918 batch mAP 0.513122559 batch PCKh 0.3125\n",
      "Trained batch 380 batch loss 0.65496707 batch mAP 0.478759766 batch PCKh 0.3125\n",
      "Trained batch 381 batch loss 0.670274317 batch mAP 0.452178955 batch PCKh 0.8125\n",
      "Trained batch 382 batch loss 0.58695972 batch mAP 0.482269287 batch PCKh 0.375\n",
      "Trained batch 383 batch loss 0.56722641 batch mAP 0.500213623 batch PCKh 0.8125\n",
      "Trained batch 384 batch loss 0.579902172 batch mAP 0.444915771 batch PCKh 0.25\n",
      "Trained batch 385 batch loss 0.696593583 batch mAP 0.471954346 batch PCKh 0.0625\n",
      "Trained batch 386 batch loss 0.643210351 batch mAP 0.454193115 batch PCKh 0.5\n",
      "Trained batch 387 batch loss 0.554590583 batch mAP 0.429870605 batch PCKh 0.5\n",
      "Trained batch 388 batch loss 0.694411755 batch mAP 0.465881348 batch PCKh 0.125\n",
      "Trained batch 389 batch loss 0.525694609 batch mAP 0.409667969 batch PCKh 0.5625\n",
      "Trained batch 390 batch loss 0.576545417 batch mAP 0.436889648 batch PCKh 0.625\n",
      "Trained batch 391 batch loss 0.541018903 batch mAP 0.452178955 batch PCKh 0.1875\n",
      "Trained batch 392 batch loss 0.516498089 batch mAP 0.458435059 batch PCKh 0.75\n",
      "Trained batch 393 batch loss 0.559045 batch mAP 0.491271973 batch PCKh 0.8125\n",
      "Trained batch 394 batch loss 0.523734331 batch mAP 0.493133545 batch PCKh 0.4375\n",
      "Trained batch 395 batch loss 0.670690715 batch mAP 0.465179443 batch PCKh 0.8125\n",
      "Trained batch 396 batch loss 0.599895835 batch mAP 0.484741211 batch PCKh 0.75\n",
      "Trained batch 397 batch loss 0.606898606 batch mAP 0.507263184 batch PCKh 0.75\n",
      "Trained batch 398 batch loss 0.538268685 batch mAP 0.604248047 batch PCKh 0.25\n",
      "Trained batch 399 batch loss 0.677808523 batch mAP 0.511779785 batch PCKh 0.125\n",
      "Trained batch 400 batch loss 0.638095915 batch mAP 0.538208 batch PCKh 0.5625\n",
      "Trained batch 401 batch loss 0.663506448 batch mAP 0.543029785 batch PCKh 0.1875\n",
      "Trained batch 402 batch loss 0.639735 batch mAP 0.521850586 batch PCKh 0.3125\n",
      "Trained batch 403 batch loss 0.656451702 batch mAP 0.486999512 batch PCKh 0.5625\n",
      "Trained batch 404 batch loss 0.654178858 batch mAP 0.496887207 batch PCKh 0.6875\n",
      "Trained batch 405 batch loss 0.714610159 batch mAP 0.46697998 batch PCKh 0\n",
      "Trained batch 406 batch loss 0.579730213 batch mAP 0.458435059 batch PCKh 0\n",
      "Trained batch 407 batch loss 0.473737657 batch mAP 0.485351562 batch PCKh 0.3125\n",
      "Trained batch 408 batch loss 0.483546823 batch mAP 0.53326416 batch PCKh 0.3125\n",
      "Trained batch 409 batch loss 0.486909 batch mAP 0.505432129 batch PCKh 0.125\n",
      "Trained batch 410 batch loss 0.559097826 batch mAP 0.542938232 batch PCKh 0.5\n",
      "Trained batch 411 batch loss 0.625769615 batch mAP 0.558227539 batch PCKh 0.5625\n",
      "Trained batch 412 batch loss 0.512498736 batch mAP 0.5987854 batch PCKh 0.375\n",
      "Trained batch 413 batch loss 0.53791821 batch mAP 0.587402344 batch PCKh 0.25\n",
      "Trained batch 414 batch loss 0.435838699 batch mAP 0.587646484 batch PCKh 0\n",
      "Trained batch 415 batch loss 0.479654372 batch mAP 0.620819092 batch PCKh 0.375\n",
      "Trained batch 416 batch loss 0.504016459 batch mAP 0.647827148 batch PCKh 0.375\n",
      "Trained batch 417 batch loss 0.4533903 batch mAP 0.65045166 batch PCKh 0.6875\n",
      "Trained batch 418 batch loss 0.480999738 batch mAP 0.664520264 batch PCKh 0.25\n",
      "Trained batch 419 batch loss 0.508749962 batch mAP 0.584075928 batch PCKh 0.4375\n",
      "Trained batch 420 batch loss 0.474105358 batch mAP 0.633209229 batch PCKh 0.375\n",
      "Trained batch 421 batch loss 0.43021071 batch mAP 0.640411377 batch PCKh 0.1875\n",
      "Trained batch 422 batch loss 0.463972688 batch mAP 0.636566162 batch PCKh 0.25\n",
      "Trained batch 423 batch loss 0.464052558 batch mAP 0.592224121 batch PCKh 0.625\n",
      "Trained batch 424 batch loss 0.496028 batch mAP 0.621551514 batch PCKh 0.3125\n",
      "Trained batch 425 batch loss 0.443276018 batch mAP 0.591369629 batch PCKh 0.25\n",
      "Trained batch 426 batch loss 0.564538 batch mAP 0.520355225 batch PCKh 0.1875\n",
      "Trained batch 427 batch loss 0.536726713 batch mAP 0.521453857 batch PCKh 0.3125\n",
      "Trained batch 428 batch loss 0.634233832 batch mAP 0.512878418 batch PCKh 0.4375\n",
      "Trained batch 429 batch loss 0.556968033 batch mAP 0.561187744 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 430 batch loss 0.427962184 batch mAP 0.630584717 batch PCKh 0.3125\n",
      "Trained batch 431 batch loss 0.432648659 batch mAP 0.626312256 batch PCKh 0.3125\n",
      "Trained batch 432 batch loss 0.539660335 batch mAP 0.568878174 batch PCKh 0.4375\n",
      "Trained batch 433 batch loss 0.44383049 batch mAP 0.524627686 batch PCKh 0.4375\n",
      "Trained batch 434 batch loss 0.506069064 batch mAP 0.586456299 batch PCKh 0.1875\n",
      "Trained batch 435 batch loss 0.44115904 batch mAP 0.527313232 batch PCKh 0.5625\n",
      "Trained batch 436 batch loss 0.417062879 batch mAP 0.589324951 batch PCKh 0.1875\n",
      "Trained batch 437 batch loss 0.501673222 batch mAP 0.627471924 batch PCKh 0.5\n",
      "Trained batch 438 batch loss 0.532286704 batch mAP 0.622589111 batch PCKh 0.4375\n",
      "Trained batch 439 batch loss 0.776293695 batch mAP 0.532928467 batch PCKh 0.0625\n",
      "Trained batch 440 batch loss 0.662970901 batch mAP 0.56539917 batch PCKh 0.4375\n",
      "Trained batch 441 batch loss 0.612793505 batch mAP 0.529968262 batch PCKh 0.5\n",
      "Trained batch 442 batch loss 0.60357219 batch mAP 0.569763184 batch PCKh 0.625\n",
      "Trained batch 443 batch loss 0.589639187 batch mAP 0.559631348 batch PCKh 0.375\n",
      "Trained batch 444 batch loss 0.626097441 batch mAP 0.550354 batch PCKh 0.25\n",
      "Trained batch 445 batch loss 0.616308 batch mAP 0.523071289 batch PCKh 0.3125\n",
      "Trained batch 446 batch loss 0.614884138 batch mAP 0.540588379 batch PCKh 0.25\n",
      "Trained batch 447 batch loss 0.575045586 batch mAP 0.491119385 batch PCKh 0.8125\n",
      "Trained batch 448 batch loss 0.563942552 batch mAP 0.493438721 batch PCKh 0.3125\n",
      "Trained batch 449 batch loss 0.518944561 batch mAP 0.5284729 batch PCKh 0.75\n",
      "Trained batch 450 batch loss 0.490244448 batch mAP 0.488311768 batch PCKh 0.8125\n",
      "Trained batch 451 batch loss 0.477074772 batch mAP 0.520477295 batch PCKh 0.8125\n",
      "Trained batch 452 batch loss 0.526067853 batch mAP 0.526794434 batch PCKh 0.6875\n",
      "Trained batch 453 batch loss 0.55726254 batch mAP 0.544555664 batch PCKh 0.875\n",
      "Trained batch 454 batch loss 0.521757543 batch mAP 0.566070557 batch PCKh 0.8125\n",
      "Trained batch 455 batch loss 0.629825771 batch mAP 0.565216064 batch PCKh 0.1875\n",
      "Trained batch 456 batch loss 0.544403255 batch mAP 0.563415527 batch PCKh 0.625\n",
      "Trained batch 457 batch loss 0.527481496 batch mAP 0.565063477 batch PCKh 0.625\n",
      "Trained batch 458 batch loss 0.626878381 batch mAP 0.523742676 batch PCKh 0.25\n",
      "Trained batch 459 batch loss 0.611398399 batch mAP 0.511810303 batch PCKh 0.5\n",
      "Trained batch 460 batch loss 0.696644425 batch mAP 0.421966553 batch PCKh 0.5\n",
      "Trained batch 461 batch loss 0.68614763 batch mAP 0.535675049 batch PCKh 0.375\n",
      "Trained batch 462 batch loss 0.565570116 batch mAP 0.543731689 batch PCKh 0.5\n",
      "Trained batch 463 batch loss 0.598006904 batch mAP 0.450256348 batch PCKh 0.75\n",
      "Trained batch 464 batch loss 0.59683013 batch mAP 0.499237061 batch PCKh 0.5625\n",
      "Trained batch 465 batch loss 0.584104359 batch mAP 0.520294189 batch PCKh 0.3125\n",
      "Trained batch 466 batch loss 0.626488388 batch mAP 0.46282959 batch PCKh 0.625\n",
      "Trained batch 467 batch loss 0.652953 batch mAP 0.416564941 batch PCKh 0.875\n",
      "Trained batch 468 batch loss 0.628751755 batch mAP 0.423034668 batch PCKh 0.4375\n",
      "Trained batch 469 batch loss 0.681818724 batch mAP 0.420684814 batch PCKh 0.3125\n",
      "Trained batch 470 batch loss 0.658551097 batch mAP 0.473602295 batch PCKh 0.625\n",
      "Trained batch 471 batch loss 0.60406816 batch mAP 0.455566406 batch PCKh 0.875\n",
      "Trained batch 472 batch loss 0.604480207 batch mAP 0.424621582 batch PCKh 0.5625\n",
      "Trained batch 473 batch loss 0.661541 batch mAP 0.413513184 batch PCKh 0.75\n",
      "Trained batch 474 batch loss 0.692231655 batch mAP 0.453460693 batch PCKh 0.3125\n",
      "Trained batch 475 batch loss 0.609793663 batch mAP 0.41519165 batch PCKh 0.5\n",
      "Trained batch 476 batch loss 0.628037035 batch mAP 0.422821045 batch PCKh 0.3125\n",
      "Trained batch 477 batch loss 0.59427923 batch mAP 0.391174316 batch PCKh 0.375\n",
      "Trained batch 478 batch loss 0.592314422 batch mAP 0.449554443 batch PCKh 0.3125\n",
      "Trained batch 479 batch loss 0.592492104 batch mAP 0.427124023 batch PCKh 0.625\n",
      "Trained batch 480 batch loss 0.618047416 batch mAP 0.479614258 batch PCKh 0.875\n",
      "Trained batch 481 batch loss 0.612784624 batch mAP 0.501861572 batch PCKh 0.8125\n",
      "Trained batch 482 batch loss 0.53875947 batch mAP 0.494476318 batch PCKh 0.375\n",
      "Trained batch 483 batch loss 0.629091084 batch mAP 0.521850586 batch PCKh 0.25\n",
      "Trained batch 484 batch loss 0.627544224 batch mAP 0.528442383 batch PCKh 0.25\n",
      "Trained batch 485 batch loss 0.613539875 batch mAP 0.514709473 batch PCKh 0.375\n",
      "Trained batch 486 batch loss 0.647605658 batch mAP 0.516723633 batch PCKh 0.125\n",
      "Trained batch 487 batch loss 0.655297875 batch mAP 0.43572998 batch PCKh 0.3125\n",
      "Trained batch 488 batch loss 0.658131 batch mAP 0.456298828 batch PCKh 0.6875\n",
      "Trained batch 489 batch loss 0.65246284 batch mAP 0.48223877 batch PCKh 0.4375\n",
      "Trained batch 490 batch loss 0.662800193 batch mAP 0.518127441 batch PCKh 0.625\n",
      "Trained batch 491 batch loss 0.651508212 batch mAP 0.469940186 batch PCKh 0.375\n",
      "Trained batch 492 batch loss 0.589524031 batch mAP 0.521575928 batch PCKh 0.875\n",
      "Trained batch 493 batch loss 0.576078 batch mAP 0.515350342 batch PCKh 0.8125\n",
      "Trained batch 494 batch loss 0.56731832 batch mAP 0.508728 batch PCKh 0.875\n",
      "Trained batch 495 batch loss 0.580077827 batch mAP 0.522094727 batch PCKh 0.75\n",
      "Trained batch 496 batch loss 0.597427487 batch mAP 0.513641357 batch PCKh 0.625\n",
      "Trained batch 497 batch loss 0.560664296 batch mAP 0.556915283 batch PCKh 0.1875\n",
      "Trained batch 498 batch loss 0.55620718 batch mAP 0.507995605 batch PCKh 0.4375\n",
      "Trained batch 499 batch loss 0.629034758 batch mAP 0.558166504 batch PCKh 0.625\n",
      "Trained batch 500 batch loss 0.70106411 batch mAP 0.508026123 batch PCKh 0.1875\n",
      "Trained batch 501 batch loss 0.617581487 batch mAP 0.529571533 batch PCKh 0.125\n",
      "Trained batch 502 batch loss 0.626089036 batch mAP 0.495361328 batch PCKh 0.375\n",
      "Trained batch 503 batch loss 0.732093096 batch mAP 0.475891113 batch PCKh 0.4375\n",
      "Trained batch 504 batch loss 0.742630541 batch mAP 0.429718018 batch PCKh 0.25\n",
      "Trained batch 505 batch loss 0.751364589 batch mAP 0.4503479 batch PCKh 0.0625\n",
      "Trained batch 506 batch loss 0.536473632 batch mAP 0.525360107 batch PCKh 0.4375\n",
      "Trained batch 507 batch loss 0.579236329 batch mAP 0.458221436 batch PCKh 0.3125\n",
      "Trained batch 508 batch loss 0.575340152 batch mAP 0.429718018 batch PCKh 0.1875\n",
      "Trained batch 509 batch loss 0.581597149 batch mAP 0.3621521 batch PCKh 0.125\n",
      "Trained batch 510 batch loss 0.612963498 batch mAP 0.387207031 batch PCKh 0.125\n",
      "Trained batch 511 batch loss 0.672065735 batch mAP 0.332519531 batch PCKh 0.125\n",
      "Trained batch 512 batch loss 0.586915374 batch mAP 0.339630127 batch PCKh 0.3125\n",
      "Trained batch 513 batch loss 0.598033845 batch mAP 0.353912354 batch PCKh 0.3125\n",
      "Trained batch 514 batch loss 0.660486281 batch mAP 0.431243896 batch PCKh 0.1875\n",
      "Trained batch 515 batch loss 0.660859227 batch mAP 0.425262451 batch PCKh 0.1875\n",
      "Trained batch 516 batch loss 0.658651292 batch mAP 0.497924805 batch PCKh 0.0625\n",
      "Trained batch 517 batch loss 0.575895071 batch mAP 0.522979736 batch PCKh 0.3125\n",
      "Trained batch 518 batch loss 0.611579 batch mAP 0.514160156 batch PCKh 0.125\n",
      "Trained batch 519 batch loss 0.525316834 batch mAP 0.516540527 batch PCKh 0.3125\n",
      "Trained batch 520 batch loss 0.469914556 batch mAP 0.478729248 batch PCKh 0.1875\n",
      "Trained batch 521 batch loss 0.593259037 batch mAP 0.491119385 batch PCKh 0.5\n",
      "Trained batch 522 batch loss 0.589571953 batch mAP 0.478881836 batch PCKh 0.3125\n",
      "Trained batch 523 batch loss 0.654699326 batch mAP 0.449859619 batch PCKh 0.5\n",
      "Trained batch 524 batch loss 0.490321785 batch mAP 0.42767334 batch PCKh 0.5625\n",
      "Trained batch 525 batch loss 0.614498615 batch mAP 0.393768311 batch PCKh 0\n",
      "Trained batch 526 batch loss 0.583223403 batch mAP 0.491577148 batch PCKh 0.625\n",
      "Trained batch 527 batch loss 0.619401455 batch mAP 0.491119385 batch PCKh 0\n",
      "Trained batch 528 batch loss 0.67776531 batch mAP 0.443725586 batch PCKh 0.1875\n",
      "Trained batch 529 batch loss 0.567648172 batch mAP 0.522399902 batch PCKh 0.6875\n",
      "Trained batch 530 batch loss 0.63368082 batch mAP 0.478820801 batch PCKh 0.6875\n",
      "Trained batch 531 batch loss 0.516059637 batch mAP 0.431732178 batch PCKh 0.375\n",
      "Trained batch 532 batch loss 0.603336811 batch mAP 0.356201172 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 533 batch loss 0.601910293 batch mAP 0.553222656 batch PCKh 0.8125\n",
      "Trained batch 534 batch loss 0.574746549 batch mAP 0.588409424 batch PCKh 0.1875\n",
      "Trained batch 535 batch loss 0.511075497 batch mAP 0.61239624 batch PCKh 0.875\n",
      "Trained batch 536 batch loss 0.600666523 batch mAP 0.544342041 batch PCKh 0.1875\n",
      "Trained batch 537 batch loss 0.559492 batch mAP 0.576812744 batch PCKh 0.1875\n",
      "Trained batch 538 batch loss 0.523711443 batch mAP 0.664276123 batch PCKh 0.5625\n",
      "Trained batch 539 batch loss 0.502113819 batch mAP 0.629303 batch PCKh 0.4375\n",
      "Trained batch 540 batch loss 0.549825132 batch mAP 0.629943848 batch PCKh 0.375\n",
      "Trained batch 541 batch loss 0.589415491 batch mAP 0.592895508 batch PCKh 0.1875\n",
      "Trained batch 542 batch loss 0.557461858 batch mAP 0.612762451 batch PCKh 0.375\n",
      "Trained batch 543 batch loss 0.635956883 batch mAP 0.542633057 batch PCKh 0.4375\n",
      "Trained batch 544 batch loss 0.586420536 batch mAP 0.530395508 batch PCKh 0.75\n",
      "Trained batch 545 batch loss 0.708980262 batch mAP 0.462280273 batch PCKh 0.125\n",
      "Trained batch 546 batch loss 0.622177601 batch mAP 0.538330078 batch PCKh 0.1875\n",
      "Trained batch 547 batch loss 0.594891787 batch mAP 0.53427124 batch PCKh 0.3125\n",
      "Trained batch 548 batch loss 0.589993358 batch mAP 0.524627686 batch PCKh 0.6875\n",
      "Trained batch 549 batch loss 0.596129775 batch mAP 0.548248291 batch PCKh 0.3125\n",
      "Trained batch 550 batch loss 0.620349586 batch mAP 0.498718262 batch PCKh 0.3125\n",
      "Trained batch 551 batch loss 0.553388417 batch mAP 0.55859375 batch PCKh 0.75\n",
      "Trained batch 552 batch loss 0.64264369 batch mAP 0.504669189 batch PCKh 0.5625\n",
      "Trained batch 553 batch loss 0.610970378 batch mAP 0.514343262 batch PCKh 0.625\n",
      "Trained batch 554 batch loss 0.632612467 batch mAP 0.508667 batch PCKh 0.5625\n",
      "Trained batch 555 batch loss 0.550229311 batch mAP 0.542022705 batch PCKh 0.25\n",
      "Trained batch 556 batch loss 0.605312467 batch mAP 0.554534912 batch PCKh 0.6875\n",
      "Trained batch 557 batch loss 0.517232358 batch mAP 0.592132568 batch PCKh 0.4375\n",
      "Trained batch 558 batch loss 0.561045945 batch mAP 0.590240479 batch PCKh 0.1875\n",
      "Trained batch 559 batch loss 0.576593876 batch mAP 0.54397583 batch PCKh 0.6875\n",
      "Trained batch 560 batch loss 0.569693506 batch mAP 0.547546387 batch PCKh 0.25\n",
      "Trained batch 561 batch loss 0.528505325 batch mAP 0.613586426 batch PCKh 0.375\n",
      "Trained batch 562 batch loss 0.505178392 batch mAP 0.620300293 batch PCKh 0.75\n",
      "Trained batch 563 batch loss 0.618139148 batch mAP 0.532623291 batch PCKh 0.75\n",
      "Trained batch 564 batch loss 0.493698 batch mAP 0.576751709 batch PCKh 0.75\n",
      "Trained batch 565 batch loss 0.580259442 batch mAP 0.553863525 batch PCKh 0.4375\n",
      "Trained batch 566 batch loss 0.51978004 batch mAP 0.549591064 batch PCKh 0.25\n",
      "Trained batch 567 batch loss 0.537510872 batch mAP 0.549743652 batch PCKh 0.5\n",
      "Trained batch 568 batch loss 0.483083397 batch mAP 0.580047607 batch PCKh 0.4375\n",
      "Trained batch 569 batch loss 0.481181085 batch mAP 0.585907 batch PCKh 0.75\n",
      "Trained batch 570 batch loss 0.514590919 batch mAP 0.59185791 batch PCKh 0.3125\n",
      "Trained batch 571 batch loss 0.516014457 batch mAP 0.545379639 batch PCKh 0.1875\n",
      "Trained batch 572 batch loss 0.5235672 batch mAP 0.591827393 batch PCKh 0.1875\n",
      "Trained batch 573 batch loss 0.472312778 batch mAP 0.618255615 batch PCKh 0.4375\n",
      "Trained batch 574 batch loss 0.558920741 batch mAP 0.639434814 batch PCKh 0.3125\n",
      "Trained batch 575 batch loss 0.440466911 batch mAP 0.663635254 batch PCKh 0.375\n",
      "Trained batch 576 batch loss 0.485235304 batch mAP 0.652099609 batch PCKh 0.4375\n",
      "Trained batch 577 batch loss 0.491771311 batch mAP 0.596313477 batch PCKh 0.25\n",
      "Trained batch 578 batch loss 0.548343956 batch mAP 0.587097168 batch PCKh 0\n",
      "Trained batch 579 batch loss 0.481473982 batch mAP 0.580169678 batch PCKh 0.75\n",
      "Trained batch 580 batch loss 0.535766721 batch mAP 0.524963379 batch PCKh 0.375\n",
      "Trained batch 581 batch loss 0.578933954 batch mAP 0.555053711 batch PCKh 0.8125\n",
      "Trained batch 582 batch loss 0.542690635 batch mAP 0.570739746 batch PCKh 0.1875\n",
      "Trained batch 583 batch loss 0.480351269 batch mAP 0.618133545 batch PCKh 0.5625\n",
      "Trained batch 584 batch loss 0.537846863 batch mAP 0.625976562 batch PCKh 0.375\n",
      "Trained batch 585 batch loss 0.518905938 batch mAP 0.603118896 batch PCKh 0.25\n",
      "Trained batch 586 batch loss 0.515995622 batch mAP 0.613769531 batch PCKh 0.375\n",
      "Trained batch 587 batch loss 0.503808379 batch mAP 0.590759277 batch PCKh 0.4375\n",
      "Trained batch 588 batch loss 0.527144313 batch mAP 0.623535156 batch PCKh 0.5625\n",
      "Trained batch 589 batch loss 0.506251693 batch mAP 0.516540527 batch PCKh 0.0625\n",
      "Trained batch 590 batch loss 0.487510443 batch mAP 0.576263428 batch PCKh 0.75\n",
      "Trained batch 591 batch loss 0.633572757 batch mAP 0.480896 batch PCKh 0.125\n",
      "Trained batch 592 batch loss 0.69093132 batch mAP 0.469696045 batch PCKh 0.6875\n",
      "Trained batch 593 batch loss 0.754024 batch mAP 0.458953857 batch PCKh 0\n",
      "Trained batch 594 batch loss 0.632351875 batch mAP 0.487609863 batch PCKh 0.125\n",
      "Trained batch 595 batch loss 0.554784656 batch mAP 0.53314209 batch PCKh 0.25\n",
      "Trained batch 596 batch loss 0.545092583 batch mAP 0.477294922 batch PCKh 0.625\n",
      "Trained batch 597 batch loss 0.633384824 batch mAP 0.479553223 batch PCKh 0.375\n",
      "Trained batch 598 batch loss 0.677447617 batch mAP 0.438049316 batch PCKh 0.1875\n",
      "Trained batch 599 batch loss 0.597960234 batch mAP 0.441253662 batch PCKh 0\n",
      "Trained batch 600 batch loss 0.655352831 batch mAP 0.471130371 batch PCKh 0\n",
      "Trained batch 601 batch loss 0.616284728 batch mAP 0.46987915 batch PCKh 0.1875\n",
      "Trained batch 602 batch loss 0.531221688 batch mAP 0.477935791 batch PCKh 0.3125\n",
      "Trained batch 603 batch loss 0.584308863 batch mAP 0.486083984 batch PCKh 0.4375\n",
      "Trained batch 604 batch loss 0.663697064 batch mAP 0.532409668 batch PCKh 0.25\n",
      "Trained batch 605 batch loss 0.511491954 batch mAP 0.582763672 batch PCKh 0.75\n",
      "Trained batch 606 batch loss 0.531765342 batch mAP 0.570068359 batch PCKh 0.75\n",
      "Trained batch 607 batch loss 0.529628038 batch mAP 0.464813232 batch PCKh 0.75\n",
      "Trained batch 608 batch loss 0.549223185 batch mAP 0.490692139 batch PCKh 0.75\n",
      "Trained batch 609 batch loss 0.559958935 batch mAP 0.479034424 batch PCKh 0.5625\n",
      "Trained batch 610 batch loss 0.682383299 batch mAP 0.414337158 batch PCKh 0.6875\n",
      "Trained batch 611 batch loss 0.545187294 batch mAP 0.517028809 batch PCKh 0.6875\n",
      "Trained batch 612 batch loss 0.502376795 batch mAP 0.495300293 batch PCKh 0.625\n",
      "Trained batch 613 batch loss 0.489909649 batch mAP 0.399078369 batch PCKh 0\n",
      "Trained batch 614 batch loss 0.414108574 batch mAP 0.529510498 batch PCKh 0.5\n",
      "Trained batch 615 batch loss 0.471880585 batch mAP 0.532592773 batch PCKh 0.625\n",
      "Trained batch 616 batch loss 0.472404361 batch mAP 0.481231689 batch PCKh 0.5625\n",
      "Trained batch 617 batch loss 0.470397353 batch mAP 0.505615234 batch PCKh 0.625\n",
      "Trained batch 618 batch loss 0.519929588 batch mAP 0.452331543 batch PCKh 0.6875\n",
      "Trained batch 619 batch loss 0.647466421 batch mAP 0.363922119 batch PCKh 0.75\n",
      "Trained batch 620 batch loss 0.534013033 batch mAP 0.458709717 batch PCKh 0.75\n",
      "Trained batch 621 batch loss 0.648295522 batch mAP 0.390716553 batch PCKh 0.5\n",
      "Trained batch 622 batch loss 0.640785813 batch mAP 0.416931152 batch PCKh 0.5\n",
      "Trained batch 623 batch loss 0.662336111 batch mAP 0.456604 batch PCKh 0.1875\n",
      "Trained batch 624 batch loss 0.622763872 batch mAP 0.518798828 batch PCKh 0.25\n",
      "Trained batch 625 batch loss 0.637200713 batch mAP 0.483612061 batch PCKh 0.3125\n",
      "Trained batch 626 batch loss 0.6260584 batch mAP 0.475769043 batch PCKh 0.4375\n",
      "Trained batch 627 batch loss 0.515702307 batch mAP 0.29196167 batch PCKh 0.25\n",
      "Trained batch 628 batch loss 0.527134538 batch mAP 0.224304199 batch PCKh 0.3125\n",
      "Trained batch 629 batch loss 0.491891533 batch mAP 0.134033203 batch PCKh 0.125\n",
      "Trained batch 630 batch loss 0.495823741 batch mAP 0.227325439 batch PCKh 0.3125\n",
      "Trained batch 631 batch loss 0.626638293 batch mAP 0.247711182 batch PCKh 0.5\n",
      "Trained batch 632 batch loss 0.628216863 batch mAP 0.266448975 batch PCKh 0.3125\n",
      "Trained batch 633 batch loss 0.596542478 batch mAP 0.273223877 batch PCKh 0.75\n",
      "Trained batch 634 batch loss 0.55559814 batch mAP 0.316497803 batch PCKh 0.875\n",
      "Trained batch 635 batch loss 0.63404566 batch mAP 0.436279297 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 636 batch loss 0.566137075 batch mAP 0.486969 batch PCKh 0.5625\n",
      "Trained batch 637 batch loss 0.550359964 batch mAP 0.452392578 batch PCKh 0.5\n",
      "Trained batch 638 batch loss 0.56640625 batch mAP 0.519317627 batch PCKh 0.3125\n",
      "Trained batch 639 batch loss 0.57811451 batch mAP 0.541503906 batch PCKh 0.5\n",
      "Trained batch 640 batch loss 0.646704137 batch mAP 0.492340088 batch PCKh 0.4375\n",
      "Trained batch 641 batch loss 0.666166 batch mAP 0.509735107 batch PCKh 0.6875\n",
      "Trained batch 642 batch loss 0.671582878 batch mAP 0.426879883 batch PCKh 0.8125\n",
      "Trained batch 643 batch loss 0.580019236 batch mAP 0.46206665 batch PCKh 0.8125\n",
      "Trained batch 644 batch loss 0.570057631 batch mAP 0.516296387 batch PCKh 0.875\n",
      "Trained batch 645 batch loss 0.556304514 batch mAP 0.541046143 batch PCKh 0.75\n",
      "Trained batch 646 batch loss 0.588670135 batch mAP 0.555236816 batch PCKh 0.8125\n",
      "Trained batch 647 batch loss 0.554848373 batch mAP 0.491638184 batch PCKh 0.75\n",
      "Trained batch 648 batch loss 0.464546561 batch mAP 0.505493164 batch PCKh 0.1875\n",
      "Trained batch 649 batch loss 0.491787493 batch mAP 0.555114746 batch PCKh 0.6875\n",
      "Trained batch 650 batch loss 0.524709284 batch mAP 0.557495117 batch PCKh 0.4375\n",
      "Trained batch 651 batch loss 0.578936696 batch mAP 0.49798584 batch PCKh 0.5\n",
      "Trained batch 652 batch loss 0.594955266 batch mAP 0.558319092 batch PCKh 0.4375\n",
      "Trained batch 653 batch loss 0.548648 batch mAP 0.491333 batch PCKh 0.4375\n",
      "Trained batch 654 batch loss 0.480745852 batch mAP 0.508361816 batch PCKh 0\n",
      "Trained batch 655 batch loss 0.601415873 batch mAP 0.464447021 batch PCKh 0.5\n",
      "Trained batch 656 batch loss 0.636089563 batch mAP 0.469390869 batch PCKh 0.25\n",
      "Trained batch 657 batch loss 0.603770494 batch mAP 0.472290039 batch PCKh 0.125\n",
      "Trained batch 658 batch loss 0.602964878 batch mAP 0.529266357 batch PCKh 0.75\n",
      "Trained batch 659 batch loss 0.534873784 batch mAP 0.462005615 batch PCKh 0.875\n",
      "Trained batch 660 batch loss 0.603135467 batch mAP 0.467498779 batch PCKh 0.25\n",
      "Trained batch 661 batch loss 0.603998184 batch mAP 0.472839355 batch PCKh 0\n",
      "Trained batch 662 batch loss 0.513075233 batch mAP 0.426147461 batch PCKh 0.6875\n",
      "Trained batch 663 batch loss 0.5732795 batch mAP 0.45803833 batch PCKh 0.3125\n",
      "Trained batch 664 batch loss 0.602116644 batch mAP 0.510192871 batch PCKh 0.25\n",
      "Trained batch 665 batch loss 0.565576613 batch mAP 0.467834473 batch PCKh 0.25\n",
      "Trained batch 666 batch loss 0.611124575 batch mAP 0.523010254 batch PCKh 0.75\n",
      "Trained batch 667 batch loss 0.672335744 batch mAP 0.526001 batch PCKh 0.75\n",
      "Trained batch 668 batch loss 0.721871078 batch mAP 0.514862061 batch PCKh 0.1875\n",
      "Trained batch 669 batch loss 0.523159206 batch mAP 0.546813965 batch PCKh 0.375\n",
      "Trained batch 670 batch loss 0.436439604 batch mAP 0.524597168 batch PCKh 0.6875\n",
      "Trained batch 671 batch loss 0.465695918 batch mAP 0.527587891 batch PCKh 0.5\n",
      "Trained batch 672 batch loss 0.420064807 batch mAP 0.513671875 batch PCKh 0.5\n",
      "Trained batch 673 batch loss 0.488063425 batch mAP 0.537658691 batch PCKh 0.3125\n",
      "Trained batch 674 batch loss 0.571208179 batch mAP 0.484008789 batch PCKh 0.1875\n",
      "Trained batch 675 batch loss 0.551517725 batch mAP 0.519989 batch PCKh 0.25\n",
      "Trained batch 676 batch loss 0.56243372 batch mAP 0.524261475 batch PCKh 0.5\n",
      "Trained batch 677 batch loss 0.551552534 batch mAP 0.569763184 batch PCKh 0.75\n",
      "Trained batch 678 batch loss 0.535167813 batch mAP 0.599578857 batch PCKh 0\n",
      "Trained batch 679 batch loss 0.521644711 batch mAP 0.558044434 batch PCKh 0.1875\n",
      "Trained batch 680 batch loss 0.592303813 batch mAP 0.550628662 batch PCKh 0.0625\n",
      "Trained batch 681 batch loss 0.546411395 batch mAP 0.559906 batch PCKh 0.625\n",
      "Trained batch 682 batch loss 0.589303851 batch mAP 0.55947876 batch PCKh 0.5\n",
      "Trained batch 683 batch loss 0.572930515 batch mAP 0.549682617 batch PCKh 0.75\n",
      "Trained batch 684 batch loss 0.629365087 batch mAP 0.506317139 batch PCKh 0.75\n",
      "Trained batch 685 batch loss 0.578702152 batch mAP 0.544952393 batch PCKh 0.4375\n",
      "Trained batch 686 batch loss 0.664162815 batch mAP 0.520446777 batch PCKh 0.1875\n",
      "Trained batch 687 batch loss 0.568751216 batch mAP 0.48425293 batch PCKh 0.1875\n",
      "Trained batch 688 batch loss 0.622456789 batch mAP 0.491516113 batch PCKh 0.75\n",
      "Trained batch 689 batch loss 0.565984488 batch mAP 0.497711182 batch PCKh 0.5625\n",
      "Trained batch 690 batch loss 0.60702765 batch mAP 0.487884521 batch PCKh 0.8125\n",
      "Trained batch 691 batch loss 0.669106722 batch mAP 0.545074463 batch PCKh 0.4375\n",
      "Trained batch 692 batch loss 0.694709122 batch mAP 0.492584229 batch PCKh 0.125\n",
      "Trained batch 693 batch loss 0.693804622 batch mAP 0.507782 batch PCKh 0.3125\n",
      "Trained batch 694 batch loss 0.713594735 batch mAP 0.522155762 batch PCKh 0.0625\n",
      "Trained batch 695 batch loss 0.558817923 batch mAP 0.509185791 batch PCKh 0.6875\n",
      "Trained batch 696 batch loss 0.574480951 batch mAP 0.516906738 batch PCKh 0.5625\n",
      "Trained batch 697 batch loss 0.469083 batch mAP 0.570495605 batch PCKh 0.375\n",
      "Trained batch 698 batch loss 0.562117457 batch mAP 0.520965576 batch PCKh 0.5\n",
      "Trained batch 699 batch loss 0.555139 batch mAP 0.541290283 batch PCKh 0.5625\n",
      "Trained batch 700 batch loss 0.648528099 batch mAP 0.596435547 batch PCKh 0.3125\n",
      "Trained batch 701 batch loss 0.438192159 batch mAP 0.586151123 batch PCKh 0.4375\n",
      "Trained batch 702 batch loss 0.544717073 batch mAP 0.547912598 batch PCKh 0.75\n",
      "Trained batch 703 batch loss 0.62311244 batch mAP 0.558044434 batch PCKh 0.375\n",
      "Trained batch 704 batch loss 0.532708704 batch mAP 0.602783203 batch PCKh 0.625\n",
      "Trained batch 705 batch loss 0.619653046 batch mAP 0.555633545 batch PCKh 0.4375\n",
      "Trained batch 706 batch loss 0.59867388 batch mAP 0.551239 batch PCKh 0.25\n",
      "Trained batch 707 batch loss 0.62630862 batch mAP 0.524536133 batch PCKh 0.3125\n",
      "Trained batch 708 batch loss 0.586760521 batch mAP 0.583343506 batch PCKh 0.4375\n",
      "Trained batch 709 batch loss 0.552664161 batch mAP 0.563812256 batch PCKh 0.5625\n",
      "Trained batch 710 batch loss 0.590797842 batch mAP 0.548126221 batch PCKh 0.5625\n",
      "Trained batch 711 batch loss 0.592586696 batch mAP 0.531677246 batch PCKh 0.125\n",
      "Trained batch 712 batch loss 0.660117269 batch mAP 0.530700684 batch PCKh 0.0625\n",
      "Trained batch 713 batch loss 0.697481811 batch mAP 0.484863281 batch PCKh 0.1875\n",
      "Trained batch 714 batch loss 0.564821601 batch mAP 0.536651611 batch PCKh 0.5625\n",
      "Trained batch 715 batch loss 0.60425365 batch mAP 0.505188 batch PCKh 0.5625\n",
      "Trained batch 716 batch loss 0.648418784 batch mAP 0.459869385 batch PCKh 0.5625\n",
      "Trained batch 717 batch loss 0.593755245 batch mAP 0.503387451 batch PCKh 0.625\n",
      "Trained batch 718 batch loss 0.565402448 batch mAP 0.468078613 batch PCKh 0.5625\n",
      "Trained batch 719 batch loss 0.540360272 batch mAP 0.50567627 batch PCKh 0.75\n",
      "Trained batch 720 batch loss 0.48024708 batch mAP 0.540802 batch PCKh 0.5625\n",
      "Trained batch 721 batch loss 0.516851723 batch mAP 0.516235352 batch PCKh 0.0625\n",
      "Trained batch 722 batch loss 0.523455 batch mAP 0.509460449 batch PCKh 0.5\n",
      "Trained batch 723 batch loss 0.570273876 batch mAP 0.544647217 batch PCKh 0.5\n",
      "Trained batch 724 batch loss 0.494665086 batch mAP 0.570251465 batch PCKh 0.1875\n",
      "Trained batch 725 batch loss 0.629760921 batch mAP 0.586608887 batch PCKh 0\n",
      "Trained batch 726 batch loss 0.585304 batch mAP 0.596588135 batch PCKh 0.5\n",
      "Trained batch 727 batch loss 0.648114681 batch mAP 0.579589844 batch PCKh 0.25\n",
      "Trained batch 728 batch loss 0.69975853 batch mAP 0.545837402 batch PCKh 0\n",
      "Trained batch 729 batch loss 0.626348853 batch mAP 0.661865234 batch PCKh 0.5\n",
      "Trained batch 730 batch loss 0.616517901 batch mAP 0.611816406 batch PCKh 0.4375\n",
      "Trained batch 731 batch loss 0.677641273 batch mAP 0.519561768 batch PCKh 0.5625\n",
      "Trained batch 732 batch loss 0.566827655 batch mAP 0.561553955 batch PCKh 0.75\n",
      "Trained batch 733 batch loss 0.527612567 batch mAP 0.498474121 batch PCKh 0.1875\n",
      "Trained batch 734 batch loss 0.464094281 batch mAP 0.470672607 batch PCKh 0.1875\n",
      "Trained batch 735 batch loss 0.56532675 batch mAP 0.538635254 batch PCKh 0.625\n",
      "Trained batch 736 batch loss 0.619185567 batch mAP 0.525817871 batch PCKh 0.375\n",
      "Trained batch 737 batch loss 0.591329813 batch mAP 0.570129395 batch PCKh 0.3125\n",
      "Trained batch 738 batch loss 0.518249452 batch mAP 0.563812256 batch PCKh 0.8125\n",
      "Trained batch 739 batch loss 0.617907047 batch mAP 0.546203613 batch PCKh 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 740 batch loss 0.653068244 batch mAP 0.545715332 batch PCKh 0.5\n",
      "Trained batch 741 batch loss 0.5727 batch mAP 0.554657 batch PCKh 0.1875\n",
      "Trained batch 742 batch loss 0.522936344 batch mAP 0.54776 batch PCKh 0.4375\n",
      "Trained batch 743 batch loss 0.569237649 batch mAP 0.524505615 batch PCKh 0.875\n",
      "Trained batch 744 batch loss 0.658067405 batch mAP 0.559509277 batch PCKh 0.3125\n",
      "Trained batch 745 batch loss 0.584351897 batch mAP 0.52520752 batch PCKh 0.25\n",
      "Trained batch 746 batch loss 0.592777133 batch mAP 0.499847412 batch PCKh 0.875\n",
      "Trained batch 747 batch loss 0.590383232 batch mAP 0.52948 batch PCKh 0.8125\n",
      "Trained batch 748 batch loss 0.59235996 batch mAP 0.535522461 batch PCKh 0.375\n",
      "Trained batch 749 batch loss 0.625822 batch mAP 0.52041626 batch PCKh 0.75\n",
      "Trained batch 750 batch loss 0.559892058 batch mAP 0.498291016 batch PCKh 0.875\n",
      "Trained batch 751 batch loss 0.582618415 batch mAP 0.527099609 batch PCKh 0.6875\n",
      "Trained batch 752 batch loss 0.530866086 batch mAP 0.560058594 batch PCKh 0.1875\n",
      "Trained batch 753 batch loss 0.551769674 batch mAP 0.520385742 batch PCKh 0.5625\n",
      "Trained batch 754 batch loss 0.512167871 batch mAP 0.566070557 batch PCKh 0.5625\n",
      "Trained batch 755 batch loss 0.536534846 batch mAP 0.505737305 batch PCKh 0.375\n",
      "Trained batch 756 batch loss 0.483605802 batch mAP 0.54019165 batch PCKh 0.75\n",
      "Trained batch 757 batch loss 0.585794 batch mAP 0.502655 batch PCKh 0.5625\n",
      "Trained batch 758 batch loss 0.601926267 batch mAP 0.513000488 batch PCKh 0.375\n",
      "Trained batch 759 batch loss 0.520531178 batch mAP 0.496246338 batch PCKh 0.4375\n",
      "Trained batch 760 batch loss 0.612626672 batch mAP 0.49029541 batch PCKh 0.6875\n",
      "Trained batch 761 batch loss 0.618785858 batch mAP 0.537841797 batch PCKh 0.75\n",
      "Trained batch 762 batch loss 0.660174549 batch mAP 0.503143311 batch PCKh 0.625\n",
      "Trained batch 763 batch loss 0.577471495 batch mAP 0.484771729 batch PCKh 0.1875\n",
      "Trained batch 764 batch loss 0.61692673 batch mAP 0.508911133 batch PCKh 0.1875\n",
      "Trained batch 765 batch loss 0.541509 batch mAP 0.614532471 batch PCKh 0.8125\n",
      "Trained batch 766 batch loss 0.553146422 batch mAP 0.570709229 batch PCKh 0.375\n",
      "Trained batch 767 batch loss 0.532938659 batch mAP 0.524963379 batch PCKh 0.125\n",
      "Trained batch 768 batch loss 0.592896223 batch mAP 0.538238525 batch PCKh 0.6875\n",
      "Trained batch 769 batch loss 0.528900206 batch mAP 0.543731689 batch PCKh 0.25\n",
      "Trained batch 770 batch loss 0.606868267 batch mAP 0.496856689 batch PCKh 0.625\n",
      "Trained batch 771 batch loss 0.566920519 batch mAP 0.525909424 batch PCKh 0.5\n",
      "Trained batch 772 batch loss 0.488096446 batch mAP 0.503753662 batch PCKh 0.1875\n",
      "Trained batch 773 batch loss 0.471480936 batch mAP 0.483551025 batch PCKh 0.625\n",
      "Trained batch 774 batch loss 0.525227308 batch mAP 0.540100098 batch PCKh 0.625\n",
      "Trained batch 775 batch loss 0.587352514 batch mAP 0.527557373 batch PCKh 0.4375\n",
      "Trained batch 776 batch loss 0.655239165 batch mAP 0.468688965 batch PCKh 0.625\n",
      "Trained batch 777 batch loss 0.594831765 batch mAP 0.486114502 batch PCKh 0.125\n",
      "Trained batch 778 batch loss 0.636378706 batch mAP 0.519928 batch PCKh 0.125\n",
      "Trained batch 779 batch loss 0.616209626 batch mAP 0.514312744 batch PCKh 0.625\n",
      "Trained batch 780 batch loss 0.574580491 batch mAP 0.528015137 batch PCKh 0.375\n",
      "Trained batch 781 batch loss 0.635379 batch mAP 0.541320801 batch PCKh 0.1875\n",
      "Trained batch 782 batch loss 0.606623054 batch mAP 0.525024414 batch PCKh 0.1875\n",
      "Trained batch 783 batch loss 0.667270362 batch mAP 0.522186279 batch PCKh 0.375\n",
      "Trained batch 784 batch loss 0.517650902 batch mAP 0.519012451 batch PCKh 0.3125\n",
      "Trained batch 785 batch loss 0.564350963 batch mAP 0.533447266 batch PCKh 0.5625\n",
      "Trained batch 786 batch loss 0.547342956 batch mAP 0.384643555 batch PCKh 0.625\n",
      "Trained batch 787 batch loss 0.599113643 batch mAP 0.448730469 batch PCKh 0.6875\n",
      "Trained batch 788 batch loss 0.46222496 batch mAP 0.602966309 batch PCKh 0.5625\n",
      "Trained batch 789 batch loss 0.476197332 batch mAP 0.456115723 batch PCKh 0.1875\n",
      "Trained batch 790 batch loss 0.449490637 batch mAP 0.514343262 batch PCKh 0.25\n",
      "Trained batch 791 batch loss 0.524188876 batch mAP 0.548126221 batch PCKh 0.5\n",
      "Trained batch 792 batch loss 0.490782589 batch mAP 0.590362549 batch PCKh 0.625\n",
      "Trained batch 793 batch loss 0.671631396 batch mAP 0.542388916 batch PCKh 0.4375\n",
      "Trained batch 794 batch loss 0.780330062 batch mAP 0.508880615 batch PCKh 0.125\n",
      "Trained batch 795 batch loss 0.651053 batch mAP 0.584136963 batch PCKh 0\n",
      "Trained batch 796 batch loss 0.560837865 batch mAP 0.620147705 batch PCKh 0.375\n",
      "Trained batch 797 batch loss 0.539934695 batch mAP 0.56918335 batch PCKh 0.75\n",
      "Trained batch 798 batch loss 0.545401156 batch mAP 0.546020508 batch PCKh 0.4375\n",
      "Trained batch 799 batch loss 0.532391191 batch mAP 0.539917 batch PCKh 0.125\n",
      "Trained batch 800 batch loss 0.539537311 batch mAP 0.561737061 batch PCKh 0.1875\n",
      "Trained batch 801 batch loss 0.603672624 batch mAP 0.514953613 batch PCKh 0.1875\n",
      "Trained batch 802 batch loss 0.530017734 batch mAP 0.539855957 batch PCKh 0.4375\n",
      "Trained batch 803 batch loss 0.576395512 batch mAP 0.528900146 batch PCKh 0.375\n",
      "Trained batch 804 batch loss 0.518614888 batch mAP 0.522766113 batch PCKh 0.5\n",
      "Trained batch 805 batch loss 0.636058867 batch mAP 0.548919678 batch PCKh 0.625\n",
      "Trained batch 806 batch loss 0.526855767 batch mAP 0.596130371 batch PCKh 0.5\n",
      "Trained batch 807 batch loss 0.521812081 batch mAP 0.581237793 batch PCKh 0.6875\n",
      "Trained batch 808 batch loss 0.469648212 batch mAP 0.536193848 batch PCKh 0.375\n",
      "Trained batch 809 batch loss 0.627654552 batch mAP 0.582855225 batch PCKh 0.3125\n",
      "Trained batch 810 batch loss 0.586611748 batch mAP 0.584503174 batch PCKh 0.5625\n",
      "Trained batch 811 batch loss 0.539980114 batch mAP 0.609771729 batch PCKh 0.6875\n",
      "Trained batch 812 batch loss 0.544691086 batch mAP 0.584625244 batch PCKh 0.3125\n",
      "Trained batch 813 batch loss 0.597834408 batch mAP 0.642456055 batch PCKh 0.625\n",
      "Trained batch 814 batch loss 0.53903991 batch mAP 0.583465576 batch PCKh 0.4375\n",
      "Trained batch 815 batch loss 0.563205719 batch mAP 0.557769775 batch PCKh 0.625\n",
      "Trained batch 816 batch loss 0.63494122 batch mAP 0.589019775 batch PCKh 0.375\n",
      "Trained batch 817 batch loss 0.602279305 batch mAP 0.560302734 batch PCKh 0.5625\n",
      "Trained batch 818 batch loss 0.492346972 batch mAP 0.609008789 batch PCKh 0.0625\n",
      "Trained batch 819 batch loss 0.509608388 batch mAP 0.621490479 batch PCKh 0.5\n",
      "Trained batch 820 batch loss 0.524317 batch mAP 0.634643555 batch PCKh 0.25\n",
      "Trained batch 821 batch loss 0.531294227 batch mAP 0.615722656 batch PCKh 0.25\n",
      "Trained batch 822 batch loss 0.49069649 batch mAP 0.668243408 batch PCKh 0.4375\n",
      "Trained batch 823 batch loss 0.533807 batch mAP 0.621337891 batch PCKh 0.1875\n",
      "Trained batch 824 batch loss 0.594946504 batch mAP 0.634521484 batch PCKh 0.3125\n",
      "Trained batch 825 batch loss 0.514816046 batch mAP 0.571899414 batch PCKh 0.25\n",
      "Trained batch 826 batch loss 0.561479747 batch mAP 0.58706665 batch PCKh 0.25\n",
      "Trained batch 827 batch loss 0.564798295 batch mAP 0.578338623 batch PCKh 0.3125\n",
      "Trained batch 828 batch loss 0.643283129 batch mAP 0.585510254 batch PCKh 0.5\n",
      "Trained batch 829 batch loss 0.612378478 batch mAP 0.525177 batch PCKh 0.375\n",
      "Trained batch 830 batch loss 0.608470678 batch mAP 0.543182373 batch PCKh 0.3125\n",
      "Trained batch 831 batch loss 0.620590508 batch mAP 0.528778076 batch PCKh 0.75\n",
      "Trained batch 832 batch loss 0.553501844 batch mAP 0.633453369 batch PCKh 0.8125\n",
      "Trained batch 833 batch loss 0.698842347 batch mAP 0.562561035 batch PCKh 0\n",
      "Trained batch 834 batch loss 0.73077929 batch mAP 0.499847412 batch PCKh 0.0625\n",
      "Trained batch 835 batch loss 0.708884597 batch mAP 0.495239258 batch PCKh 0.25\n",
      "Trained batch 836 batch loss 0.670733333 batch mAP 0.446777344 batch PCKh 0.125\n",
      "Trained batch 837 batch loss 0.623361945 batch mAP 0.504150391 batch PCKh 0.75\n",
      "Trained batch 838 batch loss 0.556635082 batch mAP 0.384094238 batch PCKh 0.5\n",
      "Trained batch 839 batch loss 0.636443734 batch mAP 0.386261 batch PCKh 0.625\n",
      "Trained batch 840 batch loss 0.57009697 batch mAP 0.401672363 batch PCKh 0.5625\n",
      "Trained batch 841 batch loss 0.546136796 batch mAP 0.440246582 batch PCKh 0.75\n",
      "Trained batch 842 batch loss 0.438982457 batch mAP 0.290771484 batch PCKh 0.5\n",
      "Trained batch 843 batch loss 0.492761135 batch mAP 0.387939453 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 844 batch loss 0.39680022 batch mAP 0.300476074 batch PCKh 0\n",
      "Trained batch 845 batch loss 0.430259 batch mAP 0.432769775 batch PCKh 0.125\n",
      "Trained batch 846 batch loss 0.524316 batch mAP 0.482788086 batch PCKh 0.5\n",
      "Trained batch 847 batch loss 0.588695824 batch mAP 0.505371094 batch PCKh 0.6875\n",
      "Trained batch 848 batch loss 0.571417 batch mAP 0.457214355 batch PCKh 0.6875\n",
      "Trained batch 849 batch loss 0.44440639 batch mAP 0.397613525 batch PCKh 0\n",
      "Trained batch 850 batch loss 0.420432508 batch mAP 0.475097656 batch PCKh 0.3125\n",
      "Trained batch 851 batch loss 0.403217554 batch mAP 0.508392334 batch PCKh 0.1875\n",
      "Trained batch 852 batch loss 0.447534978 batch mAP 0.538848877 batch PCKh 0\n",
      "Trained batch 853 batch loss 0.482668221 batch mAP 0.48526 batch PCKh 0\n",
      "Trained batch 854 batch loss 0.420287162 batch mAP 0.454315186 batch PCKh 0.5625\n",
      "Trained batch 855 batch loss 0.456077456 batch mAP 0.476318359 batch PCKh 0.1875\n",
      "Trained batch 856 batch loss 0.561072 batch mAP 0.388946533 batch PCKh 0.75\n",
      "Trained batch 857 batch loss 0.510613441 batch mAP 0.453491211 batch PCKh 0.625\n",
      "Trained batch 858 batch loss 0.627735376 batch mAP 0.276184082 batch PCKh 0.625\n",
      "Trained batch 859 batch loss 0.502437353 batch mAP 0.328979492 batch PCKh 0.625\n",
      "Trained batch 860 batch loss 0.572965205 batch mAP 0.442504883 batch PCKh 0.1875\n",
      "Trained batch 861 batch loss 0.59802407 batch mAP 0.459075928 batch PCKh 0.0625\n",
      "Trained batch 862 batch loss 0.583051383 batch mAP 0.468597412 batch PCKh 0.75\n",
      "Trained batch 863 batch loss 0.600249052 batch mAP 0.486755371 batch PCKh 0.3125\n",
      "Trained batch 864 batch loss 0.567066967 batch mAP 0.473449707 batch PCKh 0.625\n",
      "Trained batch 865 batch loss 0.555695534 batch mAP 0.541534424 batch PCKh 0.6875\n",
      "Trained batch 866 batch loss 0.58202827 batch mAP 0.49331665 batch PCKh 0.5\n",
      "Trained batch 867 batch loss 0.640924394 batch mAP 0.537963867 batch PCKh 0.375\n",
      "Trained batch 868 batch loss 0.524952888 batch mAP 0.481506348 batch PCKh 0.25\n",
      "Trained batch 869 batch loss 0.498821199 batch mAP 0.435272217 batch PCKh 0.375\n",
      "Trained batch 870 batch loss 0.578018129 batch mAP 0.342468262 batch PCKh 0.5\n",
      "Trained batch 871 batch loss 0.660455287 batch mAP 0.417144775 batch PCKh 0.625\n",
      "Trained batch 872 batch loss 0.571129441 batch mAP 0.395568848 batch PCKh 0.125\n",
      "Trained batch 873 batch loss 0.625274241 batch mAP 0.42956543 batch PCKh 0.625\n",
      "Trained batch 874 batch loss 0.6827057 batch mAP 0.410919189 batch PCKh 0.25\n",
      "Trained batch 875 batch loss 0.677269876 batch mAP 0.449707031 batch PCKh 0.4375\n",
      "Trained batch 876 batch loss 0.695429325 batch mAP 0.43762207 batch PCKh 0.125\n",
      "Trained batch 877 batch loss 0.653296232 batch mAP 0.490875244 batch PCKh 0.375\n",
      "Trained batch 878 batch loss 0.608663917 batch mAP 0.553070068 batch PCKh 0.5625\n",
      "Trained batch 879 batch loss 0.532226861 batch mAP 0.578979492 batch PCKh 0.625\n",
      "Trained batch 880 batch loss 0.66309607 batch mAP 0.521270752 batch PCKh 0.5\n",
      "Trained batch 881 batch loss 0.581818879 batch mAP 0.471557617 batch PCKh 0.5\n",
      "Trained batch 882 batch loss 0.60534215 batch mAP 0.474121094 batch PCKh 0.5625\n",
      "Trained batch 883 batch loss 0.577578843 batch mAP 0.58013916 batch PCKh 0.1875\n",
      "Trained batch 884 batch loss 0.601716161 batch mAP 0.544921875 batch PCKh 0.1875\n",
      "Trained batch 885 batch loss 0.554315209 batch mAP 0.530883789 batch PCKh 0.75\n",
      "Trained batch 886 batch loss 0.477569222 batch mAP 0.546447754 batch PCKh 0.25\n",
      "Trained batch 887 batch loss 0.523459077 batch mAP 0.497375488 batch PCKh 0.5\n",
      "Trained batch 888 batch loss 0.495189369 batch mAP 0.563171387 batch PCKh 0.25\n",
      "Trained batch 889 batch loss 0.584994197 batch mAP 0.583496094 batch PCKh 0.375\n",
      "Trained batch 890 batch loss 0.624974728 batch mAP 0.528015137 batch PCKh 0\n",
      "Trained batch 891 batch loss 0.607226 batch mAP 0.537902832 batch PCKh 0.1875\n",
      "Trained batch 892 batch loss 0.665022373 batch mAP 0.499786377 batch PCKh 0.1875\n",
      "Trained batch 893 batch loss 0.640176654 batch mAP 0.476379395 batch PCKh 0.3125\n",
      "Trained batch 894 batch loss 0.614477277 batch mAP 0.500152588 batch PCKh 0.5\n",
      "Trained batch 895 batch loss 0.549959958 batch mAP 0.466400146 batch PCKh 0.5625\n",
      "Trained batch 896 batch loss 0.558155 batch mAP 0.530639648 batch PCKh 0.75\n",
      "Trained batch 897 batch loss 0.529380441 batch mAP 0.483398438 batch PCKh 0.3125\n",
      "Trained batch 898 batch loss 0.486459315 batch mAP 0.560058594 batch PCKh 0.6875\n",
      "Trained batch 899 batch loss 0.529826641 batch mAP 0.529541 batch PCKh 0.25\n",
      "Trained batch 900 batch loss 0.519617796 batch mAP 0.554351807 batch PCKh 0.5\n",
      "Trained batch 901 batch loss 0.532549679 batch mAP 0.524017334 batch PCKh 0.5\n",
      "Trained batch 902 batch loss 0.457110971 batch mAP 0.606536865 batch PCKh 0.25\n",
      "Trained batch 903 batch loss 0.535959303 batch mAP 0.621185303 batch PCKh 0.625\n",
      "Trained batch 904 batch loss 0.558829665 batch mAP 0.548980713 batch PCKh 0.6875\n",
      "Trained batch 905 batch loss 0.532721 batch mAP 0.575408936 batch PCKh 0.6875\n",
      "Trained batch 906 batch loss 0.538119316 batch mAP 0.595977783 batch PCKh 0.5625\n",
      "Trained batch 907 batch loss 0.47134316 batch mAP 0.568573 batch PCKh 0.375\n",
      "Trained batch 908 batch loss 0.472817838 batch mAP 0.614532471 batch PCKh 0.25\n",
      "Trained batch 909 batch loss 0.595187724 batch mAP 0.611358643 batch PCKh 0.4375\n",
      "Trained batch 910 batch loss 0.546082735 batch mAP 0.591644287 batch PCKh 0.375\n",
      "Trained batch 911 batch loss 0.588286698 batch mAP 0.619659424 batch PCKh 0.3125\n",
      "Trained batch 912 batch loss 0.614574134 batch mAP 0.554260254 batch PCKh 0.25\n",
      "Trained batch 913 batch loss 0.632308483 batch mAP 0.598327637 batch PCKh 0.1875\n",
      "Trained batch 914 batch loss 0.686161637 batch mAP 0.538147 batch PCKh 0.1875\n",
      "Trained batch 915 batch loss 0.577329695 batch mAP 0.546905518 batch PCKh 0.375\n",
      "Trained batch 916 batch loss 0.535484374 batch mAP 0.534942627 batch PCKh 0.25\n",
      "Trained batch 917 batch loss 0.548924088 batch mAP 0.537078857 batch PCKh 0.0625\n",
      "Trained batch 918 batch loss 0.503563344 batch mAP 0.528564453 batch PCKh 0.1875\n",
      "Trained batch 919 batch loss 0.477870673 batch mAP 0.53326416 batch PCKh 0.5625\n",
      "Trained batch 920 batch loss 0.545156062 batch mAP 0.503417969 batch PCKh 0.3125\n",
      "Trained batch 921 batch loss 0.516522884 batch mAP 0.53012085 batch PCKh 0.5\n",
      "Trained batch 922 batch loss 0.495540023 batch mAP 0.554901123 batch PCKh 0.375\n",
      "Trained batch 923 batch loss 0.541568816 batch mAP 0.599090576 batch PCKh 0.625\n",
      "Trained batch 924 batch loss 0.556643844 batch mAP 0.563598633 batch PCKh 0.4375\n",
      "Trained batch 925 batch loss 0.480725676 batch mAP 0.554107666 batch PCKh 0.0625\n",
      "Trained batch 926 batch loss 0.594660878 batch mAP 0.526580811 batch PCKh 0.625\n",
      "Trained batch 927 batch loss 0.552583456 batch mAP 0.531707764 batch PCKh 0.3125\n",
      "Trained batch 928 batch loss 0.541151702 batch mAP 0.546447754 batch PCKh 0.5\n",
      "Trained batch 929 batch loss 0.6473943 batch mAP 0.521850586 batch PCKh 0.125\n",
      "Trained batch 930 batch loss 0.550877333 batch mAP 0.539367676 batch PCKh 0.3125\n",
      "Trained batch 931 batch loss 0.585397 batch mAP 0.519470215 batch PCKh 0\n",
      "Trained batch 932 batch loss 0.537159741 batch mAP 0.57321167 batch PCKh 0.3125\n",
      "Trained batch 933 batch loss 0.57685256 batch mAP 0.545776367 batch PCKh 0.375\n",
      "Trained batch 934 batch loss 0.641865611 batch mAP 0.426757812 batch PCKh 0.1875\n",
      "Trained batch 935 batch loss 0.623673558 batch mAP 0.485168457 batch PCKh 0.6875\n",
      "Trained batch 936 batch loss 0.600771546 batch mAP 0.532440186 batch PCKh 0.375\n",
      "Trained batch 937 batch loss 0.649278581 batch mAP 0.539855957 batch PCKh 0.375\n",
      "Trained batch 938 batch loss 0.667503476 batch mAP 0.471099854 batch PCKh 0.1875\n",
      "Trained batch 939 batch loss 0.726542473 batch mAP 0.439239502 batch PCKh 0.5625\n",
      "Trained batch 940 batch loss 0.767897129 batch mAP 0.442657471 batch PCKh 0.125\n",
      "Trained batch 941 batch loss 0.649836838 batch mAP 0.387817383 batch PCKh 0\n",
      "Trained batch 942 batch loss 0.758353174 batch mAP 0.326660156 batch PCKh 0\n",
      "Trained batch 943 batch loss 0.609573483 batch mAP 0.355041504 batch PCKh 0.25\n",
      "Trained batch 944 batch loss 0.576524734 batch mAP 0.252105713 batch PCKh 0.0625\n",
      "Trained batch 945 batch loss 0.526127517 batch mAP 0.236328125 batch PCKh 0.3125\n",
      "Trained batch 946 batch loss 0.515080273 batch mAP 0.176086426 batch PCKh 0.1875\n",
      "Trained batch 947 batch loss 0.543953121 batch mAP 0.248809814 batch PCKh 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 948 batch loss 0.602163613 batch mAP 0.265808105 batch PCKh 0.4375\n",
      "Trained batch 949 batch loss 0.660538673 batch mAP 0.279937744 batch PCKh 0.1875\n",
      "Trained batch 950 batch loss 0.689522 batch mAP 0.271087646 batch PCKh 0.3125\n",
      "Trained batch 951 batch loss 0.729461968 batch mAP 0.3019104 batch PCKh 0.1875\n",
      "Trained batch 952 batch loss 0.60640043 batch mAP 0.402069092 batch PCKh 0\n",
      "Trained batch 953 batch loss 0.642854393 batch mAP 0.414611816 batch PCKh 0.5625\n",
      "Trained batch 954 batch loss 0.6288656 batch mAP 0.459320068 batch PCKh 0.5\n",
      "Trained batch 955 batch loss 0.55013597 batch mAP 0.51663208 batch PCKh 0.375\n",
      "Trained batch 956 batch loss 0.567771554 batch mAP 0.558624268 batch PCKh 0.5625\n",
      "Trained batch 957 batch loss 0.5360111 batch mAP 0.570556641 batch PCKh 0.5625\n",
      "Trained batch 958 batch loss 0.578325629 batch mAP 0.515686035 batch PCKh 0.1875\n",
      "Trained batch 959 batch loss 0.597012937 batch mAP 0.540985107 batch PCKh 0.125\n",
      "Trained batch 960 batch loss 0.585444093 batch mAP 0.530639648 batch PCKh 0.5\n",
      "Trained batch 961 batch loss 0.663690686 batch mAP 0.479125977 batch PCKh 0.625\n",
      "Trained batch 962 batch loss 0.59675324 batch mAP 0.512451172 batch PCKh 0.25\n",
      "Trained batch 963 batch loss 0.613287568 batch mAP 0.490081787 batch PCKh 0.75\n",
      "Trained batch 964 batch loss 0.633844435 batch mAP 0.481384277 batch PCKh 0.5\n",
      "Trained batch 965 batch loss 0.639092922 batch mAP 0.460906982 batch PCKh 0.5\n",
      "Trained batch 966 batch loss 0.554686427 batch mAP 0.564697266 batch PCKh 0.5\n",
      "Trained batch 967 batch loss 0.645703733 batch mAP 0.520904541 batch PCKh 0.75\n",
      "Trained batch 968 batch loss 0.559717417 batch mAP 0.595214844 batch PCKh 0.6875\n",
      "Trained batch 969 batch loss 0.655132949 batch mAP 0.471984863 batch PCKh 0\n",
      "Trained batch 970 batch loss 0.629379153 batch mAP 0.503143311 batch PCKh 0.75\n",
      "Trained batch 971 batch loss 0.617848158 batch mAP 0.55960083 batch PCKh 0.8125\n",
      "Trained batch 972 batch loss 0.611994505 batch mAP 0.567779541 batch PCKh 0.4375\n",
      "Trained batch 973 batch loss 0.562061906 batch mAP 0.603302 batch PCKh 0.5625\n",
      "Trained batch 974 batch loss 0.596227527 batch mAP 0.540161133 batch PCKh 0.6875\n",
      "Trained batch 975 batch loss 0.631314 batch mAP 0.516204834 batch PCKh 0\n",
      "Trained batch 976 batch loss 0.684899747 batch mAP 0.472290039 batch PCKh 0\n",
      "Trained batch 977 batch loss 0.561468422 batch mAP 0.518707275 batch PCKh 0.5625\n",
      "Trained batch 978 batch loss 0.569470465 batch mAP 0.561950684 batch PCKh 0.5625\n",
      "Trained batch 979 batch loss 0.683193684 batch mAP 0.545776367 batch PCKh 0.375\n",
      "Trained batch 980 batch loss 0.545297503 batch mAP 0.534820557 batch PCKh 0.25\n",
      "Trained batch 981 batch loss 0.648221433 batch mAP 0.52722168 batch PCKh 0.5\n",
      "Trained batch 982 batch loss 0.563587666 batch mAP 0.499633789 batch PCKh 0.625\n",
      "Trained batch 983 batch loss 0.613707304 batch mAP 0.467803955 batch PCKh 0.8125\n",
      "Trained batch 984 batch loss 0.601662397 batch mAP 0.480804443 batch PCKh 0.6875\n",
      "Trained batch 985 batch loss 0.574582338 batch mAP 0.492340088 batch PCKh 0.5625\n",
      "Trained batch 986 batch loss 0.562638521 batch mAP 0.492492676 batch PCKh 0.625\n",
      "Trained batch 987 batch loss 0.562510312 batch mAP 0.516479492 batch PCKh 0.4375\n",
      "Trained batch 988 batch loss 0.608849 batch mAP 0.443511963 batch PCKh 0.1875\n",
      "Trained batch 989 batch loss 0.572889 batch mAP 0.446167 batch PCKh 0.375\n",
      "Trained batch 990 batch loss 0.67241776 batch mAP 0.448059082 batch PCKh 0.5625\n",
      "Trained batch 991 batch loss 0.632840812 batch mAP 0.429077148 batch PCKh 0.5625\n",
      "Trained batch 992 batch loss 0.546420574 batch mAP 0.50793457 batch PCKh 0.5625\n",
      "Trained batch 993 batch loss 0.566408634 batch mAP 0.486297607 batch PCKh 0.625\n",
      "Trained batch 994 batch loss 0.557291448 batch mAP 0.505523682 batch PCKh 0.5\n",
      "Trained batch 995 batch loss 0.519380033 batch mAP 0.54397583 batch PCKh 0.625\n",
      "Trained batch 996 batch loss 0.481505334 batch mAP 0.553375244 batch PCKh 0.5625\n",
      "Trained batch 997 batch loss 0.566554904 batch mAP 0.562774658 batch PCKh 0.125\n",
      "Trained batch 998 batch loss 0.538948298 batch mAP 0.52130127 batch PCKh 0.375\n",
      "Trained batch 999 batch loss 0.532047749 batch mAP 0.570800781 batch PCKh 0.75\n",
      "Trained batch 1000 batch loss 0.529298961 batch mAP 0.594146729 batch PCKh 0.4375\n",
      "Trained batch 1001 batch loss 0.523179591 batch mAP 0.534698486 batch PCKh 0.5625\n",
      "Trained batch 1002 batch loss 0.498624116 batch mAP 0.569519043 batch PCKh 0.5625\n",
      "Trained batch 1003 batch loss 0.541133285 batch mAP 0.53604126 batch PCKh 0.375\n",
      "Trained batch 1004 batch loss 0.582113862 batch mAP 0.512481689 batch PCKh 0.625\n",
      "Trained batch 1005 batch loss 0.578865 batch mAP 0.55859375 batch PCKh 0.25\n",
      "Trained batch 1006 batch loss 0.55744946 batch mAP 0.59576416 batch PCKh 0.375\n",
      "Trained batch 1007 batch loss 0.529591501 batch mAP 0.556884766 batch PCKh 0.25\n",
      "Trained batch 1008 batch loss 0.573741257 batch mAP 0.54574585 batch PCKh 0.625\n",
      "Trained batch 1009 batch loss 0.583987772 batch mAP 0.539550781 batch PCKh 0.875\n",
      "Trained batch 1010 batch loss 0.584892273 batch mAP 0.533813477 batch PCKh 0.5\n",
      "Trained batch 1011 batch loss 0.639544487 batch mAP 0.486083984 batch PCKh 0\n",
      "Trained batch 1012 batch loss 0.688360453 batch mAP 0.503540039 batch PCKh 0.25\n",
      "Trained batch 1013 batch loss 0.574305177 batch mAP 0.577514648 batch PCKh 0.4375\n",
      "Trained batch 1014 batch loss 0.598029 batch mAP 0.552032471 batch PCKh 0.375\n",
      "Trained batch 1015 batch loss 0.639753 batch mAP 0.532684326 batch PCKh 0.5\n",
      "Trained batch 1016 batch loss 0.556205273 batch mAP 0.599731445 batch PCKh 0.625\n",
      "Trained batch 1017 batch loss 0.604564071 batch mAP 0.563140869 batch PCKh 0.3125\n",
      "Trained batch 1018 batch loss 0.564948678 batch mAP 0.607330322 batch PCKh 0.5\n",
      "Trained batch 1019 batch loss 0.572317958 batch mAP 0.64654541 batch PCKh 0.3125\n",
      "Trained batch 1020 batch loss 0.577667117 batch mAP 0.57019043 batch PCKh 0.5625\n",
      "Trained batch 1021 batch loss 0.498782754 batch mAP 0.57043457 batch PCKh 0.5\n",
      "Trained batch 1022 batch loss 0.474513829 batch mAP 0.585022 batch PCKh 0.625\n",
      "Trained batch 1023 batch loss 0.556566179 batch mAP 0.513031 batch PCKh 0.75\n",
      "Trained batch 1024 batch loss 0.541999 batch mAP 0.538909912 batch PCKh 0.75\n",
      "Trained batch 1025 batch loss 0.501125455 batch mAP 0.536956787 batch PCKh 0.5625\n",
      "Trained batch 1026 batch loss 0.569657087 batch mAP 0.519989 batch PCKh 0.5\n",
      "Trained batch 1027 batch loss 0.530184746 batch mAP 0.510131836 batch PCKh 0.375\n",
      "Trained batch 1028 batch loss 0.602412939 batch mAP 0.45980835 batch PCKh 0.75\n",
      "Trained batch 1029 batch loss 0.697279632 batch mAP 0.515625 batch PCKh 0.5625\n",
      "Trained batch 1030 batch loss 0.664452434 batch mAP 0.554107666 batch PCKh 0.75\n",
      "Trained batch 1031 batch loss 0.544182897 batch mAP 0.531188965 batch PCKh 0.375\n",
      "Trained batch 1032 batch loss 0.589803874 batch mAP 0.541687 batch PCKh 0.5625\n",
      "Trained batch 1033 batch loss 0.570847869 batch mAP 0.520507812 batch PCKh 0.6875\n",
      "Trained batch 1034 batch loss 0.532679 batch mAP 0.497009277 batch PCKh 0.0625\n",
      "Trained batch 1035 batch loss 0.571775675 batch mAP 0.504333496 batch PCKh 0.625\n",
      "Trained batch 1036 batch loss 0.562502444 batch mAP 0.509887695 batch PCKh 0.4375\n",
      "Trained batch 1037 batch loss 0.585278 batch mAP 0.549743652 batch PCKh 0.625\n",
      "Trained batch 1038 batch loss 0.647010922 batch mAP 0.564086914 batch PCKh 0.75\n",
      "Trained batch 1039 batch loss 0.50085783 batch mAP 0.53024292 batch PCKh 0.25\n",
      "Trained batch 1040 batch loss 0.49015516 batch mAP 0.554962158 batch PCKh 0.25\n",
      "Trained batch 1041 batch loss 0.494182736 batch mAP 0.569152832 batch PCKh 0.75\n",
      "Trained batch 1042 batch loss 0.527492523 batch mAP 0.545105 batch PCKh 0.75\n",
      "Trained batch 1043 batch loss 0.513750672 batch mAP 0.59854126 batch PCKh 0.4375\n",
      "Trained batch 1044 batch loss 0.515180707 batch mAP 0.534606934 batch PCKh 0.3125\n",
      "Trained batch 1045 batch loss 0.533864141 batch mAP 0.57623291 batch PCKh 0.6875\n",
      "Trained batch 1046 batch loss 0.517747402 batch mAP 0.568634033 batch PCKh 0.5625\n",
      "Trained batch 1047 batch loss 0.515493393 batch mAP 0.581115723 batch PCKh 0.4375\n",
      "Trained batch 1048 batch loss 0.476930588 batch mAP 0.653381348 batch PCKh 0.75\n",
      "Trained batch 1049 batch loss 0.599175513 batch mAP 0.565826416 batch PCKh 0.3125\n",
      "Trained batch 1050 batch loss 0.630263209 batch mAP 0.513031 batch PCKh 0.75\n",
      "Trained batch 1051 batch loss 0.647953629 batch mAP 0.507507324 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1052 batch loss 0.549205542 batch mAP 0.5390625 batch PCKh 0.125\n",
      "Trained batch 1053 batch loss 0.60704565 batch mAP 0.520568848 batch PCKh 0.0625\n",
      "Trained batch 1054 batch loss 0.594818354 batch mAP 0.551269531 batch PCKh 0.75\n",
      "Trained batch 1055 batch loss 0.557346 batch mAP 0.566558838 batch PCKh 0.25\n",
      "Trained batch 1056 batch loss 0.567578793 batch mAP 0.514923096 batch PCKh 0.75\n",
      "Trained batch 1057 batch loss 0.535043657 batch mAP 0.534088135 batch PCKh 0.1875\n",
      "Trained batch 1058 batch loss 0.460758626 batch mAP 0.506408691 batch PCKh 0.25\n",
      "Trained batch 1059 batch loss 0.491170764 batch mAP 0.502807617 batch PCKh 0\n",
      "Trained batch 1060 batch loss 0.512290657 batch mAP 0.492950439 batch PCKh 0.1875\n",
      "Trained batch 1061 batch loss 0.602827311 batch mAP 0.573852539 batch PCKh 0.625\n",
      "Trained batch 1062 batch loss 0.527994514 batch mAP 0.566375732 batch PCKh 0.5625\n",
      "Trained batch 1063 batch loss 0.507961214 batch mAP 0.600616455 batch PCKh 0.5625\n",
      "Trained batch 1064 batch loss 0.570239723 batch mAP 0.569213867 batch PCKh 0.5625\n",
      "Trained batch 1065 batch loss 0.579676747 batch mAP 0.549987793 batch PCKh 0.0625\n",
      "Trained batch 1066 batch loss 0.637456477 batch mAP 0.541381836 batch PCKh 0.375\n",
      "Trained batch 1067 batch loss 0.590409517 batch mAP 0.568054199 batch PCKh 0.4375\n",
      "Trained batch 1068 batch loss 0.633874416 batch mAP 0.577636719 batch PCKh 0.25\n",
      "Trained batch 1069 batch loss 0.650988817 batch mAP 0.532928467 batch PCKh 0.4375\n",
      "Trained batch 1070 batch loss 0.5760414 batch mAP 0.607635498 batch PCKh 0.75\n",
      "Trained batch 1071 batch loss 0.600386262 batch mAP 0.559326172 batch PCKh 0.25\n",
      "Trained batch 1072 batch loss 0.575183153 batch mAP 0.545410156 batch PCKh 0.6875\n",
      "Trained batch 1073 batch loss 0.583676457 batch mAP 0.557006836 batch PCKh 0.75\n",
      "Trained batch 1074 batch loss 0.59309423 batch mAP 0.557983398 batch PCKh 0.3125\n",
      "Trained batch 1075 batch loss 0.541588843 batch mAP 0.600524902 batch PCKh 0.5625\n",
      "Trained batch 1076 batch loss 0.567371607 batch mAP 0.503753662 batch PCKh 0.375\n",
      "Trained batch 1077 batch loss 0.542059422 batch mAP 0.538818359 batch PCKh 0.25\n",
      "Trained batch 1078 batch loss 0.594736516 batch mAP 0.533905 batch PCKh 0.625\n",
      "Trained batch 1079 batch loss 0.568074644 batch mAP 0.529205322 batch PCKh 0.3125\n",
      "Trained batch 1080 batch loss 0.547572136 batch mAP 0.511901855 batch PCKh 0.0625\n",
      "Trained batch 1081 batch loss 0.601262093 batch mAP 0.525604248 batch PCKh 0.0625\n",
      "Trained batch 1082 batch loss 0.564672709 batch mAP 0.593536377 batch PCKh 0.6875\n",
      "Trained batch 1083 batch loss 0.630636811 batch mAP 0.52230835 batch PCKh 0.8125\n",
      "Trained batch 1084 batch loss 0.570066929 batch mAP 0.530029297 batch PCKh 0.3125\n",
      "Trained batch 1085 batch loss 0.577862382 batch mAP 0.536987305 batch PCKh 0.0625\n",
      "Trained batch 1086 batch loss 0.62106353 batch mAP 0.480224609 batch PCKh 0.5\n",
      "Trained batch 1087 batch loss 0.609532237 batch mAP 0.516204834 batch PCKh 0.1875\n",
      "Trained batch 1088 batch loss 0.567302942 batch mAP 0.552642822 batch PCKh 0.4375\n",
      "Trained batch 1089 batch loss 0.544784606 batch mAP 0.528991699 batch PCKh 0.5\n",
      "Trained batch 1090 batch loss 0.584066272 batch mAP 0.573303223 batch PCKh 0.6875\n",
      "Trained batch 1091 batch loss 0.573477507 batch mAP 0.55078125 batch PCKh 0.5625\n",
      "Trained batch 1092 batch loss 0.640682101 batch mAP 0.522369385 batch PCKh 0.3125\n",
      "Trained batch 1093 batch loss 0.531900525 batch mAP 0.576263428 batch PCKh 0.1875\n",
      "Trained batch 1094 batch loss 0.482666731 batch mAP 0.519348145 batch PCKh 0.125\n",
      "Trained batch 1095 batch loss 0.548820496 batch mAP 0.517913818 batch PCKh 0.5\n",
      "Trained batch 1096 batch loss 0.566394925 batch mAP 0.584564209 batch PCKh 0.25\n",
      "Trained batch 1097 batch loss 0.51603967 batch mAP 0.596405 batch PCKh 0.5625\n",
      "Trained batch 1098 batch loss 0.562278032 batch mAP 0.62902832 batch PCKh 0.6875\n",
      "Trained batch 1099 batch loss 0.624878705 batch mAP 0.596588135 batch PCKh 0.75\n",
      "Trained batch 1100 batch loss 0.511054516 batch mAP 0.589233398 batch PCKh 0.25\n",
      "Trained batch 1101 batch loss 0.515413523 batch mAP 0.557861328 batch PCKh 0.375\n",
      "Trained batch 1102 batch loss 0.535826147 batch mAP 0.545166 batch PCKh 0.25\n",
      "Trained batch 1103 batch loss 0.427579165 batch mAP 0.600738525 batch PCKh 0.25\n",
      "Trained batch 1104 batch loss 0.466969967 batch mAP 0.554992676 batch PCKh 0.3125\n",
      "Trained batch 1105 batch loss 0.449342668 batch mAP 0.553924561 batch PCKh 0.4375\n",
      "Trained batch 1106 batch loss 0.486092269 batch mAP 0.574859619 batch PCKh 0.5625\n",
      "Trained batch 1107 batch loss 0.530284703 batch mAP 0.553863525 batch PCKh 0.5\n",
      "Trained batch 1108 batch loss 0.589616895 batch mAP 0.538879395 batch PCKh 0.5625\n",
      "Trained batch 1109 batch loss 0.511553645 batch mAP 0.598388672 batch PCKh 0.5625\n",
      "Trained batch 1110 batch loss 0.551192403 batch mAP 0.523956299 batch PCKh 0.5\n",
      "Trained batch 1111 batch loss 0.482286096 batch mAP 0.529907227 batch PCKh 0.375\n",
      "Trained batch 1112 batch loss 0.510814428 batch mAP 0.598114 batch PCKh 0.5625\n",
      "Trained batch 1113 batch loss 0.553106248 batch mAP 0.55770874 batch PCKh 0.375\n",
      "Trained batch 1114 batch loss 0.467647672 batch mAP 0.53237915 batch PCKh 0.75\n",
      "Trained batch 1115 batch loss 0.526526928 batch mAP 0.557281494 batch PCKh 0.3125\n",
      "Trained batch 1116 batch loss 0.601637661 batch mAP 0.51083374 batch PCKh 0.1875\n",
      "Trained batch 1117 batch loss 0.510994077 batch mAP 0.593536377 batch PCKh 0.625\n",
      "Trained batch 1118 batch loss 0.492002964 batch mAP 0.571014404 batch PCKh 0.0625\n",
      "Trained batch 1119 batch loss 0.703700781 batch mAP 0.53717041 batch PCKh 0.125\n",
      "Trained batch 1120 batch loss 0.634517312 batch mAP 0.57220459 batch PCKh 0\n",
      "Trained batch 1121 batch loss 0.551244795 batch mAP 0.589386 batch PCKh 0.6875\n",
      "Trained batch 1122 batch loss 0.61206162 batch mAP 0.609893799 batch PCKh 0.25\n",
      "Trained batch 1123 batch loss 0.59707588 batch mAP 0.577636719 batch PCKh 0.1875\n",
      "Trained batch 1124 batch loss 0.599076927 batch mAP 0.626983643 batch PCKh 0.4375\n",
      "Trained batch 1125 batch loss 0.608176 batch mAP 0.60345459 batch PCKh 0.3125\n",
      "Trained batch 1126 batch loss 0.67354691 batch mAP 0.58605957 batch PCKh 0.375\n",
      "Trained batch 1127 batch loss 0.578831613 batch mAP 0.532714844 batch PCKh 0.375\n",
      "Trained batch 1128 batch loss 0.616183639 batch mAP 0.536499 batch PCKh 0.0625\n",
      "Trained batch 1129 batch loss 0.663750887 batch mAP 0.500824 batch PCKh 0\n",
      "Trained batch 1130 batch loss 0.6748752 batch mAP 0.528442383 batch PCKh 0.125\n",
      "Trained batch 1131 batch loss 0.681966066 batch mAP 0.550140381 batch PCKh 0.25\n",
      "Trained batch 1132 batch loss 0.62948513 batch mAP 0.514099121 batch PCKh 0.5\n",
      "Trained batch 1133 batch loss 0.58464551 batch mAP 0.529083252 batch PCKh 0.8125\n",
      "Trained batch 1134 batch loss 0.582285702 batch mAP 0.535858154 batch PCKh 0.8125\n",
      "Trained batch 1135 batch loss 0.664965868 batch mAP 0.471557617 batch PCKh 0.25\n",
      "Trained batch 1136 batch loss 0.668838918 batch mAP 0.489868164 batch PCKh 0.4375\n",
      "Trained batch 1137 batch loss 0.631560206 batch mAP 0.500030518 batch PCKh 0.4375\n",
      "Trained batch 1138 batch loss 0.632684052 batch mAP 0.511199951 batch PCKh 0.3125\n",
      "Trained batch 1139 batch loss 0.67128253 batch mAP 0.509521484 batch PCKh 0.3125\n",
      "Trained batch 1140 batch loss 0.572225034 batch mAP 0.525665283 batch PCKh 0\n",
      "Trained batch 1141 batch loss 0.567777812 batch mAP 0.564331055 batch PCKh 0.875\n",
      "Trained batch 1142 batch loss 0.611783624 batch mAP 0.555603 batch PCKh 0.3125\n",
      "Trained batch 1143 batch loss 0.549937725 batch mAP 0.537536621 batch PCKh 0.25\n",
      "Trained batch 1144 batch loss 0.545391679 batch mAP 0.613769531 batch PCKh 0.125\n",
      "Trained batch 1145 batch loss 0.620210767 batch mAP 0.561737061 batch PCKh 0.3125\n",
      "Trained batch 1146 batch loss 0.553929329 batch mAP 0.56427 batch PCKh 0.875\n",
      "Trained batch 1147 batch loss 0.572572827 batch mAP 0.528564453 batch PCKh 0\n",
      "Trained batch 1148 batch loss 0.539060175 batch mAP 0.565612793 batch PCKh 0.1875\n",
      "Trained batch 1149 batch loss 0.61070466 batch mAP 0.543609619 batch PCKh 0.5625\n",
      "Trained batch 1150 batch loss 0.572999358 batch mAP 0.578918457 batch PCKh 0.875\n",
      "Trained batch 1151 batch loss 0.595240712 batch mAP 0.572967529 batch PCKh 0.6875\n",
      "Trained batch 1152 batch loss 0.635200918 batch mAP 0.55770874 batch PCKh 0.5\n",
      "Trained batch 1153 batch loss 0.652896523 batch mAP 0.572021484 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1154 batch loss 0.652752876 batch mAP 0.545166 batch PCKh 0.25\n",
      "Trained batch 1155 batch loss 0.638172388 batch mAP 0.537323 batch PCKh 0.375\n",
      "Trained batch 1156 batch loss 0.53388 batch mAP 0.572753906 batch PCKh 0.625\n",
      "Trained batch 1157 batch loss 0.61710608 batch mAP 0.545593262 batch PCKh 0.5\n",
      "Trained batch 1158 batch loss 0.687470675 batch mAP 0.522949219 batch PCKh 0\n",
      "Trained batch 1159 batch loss 0.623846173 batch mAP 0.600036621 batch PCKh 0\n",
      "Trained batch 1160 batch loss 0.545458 batch mAP 0.630462646 batch PCKh 0.6875\n",
      "Trained batch 1161 batch loss 0.482617497 batch mAP 0.613433838 batch PCKh 0.4375\n",
      "Trained batch 1162 batch loss 0.486280054 batch mAP 0.598907471 batch PCKh 0.5\n",
      "Trained batch 1163 batch loss 0.492002755 batch mAP 0.59262085 batch PCKh 0.625\n",
      "Trained batch 1164 batch loss 0.525762916 batch mAP 0.564575195 batch PCKh 0.625\n",
      "Trained batch 1165 batch loss 0.573095322 batch mAP 0.543762207 batch PCKh 0.1875\n",
      "Trained batch 1166 batch loss 0.557623625 batch mAP 0.525787354 batch PCKh 0.0625\n",
      "Trained batch 1167 batch loss 0.634645462 batch mAP 0.514831543 batch PCKh 0.5625\n",
      "Trained batch 1168 batch loss 0.670492232 batch mAP 0.485992432 batch PCKh 0\n",
      "Trained batch 1169 batch loss 0.577410042 batch mAP 0.533325195 batch PCKh 0.375\n",
      "Trained batch 1170 batch loss 0.585669696 batch mAP 0.628662109 batch PCKh 0.25\n",
      "Trained batch 1171 batch loss 0.569873869 batch mAP 0.609619141 batch PCKh 0.625\n",
      "Trained batch 1172 batch loss 0.596995234 batch mAP 0.531799316 batch PCKh 0.3125\n",
      "Trained batch 1173 batch loss 0.563611 batch mAP 0.510528564 batch PCKh 0.5625\n",
      "Trained batch 1174 batch loss 0.509566307 batch mAP 0.489990234 batch PCKh 0.375\n",
      "Trained batch 1175 batch loss 0.53536427 batch mAP 0.507232666 batch PCKh 0.1875\n",
      "Trained batch 1176 batch loss 0.649176419 batch mAP 0.509796143 batch PCKh 0.0625\n",
      "Trained batch 1177 batch loss 0.580281734 batch mAP 0.500366211 batch PCKh 0\n",
      "Trained batch 1178 batch loss 0.638654113 batch mAP 0.531433105 batch PCKh 0.1875\n",
      "Trained batch 1179 batch loss 0.618814945 batch mAP 0.59564209 batch PCKh 0.75\n",
      "Trained batch 1180 batch loss 0.546973944 batch mAP 0.560852051 batch PCKh 0.6875\n",
      "Trained batch 1181 batch loss 0.571994126 batch mAP 0.498687744 batch PCKh 0.1875\n",
      "Trained batch 1182 batch loss 0.596279502 batch mAP 0.519287109 batch PCKh 0\n",
      "Trained batch 1183 batch loss 0.521327674 batch mAP 0.520050049 batch PCKh 0.75\n",
      "Trained batch 1184 batch loss 0.538421154 batch mAP 0.444519043 batch PCKh 0.75\n",
      "Trained batch 1185 batch loss 0.566712499 batch mAP 0.518157959 batch PCKh 0.75\n",
      "Trained batch 1186 batch loss 0.458274424 batch mAP 0.502380371 batch PCKh 0.5\n",
      "Trained batch 1187 batch loss 0.460917592 batch mAP 0.505950928 batch PCKh 0.5\n",
      "Trained batch 1188 batch loss 0.515200615 batch mAP 0.495574951 batch PCKh 0.6875\n",
      "Trained batch 1189 batch loss 0.562548935 batch mAP 0.528869629 batch PCKh 0.6875\n",
      "Trained batch 1190 batch loss 0.582962155 batch mAP 0.508544922 batch PCKh 0.875\n",
      "Trained batch 1191 batch loss 0.599602103 batch mAP 0.560913086 batch PCKh 0.3125\n",
      "Trained batch 1192 batch loss 0.535112083 batch mAP 0.576934814 batch PCKh 0.5625\n",
      "Trained batch 1193 batch loss 0.559844732 batch mAP 0.583984375 batch PCKh 0.3125\n",
      "Trained batch 1194 batch loss 0.571277738 batch mAP 0.571105957 batch PCKh 0.3125\n",
      "Trained batch 1195 batch loss 0.535662293 batch mAP 0.616729736 batch PCKh 0.4375\n",
      "Trained batch 1196 batch loss 0.490172267 batch mAP 0.534729 batch PCKh 0.125\n",
      "Trained batch 1197 batch loss 0.491047263 batch mAP 0.531829834 batch PCKh 0.75\n",
      "Trained batch 1198 batch loss 0.527131677 batch mAP 0.535339355 batch PCKh 0.5625\n",
      "Trained batch 1199 batch loss 0.495512635 batch mAP 0.48324585 batch PCKh 0.6875\n",
      "Trained batch 1200 batch loss 0.601938963 batch mAP 0.556610107 batch PCKh 0.625\n",
      "Trained batch 1201 batch loss 0.672759175 batch mAP 0.506317139 batch PCKh 0.625\n",
      "Trained batch 1202 batch loss 0.716010869 batch mAP 0.402069092 batch PCKh 0.0625\n",
      "Trained batch 1203 batch loss 0.655384898 batch mAP 0.519592285 batch PCKh 0.8125\n",
      "Trained batch 1204 batch loss 0.537663639 batch mAP 0.559204102 batch PCKh 0.375\n",
      "Trained batch 1205 batch loss 0.540426135 batch mAP 0.51348877 batch PCKh 0.6875\n",
      "Trained batch 1206 batch loss 0.592814922 batch mAP 0.515319824 batch PCKh 0.5\n",
      "Trained batch 1207 batch loss 0.612635255 batch mAP 0.45602417 batch PCKh 0.6875\n",
      "Trained batch 1208 batch loss 0.415910959 batch mAP 0.555786133 batch PCKh 0.3125\n",
      "Trained batch 1209 batch loss 0.478986531 batch mAP 0.531646729 batch PCKh 0.6875\n",
      "Trained batch 1210 batch loss 0.398243904 batch mAP 0.667572 batch PCKh 0.5\n",
      "Trained batch 1211 batch loss 0.370494038 batch mAP 0.724151611 batch PCKh 0.5\n",
      "Trained batch 1212 batch loss 0.408266723 batch mAP 0.667419434 batch PCKh 0.625\n",
      "Trained batch 1213 batch loss 0.376421124 batch mAP 0.710968 batch PCKh 0.5\n",
      "Trained batch 1214 batch loss 0.472532302 batch mAP 0.615783691 batch PCKh 0.375\n",
      "Trained batch 1215 batch loss 0.517102838 batch mAP 0.596740723 batch PCKh 0.75\n",
      "Trained batch 1216 batch loss 0.506297171 batch mAP 0.543273926 batch PCKh 0.6875\n",
      "Trained batch 1217 batch loss 0.56281811 batch mAP 0.582885742 batch PCKh 0.625\n",
      "Trained batch 1218 batch loss 0.491504431 batch mAP 0.6434021 batch PCKh 0.4375\n",
      "Trained batch 1219 batch loss 0.539463401 batch mAP 0.615081787 batch PCKh 0.5\n",
      "Trained batch 1220 batch loss 0.56496489 batch mAP 0.635101318 batch PCKh 0.625\n",
      "Trained batch 1221 batch loss 0.502321 batch mAP 0.602996826 batch PCKh 0.25\n",
      "Trained batch 1222 batch loss 0.515082836 batch mAP 0.554718 batch PCKh 0.4375\n",
      "Trained batch 1223 batch loss 0.493919224 batch mAP 0.648071289 batch PCKh 0.625\n",
      "Trained batch 1224 batch loss 0.545551538 batch mAP 0.586914062 batch PCKh 0.3125\n",
      "Trained batch 1225 batch loss 0.43964529 batch mAP 0.63269043 batch PCKh 0.375\n",
      "Trained batch 1226 batch loss 0.576984286 batch mAP 0.571380615 batch PCKh 0.3125\n",
      "Trained batch 1227 batch loss 0.613689661 batch mAP 0.520294189 batch PCKh 0.5\n",
      "Trained batch 1228 batch loss 0.540534854 batch mAP 0.576904297 batch PCKh 0.3125\n",
      "Trained batch 1229 batch loss 0.551619649 batch mAP 0.558624268 batch PCKh 0.0625\n",
      "Trained batch 1230 batch loss 0.521405399 batch mAP 0.51953125 batch PCKh 0\n",
      "Trained batch 1231 batch loss 0.534522533 batch mAP 0.401886 batch PCKh 0.1875\n",
      "Trained batch 1232 batch loss 0.523685396 batch mAP 0.439453125 batch PCKh 0.5\n",
      "Trained batch 1233 batch loss 0.492041647 batch mAP 0.418762207 batch PCKh 0.25\n",
      "Trained batch 1234 batch loss 0.391786218 batch mAP 0.449981689 batch PCKh 0\n",
      "Trained batch 1235 batch loss 0.399401605 batch mAP 0.450927734 batch PCKh 0\n",
      "Trained batch 1236 batch loss 0.483847976 batch mAP 0.408294678 batch PCKh 0.25\n",
      "Trained batch 1237 batch loss 0.481366515 batch mAP 0.406585693 batch PCKh 0\n",
      "Trained batch 1238 batch loss 0.631618559 batch mAP 0.391418457 batch PCKh 0.125\n",
      "Trained batch 1239 batch loss 0.612827539 batch mAP 0.436004639 batch PCKh 0.3125\n",
      "Trained batch 1240 batch loss 0.526128948 batch mAP 0.489807129 batch PCKh 0.5\n",
      "Trained batch 1241 batch loss 0.600306511 batch mAP 0.425445557 batch PCKh 0.6875\n",
      "Trained batch 1242 batch loss 0.544012189 batch mAP 0.44631958 batch PCKh 0.0625\n",
      "Trained batch 1243 batch loss 0.540426254 batch mAP 0.432159424 batch PCKh 0.0625\n",
      "Trained batch 1244 batch loss 0.554104686 batch mAP 0.473236084 batch PCKh 0.5625\n",
      "Trained batch 1245 batch loss 0.512213707 batch mAP 0.504547119 batch PCKh 0.875\n",
      "Trained batch 1246 batch loss 0.579717219 batch mAP 0.518463135 batch PCKh 0.875\n",
      "Trained batch 1247 batch loss 0.552446544 batch mAP 0.498321533 batch PCKh 0.5\n",
      "Trained batch 1248 batch loss 0.460763276 batch mAP 0.426177979 batch PCKh 0.5\n",
      "Trained batch 1249 batch loss 0.492834419 batch mAP 0.401611328 batch PCKh 0.5\n",
      "Trained batch 1250 batch loss 0.535041094 batch mAP 0.375091553 batch PCKh 0.5\n",
      "Trained batch 1251 batch loss 0.557522595 batch mAP 0.386016846 batch PCKh 0.4375\n",
      "Trained batch 1252 batch loss 0.566204667 batch mAP 0.381866455 batch PCKh 0.5\n",
      "Trained batch 1253 batch loss 0.540720105 batch mAP 0.46975708 batch PCKh 0\n",
      "Trained batch 1254 batch loss 0.503383815 batch mAP 0.419433594 batch PCKh 0.6875\n",
      "Trained batch 1255 batch loss 0.583310127 batch mAP 0.380859375 batch PCKh 0\n",
      "Trained batch 1256 batch loss 0.644882679 batch mAP 0.43170166 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1257 batch loss 0.571922243 batch mAP 0.493988037 batch PCKh 0.6875\n",
      "Trained batch 1258 batch loss 0.520286322 batch mAP 0.559143066 batch PCKh 0.3125\n",
      "Trained batch 1259 batch loss 0.476753563 batch mAP 0.551025391 batch PCKh 0\n",
      "Trained batch 1260 batch loss 0.441833854 batch mAP 0.525390625 batch PCKh 0.125\n",
      "Trained batch 1261 batch loss 0.440399528 batch mAP 0.538787842 batch PCKh 0\n",
      "Trained batch 1262 batch loss 0.410891712 batch mAP 0.562286377 batch PCKh 0\n",
      "Trained batch 1263 batch loss 0.518631101 batch mAP 0.524841309 batch PCKh 0.75\n",
      "Trained batch 1264 batch loss 0.532318354 batch mAP 0.524841309 batch PCKh 0.1875\n",
      "Trained batch 1265 batch loss 0.657042 batch mAP 0.530395508 batch PCKh 0.5625\n",
      "Trained batch 1266 batch loss 0.655665398 batch mAP 0.496673584 batch PCKh 0.6875\n",
      "Trained batch 1267 batch loss 0.663109958 batch mAP 0.510467529 batch PCKh 0.25\n",
      "Trained batch 1268 batch loss 0.684299 batch mAP 0.507995605 batch PCKh 0.3125\n",
      "Trained batch 1269 batch loss 0.830875039 batch mAP 0.411682129 batch PCKh 0.0625\n",
      "Trained batch 1270 batch loss 0.603276908 batch mAP 0.546539307 batch PCKh 0.1875\n",
      "Trained batch 1271 batch loss 0.536405 batch mAP 0.540374756 batch PCKh 0.125\n",
      "Trained batch 1272 batch loss 0.612615108 batch mAP 0.430419922 batch PCKh 0.125\n",
      "Trained batch 1273 batch loss 0.503686488 batch mAP 0.462127686 batch PCKh 0.5\n",
      "Trained batch 1274 batch loss 0.573036134 batch mAP 0.413421631 batch PCKh 0.5625\n",
      "Trained batch 1275 batch loss 0.617640257 batch mAP 0.33480835 batch PCKh 0.25\n",
      "Trained batch 1276 batch loss 0.642408371 batch mAP 0.361846924 batch PCKh 0.3125\n",
      "Trained batch 1277 batch loss 0.630296886 batch mAP 0.320617676 batch PCKh 0.375\n",
      "Trained batch 1278 batch loss 0.587391257 batch mAP 0.281555176 batch PCKh 0.625\n",
      "Trained batch 1279 batch loss 0.605854154 batch mAP 0.240631104 batch PCKh 0.3125\n",
      "Trained batch 1280 batch loss 0.595262289 batch mAP 0.345703125 batch PCKh 0.375\n",
      "Trained batch 1281 batch loss 0.614223957 batch mAP 0.406097412 batch PCKh 0.6875\n",
      "Trained batch 1282 batch loss 0.570767939 batch mAP 0.463989258 batch PCKh 0.1875\n",
      "Trained batch 1283 batch loss 0.602819264 batch mAP 0.523101807 batch PCKh 0.375\n",
      "Trained batch 1284 batch loss 0.585045934 batch mAP 0.505950928 batch PCKh 0.75\n",
      "Trained batch 1285 batch loss 0.649359107 batch mAP 0.449829102 batch PCKh 0.1875\n",
      "Trained batch 1286 batch loss 0.558713555 batch mAP 0.504669189 batch PCKh 0.625\n",
      "Trained batch 1287 batch loss 0.558365226 batch mAP 0.533508301 batch PCKh 0.375\n",
      "Trained batch 1288 batch loss 0.509431779 batch mAP 0.575408936 batch PCKh 0.375\n",
      "Trained batch 1289 batch loss 0.610018253 batch mAP 0.52230835 batch PCKh 0.625\n",
      "Trained batch 1290 batch loss 0.566113114 batch mAP 0.550170898 batch PCKh 0.3125\n",
      "Trained batch 1291 batch loss 0.494888544 batch mAP 0.636657715 batch PCKh 0.4375\n",
      "Trained batch 1292 batch loss 0.527564168 batch mAP 0.595489502 batch PCKh 0.1875\n",
      "Trained batch 1293 batch loss 0.520892739 batch mAP 0.518829346 batch PCKh 0.375\n",
      "Trained batch 1294 batch loss 0.547142327 batch mAP 0.638214111 batch PCKh 0.6875\n",
      "Trained batch 1295 batch loss 0.535624743 batch mAP 0.641967773 batch PCKh 0.25\n",
      "Trained batch 1296 batch loss 0.625435293 batch mAP 0.560394287 batch PCKh 0.6875\n",
      "Trained batch 1297 batch loss 0.638034225 batch mAP 0.610107422 batch PCKh 0.0625\n",
      "Trained batch 1298 batch loss 0.537517726 batch mAP 0.59979248 batch PCKh 0.5625\n",
      "Trained batch 1299 batch loss 0.494969606 batch mAP 0.602325439 batch PCKh 0.75\n",
      "Trained batch 1300 batch loss 0.470192581 batch mAP 0.52532959 batch PCKh 0.25\n",
      "Trained batch 1301 batch loss 0.489584863 batch mAP 0.54006958 batch PCKh 0.125\n",
      "Trained batch 1302 batch loss 0.532313466 batch mAP 0.550720215 batch PCKh 0.375\n",
      "Trained batch 1303 batch loss 0.496463478 batch mAP 0.55065918 batch PCKh 0.25\n",
      "Trained batch 1304 batch loss 0.632123291 batch mAP 0.487884521 batch PCKh 0.375\n",
      "Trained batch 1305 batch loss 0.59015131 batch mAP 0.518707275 batch PCKh 0.8125\n",
      "Trained batch 1306 batch loss 0.563678324 batch mAP 0.553131104 batch PCKh 0.4375\n",
      "Trained batch 1307 batch loss 0.600361943 batch mAP 0.527313232 batch PCKh 0.6875\n",
      "Trained batch 1308 batch loss 0.603024721 batch mAP 0.493164062 batch PCKh 0.125\n",
      "Trained batch 1309 batch loss 0.625015199 batch mAP 0.425323486 batch PCKh 0.375\n",
      "Trained batch 1310 batch loss 0.649005949 batch mAP 0.433410645 batch PCKh 0.5\n",
      "Trained batch 1311 batch loss 0.572369277 batch mAP 0.440094 batch PCKh 0.4375\n",
      "Trained batch 1312 batch loss 0.477934241 batch mAP 0.323547363 batch PCKh 0.5625\n",
      "Trained batch 1313 batch loss 0.472367972 batch mAP 0.306274414 batch PCKh 0.4375\n",
      "Trained batch 1314 batch loss 0.456609309 batch mAP 0.381713867 batch PCKh 0\n",
      "Trained batch 1315 batch loss 0.555461943 batch mAP 0.364105225 batch PCKh 0.5625\n",
      "Trained batch 1316 batch loss 0.567934036 batch mAP 0.342468262 batch PCKh 0.625\n",
      "Trained batch 1317 batch loss 0.643571138 batch mAP 0.354400635 batch PCKh 0.625\n",
      "Trained batch 1318 batch loss 0.620862544 batch mAP 0.278289795 batch PCKh 0.75\n",
      "Trained batch 1319 batch loss 0.656009316 batch mAP 0.334381104 batch PCKh 0.3125\n",
      "Trained batch 1320 batch loss 0.624898672 batch mAP 0.25402832 batch PCKh 0.5625\n",
      "Trained batch 1321 batch loss 0.670435071 batch mAP 0.227233887 batch PCKh 0.3125\n",
      "Trained batch 1322 batch loss 0.573001742 batch mAP 0.308654785 batch PCKh 0.625\n",
      "Trained batch 1323 batch loss 0.601527572 batch mAP 0.432678223 batch PCKh 0.375\n",
      "Trained batch 1324 batch loss 0.709651768 batch mAP 0.509155273 batch PCKh 0.125\n",
      "Trained batch 1325 batch loss 0.660532534 batch mAP 0.482666016 batch PCKh 0.375\n",
      "Trained batch 1326 batch loss 0.666149914 batch mAP 0.547088623 batch PCKh 0.125\n",
      "Trained batch 1327 batch loss 0.618550599 batch mAP 0.545593262 batch PCKh 0.25\n",
      "Trained batch 1328 batch loss 0.528116941 batch mAP 0.567626953 batch PCKh 0.3125\n",
      "Trained batch 1329 batch loss 0.65544194 batch mAP 0.577026367 batch PCKh 0.125\n",
      "Trained batch 1330 batch loss 0.620533 batch mAP 0.541931152 batch PCKh 0.75\n",
      "Trained batch 1331 batch loss 0.594307423 batch mAP 0.506835938 batch PCKh 0.4375\n",
      "Trained batch 1332 batch loss 0.585005701 batch mAP 0.53704834 batch PCKh 0.5625\n",
      "Trained batch 1333 batch loss 0.587678134 batch mAP 0.568786621 batch PCKh 0.6875\n",
      "Trained batch 1334 batch loss 0.615756273 batch mAP 0.517303467 batch PCKh 0.6875\n",
      "Trained batch 1335 batch loss 0.657109857 batch mAP 0.505615234 batch PCKh 0.25\n",
      "Trained batch 1336 batch loss 0.557536483 batch mAP 0.57333374 batch PCKh 0.875\n",
      "Trained batch 1337 batch loss 0.590386271 batch mAP 0.448638916 batch PCKh 0.0625\n",
      "Trained batch 1338 batch loss 0.725286245 batch mAP 0.455841064 batch PCKh 0\n",
      "Trained batch 1339 batch loss 0.622097731 batch mAP 0.552703857 batch PCKh 0.6875\n",
      "Trained batch 1340 batch loss 0.605505526 batch mAP 0.485015869 batch PCKh 0.1875\n",
      "Trained batch 1341 batch loss 0.588335514 batch mAP 0.45324707 batch PCKh 0.625\n",
      "Trained batch 1342 batch loss 0.571998775 batch mAP 0.494537354 batch PCKh 0.375\n",
      "Trained batch 1343 batch loss 0.630215406 batch mAP 0.444458 batch PCKh 0.25\n",
      "Trained batch 1344 batch loss 0.652988434 batch mAP 0.397491455 batch PCKh 0.125\n",
      "Trained batch 1345 batch loss 0.545788765 batch mAP 0.454925537 batch PCKh 0.5625\n",
      "Trained batch 1346 batch loss 0.528479755 batch mAP 0.462127686 batch PCKh 0.125\n",
      "Trained batch 1347 batch loss 0.597420573 batch mAP 0.432617188 batch PCKh 0\n",
      "Trained batch 1348 batch loss 0.652629614 batch mAP 0.513336182 batch PCKh 0\n",
      "Trained batch 1349 batch loss 0.714828074 batch mAP 0.512695312 batch PCKh 0.1875\n",
      "Trained batch 1350 batch loss 0.62054193 batch mAP 0.593902588 batch PCKh 0.375\n",
      "Trained batch 1351 batch loss 0.676368296 batch mAP 0.553375244 batch PCKh 0.1875\n",
      "Trained batch 1352 batch loss 0.681431532 batch mAP 0.551330566 batch PCKh 0.3125\n",
      "Trained batch 1353 batch loss 0.659265757 batch mAP 0.500427246 batch PCKh 0.25\n",
      "Trained batch 1354 batch loss 0.681153774 batch mAP 0.439483643 batch PCKh 0.6875\n",
      "Trained batch 1355 batch loss 0.6009745 batch mAP 0.376861572 batch PCKh 0.625\n",
      "Trained batch 1356 batch loss 0.612985134 batch mAP 0.356506348 batch PCKh 0.625\n",
      "Trained batch 1357 batch loss 0.648186445 batch mAP 0.34942627 batch PCKh 0.875\n",
      "Trained batch 1358 batch loss 0.574877441 batch mAP 0.424469 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1359 batch loss 0.580502748 batch mAP 0.416534424 batch PCKh 0.375\n",
      "Trained batch 1360 batch loss 0.558346152 batch mAP 0.496521 batch PCKh 0.5625\n",
      "Trained batch 1361 batch loss 0.60804081 batch mAP 0.456176758 batch PCKh 0\n",
      "Trained batch 1362 batch loss 0.616302252 batch mAP 0.438049316 batch PCKh 0.5\n",
      "Trained batch 1363 batch loss 0.631587327 batch mAP 0.516113281 batch PCKh 0.125\n",
      "Trained batch 1364 batch loss 0.624695778 batch mAP 0.609039307 batch PCKh 0.3125\n",
      "Trained batch 1365 batch loss 0.544065654 batch mAP 0.570678711 batch PCKh 0.1875\n",
      "Trained batch 1366 batch loss 0.581366777 batch mAP 0.528564453 batch PCKh 0.8125\n",
      "Trained batch 1367 batch loss 0.613661289 batch mAP 0.533325195 batch PCKh 0.4375\n",
      "Trained batch 1368 batch loss 0.576580942 batch mAP 0.534820557 batch PCKh 0.4375\n",
      "Trained batch 1369 batch loss 0.634164751 batch mAP 0.511108398 batch PCKh 0.5625\n",
      "Trained batch 1370 batch loss 0.562234879 batch mAP 0.520294189 batch PCKh 0.375\n",
      "Trained batch 1371 batch loss 0.580637217 batch mAP 0.444061279 batch PCKh 0.8125\n",
      "Trained batch 1372 batch loss 0.587943435 batch mAP 0.467498779 batch PCKh 0.1875\n",
      "Trained batch 1373 batch loss 0.502481043 batch mAP 0.486816406 batch PCKh 0.0625\n",
      "Trained batch 1374 batch loss 0.566861928 batch mAP 0.542022705 batch PCKh 0.3125\n",
      "Trained batch 1375 batch loss 0.49890998 batch mAP 0.540802 batch PCKh 0.4375\n",
      "Trained batch 1376 batch loss 0.498499483 batch mAP 0.473144531 batch PCKh 0.875\n",
      "Trained batch 1377 batch loss 0.47470206 batch mAP 0.509063721 batch PCKh 0.625\n",
      "Trained batch 1378 batch loss 0.46581322 batch mAP 0.506469727 batch PCKh 0.5625\n",
      "Trained batch 1379 batch loss 0.479717344 batch mAP 0.581329346 batch PCKh 0.75\n",
      "Trained batch 1380 batch loss 0.552556574 batch mAP 0.532836914 batch PCKh 0.875\n",
      "Trained batch 1381 batch loss 0.610579491 batch mAP 0.463256836 batch PCKh 0.5625\n",
      "Trained batch 1382 batch loss 0.670556188 batch mAP 0.500183105 batch PCKh 0.125\n",
      "Trained batch 1383 batch loss 0.573311329 batch mAP 0.500366211 batch PCKh 0.75\n",
      "Trained batch 1384 batch loss 0.603615 batch mAP 0.47052002 batch PCKh 0.8125\n",
      "Trained batch 1385 batch loss 0.685416341 batch mAP 0.462799072 batch PCKh 0.1875\n",
      "Trained batch 1386 batch loss 0.672780633 batch mAP 0.489624023 batch PCKh 0.3125\n",
      "Trained batch 1387 batch loss 0.618017912 batch mAP 0.488464355 batch PCKh 0.3125\n",
      "Trained batch 1388 batch loss 0.631369591 batch mAP 0.50289917 batch PCKh 0.25\n",
      "Trained batch 1389 batch loss 0.647495568 batch mAP 0.473236084 batch PCKh 0.4375\n",
      "Trained batch 1390 batch loss 0.655522943 batch mAP 0.453491211 batch PCKh 0.625\n",
      "Trained batch 1391 batch loss 0.624834239 batch mAP 0.460327148 batch PCKh 0.0625\n",
      "Trained batch 1392 batch loss 0.601556778 batch mAP 0.461730957 batch PCKh 0.25\n",
      "Trained batch 1393 batch loss 0.683447778 batch mAP 0.462677 batch PCKh 0.8125\n",
      "Trained batch 1394 batch loss 0.569393039 batch mAP 0.483642578 batch PCKh 0.6875\n",
      "Trained batch 1395 batch loss 0.6232059 batch mAP 0.441192627 batch PCKh 0.5625\n",
      "Trained batch 1396 batch loss 0.597493291 batch mAP 0.525848389 batch PCKh 0.5625\n",
      "Trained batch 1397 batch loss 0.592546 batch mAP 0.514770508 batch PCKh 0.375\n",
      "Trained batch 1398 batch loss 0.644805491 batch mAP 0.541473389 batch PCKh 0.8125\n",
      "Trained batch 1399 batch loss 0.689460754 batch mAP 0.5390625 batch PCKh 0.75\n",
      "Trained batch 1400 batch loss 0.641474 batch mAP 0.526550293 batch PCKh 0.75\n",
      "Trained batch 1401 batch loss 0.505650103 batch mAP 0.567169189 batch PCKh 0.3125\n",
      "Trained batch 1402 batch loss 0.539970815 batch mAP 0.599273682 batch PCKh 0.4375\n",
      "Trained batch 1403 batch loss 0.542981267 batch mAP 0.559936523 batch PCKh 0.4375\n",
      "Trained batch 1404 batch loss 0.609391451 batch mAP 0.490875244 batch PCKh 0\n",
      "Trained batch 1405 batch loss 0.507625937 batch mAP 0.480682373 batch PCKh 0.1875\n",
      "Trained batch 1406 batch loss 0.513264537 batch mAP 0.513122559 batch PCKh 0.125\n",
      "Trained batch 1407 batch loss 0.400120735 batch mAP 0.49230957 batch PCKh 0.5625\n",
      "Trained batch 1408 batch loss 0.546779037 batch mAP 0.496887207 batch PCKh 0.375\n",
      "Trained batch 1409 batch loss 0.477168679 batch mAP 0.522460938 batch PCKh 0.75\n",
      "Trained batch 1410 batch loss 0.42981419 batch mAP 0.540283203 batch PCKh 0.5\n",
      "Trained batch 1411 batch loss 0.509044588 batch mAP 0.480407715 batch PCKh 0.6875\n",
      "Trained batch 1412 batch loss 0.561928928 batch mAP 0.452911377 batch PCKh 0.875\n",
      "Trained batch 1413 batch loss 0.529412091 batch mAP 0.473175049 batch PCKh 0.75\n",
      "Trained batch 1414 batch loss 0.495589674 batch mAP 0.51373291 batch PCKh 0.4375\n",
      "Trained batch 1415 batch loss 0.426634371 batch mAP 0.572845459 batch PCKh 0.75\n",
      "Trained batch 1416 batch loss 0.383468091 batch mAP 0.597381592 batch PCKh 0.5\n",
      "Trained batch 1417 batch loss 0.427591354 batch mAP 0.574768066 batch PCKh 0\n",
      "Trained batch 1418 batch loss 0.469371319 batch mAP 0.526153564 batch PCKh 0\n",
      "Trained batch 1419 batch loss 0.380227804 batch mAP 0.581451416 batch PCKh 0\n",
      "Trained batch 1420 batch loss 0.370782554 batch mAP 0.584991455 batch PCKh 0.3125\n",
      "Trained batch 1421 batch loss 0.485967577 batch mAP 0.533508301 batch PCKh 0.5625\n",
      "Trained batch 1422 batch loss 0.514933348 batch mAP 0.53503418 batch PCKh 0.375\n",
      "Trained batch 1423 batch loss 0.522152662 batch mAP 0.564483643 batch PCKh 0.5\n",
      "Trained batch 1424 batch loss 0.615965486 batch mAP 0.507110596 batch PCKh 0.4375\n",
      "Trained batch 1425 batch loss 0.514157295 batch mAP 0.608551 batch PCKh 0.375\n",
      "Trained batch 1426 batch loss 0.600400805 batch mAP 0.570465088 batch PCKh 0.375\n",
      "Trained batch 1427 batch loss 0.563064098 batch mAP 0.583068848 batch PCKh 0.1875\n",
      "Trained batch 1428 batch loss 0.559949517 batch mAP 0.611236572 batch PCKh 0.25\n",
      "Trained batch 1429 batch loss 0.507523417 batch mAP 0.576629639 batch PCKh 0.4375\n",
      "Trained batch 1430 batch loss 0.569903 batch mAP 0.561676 batch PCKh 0.4375\n",
      "Trained batch 1431 batch loss 0.517847776 batch mAP 0.584716797 batch PCKh 0.5625\n",
      "Trained batch 1432 batch loss 0.560647845 batch mAP 0.54284668 batch PCKh 0.1875\n",
      "Trained batch 1433 batch loss 0.686859846 batch mAP 0.528198242 batch PCKh 0.125\n",
      "Trained batch 1434 batch loss 0.647304952 batch mAP 0.480926514 batch PCKh 0.375\n",
      "Trained batch 1435 batch loss 0.505063951 batch mAP 0.577972412 batch PCKh 0.1875\n",
      "Trained batch 1436 batch loss 0.445723593 batch mAP 0.557342529 batch PCKh 0.3125\n",
      "Trained batch 1437 batch loss 0.64398849 batch mAP 0.501922607 batch PCKh 0.5625\n",
      "Trained batch 1438 batch loss 0.616591454 batch mAP 0.529937744 batch PCKh 0.1875\n",
      "Trained batch 1439 batch loss 0.638760686 batch mAP 0.454345703 batch PCKh 0.3125\n",
      "Trained batch 1440 batch loss 0.612330616 batch mAP 0.435821533 batch PCKh 0.1875\n",
      "Trained batch 1441 batch loss 0.622793734 batch mAP 0.318237305 batch PCKh 0.25\n",
      "Trained batch 1442 batch loss 0.537505627 batch mAP 0.142608643 batch PCKh 0.125\n",
      "Trained batch 1443 batch loss 0.469190836 batch mAP 0.0817871094 batch PCKh 0.125\n",
      "Trained batch 1444 batch loss 0.505743623 batch mAP 0.0723266602 batch PCKh 0.0625\n",
      "Trained batch 1445 batch loss 0.660593629 batch mAP 0.0873413086 batch PCKh 0.125\n",
      "Trained batch 1446 batch loss 0.673714221 batch mAP 0.220031738 batch PCKh 0.8125\n",
      "Trained batch 1447 batch loss 0.670341372 batch mAP 0.358886719 batch PCKh 0.3125\n",
      "Trained batch 1448 batch loss 0.630352259 batch mAP 0.434906 batch PCKh 0.6875\n",
      "Trained batch 1449 batch loss 0.567943275 batch mAP 0.586456299 batch PCKh 0.1875\n",
      "Trained batch 1450 batch loss 0.57677722 batch mAP 0.578033447 batch PCKh 0.4375\n",
      "Trained batch 1451 batch loss 0.517832518 batch mAP 0.622894287 batch PCKh 0.4375\n",
      "Trained batch 1452 batch loss 0.444361031 batch mAP 0.649200439 batch PCKh 0.375\n",
      "Trained batch 1453 batch loss 0.450950086 batch mAP 0.659423828 batch PCKh 0.375\n",
      "Trained batch 1454 batch loss 0.487407357 batch mAP 0.563934326 batch PCKh 0.5625\n",
      "Trained batch 1455 batch loss 0.505083203 batch mAP 0.497619629 batch PCKh 0.4375\n",
      "Trained batch 1456 batch loss 0.482888818 batch mAP 0.451904297 batch PCKh 0.375\n",
      "Trained batch 1457 batch loss 0.495992243 batch mAP 0.478149414 batch PCKh 0.4375\n",
      "Trained batch 1458 batch loss 0.487160563 batch mAP 0.553619385 batch PCKh 0.375\n",
      "Trained batch 1459 batch loss 0.476928174 batch mAP 0.579345703 batch PCKh 0.375\n",
      "Trained batch 1460 batch loss 0.491918653 batch mAP 0.614898682 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1461 batch loss 0.471209526 batch mAP 0.537384033 batch PCKh 0.0625\n",
      "Trained batch 1462 batch loss 0.544319153 batch mAP 0.473083496 batch PCKh 0.375\n",
      "Trained batch 1463 batch loss 0.514905572 batch mAP 0.538482666 batch PCKh 0.375\n",
      "Trained batch 1464 batch loss 0.528211117 batch mAP 0.58078 batch PCKh 0.75\n",
      "Trained batch 1465 batch loss 0.542764783 batch mAP 0.532959 batch PCKh 0.875\n",
      "Trained batch 1466 batch loss 0.599161506 batch mAP 0.537689209 batch PCKh 0.75\n",
      "Trained batch 1467 batch loss 0.503715932 batch mAP 0.606628418 batch PCKh 0.8125\n",
      "Trained batch 1468 batch loss 0.567336 batch mAP 0.491607666 batch PCKh 0.875\n",
      "Trained batch 1469 batch loss 0.573712 batch mAP 0.527435303 batch PCKh 0.75\n",
      "Trained batch 1470 batch loss 0.616035759 batch mAP 0.520965576 batch PCKh 0.875\n",
      "Trained batch 1471 batch loss 0.584781766 batch mAP 0.506164551 batch PCKh 0.75\n",
      "Trained batch 1472 batch loss 0.565328062 batch mAP 0.484436035 batch PCKh 0.75\n",
      "Trained batch 1473 batch loss 0.603880882 batch mAP 0.508728 batch PCKh 0.3125\n",
      "Trained batch 1474 batch loss 0.572845578 batch mAP 0.494384766 batch PCKh 0.5\n",
      "Trained batch 1475 batch loss 0.634858668 batch mAP 0.459136963 batch PCKh 0.25\n",
      "Trained batch 1476 batch loss 0.613508 batch mAP 0.387542725 batch PCKh 0.0625\n",
      "Trained batch 1477 batch loss 0.461783409 batch mAP 0.414154053 batch PCKh 0.3125\n",
      "Trained batch 1478 batch loss 0.52304244 batch mAP 0.498443604 batch PCKh 0\n",
      "Trained batch 1479 batch loss 0.614882588 batch mAP 0.425292969 batch PCKh 0.0625\n",
      "Trained batch 1480 batch loss 0.605318666 batch mAP 0.398986816 batch PCKh 0.5\n",
      "Trained batch 1481 batch loss 0.531358361 batch mAP 0.408538818 batch PCKh 0\n",
      "Trained batch 1482 batch loss 0.609979212 batch mAP 0.42288208 batch PCKh 0.25\n",
      "Trained batch 1483 batch loss 0.629186869 batch mAP 0.436706543 batch PCKh 0.75\n",
      "Trained batch 1484 batch loss 0.674743354 batch mAP 0.457336426 batch PCKh 0.5625\n",
      "Trained batch 1485 batch loss 0.661600709 batch mAP 0.481689453 batch PCKh 0.875\n",
      "Trained batch 1486 batch loss 0.677042484 batch mAP 0.470306396 batch PCKh 0.75\n",
      "Trained batch 1487 batch loss 0.669753253 batch mAP 0.451629639 batch PCKh 0.0625\n",
      "Trained batch 1488 batch loss 0.607986748 batch mAP 0.480865479 batch PCKh 0.25\n",
      "Trained batch 1489 batch loss 0.639315546 batch mAP 0.539764404 batch PCKh 0.75\n",
      "Trained batch 1490 batch loss 0.556011915 batch mAP 0.508087158 batch PCKh 0.75\n",
      "Trained batch 1491 batch loss 0.60533452 batch mAP 0.50579834 batch PCKh 0.8125\n",
      "Trained batch 1492 batch loss 0.610259414 batch mAP 0.480255127 batch PCKh 0.625\n",
      "Trained batch 1493 batch loss 0.531501174 batch mAP 0.550384521 batch PCKh 0.6875\n",
      "Trained batch 1494 batch loss 0.568881691 batch mAP 0.494445801 batch PCKh 0.625\n",
      "Trained batch 1495 batch loss 0.628816962 batch mAP 0.482299805 batch PCKh 0.5\n",
      "Trained batch 1496 batch loss 0.602753162 batch mAP 0.547485352 batch PCKh 0.3125\n",
      "Trained batch 1497 batch loss 0.559186697 batch mAP 0.581817627 batch PCKh 0.5625\n",
      "Trained batch 1498 batch loss 0.613489032 batch mAP 0.60635376 batch PCKh 0.25\n",
      "Trained batch 1499 batch loss 0.632303178 batch mAP 0.578582764 batch PCKh 0.8125\n",
      "Trained batch 1500 batch loss 0.619438171 batch mAP 0.528259277 batch PCKh 0.5\n",
      "Trained batch 1501 batch loss 0.604067624 batch mAP 0.570159912 batch PCKh 0.3125\n",
      "Trained batch 1502 batch loss 0.674905658 batch mAP 0.445800781 batch PCKh 0.1875\n",
      "Trained batch 1503 batch loss 0.666449606 batch mAP 0.58984375 batch PCKh 0.375\n",
      "Trained batch 1504 batch loss 0.677272439 batch mAP 0.564788818 batch PCKh 0.25\n",
      "Trained batch 1505 batch loss 0.638066649 batch mAP 0.522369385 batch PCKh 0.125\n",
      "Trained batch 1506 batch loss 0.630561352 batch mAP 0.500335693 batch PCKh 0.5\n",
      "Trained batch 1507 batch loss 0.644301414 batch mAP 0.49420166 batch PCKh 0.375\n",
      "Trained batch 1508 batch loss 0.682604849 batch mAP 0.437194824 batch PCKh 0.5625\n",
      "Trained batch 1509 batch loss 0.687258482 batch mAP 0.52432251 batch PCKh 0.5625\n",
      "Trained batch 1510 batch loss 0.672192395 batch mAP 0.527618408 batch PCKh 0.625\n",
      "Trained batch 1511 batch loss 0.704671144 batch mAP 0.500671387 batch PCKh 0\n",
      "Trained batch 1512 batch loss 0.614692032 batch mAP 0.514251709 batch PCKh 0.4375\n",
      "Trained batch 1513 batch loss 0.694688618 batch mAP 0.472259521 batch PCKh 0.1875\n",
      "Trained batch 1514 batch loss 0.626165688 batch mAP 0.495513916 batch PCKh 0.6875\n",
      "Trained batch 1515 batch loss 0.638855875 batch mAP 0.529907227 batch PCKh 0.5\n",
      "Trained batch 1516 batch loss 0.605646551 batch mAP 0.525268555 batch PCKh 0.4375\n",
      "Trained batch 1517 batch loss 0.577802777 batch mAP 0.515777588 batch PCKh 0.875\n",
      "Trained batch 1518 batch loss 0.652609587 batch mAP 0.484741211 batch PCKh 0.125\n",
      "Trained batch 1519 batch loss 0.569369733 batch mAP 0.608215332 batch PCKh 0.4375\n",
      "Trained batch 1520 batch loss 0.615265369 batch mAP 0.596405 batch PCKh 0.4375\n",
      "Trained batch 1521 batch loss 0.600458503 batch mAP 0.575134277 batch PCKh 0.0625\n",
      "Trained batch 1522 batch loss 0.556550503 batch mAP 0.583618164 batch PCKh 0.375\n",
      "Trained batch 1523 batch loss 0.632385612 batch mAP 0.506744385 batch PCKh 0.25\n",
      "Trained batch 1524 batch loss 0.574074745 batch mAP 0.568084717 batch PCKh 0.6875\n",
      "Trained batch 1525 batch loss 0.592006564 batch mAP 0.511901855 batch PCKh 0.75\n",
      "Trained batch 1526 batch loss 0.581628919 batch mAP 0.518615723 batch PCKh 0.8125\n",
      "Trained batch 1527 batch loss 0.508331597 batch mAP 0.503601074 batch PCKh 0.6875\n",
      "Trained batch 1528 batch loss 0.57717 batch mAP 0.569091797 batch PCKh 0.3125\n",
      "Trained batch 1529 batch loss 0.592008233 batch mAP 0.571258545 batch PCKh 0.6875\n",
      "Trained batch 1530 batch loss 0.553769231 batch mAP 0.556488037 batch PCKh 0.25\n",
      "Trained batch 1531 batch loss 0.547220469 batch mAP 0.582763672 batch PCKh 0.0625\n",
      "Trained batch 1532 batch loss 0.534067154 batch mAP 0.599823 batch PCKh 0.5\n",
      "Trained batch 1533 batch loss 0.528040946 batch mAP 0.589599609 batch PCKh 0.4375\n",
      "Trained batch 1534 batch loss 0.542858779 batch mAP 0.543487549 batch PCKh 0.6875\n",
      "Trained batch 1535 batch loss 0.582681835 batch mAP 0.577697754 batch PCKh 0.5625\n",
      "Trained batch 1536 batch loss 0.640804648 batch mAP 0.524353 batch PCKh 0.625\n",
      "Trained batch 1537 batch loss 0.602757573 batch mAP 0.509490967 batch PCKh 0.6875\n",
      "Trained batch 1538 batch loss 0.67238003 batch mAP 0.501037598 batch PCKh 0.4375\n",
      "Trained batch 1539 batch loss 0.646287918 batch mAP 0.453491211 batch PCKh 0.6875\n",
      "Trained batch 1540 batch loss 0.637716413 batch mAP 0.519958496 batch PCKh 0.875\n",
      "Trained batch 1541 batch loss 0.627976716 batch mAP 0.491088867 batch PCKh 0.8125\n",
      "Trained batch 1542 batch loss 0.595359802 batch mAP 0.503265381 batch PCKh 0.625\n",
      "Trained batch 1543 batch loss 0.659582555 batch mAP 0.506408691 batch PCKh 0.75\n",
      "Trained batch 1544 batch loss 0.585311532 batch mAP 0.561523438 batch PCKh 0.375\n",
      "Trained batch 1545 batch loss 0.57706821 batch mAP 0.512664795 batch PCKh 0.3125\n",
      "Trained batch 1546 batch loss 0.627177715 batch mAP 0.514251709 batch PCKh 0.125\n",
      "Trained batch 1547 batch loss 0.584187031 batch mAP 0.462890625 batch PCKh 0.8125\n",
      "Trained batch 1548 batch loss 0.653641582 batch mAP 0.518371582 batch PCKh 0.3125\n",
      "Trained batch 1549 batch loss 0.565739155 batch mAP 0.522827148 batch PCKh 0.625\n",
      "Trained batch 1550 batch loss 0.595139623 batch mAP 0.511383057 batch PCKh 0.0625\n",
      "Trained batch 1551 batch loss 0.56430316 batch mAP 0.502929688 batch PCKh 0.4375\n",
      "Trained batch 1552 batch loss 0.640237689 batch mAP 0.48336792 batch PCKh 0.5\n",
      "Trained batch 1553 batch loss 0.642438889 batch mAP 0.468475342 batch PCKh 0.1875\n",
      "Trained batch 1554 batch loss 0.556248546 batch mAP 0.536834717 batch PCKh 0.625\n",
      "Trained batch 1555 batch loss 0.496117055 batch mAP 0.513885498 batch PCKh 0.1875\n",
      "Trained batch 1556 batch loss 0.625462651 batch mAP 0.461425781 batch PCKh 0.0625\n",
      "Trained batch 1557 batch loss 0.467484951 batch mAP 0.570495605 batch PCKh 0.75\n",
      "Trained batch 1558 batch loss 0.501041889 batch mAP 0.57434082 batch PCKh 0.5625\n",
      "Trained batch 1559 batch loss 0.461128354 batch mAP 0.555206299 batch PCKh 0.4375\n",
      "Trained batch 1560 batch loss 0.545050859 batch mAP 0.46282959 batch PCKh 0.75\n",
      "Trained batch 1561 batch loss 0.520697892 batch mAP 0.576324463 batch PCKh 0.5625\n",
      "Trained batch 1562 batch loss 0.555389047 batch mAP 0.516174316 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1563 batch loss 0.5207026 batch mAP 0.44720459 batch PCKh 0\n",
      "Trained batch 1564 batch loss 0.540003836 batch mAP 0.574920654 batch PCKh 0.1875\n",
      "Trained batch 1565 batch loss 0.588144064 batch mAP 0.442474365 batch PCKh 0\n",
      "Trained batch 1566 batch loss 0.665143 batch mAP 0.282897949 batch PCKh 0.5625\n",
      "Trained batch 1567 batch loss 0.70475316 batch mAP 0.405212402 batch PCKh 0.1875\n",
      "Trained batch 1568 batch loss 0.667268038 batch mAP 0.528686523 batch PCKh 0.1875\n",
      "Trained batch 1569 batch loss 0.551080525 batch mAP 0.579498291 batch PCKh 0.3125\n",
      "Trained batch 1570 batch loss 0.538037717 batch mAP 0.573364258 batch PCKh 0.125\n",
      "Trained batch 1571 batch loss 0.509891272 batch mAP 0.599609375 batch PCKh 0.0625\n",
      "Trained batch 1572 batch loss 0.426196963 batch mAP 0.589111328 batch PCKh 0.375\n",
      "Trained batch 1573 batch loss 0.407945067 batch mAP 0.590332031 batch PCKh 0.375\n",
      "Trained batch 1574 batch loss 0.521722436 batch mAP 0.575439453 batch PCKh 0\n",
      "Trained batch 1575 batch loss 0.533368349 batch mAP 0.582733154 batch PCKh 0.5625\n",
      "Trained batch 1576 batch loss 0.557911 batch mAP 0.605926514 batch PCKh 0.1875\n",
      "Trained batch 1577 batch loss 0.573020816 batch mAP 0.611541748 batch PCKh 0.25\n",
      "Trained batch 1578 batch loss 0.628576219 batch mAP 0.606903076 batch PCKh 0\n",
      "Trained batch 1579 batch loss 0.642299771 batch mAP 0.523010254 batch PCKh 0.625\n",
      "Trained batch 1580 batch loss 0.60055244 batch mAP 0.556732178 batch PCKh 0.6875\n",
      "Trained batch 1581 batch loss 0.638543367 batch mAP 0.535064697 batch PCKh 0.3125\n",
      "Trained batch 1582 batch loss 0.611325383 batch mAP 0.490936279 batch PCKh 0.3125\n",
      "Trained batch 1583 batch loss 0.489225805 batch mAP 0.668304443 batch PCKh 0.3125\n",
      "Trained batch 1584 batch loss 0.556037426 batch mAP 0.621765137 batch PCKh 0.5\n",
      "Trained batch 1585 batch loss 0.557919741 batch mAP 0.562316895 batch PCKh 0.1875\n",
      "Trained batch 1586 batch loss 0.5920313 batch mAP 0.614379883 batch PCKh 0.375\n",
      "Trained batch 1587 batch loss 0.549857259 batch mAP 0.615936279 batch PCKh 0.3125\n",
      "Trained batch 1588 batch loss 0.540451884 batch mAP 0.549041748 batch PCKh 0\n",
      "Trained batch 1589 batch loss 0.523420572 batch mAP 0.496582031 batch PCKh 0.1875\n",
      "Trained batch 1590 batch loss 0.617728829 batch mAP 0.533660889 batch PCKh 0.625\n",
      "Trained batch 1591 batch loss 0.5750103 batch mAP 0.491394043 batch PCKh 0.375\n",
      "Trained batch 1592 batch loss 0.588242054 batch mAP 0.532470703 batch PCKh 0.375\n",
      "Trained batch 1593 batch loss 0.593942642 batch mAP 0.494049072 batch PCKh 0.25\n",
      "Trained batch 1594 batch loss 0.734532833 batch mAP 0.513458252 batch PCKh 0\n",
      "Trained batch 1595 batch loss 0.666157663 batch mAP 0.521484375 batch PCKh 0.375\n",
      "Trained batch 1596 batch loss 0.554937661 batch mAP 0.50982666 batch PCKh 0.6875\n",
      "Trained batch 1597 batch loss 0.611775041 batch mAP 0.501800537 batch PCKh 0.8125\n",
      "Trained batch 1598 batch loss 0.445410341 batch mAP 0.454833984 batch PCKh 0.5625\n",
      "Trained batch 1599 batch loss 0.464907348 batch mAP 0.391693115 batch PCKh 0.5625\n",
      "Trained batch 1600 batch loss 0.549664259 batch mAP 0.434570312 batch PCKh 0.4375\n",
      "Trained batch 1601 batch loss 0.47359854 batch mAP 0.367919922 batch PCKh 0.625\n",
      "Trained batch 1602 batch loss 0.490358889 batch mAP 0.473114 batch PCKh 0.3125\n",
      "Trained batch 1603 batch loss 0.522276938 batch mAP 0.434356689 batch PCKh 0.6875\n",
      "Trained batch 1604 batch loss 0.602333 batch mAP 0.52432251 batch PCKh 0.3125\n",
      "Trained batch 1605 batch loss 0.732790112 batch mAP 0.527618408 batch PCKh 0.125\n",
      "Trained batch 1606 batch loss 0.683682084 batch mAP 0.443939209 batch PCKh 0.5\n",
      "Trained batch 1607 batch loss 0.694457233 batch mAP 0.519470215 batch PCKh 0.5\n",
      "Trained batch 1608 batch loss 0.6225366 batch mAP 0.490722656 batch PCKh 0.125\n",
      "Trained batch 1609 batch loss 0.576568305 batch mAP 0.531433105 batch PCKh 0.25\n",
      "Trained batch 1610 batch loss 0.654856145 batch mAP 0.589355469 batch PCKh 0.125\n",
      "Trained batch 1611 batch loss 0.645869851 batch mAP 0.554595947 batch PCKh 0.3125\n",
      "Trained batch 1612 batch loss 0.586303115 batch mAP 0.541473389 batch PCKh 0.6875\n",
      "Trained batch 1613 batch loss 0.523248911 batch mAP 0.531158447 batch PCKh 0.3125\n",
      "Trained batch 1614 batch loss 0.533918738 batch mAP 0.527862549 batch PCKh 0.375\n",
      "Trained batch 1615 batch loss 0.54992497 batch mAP 0.524688721 batch PCKh 0.1875\n",
      "Trained batch 1616 batch loss 0.634617805 batch mAP 0.544494629 batch PCKh 0.625\n",
      "Trained batch 1617 batch loss 0.553151488 batch mAP 0.573944092 batch PCKh 0.75\n",
      "Trained batch 1618 batch loss 0.619601309 batch mAP 0.56463623 batch PCKh 0.4375\n",
      "Trained batch 1619 batch loss 0.54678607 batch mAP 0.541626 batch PCKh 0.5\n",
      "Trained batch 1620 batch loss 0.5000875 batch mAP 0.509887695 batch PCKh 0.3125\n",
      "Trained batch 1621 batch loss 0.518878698 batch mAP 0.475921631 batch PCKh 0.5625\n",
      "Trained batch 1622 batch loss 0.489002168 batch mAP 0.491882324 batch PCKh 0.6875\n",
      "Trained batch 1623 batch loss 0.503965855 batch mAP 0.490356445 batch PCKh 0.5625\n",
      "Trained batch 1624 batch loss 0.504954934 batch mAP 0.496612549 batch PCKh 0.375\n",
      "Trained batch 1625 batch loss 0.473936081 batch mAP 0.508270264 batch PCKh 0.5\n",
      "Trained batch 1626 batch loss 0.537000239 batch mAP 0.516387939 batch PCKh 0.75\n",
      "Trained batch 1627 batch loss 0.563576579 batch mAP 0.597930908 batch PCKh 0.3125\n",
      "Trained batch 1628 batch loss 0.514734745 batch mAP 0.571807861 batch PCKh 0.6875\n",
      "Trained batch 1629 batch loss 0.499305069 batch mAP 0.595367432 batch PCKh 0.5\n",
      "Trained batch 1630 batch loss 0.479557663 batch mAP 0.592651367 batch PCKh 0.4375\n",
      "Trained batch 1631 batch loss 0.531208515 batch mAP 0.562805176 batch PCKh 0.5625\n",
      "Trained batch 1632 batch loss 0.557623386 batch mAP 0.574401855 batch PCKh 0.75\n",
      "Trained batch 1633 batch loss 0.614648879 batch mAP 0.524078369 batch PCKh 0.875\n",
      "Trained batch 1634 batch loss 0.503753185 batch mAP 0.528198242 batch PCKh 0.3125\n",
      "Trained batch 1635 batch loss 0.584715247 batch mAP 0.456359863 batch PCKh 0.6875\n",
      "Trained batch 1636 batch loss 0.601433456 batch mAP 0.46472168 batch PCKh 0.5625\n",
      "Trained batch 1637 batch loss 0.615293145 batch mAP 0.418609619 batch PCKh 0.375\n",
      "Trained batch 1638 batch loss 0.543483377 batch mAP 0.469512939 batch PCKh 0.6875\n",
      "Trained batch 1639 batch loss 0.487757355 batch mAP 0.589660645 batch PCKh 0.6875\n",
      "Trained batch 1640 batch loss 0.478213757 batch mAP 0.563873291 batch PCKh 0.6875\n",
      "Trained batch 1641 batch loss 0.612260699 batch mAP 0.54422 batch PCKh 0.875\n",
      "Trained batch 1642 batch loss 0.595022321 batch mAP 0.517059326 batch PCKh 0.75\n",
      "Trained batch 1643 batch loss 0.666570485 batch mAP 0.463592529 batch PCKh 0.1875\n",
      "Trained batch 1644 batch loss 0.670890689 batch mAP 0.495697021 batch PCKh 0.1875\n",
      "Trained batch 1645 batch loss 0.646757245 batch mAP 0.608428955 batch PCKh 0.3125\n",
      "Trained batch 1646 batch loss 0.578967452 batch mAP 0.537475586 batch PCKh 0.25\n",
      "Trained batch 1647 batch loss 0.534629226 batch mAP 0.516967773 batch PCKh 0.375\n",
      "Trained batch 1648 batch loss 0.538555264 batch mAP 0.490783691 batch PCKh 0.1875\n",
      "Trained batch 1649 batch loss 0.580363452 batch mAP 0.516113281 batch PCKh 0.625\n",
      "Trained batch 1650 batch loss 0.555564165 batch mAP 0.523254395 batch PCKh 0.8125\n",
      "Trained batch 1651 batch loss 0.615880728 batch mAP 0.51651 batch PCKh 0.6875\n",
      "Trained batch 1652 batch loss 0.571155787 batch mAP 0.602935791 batch PCKh 0.4375\n",
      "Trained batch 1653 batch loss 0.494181216 batch mAP 0.595001221 batch PCKh 0.125\n",
      "Trained batch 1654 batch loss 0.499503344 batch mAP 0.586273193 batch PCKh 0.625\n",
      "Trained batch 1655 batch loss 0.485792577 batch mAP 0.608856201 batch PCKh 0.375\n",
      "Trained batch 1656 batch loss 0.560863733 batch mAP 0.593536377 batch PCKh 0.375\n",
      "Trained batch 1657 batch loss 0.661451757 batch mAP 0.511291504 batch PCKh 0.4375\n",
      "Trained batch 1658 batch loss 0.600744665 batch mAP 0.543273926 batch PCKh 0.625\n",
      "Trained batch 1659 batch loss 0.618229687 batch mAP 0.54019165 batch PCKh 0.375\n",
      "Trained batch 1660 batch loss 0.627537847 batch mAP 0.559814453 batch PCKh 0.3125\n",
      "Trained batch 1661 batch loss 0.595936239 batch mAP 0.547973633 batch PCKh 0.625\n",
      "Trained batch 1662 batch loss 0.589711308 batch mAP 0.564178467 batch PCKh 0.4375\n",
      "Trained batch 1663 batch loss 0.605177522 batch mAP 0.509552 batch PCKh 0.75\n",
      "Trained batch 1664 batch loss 0.576417625 batch mAP 0.602783203 batch PCKh 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1665 batch loss 0.598479629 batch mAP 0.564331055 batch PCKh 0.625\n",
      "Trained batch 1666 batch loss 0.56772238 batch mAP 0.564819336 batch PCKh 0\n",
      "Trained batch 1667 batch loss 0.650064528 batch mAP 0.523101807 batch PCKh 0.25\n",
      "Trained batch 1668 batch loss 0.634901881 batch mAP 0.512115479 batch PCKh 0.25\n",
      "Trained batch 1669 batch loss 0.659194589 batch mAP 0.492218018 batch PCKh 0.25\n",
      "Trained batch 1670 batch loss 0.624559522 batch mAP 0.556182861 batch PCKh 0.3125\n",
      "Trained batch 1671 batch loss 0.559543133 batch mAP 0.580596924 batch PCKh 0.6875\n",
      "Trained batch 1672 batch loss 0.519857883 batch mAP 0.555175781 batch PCKh 0.3125\n",
      "Trained batch 1673 batch loss 0.552675486 batch mAP 0.485290527 batch PCKh 0.3125\n",
      "Trained batch 1674 batch loss 0.532256961 batch mAP 0.563903809 batch PCKh 0.3125\n",
      "Trained batch 1675 batch loss 0.528414726 batch mAP 0.602081299 batch PCKh 0.25\n",
      "Trained batch 1676 batch loss 0.572569132 batch mAP 0.521026611 batch PCKh 0.25\n",
      "Trained batch 1677 batch loss 0.56974 batch mAP 0.427856445 batch PCKh 0.6875\n",
      "Trained batch 1678 batch loss 0.540696442 batch mAP 0.454986572 batch PCKh 0.6875\n",
      "Trained batch 1679 batch loss 0.46724838 batch mAP 0.511322 batch PCKh 0.125\n",
      "Trained batch 1680 batch loss 0.477556288 batch mAP 0.486816406 batch PCKh 0.1875\n",
      "Trained batch 1681 batch loss 0.537366509 batch mAP 0.464782715 batch PCKh 0.1875\n",
      "Trained batch 1682 batch loss 0.567363083 batch mAP 0.499328613 batch PCKh 0.25\n",
      "Trained batch 1683 batch loss 0.596593678 batch mAP 0.519104 batch PCKh 0.6875\n",
      "Trained batch 1684 batch loss 0.497165054 batch mAP 0.594848633 batch PCKh 0.25\n",
      "Trained batch 1685 batch loss 0.528856277 batch mAP 0.557434082 batch PCKh 0.6875\n",
      "Trained batch 1686 batch loss 0.556078732 batch mAP 0.586669922 batch PCKh 0.375\n",
      "Trained batch 1687 batch loss 0.58183074 batch mAP 0.556518555 batch PCKh 0.25\n",
      "Trained batch 1688 batch loss 0.492236882 batch mAP 0.590881348 batch PCKh 0.75\n",
      "Trained batch 1689 batch loss 0.572244883 batch mAP 0.49005127 batch PCKh 0.625\n",
      "Trained batch 1690 batch loss 0.516191244 batch mAP 0.548309326 batch PCKh 0.625\n",
      "Trained batch 1691 batch loss 0.559594512 batch mAP 0.460906982 batch PCKh 0.75\n",
      "Trained batch 1692 batch loss 0.625346422 batch mAP 0.458282471 batch PCKh 0.1875\n",
      "Trained batch 1693 batch loss 0.620844483 batch mAP 0.515991211 batch PCKh 0.5\n",
      "Trained batch 1694 batch loss 0.639325202 batch mAP 0.499816895 batch PCKh 0.1875\n",
      "Trained batch 1695 batch loss 0.667457879 batch mAP 0.497283936 batch PCKh 0.375\n",
      "Trained batch 1696 batch loss 0.627990484 batch mAP 0.518585205 batch PCKh 0.5625\n",
      "Trained batch 1697 batch loss 0.63831836 batch mAP 0.498565674 batch PCKh 0.3125\n",
      "Trained batch 1698 batch loss 0.605085313 batch mAP 0.54599 batch PCKh 0.8125\n",
      "Trained batch 1699 batch loss 0.583664775 batch mAP 0.570465088 batch PCKh 0.4375\n",
      "Trained batch 1700 batch loss 0.524582386 batch mAP 0.516082764 batch PCKh 0.125\n",
      "Trained batch 1701 batch loss 0.603079 batch mAP 0.448913574 batch PCKh 0.1875\n",
      "Trained batch 1702 batch loss 0.572908759 batch mAP 0.477935791 batch PCKh 0.1875\n",
      "Trained batch 1703 batch loss 0.603067756 batch mAP 0.476257324 batch PCKh 0.75\n",
      "Trained batch 1704 batch loss 0.562580526 batch mAP 0.523681641 batch PCKh 0.875\n",
      "Trained batch 1705 batch loss 0.621029 batch mAP 0.559509277 batch PCKh 0.25\n",
      "Trained batch 1706 batch loss 0.49584052 batch mAP 0.596832275 batch PCKh 0.625\n",
      "Trained batch 1707 batch loss 0.591815 batch mAP 0.558288574 batch PCKh 0.5625\n",
      "Trained batch 1708 batch loss 0.614099085 batch mAP 0.518341064 batch PCKh 0.25\n",
      "Trained batch 1709 batch loss 0.533695579 batch mAP 0.510528564 batch PCKh 0.25\n",
      "Trained batch 1710 batch loss 0.666332483 batch mAP 0.525115967 batch PCKh 0.625\n",
      "Trained batch 1711 batch loss 0.628816664 batch mAP 0.486114502 batch PCKh 0.6875\n",
      "Trained batch 1712 batch loss 0.659347951 batch mAP 0.439208984 batch PCKh 0.125\n",
      "Trained batch 1713 batch loss 0.578482807 batch mAP 0.484313965 batch PCKh 0.125\n",
      "Trained batch 1714 batch loss 0.586053789 batch mAP 0.476898193 batch PCKh 0.5625\n",
      "Trained batch 1715 batch loss 0.539800942 batch mAP 0.536743164 batch PCKh 0.6875\n",
      "Trained batch 1716 batch loss 0.56517607 batch mAP 0.52532959 batch PCKh 0.5625\n",
      "Trained batch 1717 batch loss 0.553100348 batch mAP 0.557556152 batch PCKh 0.375\n",
      "Trained batch 1718 batch loss 0.680506349 batch mAP 0.538909912 batch PCKh 0.25\n",
      "Trained batch 1719 batch loss 0.583652 batch mAP 0.542114258 batch PCKh 0.4375\n",
      "Trained batch 1720 batch loss 0.445126861 batch mAP 0.610748291 batch PCKh 0.3125\n",
      "Trained batch 1721 batch loss 0.537175417 batch mAP 0.608703613 batch PCKh 0.5\n",
      "Trained batch 1722 batch loss 0.52414757 batch mAP 0.60836792 batch PCKh 0.4375\n",
      "Trained batch 1723 batch loss 0.545560598 batch mAP 0.571563721 batch PCKh 0.375\n",
      "Trained batch 1724 batch loss 0.614230275 batch mAP 0.573303223 batch PCKh 0.375\n",
      "Trained batch 1725 batch loss 0.595164776 batch mAP 0.579284668 batch PCKh 0.4375\n",
      "Trained batch 1726 batch loss 0.509123921 batch mAP 0.684570312 batch PCKh 0.4375\n",
      "Trained batch 1727 batch loss 0.499441326 batch mAP 0.680603 batch PCKh 0.4375\n",
      "Trained batch 1728 batch loss 0.471834868 batch mAP 0.680053711 batch PCKh 0.5\n",
      "Trained batch 1729 batch loss 0.480857879 batch mAP 0.659790039 batch PCKh 0.3125\n",
      "Trained batch 1730 batch loss 0.503685057 batch mAP 0.662841797 batch PCKh 0.4375\n",
      "Trained batch 1731 batch loss 0.559959412 batch mAP 0.641967773 batch PCKh 0.6875\n",
      "Trained batch 1732 batch loss 0.527334869 batch mAP 0.657592773 batch PCKh 0.5\n",
      "Trained batch 1733 batch loss 0.576315403 batch mAP 0.589050293 batch PCKh 0.8125\n",
      "Trained batch 1734 batch loss 0.627633214 batch mAP 0.518676758 batch PCKh 0.875\n",
      "Trained batch 1735 batch loss 0.606853604 batch mAP 0.492248535 batch PCKh 0.625\n",
      "Trained batch 1736 batch loss 0.532372534 batch mAP 0.557403564 batch PCKh 0.625\n",
      "Trained batch 1737 batch loss 0.503416717 batch mAP 0.502990723 batch PCKh 0.8125\n",
      "Trained batch 1738 batch loss 0.518208325 batch mAP 0.493652344 batch PCKh 0.75\n",
      "Trained batch 1739 batch loss 0.54917568 batch mAP 0.519805908 batch PCKh 0.625\n",
      "Trained batch 1740 batch loss 0.549446702 batch mAP 0.561309814 batch PCKh 0.8125\n",
      "Trained batch 1741 batch loss 0.576122701 batch mAP 0.550872803 batch PCKh 0.3125\n",
      "Trained batch 1742 batch loss 0.659767091 batch mAP 0.550415039 batch PCKh 0.5625\n",
      "Trained batch 1743 batch loss 0.578897774 batch mAP 0.540222168 batch PCKh 0.4375\n",
      "Trained batch 1744 batch loss 0.533759236 batch mAP 0.505737305 batch PCKh 0.1875\n",
      "Trained batch 1745 batch loss 0.601268351 batch mAP 0.504180908 batch PCKh 0.3125\n",
      "Trained batch 1746 batch loss 0.571469069 batch mAP 0.557830811 batch PCKh 0.5\n",
      "Trained batch 1747 batch loss 0.647410035 batch mAP 0.518768311 batch PCKh 0.4375\n",
      "Trained batch 1748 batch loss 0.616542399 batch mAP 0.491912842 batch PCKh 0.25\n",
      "Trained batch 1749 batch loss 0.621286392 batch mAP 0.505859375 batch PCKh 0.5\n",
      "Trained batch 1750 batch loss 0.657531798 batch mAP 0.487579346 batch PCKh 0\n",
      "Trained batch 1751 batch loss 0.628034115 batch mAP 0.420013428 batch PCKh 0.0625\n",
      "Trained batch 1752 batch loss 0.683354437 batch mAP 0.435974121 batch PCKh 0\n",
      "Trained batch 1753 batch loss 0.629570603 batch mAP 0.470916748 batch PCKh 0\n",
      "Trained batch 1754 batch loss 0.706577659 batch mAP 0.425994873 batch PCKh 0\n",
      "Trained batch 1755 batch loss 0.679201186 batch mAP 0.448913574 batch PCKh 0\n",
      "Trained batch 1756 batch loss 0.67837745 batch mAP 0.417327881 batch PCKh 0.1875\n",
      "Trained batch 1757 batch loss 0.573988795 batch mAP 0.520629883 batch PCKh 0.25\n",
      "Trained batch 1758 batch loss 0.551114142 batch mAP 0.579956055 batch PCKh 0.375\n",
      "Trained batch 1759 batch loss 0.516630828 batch mAP 0.544281 batch PCKh 0.375\n",
      "Trained batch 1760 batch loss 0.56545943 batch mAP 0.537567139 batch PCKh 0.75\n",
      "Trained batch 1761 batch loss 0.639497221 batch mAP 0.472717285 batch PCKh 0.625\n",
      "Trained batch 1762 batch loss 0.63901186 batch mAP 0.454284668 batch PCKh 0.4375\n",
      "Trained batch 1763 batch loss 0.695008159 batch mAP 0.494018555 batch PCKh 0.125\n",
      "Trained batch 1764 batch loss 0.510768116 batch mAP 0.582824707 batch PCKh 0.0625\n",
      "Trained batch 1765 batch loss 0.596556425 batch mAP 0.575195312 batch PCKh 0.125\n",
      "Trained batch 1766 batch loss 0.544447064 batch mAP 0.64050293 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1767 batch loss 0.57735008 batch mAP 0.625579834 batch PCKh 0.3125\n",
      "Trained batch 1768 batch loss 0.516554058 batch mAP 0.625488281 batch PCKh 0.8125\n",
      "Trained batch 1769 batch loss 0.538907945 batch mAP 0.599121094 batch PCKh 0.75\n",
      "Trained batch 1770 batch loss 0.520467579 batch mAP 0.605621338 batch PCKh 0.375\n",
      "Trained batch 1771 batch loss 0.56474328 batch mAP 0.562042236 batch PCKh 0.4375\n",
      "Trained batch 1772 batch loss 0.499952912 batch mAP 0.64074707 batch PCKh 0.3125\n",
      "Trained batch 1773 batch loss 0.477495879 batch mAP 0.631103516 batch PCKh 0.4375\n",
      "Trained batch 1774 batch loss 0.563438773 batch mAP 0.621612549 batch PCKh 0.5\n",
      "Trained batch 1775 batch loss 0.601575851 batch mAP 0.635376 batch PCKh 0\n",
      "Trained batch 1776 batch loss 0.4935669 batch mAP 0.639678955 batch PCKh 0.4375\n",
      "Trained batch 1777 batch loss 0.509840369 batch mAP 0.625518799 batch PCKh 0.3125\n",
      "Trained batch 1778 batch loss 0.588889956 batch mAP 0.624420166 batch PCKh 0.3125\n",
      "Trained batch 1779 batch loss 0.58354485 batch mAP 0.575805664 batch PCKh 0.3125\n",
      "Trained batch 1780 batch loss 0.693239689 batch mAP 0.537109375 batch PCKh 0.25\n",
      "Trained batch 1781 batch loss 0.616004765 batch mAP 0.57711792 batch PCKh 0.375\n",
      "Trained batch 1782 batch loss 0.552716076 batch mAP 0.538269043 batch PCKh 0.3125\n",
      "Trained batch 1783 batch loss 0.57520926 batch mAP 0.573516846 batch PCKh 0.25\n",
      "Trained batch 1784 batch loss 0.601717472 batch mAP 0.597900391 batch PCKh 0.1875\n",
      "Trained batch 1785 batch loss 0.581139 batch mAP 0.500549316 batch PCKh 0.5\n",
      "Trained batch 1786 batch loss 0.603724957 batch mAP 0.578674316 batch PCKh 0.75\n",
      "Trained batch 1787 batch loss 0.552845359 batch mAP 0.608398438 batch PCKh 0.375\n",
      "Trained batch 1788 batch loss 0.618464351 batch mAP 0.510345459 batch PCKh 0.1875\n",
      "Trained batch 1789 batch loss 0.596925795 batch mAP 0.538513184 batch PCKh 0.0625\n",
      "Trained batch 1790 batch loss 0.535534739 batch mAP 0.536499 batch PCKh 0.1875\n",
      "Trained batch 1791 batch loss 0.663826048 batch mAP 0.524780273 batch PCKh 0.25\n",
      "Trained batch 1792 batch loss 0.607202888 batch mAP 0.538299561 batch PCKh 0.4375\n",
      "Trained batch 1793 batch loss 0.555502176 batch mAP 0.538360596 batch PCKh 0.75\n",
      "Trained batch 1794 batch loss 0.571707249 batch mAP 0.578674316 batch PCKh 0.75\n",
      "Trained batch 1795 batch loss 0.629564047 batch mAP 0.531921387 batch PCKh 0.25\n",
      "Trained batch 1796 batch loss 0.655644298 batch mAP 0.529174805 batch PCKh 0.6875\n",
      "Trained batch 1797 batch loss 0.633274138 batch mAP 0.492584229 batch PCKh 0.4375\n",
      "Trained batch 1798 batch loss 0.580776036 batch mAP 0.535522461 batch PCKh 0.625\n",
      "Trained batch 1799 batch loss 0.696090817 batch mAP 0.516540527 batch PCKh 0.0625\n",
      "Trained batch 1800 batch loss 0.61160934 batch mAP 0.52645874 batch PCKh 0.875\n",
      "Trained batch 1801 batch loss 0.611894 batch mAP 0.498016357 batch PCKh 0.6875\n",
      "Trained batch 1802 batch loss 0.612278163 batch mAP 0.423248291 batch PCKh 0.3125\n",
      "Trained batch 1803 batch loss 0.636990607 batch mAP 0.446655273 batch PCKh 0.75\n",
      "Trained batch 1804 batch loss 0.615942836 batch mAP 0.441894531 batch PCKh 0.5625\n",
      "Trained batch 1805 batch loss 0.581618905 batch mAP 0.476837158 batch PCKh 0.75\n",
      "Trained batch 1806 batch loss 0.507028 batch mAP 0.497192383 batch PCKh 0.4375\n",
      "Trained batch 1807 batch loss 0.563005865 batch mAP 0.532043457 batch PCKh 0.5625\n",
      "Trained batch 1808 batch loss 0.656850517 batch mAP 0.507598877 batch PCKh 0.75\n",
      "Trained batch 1809 batch loss 0.53782022 batch mAP 0.596618652 batch PCKh 0.8125\n",
      "Trained batch 1810 batch loss 0.558858156 batch mAP 0.534179688 batch PCKh 0.1875\n",
      "Trained batch 1811 batch loss 0.593088746 batch mAP 0.522399902 batch PCKh 0.375\n",
      "Trained batch 1812 batch loss 0.61247468 batch mAP 0.57699585 batch PCKh 0.6875\n",
      "Trained batch 1813 batch loss 0.575369716 batch mAP 0.586212158 batch PCKh 0.75\n",
      "Trained batch 1814 batch loss 0.599492788 batch mAP 0.55456543 batch PCKh 0.8125\n",
      "Trained batch 1815 batch loss 0.558790565 batch mAP 0.498321533 batch PCKh 0.0625\n",
      "Trained batch 1816 batch loss 0.630209088 batch mAP 0.57119751 batch PCKh 0.75\n",
      "Trained batch 1817 batch loss 0.568137765 batch mAP 0.58895874 batch PCKh 0.25\n",
      "Trained batch 1818 batch loss 0.604240239 batch mAP 0.598083496 batch PCKh 0.1875\n",
      "Trained batch 1819 batch loss 0.63231039 batch mAP 0.58605957 batch PCKh 0.1875\n",
      "Trained batch 1820 batch loss 0.537126899 batch mAP 0.638885498 batch PCKh 0.4375\n",
      "Trained batch 1821 batch loss 0.560286582 batch mAP 0.601501465 batch PCKh 0.5\n",
      "Trained batch 1822 batch loss 0.632028341 batch mAP 0.58694458 batch PCKh 0.5625\n",
      "Trained batch 1823 batch loss 0.58595854 batch mAP 0.586517334 batch PCKh 0.4375\n",
      "Trained batch 1824 batch loss 0.572746515 batch mAP 0.561615 batch PCKh 0.5625\n",
      "Trained batch 1825 batch loss 0.501656413 batch mAP 0.601928711 batch PCKh 0.0625\n",
      "Trained batch 1826 batch loss 0.605532765 batch mAP 0.587097168 batch PCKh 0.125\n",
      "Trained batch 1827 batch loss 0.5924052 batch mAP 0.556884766 batch PCKh 0.25\n",
      "Trained batch 1828 batch loss 0.552497864 batch mAP 0.568756104 batch PCKh 0.3125\n",
      "Trained batch 1829 batch loss 0.609821916 batch mAP 0.517822266 batch PCKh 0.5625\n",
      "Trained batch 1830 batch loss 0.561186671 batch mAP 0.516174316 batch PCKh 0.1875\n",
      "Trained batch 1831 batch loss 0.617584 batch mAP 0.530700684 batch PCKh 0.5625\n",
      "Trained batch 1832 batch loss 0.595006466 batch mAP 0.50579834 batch PCKh 0.125\n",
      "Trained batch 1833 batch loss 0.589533865 batch mAP 0.574981689 batch PCKh 0.25\n",
      "Trained batch 1834 batch loss 0.528535247 batch mAP 0.597015381 batch PCKh 0.625\n",
      "Trained batch 1835 batch loss 0.549659729 batch mAP 0.573883057 batch PCKh 0.6875\n",
      "Trained batch 1836 batch loss 0.582680941 batch mAP 0.613983154 batch PCKh 0.3125\n",
      "Trained batch 1837 batch loss 0.624203324 batch mAP 0.630279541 batch PCKh 0.375\n",
      "Trained batch 1838 batch loss 0.569426239 batch mAP 0.643920898 batch PCKh 0.625\n",
      "Trained batch 1839 batch loss 0.602925897 batch mAP 0.62890625 batch PCKh 0.8125\n",
      "Trained batch 1840 batch loss 0.596734405 batch mAP 0.557891846 batch PCKh 0.5\n",
      "Trained batch 1841 batch loss 0.599422336 batch mAP 0.531311035 batch PCKh 0.5625\n",
      "Trained batch 1842 batch loss 0.664641261 batch mAP 0.550140381 batch PCKh 0.625\n",
      "Trained batch 1843 batch loss 0.560170054 batch mAP 0.5987854 batch PCKh 0.6875\n",
      "Trained batch 1844 batch loss 0.622081339 batch mAP 0.489715576 batch PCKh 0.375\n",
      "Trained batch 1845 batch loss 0.494536042 batch mAP 0.510955811 batch PCKh 0\n",
      "Trained batch 1846 batch loss 0.517430902 batch mAP 0.357269287 batch PCKh 0.5625\n",
      "Trained batch 1847 batch loss 0.550403237 batch mAP 0.392211914 batch PCKh 0.25\n",
      "Trained batch 1848 batch loss 0.541069925 batch mAP 0.570526123 batch PCKh 0.8125\n",
      "Trained batch 1849 batch loss 0.577053547 batch mAP 0.60534668 batch PCKh 0.1875\n",
      "Trained batch 1850 batch loss 0.580870152 batch mAP 0.546966553 batch PCKh 0.625\n",
      "Trained batch 1851 batch loss 0.603571117 batch mAP 0.59854126 batch PCKh 0.5625\n",
      "Trained batch 1852 batch loss 0.606691658 batch mAP 0.574676514 batch PCKh 0.75\n",
      "Trained batch 1853 batch loss 0.59677875 batch mAP 0.505401611 batch PCKh 0.4375\n",
      "Trained batch 1854 batch loss 0.590694129 batch mAP 0.56036377 batch PCKh 0.25\n",
      "Trained batch 1855 batch loss 0.609756768 batch mAP 0.524078369 batch PCKh 0.3125\n",
      "Trained batch 1856 batch loss 0.641078353 batch mAP 0.568511963 batch PCKh 0.625\n",
      "Trained batch 1857 batch loss 0.667507589 batch mAP 0.501098633 batch PCKh 0.125\n",
      "Trained batch 1858 batch loss 0.616284072 batch mAP 0.559936523 batch PCKh 0.5625\n",
      "Trained batch 1859 batch loss 0.587066889 batch mAP 0.579223633 batch PCKh 0.8125\n",
      "Trained batch 1860 batch loss 0.658156395 batch mAP 0.533416748 batch PCKh 0.25\n",
      "Trained batch 1861 batch loss 0.66301322 batch mAP 0.566375732 batch PCKh 0.3125\n",
      "Trained batch 1862 batch loss 0.633362651 batch mAP 0.568084717 batch PCKh 0.6875\n",
      "Trained batch 1863 batch loss 0.592875123 batch mAP 0.558990479 batch PCKh 0.25\n",
      "Trained batch 1864 batch loss 0.483582377 batch mAP 0.576049805 batch PCKh 0.4375\n",
      "Trained batch 1865 batch loss 0.563945889 batch mAP 0.520599365 batch PCKh 0.625\n",
      "Trained batch 1866 batch loss 0.540338635 batch mAP 0.557159424 batch PCKh 0.3125\n",
      "Trained batch 1867 batch loss 0.539580762 batch mAP 0.551727295 batch PCKh 0.4375\n",
      "Trained batch 1868 batch loss 0.591315866 batch mAP 0.550079346 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1869 batch loss 0.598446786 batch mAP 0.509857178 batch PCKh 0.5625\n",
      "Trained batch 1870 batch loss 0.508402228 batch mAP 0.54284668 batch PCKh 0.375\n",
      "Trained batch 1871 batch loss 0.530826211 batch mAP 0.546020508 batch PCKh 0.5\n",
      "Trained batch 1872 batch loss 0.558438122 batch mAP 0.542358398 batch PCKh 0.5\n",
      "Trained batch 1873 batch loss 0.47621417 batch mAP 0.562438965 batch PCKh 0.1875\n",
      "Trained batch 1874 batch loss 0.507051408 batch mAP 0.57119751 batch PCKh 0.6875\n",
      "Trained batch 1875 batch loss 0.642219186 batch mAP 0.544769287 batch PCKh 0.375\n",
      "Trained batch 1876 batch loss 0.564414263 batch mAP 0.584472656 batch PCKh 0.375\n",
      "Trained batch 1877 batch loss 0.587294221 batch mAP 0.55456543 batch PCKh 0.0625\n",
      "Trained batch 1878 batch loss 0.698675632 batch mAP 0.51828 batch PCKh 0\n",
      "Trained batch 1879 batch loss 0.641878 batch mAP 0.519042969 batch PCKh 0\n",
      "Trained batch 1880 batch loss 0.620232582 batch mAP 0.535644531 batch PCKh 0.5\n",
      "Trained batch 1881 batch loss 0.687124431 batch mAP 0.503662109 batch PCKh 0.3125\n",
      "Trained batch 1882 batch loss 0.630441904 batch mAP 0.525024414 batch PCKh 0.625\n",
      "Trained batch 1883 batch loss 0.606942594 batch mAP 0.524536133 batch PCKh 0.875\n",
      "Trained batch 1884 batch loss 0.572689176 batch mAP 0.496276855 batch PCKh 0.875\n",
      "Trained batch 1885 batch loss 0.603783548 batch mAP 0.45211792 batch PCKh 0.8125\n",
      "Trained batch 1886 batch loss 0.612217784 batch mAP 0.411773682 batch PCKh 0.625\n",
      "Trained batch 1887 batch loss 0.676097214 batch mAP 0.440216064 batch PCKh 0.625\n",
      "Trained batch 1888 batch loss 0.533335745 batch mAP 0.465667725 batch PCKh 0.625\n",
      "Trained batch 1889 batch loss 0.560380578 batch mAP 0.475341797 batch PCKh 0.4375\n",
      "Trained batch 1890 batch loss 0.603638291 batch mAP 0.49206543 batch PCKh 0.625\n",
      "Trained batch 1891 batch loss 0.55283 batch mAP 0.513671875 batch PCKh 0.75\n",
      "Trained batch 1892 batch loss 0.602568805 batch mAP 0.497375488 batch PCKh 0.4375\n",
      "Trained batch 1893 batch loss 0.628059 batch mAP 0.506744385 batch PCKh 0.75\n",
      "Trained batch 1894 batch loss 0.478627145 batch mAP 0.601654053 batch PCKh 0.5625\n",
      "Trained batch 1895 batch loss 0.58181262 batch mAP 0.61239624 batch PCKh 0.5\n",
      "Trained batch 1896 batch loss 0.517166913 batch mAP 0.595367432 batch PCKh 0.1875\n",
      "Trained batch 1897 batch loss 0.526846051 batch mAP 0.610778809 batch PCKh 0.3125\n",
      "Trained batch 1898 batch loss 0.628639102 batch mAP 0.619567871 batch PCKh 0.1875\n",
      "Trained batch 1899 batch loss 0.616326809 batch mAP 0.582489 batch PCKh 0.375\n",
      "Trained batch 1900 batch loss 0.541200876 batch mAP 0.604034424 batch PCKh 0.1875\n",
      "Trained batch 1901 batch loss 0.575348735 batch mAP 0.594818115 batch PCKh 0.5\n",
      "Trained batch 1902 batch loss 0.556250572 batch mAP 0.571044922 batch PCKh 0.625\n",
      "Trained batch 1903 batch loss 0.551226616 batch mAP 0.495819092 batch PCKh 0.375\n",
      "Trained batch 1904 batch loss 0.529854655 batch mAP 0.476745605 batch PCKh 0.75\n",
      "Trained batch 1905 batch loss 0.545185328 batch mAP 0.516113281 batch PCKh 0.5625\n",
      "Trained batch 1906 batch loss 0.534133434 batch mAP 0.51348877 batch PCKh 0.625\n",
      "Trained batch 1907 batch loss 0.581226587 batch mAP 0.548675537 batch PCKh 0.625\n",
      "Trained batch 1908 batch loss 0.507163584 batch mAP 0.553863525 batch PCKh 0.6875\n",
      "Trained batch 1909 batch loss 0.567972898 batch mAP 0.552124 batch PCKh 0.5\n",
      "Trained batch 1910 batch loss 0.625937581 batch mAP 0.52532959 batch PCKh 0.625\n",
      "Trained batch 1911 batch loss 0.622277856 batch mAP 0.581848145 batch PCKh 0.5\n",
      "Trained batch 1912 batch loss 0.610168457 batch mAP 0.564147949 batch PCKh 0.5625\n",
      "Trained batch 1913 batch loss 0.604782343 batch mAP 0.595275879 batch PCKh 0.4375\n",
      "Trained batch 1914 batch loss 0.530700922 batch mAP 0.595184326 batch PCKh 0.4375\n",
      "Trained batch 1915 batch loss 0.625656843 batch mAP 0.557830811 batch PCKh 0.5\n",
      "Trained batch 1916 batch loss 0.641941 batch mAP 0.508270264 batch PCKh 0.5\n",
      "Trained batch 1917 batch loss 0.545693815 batch mAP 0.574554443 batch PCKh 0.375\n",
      "Trained batch 1918 batch loss 0.639657795 batch mAP 0.48034668 batch PCKh 0.125\n",
      "Trained batch 1919 batch loss 0.593693793 batch mAP 0.533905 batch PCKh 0.6875\n",
      "Trained batch 1920 batch loss 0.649082422 batch mAP 0.53427124 batch PCKh 0.25\n",
      "Trained batch 1921 batch loss 0.588751674 batch mAP 0.485900879 batch PCKh 0.25\n",
      "Trained batch 1922 batch loss 0.610242367 batch mAP 0.521606445 batch PCKh 0.25\n",
      "Trained batch 1923 batch loss 0.590052485 batch mAP 0.549194336 batch PCKh 0.375\n",
      "Trained batch 1924 batch loss 0.609084249 batch mAP 0.559173584 batch PCKh 0.25\n",
      "Trained batch 1925 batch loss 0.613046169 batch mAP 0.524353 batch PCKh 0.5625\n",
      "Trained batch 1926 batch loss 0.484183311 batch mAP 0.592285156 batch PCKh 0.75\n",
      "Trained batch 1927 batch loss 0.532190204 batch mAP 0.596038818 batch PCKh 0.1875\n",
      "Trained batch 1928 batch loss 0.554914415 batch mAP 0.612640381 batch PCKh 0.6875\n",
      "Trained batch 1929 batch loss 0.565408945 batch mAP 0.5496521 batch PCKh 0.5\n",
      "Trained batch 1930 batch loss 0.502274513 batch mAP 0.611724854 batch PCKh 0.6875\n",
      "Trained batch 1931 batch loss 0.535295367 batch mAP 0.616149902 batch PCKh 0.375\n",
      "Trained batch 1932 batch loss 0.582869232 batch mAP 0.573547363 batch PCKh 0.5\n",
      "Trained batch 1933 batch loss 0.613810778 batch mAP 0.603363037 batch PCKh 0.4375\n",
      "Trained batch 1934 batch loss 0.577971816 batch mAP 0.604858398 batch PCKh 0.25\n",
      "Trained batch 1935 batch loss 0.517339051 batch mAP 0.5703125 batch PCKh 0.3125\n",
      "Trained batch 1936 batch loss 0.482842743 batch mAP 0.583496094 batch PCKh 0.1875\n",
      "Trained batch 1937 batch loss 0.547879219 batch mAP 0.620574951 batch PCKh 0.75\n",
      "Trained batch 1938 batch loss 0.508842051 batch mAP 0.620483398 batch PCKh 0.4375\n",
      "Trained batch 1939 batch loss 0.508547902 batch mAP 0.656341553 batch PCKh 0.1875\n",
      "Trained batch 1940 batch loss 0.46698454 batch mAP 0.603942871 batch PCKh 0.25\n",
      "Trained batch 1941 batch loss 0.453318179 batch mAP 0.594818115 batch PCKh 0.5\n",
      "Trained batch 1942 batch loss 0.520958126 batch mAP 0.567321777 batch PCKh 0.8125\n",
      "Trained batch 1943 batch loss 0.592541933 batch mAP 0.54901123 batch PCKh 0.3125\n",
      "Trained batch 1944 batch loss 0.619788527 batch mAP 0.497283936 batch PCKh 0.375\n",
      "Trained batch 1945 batch loss 0.618294418 batch mAP 0.493011475 batch PCKh 0.5\n",
      "Trained batch 1946 batch loss 0.617425859 batch mAP 0.501861572 batch PCKh 0.625\n",
      "Trained batch 1947 batch loss 0.628964365 batch mAP 0.544158936 batch PCKh 0.375\n",
      "Trained batch 1948 batch loss 0.573453903 batch mAP 0.473968506 batch PCKh 0.5\n",
      "Trained batch 1949 batch loss 0.559037745 batch mAP 0.513946533 batch PCKh 0.4375\n",
      "Trained batch 1950 batch loss 0.607303441 batch mAP 0.502288818 batch PCKh 0.8125\n",
      "Trained batch 1951 batch loss 0.604089379 batch mAP 0.522796631 batch PCKh 0.375\n",
      "Trained batch 1952 batch loss 0.665002227 batch mAP 0.513702393 batch PCKh 0.4375\n",
      "Trained batch 1953 batch loss 0.625424445 batch mAP 0.49810791 batch PCKh 0.4375\n",
      "Trained batch 1954 batch loss 0.597379923 batch mAP 0.491210938 batch PCKh 0.375\n",
      "Trained batch 1955 batch loss 0.583801866 batch mAP 0.525909424 batch PCKh 0.5625\n",
      "Trained batch 1956 batch loss 0.563953221 batch mAP 0.501586914 batch PCKh 0.6875\n",
      "Trained batch 1957 batch loss 0.56912756 batch mAP 0.481170654 batch PCKh 0\n",
      "Trained batch 1958 batch loss 0.580366194 batch mAP 0.535675049 batch PCKh 0.3125\n",
      "Trained batch 1959 batch loss 0.695462584 batch mAP 0.455566406 batch PCKh 0.125\n",
      "Trained batch 1960 batch loss 0.603809834 batch mAP 0.522094727 batch PCKh 0.375\n",
      "Trained batch 1961 batch loss 0.600068331 batch mAP 0.351715088 batch PCKh 0.375\n",
      "Trained batch 1962 batch loss 0.488085032 batch mAP 0.441223145 batch PCKh 0.625\n",
      "Trained batch 1963 batch loss 0.504199147 batch mAP 0.446411133 batch PCKh 0.75\n",
      "Trained batch 1964 batch loss 0.441797078 batch mAP 0.444519043 batch PCKh 0.5625\n",
      "Trained batch 1965 batch loss 0.594481826 batch mAP 0.484100342 batch PCKh 0.0625\n",
      "Trained batch 1966 batch loss 0.559354842 batch mAP 0.455657959 batch PCKh 0.0625\n",
      "Trained batch 1967 batch loss 0.561643481 batch mAP 0.467437744 batch PCKh 0.5625\n",
      "Trained batch 1968 batch loss 0.607115686 batch mAP 0.451721191 batch PCKh 0.5625\n",
      "Trained batch 1969 batch loss 0.663356483 batch mAP 0.413269043 batch PCKh 0.375\n",
      "Trained batch 1970 batch loss 0.571744263 batch mAP 0.509185791 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1971 batch loss 0.591312468 batch mAP 0.492218018 batch PCKh 0.1875\n",
      "Trained batch 1972 batch loss 0.513071716 batch mAP 0.55267334 batch PCKh 0.625\n",
      "Trained batch 1973 batch loss 0.591882944 batch mAP 0.511413574 batch PCKh 0.75\n",
      "Trained batch 1974 batch loss 0.641191185 batch mAP 0.536132812 batch PCKh 0.25\n",
      "Trained batch 1975 batch loss 0.5655635 batch mAP 0.542449951 batch PCKh 0.5\n",
      "Trained batch 1976 batch loss 0.587487936 batch mAP 0.504760742 batch PCKh 0.75\n",
      "Trained batch 1977 batch loss 0.533185601 batch mAP 0.522155762 batch PCKh 0.375\n",
      "Trained batch 1978 batch loss 0.529084861 batch mAP 0.509490967 batch PCKh 0.875\n",
      "Trained batch 1979 batch loss 0.580095053 batch mAP 0.493713379 batch PCKh 0.8125\n",
      "Trained batch 1980 batch loss 0.520526946 batch mAP 0.537628174 batch PCKh 0.625\n",
      "Trained batch 1981 batch loss 0.608132124 batch mAP 0.450439453 batch PCKh 0.5\n",
      "Trained batch 1982 batch loss 0.558998048 batch mAP 0.500701904 batch PCKh 0.5625\n",
      "Trained batch 1983 batch loss 0.523636818 batch mAP 0.468017578 batch PCKh 0.125\n",
      "Trained batch 1984 batch loss 0.561636806 batch mAP 0.543792725 batch PCKh 0.5\n",
      "Trained batch 1985 batch loss 0.570178628 batch mAP 0.547821045 batch PCKh 0.4375\n",
      "Trained batch 1986 batch loss 0.655420661 batch mAP 0.526733398 batch PCKh 0.5625\n",
      "Trained batch 1987 batch loss 0.560934484 batch mAP 0.537384033 batch PCKh 0.25\n",
      "Trained batch 1988 batch loss 0.567018092 batch mAP 0.527648926 batch PCKh 0.3125\n",
      "Trained batch 1989 batch loss 0.558317542 batch mAP 0.590057373 batch PCKh 0.375\n",
      "Trained batch 1990 batch loss 0.466309667 batch mAP 0.570068359 batch PCKh 0.75\n",
      "Trained batch 1991 batch loss 0.44867146 batch mAP 0.588989258 batch PCKh 0.4375\n",
      "Trained batch 1992 batch loss 0.486722827 batch mAP 0.533691406 batch PCKh 0.1875\n",
      "Trained batch 1993 batch loss 0.613708794 batch mAP 0.478515625 batch PCKh 0.0625\n",
      "Trained batch 1994 batch loss 0.687939048 batch mAP 0.507659912 batch PCKh 0\n",
      "Trained batch 1995 batch loss 0.544411361 batch mAP 0.60723877 batch PCKh 0.375\n",
      "Trained batch 1996 batch loss 0.574412167 batch mAP 0.619049072 batch PCKh 0.3125\n",
      "Trained batch 1997 batch loss 0.491440475 batch mAP 0.654052734 batch PCKh 0.625\n",
      "Trained batch 1998 batch loss 0.532150567 batch mAP 0.683258057 batch PCKh 0.25\n",
      "Trained batch 1999 batch loss 0.538130581 batch mAP 0.617157 batch PCKh 0.3125\n",
      "Trained batch 2000 batch loss 0.485065669 batch mAP 0.668609619 batch PCKh 0.625\n",
      "Trained batch 2001 batch loss 0.643191218 batch mAP 0.53817749 batch PCKh 0.6875\n",
      "Trained batch 2002 batch loss 0.478000492 batch mAP 0.619812 batch PCKh 0.5\n",
      "Trained batch 2003 batch loss 0.53815943 batch mAP 0.532928467 batch PCKh 0.625\n",
      "Trained batch 2004 batch loss 0.581095338 batch mAP 0.56829834 batch PCKh 0.625\n",
      "Trained batch 2005 batch loss 0.65954423 batch mAP 0.513916 batch PCKh 0.125\n",
      "Trained batch 2006 batch loss 0.578966 batch mAP 0.565856934 batch PCKh 0.375\n",
      "Trained batch 2007 batch loss 0.557685912 batch mAP 0.501190186 batch PCKh 0.5\n",
      "Trained batch 2008 batch loss 0.6119169 batch mAP 0.512786865 batch PCKh 0.4375\n",
      "Trained batch 2009 batch loss 0.670369625 batch mAP 0.458221436 batch PCKh 0.75\n",
      "Trained batch 2010 batch loss 0.660244226 batch mAP 0.468963623 batch PCKh 0.8125\n",
      "Trained batch 2011 batch loss 0.603527427 batch mAP 0.517547607 batch PCKh 0.75\n",
      "Trained batch 2012 batch loss 0.690746069 batch mAP 0.498565674 batch PCKh 0.375\n",
      "Trained batch 2013 batch loss 0.528388143 batch mAP 0.557342529 batch PCKh 0\n",
      "Trained batch 2014 batch loss 0.540641904 batch mAP 0.572753906 batch PCKh 0.3125\n",
      "Trained batch 2015 batch loss 0.561937928 batch mAP 0.561248779 batch PCKh 0.3125\n",
      "Trained batch 2016 batch loss 0.605199337 batch mAP 0.519989 batch PCKh 0.25\n",
      "Trained batch 2017 batch loss 0.597893476 batch mAP 0.499267578 batch PCKh 0.25\n",
      "Trained batch 2018 batch loss 0.554853678 batch mAP 0.530212402 batch PCKh 0.4375\n",
      "Trained batch 2019 batch loss 0.550119638 batch mAP 0.537139893 batch PCKh 0.3125\n",
      "Trained batch 2020 batch loss 0.549739242 batch mAP 0.514221191 batch PCKh 0.375\n",
      "Trained batch 2021 batch loss 0.635340929 batch mAP 0.441162109 batch PCKh 0.3125\n",
      "Trained batch 2022 batch loss 0.595086277 batch mAP 0.501800537 batch PCKh 0.625\n",
      "Trained batch 2023 batch loss 0.635650516 batch mAP 0.492614746 batch PCKh 0.3125\n",
      "Trained batch 2024 batch loss 0.632529259 batch mAP 0.483978271 batch PCKh 0.5625\n",
      "Trained batch 2025 batch loss 0.614409208 batch mAP 0.501495361 batch PCKh 0.625\n",
      "Trained batch 2026 batch loss 0.632915318 batch mAP 0.484588623 batch PCKh 0.25\n",
      "Trained batch 2027 batch loss 0.528746188 batch mAP 0.493103027 batch PCKh 0.3125\n",
      "Trained batch 2028 batch loss 0.558109879 batch mAP 0.339080811 batch PCKh 0.4375\n",
      "Trained batch 2029 batch loss 0.648798883 batch mAP 0.319396973 batch PCKh 0.625\n",
      "Trained batch 2030 batch loss 0.56094873 batch mAP 0.418518066 batch PCKh 0.25\n",
      "Trained batch 2031 batch loss 0.620592952 batch mAP 0.400543213 batch PCKh 0.25\n",
      "Trained batch 2032 batch loss 0.622583389 batch mAP 0.436950684 batch PCKh 0.3125\n",
      "Trained batch 2033 batch loss 0.678267896 batch mAP 0.35760498 batch PCKh 0.75\n",
      "Trained batch 2034 batch loss 0.63779664 batch mAP 0.478271484 batch PCKh 0.375\n",
      "Trained batch 2035 batch loss 0.620274484 batch mAP 0.506988525 batch PCKh 0.4375\n",
      "Trained batch 2036 batch loss 0.513160706 batch mAP 0.577636719 batch PCKh 0.8125\n",
      "Trained batch 2037 batch loss 0.576862812 batch mAP 0.565094 batch PCKh 0.75\n",
      "Trained batch 2038 batch loss 0.543993711 batch mAP 0.597137451 batch PCKh 0.5625\n",
      "Trained batch 2039 batch loss 0.556556463 batch mAP 0.613586426 batch PCKh 0.75\n",
      "Trained batch 2040 batch loss 0.578987718 batch mAP 0.538879395 batch PCKh 0.5625\n",
      "Trained batch 2041 batch loss 0.508051038 batch mAP 0.615081787 batch PCKh 0.4375\n",
      "Trained batch 2042 batch loss 0.595793247 batch mAP 0.580413818 batch PCKh 0.375\n",
      "Trained batch 2043 batch loss 0.670673311 batch mAP 0.524505615 batch PCKh 0.5625\n",
      "Trained batch 2044 batch loss 0.640000582 batch mAP 0.529602051 batch PCKh 0.125\n",
      "Trained batch 2045 batch loss 0.715417802 batch mAP 0.486694336 batch PCKh 0.1875\n",
      "Trained batch 2046 batch loss 0.673831463 batch mAP 0.519622803 batch PCKh 0.1875\n",
      "Trained batch 2047 batch loss 0.540838 batch mAP 0.613220215 batch PCKh 0.25\n",
      "Trained batch 2048 batch loss 0.647608459 batch mAP 0.514221191 batch PCKh 0.6875\n",
      "Trained batch 2049 batch loss 0.571481347 batch mAP 0.573577881 batch PCKh 0.5625\n",
      "Trained batch 2050 batch loss 0.601983 batch mAP 0.53024292 batch PCKh 0.5\n",
      "Trained batch 2051 batch loss 0.551806331 batch mAP 0.45690918 batch PCKh 0.5\n",
      "Trained batch 2052 batch loss 0.540850163 batch mAP 0.518219 batch PCKh 0.75\n",
      "Trained batch 2053 batch loss 0.551633835 batch mAP 0.500579834 batch PCKh 0.5\n",
      "Trained batch 2054 batch loss 0.591754854 batch mAP 0.462036133 batch PCKh 0.4375\n",
      "Trained batch 2055 batch loss 0.506941378 batch mAP 0.552642822 batch PCKh 0.4375\n",
      "Trained batch 2056 batch loss 0.518639684 batch mAP 0.560302734 batch PCKh 0.5\n",
      "Trained batch 2057 batch loss 0.628412724 batch mAP 0.551513672 batch PCKh 0.1875\n",
      "Trained batch 2058 batch loss 0.593891263 batch mAP 0.545166 batch PCKh 0.625\n",
      "Trained batch 2059 batch loss 0.531148911 batch mAP 0.58026123 batch PCKh 0.75\n",
      "Trained batch 2060 batch loss 0.575659454 batch mAP 0.517059326 batch PCKh 0.3125\n",
      "Trained batch 2061 batch loss 0.522345901 batch mAP 0.558380127 batch PCKh 0.375\n",
      "Trained batch 2062 batch loss 0.50326252 batch mAP 0.496032715 batch PCKh 0.375\n",
      "Trained batch 2063 batch loss 0.566062152 batch mAP 0.545257568 batch PCKh 0.1875\n",
      "Trained batch 2064 batch loss 0.436190724 batch mAP 0.51574707 batch PCKh 0.125\n",
      "Trained batch 2065 batch loss 0.457983017 batch mAP 0.656585693 batch PCKh 0.125\n",
      "Trained batch 2066 batch loss 0.468715221 batch mAP 0.612762451 batch PCKh 0.3125\n",
      "Trained batch 2067 batch loss 0.435497224 batch mAP 0.669586182 batch PCKh 0.3125\n",
      "Trained batch 2068 batch loss 0.491452336 batch mAP 0.630004883 batch PCKh 0.3125\n",
      "Trained batch 2069 batch loss 0.564504683 batch mAP 0.613616943 batch PCKh 0.5625\n",
      "Trained batch 2070 batch loss 0.631370246 batch mAP 0.584625244 batch PCKh 0.1875\n",
      "Trained batch 2071 batch loss 0.687648058 batch mAP 0.417541504 batch PCKh 0.3125\n",
      "Trained batch 2072 batch loss 0.760792 batch mAP 0.357025146 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2073 batch loss 0.702689052 batch mAP 0.424865723 batch PCKh 0.375\n",
      "Trained batch 2074 batch loss 0.592636406 batch mAP 0.507232666 batch PCKh 0.75\n",
      "Trained batch 2075 batch loss 0.732935548 batch mAP 0.442962646 batch PCKh 0.5\n",
      "Trained batch 2076 batch loss 0.608487427 batch mAP 0.480011 batch PCKh 0.875\n",
      "Trained batch 2077 batch loss 0.558407605 batch mAP 0.313934326 batch PCKh 0.75\n",
      "Trained batch 2078 batch loss 0.675309181 batch mAP 0.414367676 batch PCKh 0.6875\n",
      "Trained batch 2079 batch loss 0.619899452 batch mAP 0.409088135 batch PCKh 0.3125\n",
      "Trained batch 2080 batch loss 0.684051931 batch mAP 0.411224365 batch PCKh 0.75\n",
      "Trained batch 2081 batch loss 0.712515414 batch mAP 0.421539307 batch PCKh 0.25\n",
      "Trained batch 2082 batch loss 0.581238329 batch mAP 0.44732666 batch PCKh 0.375\n",
      "Trained batch 2083 batch loss 0.579222918 batch mAP 0.500152588 batch PCKh 0.5625\n",
      "Trained batch 2084 batch loss 0.538091481 batch mAP 0.536773682 batch PCKh 0.5625\n",
      "Trained batch 2085 batch loss 0.520860851 batch mAP 0.513458252 batch PCKh 0.3125\n",
      "Trained batch 2086 batch loss 0.614735961 batch mAP 0.553039551 batch PCKh 0.6875\n",
      "Trained batch 2087 batch loss 0.57175 batch mAP 0.523895264 batch PCKh 0.25\n",
      "Trained batch 2088 batch loss 0.642226338 batch mAP 0.563293457 batch PCKh 0.4375\n",
      "Trained batch 2089 batch loss 0.576075256 batch mAP 0.505767822 batch PCKh 0.125\n",
      "Trained batch 2090 batch loss 0.587007701 batch mAP 0.51171875 batch PCKh 0.375\n",
      "Trained batch 2091 batch loss 0.520901501 batch mAP 0.560150146 batch PCKh 0.25\n",
      "Trained batch 2092 batch loss 0.44757849 batch mAP 0.646026611 batch PCKh 0.3125\n",
      "Trained batch 2093 batch loss 0.606834412 batch mAP 0.517608643 batch PCKh 0.25\n",
      "Trained batch 2094 batch loss 0.555596948 batch mAP 0.53503418 batch PCKh 0.1875\n",
      "Trained batch 2095 batch loss 0.590377569 batch mAP 0.584869385 batch PCKh 0.5625\n",
      "Trained batch 2096 batch loss 0.537557781 batch mAP 0.578460693 batch PCKh 0.4375\n",
      "Trained batch 2097 batch loss 0.56565237 batch mAP 0.582183838 batch PCKh 0.6875\n",
      "Trained batch 2098 batch loss 0.524155796 batch mAP 0.586273193 batch PCKh 0.75\n",
      "Trained batch 2099 batch loss 0.590902805 batch mAP 0.558410645 batch PCKh 0.625\n",
      "Trained batch 2100 batch loss 0.553941667 batch mAP 0.519134521 batch PCKh 0.5625\n",
      "Trained batch 2101 batch loss 0.557817221 batch mAP 0.593414307 batch PCKh 0.5\n",
      "Trained batch 2102 batch loss 0.607389927 batch mAP 0.499786377 batch PCKh 0.6875\n",
      "Trained batch 2103 batch loss 0.566 batch mAP 0.606506348 batch PCKh 0.4375\n",
      "Trained batch 2104 batch loss 0.500753403 batch mAP 0.659118652 batch PCKh 0.3125\n",
      "Trained batch 2105 batch loss 0.626556575 batch mAP 0.594512939 batch PCKh 0\n",
      "Trained batch 2106 batch loss 0.511390448 batch mAP 0.616424561 batch PCKh 0\n",
      "Trained batch 2107 batch loss 0.588085711 batch mAP 0.538787842 batch PCKh 0.1875\n",
      "Trained batch 2108 batch loss 0.621708095 batch mAP 0.483673096 batch PCKh 0.25\n",
      "Trained batch 2109 batch loss 0.551143885 batch mAP 0.557525635 batch PCKh 0.3125\n",
      "Trained batch 2110 batch loss 0.56306541 batch mAP 0.516357422 batch PCKh 0.5\n",
      "Trained batch 2111 batch loss 0.560334384 batch mAP 0.539581299 batch PCKh 0.625\n",
      "Trained batch 2112 batch loss 0.553805649 batch mAP 0.552246094 batch PCKh 0.375\n",
      "Trained batch 2113 batch loss 0.524554133 batch mAP 0.527435303 batch PCKh 0.6875\n",
      "Trained batch 2114 batch loss 0.652406096 batch mAP 0.520294189 batch PCKh 0.5625\n",
      "Trained batch 2115 batch loss 0.653815389 batch mAP 0.492126465 batch PCKh 0.3125\n",
      "Trained batch 2116 batch loss 0.554335117 batch mAP 0.493988037 batch PCKh 0.375\n",
      "Trained batch 2117 batch loss 0.55066216 batch mAP 0.522613525 batch PCKh 0.5\n",
      "Trained batch 2118 batch loss 0.53175813 batch mAP 0.502105713 batch PCKh 0.1875\n",
      "Trained batch 2119 batch loss 0.601772785 batch mAP 0.545043945 batch PCKh 0.625\n",
      "Trained batch 2120 batch loss 0.580740571 batch mAP 0.49331665 batch PCKh 0.125\n",
      "Trained batch 2121 batch loss 0.530868292 batch mAP 0.538269043 batch PCKh 0.1875\n",
      "Trained batch 2122 batch loss 0.511684835 batch mAP 0.522369385 batch PCKh 0.5\n",
      "Trained batch 2123 batch loss 0.578646779 batch mAP 0.571746826 batch PCKh 0.375\n",
      "Trained batch 2124 batch loss 0.563152194 batch mAP 0.596984863 batch PCKh 0.5625\n",
      "Trained batch 2125 batch loss 0.571717381 batch mAP 0.587890625 batch PCKh 0.4375\n",
      "Trained batch 2126 batch loss 0.496733487 batch mAP 0.551544189 batch PCKh 0.3125\n",
      "Trained batch 2127 batch loss 0.468991131 batch mAP 0.548034668 batch PCKh 0\n",
      "Trained batch 2128 batch loss 0.472961307 batch mAP 0.54953 batch PCKh 0.375\n",
      "Trained batch 2129 batch loss 0.467935741 batch mAP 0.531494141 batch PCKh 0.5\n",
      "Trained batch 2130 batch loss 0.518254519 batch mAP 0.586456299 batch PCKh 0.625\n",
      "Trained batch 2131 batch loss 0.530214846 batch mAP 0.550292969 batch PCKh 0.1875\n",
      "Trained batch 2132 batch loss 0.527651727 batch mAP 0.59262085 batch PCKh 0.75\n",
      "Trained batch 2133 batch loss 0.508747816 batch mAP 0.604522705 batch PCKh 0.375\n",
      "Trained batch 2134 batch loss 0.499898493 batch mAP 0.585693359 batch PCKh 0.25\n",
      "Trained batch 2135 batch loss 0.533326626 batch mAP 0.597747803 batch PCKh 0.75\n",
      "Trained batch 2136 batch loss 0.565969288 batch mAP 0.613220215 batch PCKh 0.6875\n",
      "Trained batch 2137 batch loss 0.53568989 batch mAP 0.619903564 batch PCKh 0.375\n",
      "Trained batch 2138 batch loss 0.491872072 batch mAP 0.648712158 batch PCKh 0.4375\n",
      "Trained batch 2139 batch loss 0.349475831 batch mAP 0.718719482 batch PCKh 0.5\n",
      "Trained batch 2140 batch loss 0.57767278 batch mAP 0.490631104 batch PCKh 0.4375\n",
      "Trained batch 2141 batch loss 0.509434581 batch mAP 0.550476074 batch PCKh 0.5\n",
      "Trained batch 2142 batch loss 0.632973731 batch mAP 0.424987793 batch PCKh 0.4375\n",
      "Trained batch 2143 batch loss 0.618448257 batch mAP 0.501190186 batch PCKh 0.1875\n",
      "Trained batch 2144 batch loss 0.508546 batch mAP 0.594512939 batch PCKh 0.5\n",
      "Trained batch 2145 batch loss 0.57746923 batch mAP 0.558136 batch PCKh 0.1875\n",
      "Trained batch 2146 batch loss 0.598522663 batch mAP 0.509124756 batch PCKh 0.625\n",
      "Trained batch 2147 batch loss 0.544917822 batch mAP 0.505523682 batch PCKh 0.125\n",
      "Trained batch 2148 batch loss 0.412226945 batch mAP 0.548614502 batch PCKh 0.375\n",
      "Trained batch 2149 batch loss 0.493401 batch mAP 0.491546631 batch PCKh 0.5\n",
      "Trained batch 2150 batch loss 0.502650619 batch mAP 0.501678467 batch PCKh 0.25\n",
      "Trained batch 2151 batch loss 0.525864601 batch mAP 0.520721436 batch PCKh 0.5625\n",
      "Trained batch 2152 batch loss 0.500702262 batch mAP 0.522003174 batch PCKh 0.625\n",
      "Trained batch 2153 batch loss 0.525901 batch mAP 0.463928223 batch PCKh 0.375\n",
      "Trained batch 2154 batch loss 0.568973184 batch mAP 0.494232178 batch PCKh 0.3125\n",
      "Trained batch 2155 batch loss 0.581855834 batch mAP 0.549865723 batch PCKh 0.3125\n",
      "Trained batch 2156 batch loss 0.632120132 batch mAP 0.508270264 batch PCKh 0.625\n",
      "Trained batch 2157 batch loss 0.570164144 batch mAP 0.548461914 batch PCKh 0.4375\n",
      "Trained batch 2158 batch loss 0.602891445 batch mAP 0.538452148 batch PCKh 0.5625\n",
      "Trained batch 2159 batch loss 0.689240694 batch mAP 0.486633301 batch PCKh 0.3125\n",
      "Trained batch 2160 batch loss 0.643447578 batch mAP 0.508056641 batch PCKh 0.5\n",
      "Trained batch 2161 batch loss 0.675474763 batch mAP 0.474212646 batch PCKh 0.3125\n",
      "Trained batch 2162 batch loss 0.66458267 batch mAP 0.455688477 batch PCKh 0\n",
      "Trained batch 2163 batch loss 0.626965404 batch mAP 0.417877197 batch PCKh 0.6875\n",
      "Trained batch 2164 batch loss 0.553129435 batch mAP 0.463409424 batch PCKh 0.4375\n",
      "Trained batch 2165 batch loss 0.586642623 batch mAP 0.373809814 batch PCKh 0.8125\n",
      "Trained batch 2166 batch loss 0.566261053 batch mAP 0.412506104 batch PCKh 0.6875\n",
      "Trained batch 2167 batch loss 0.526527226 batch mAP 0.389434814 batch PCKh 0.375\n",
      "Trained batch 2168 batch loss 0.555328846 batch mAP 0.345916748 batch PCKh 0.4375\n",
      "Trained batch 2169 batch loss 0.652025223 batch mAP 0.348327637 batch PCKh 0.4375\n",
      "Trained batch 2170 batch loss 0.686000049 batch mAP 0.375183105 batch PCKh 0.25\n",
      "Trained batch 2171 batch loss 0.717936873 batch mAP 0.446472168 batch PCKh 0\n",
      "Trained batch 2172 batch loss 0.665440798 batch mAP 0.449737549 batch PCKh 0.5625\n",
      "Trained batch 2173 batch loss 0.534071684 batch mAP 0.443237305 batch PCKh 0.625\n",
      "Trained batch 2174 batch loss 0.529284298 batch mAP 0.477264404 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2175 batch loss 0.4973225 batch mAP 0.520843506 batch PCKh 0.5\n",
      "Trained batch 2176 batch loss 0.432442307 batch mAP 0.508483887 batch PCKh 0.3125\n",
      "Trained batch 2177 batch loss 0.507172823 batch mAP 0.487335205 batch PCKh 0.0625\n",
      "Trained batch 2178 batch loss 0.629126 batch mAP 0.461853027 batch PCKh 0.125\n",
      "Trained batch 2179 batch loss 0.672548771 batch mAP 0.496856689 batch PCKh 0\n",
      "Trained batch 2180 batch loss 0.64149332 batch mAP 0.439666748 batch PCKh 0.0625\n",
      "Trained batch 2181 batch loss 0.602088 batch mAP 0.536651611 batch PCKh 0.5625\n",
      "Trained batch 2182 batch loss 0.620723963 batch mAP 0.51864624 batch PCKh 0.1875\n",
      "Trained batch 2183 batch loss 0.645390868 batch mAP 0.533630371 batch PCKh 0.5\n",
      "Trained batch 2184 batch loss 0.618756831 batch mAP 0.530029297 batch PCKh 0.0625\n",
      "Trained batch 2185 batch loss 0.622320652 batch mAP 0.487426758 batch PCKh 0.6875\n",
      "Trained batch 2186 batch loss 0.634251833 batch mAP 0.49118042 batch PCKh 0.375\n",
      "Trained batch 2187 batch loss 0.580182612 batch mAP 0.562988281 batch PCKh 0.0625\n",
      "Trained batch 2188 batch loss 0.59343797 batch mAP 0.594146729 batch PCKh 0.3125\n",
      "Trained batch 2189 batch loss 0.626254857 batch mAP 0.564971924 batch PCKh 0.1875\n",
      "Trained batch 2190 batch loss 0.557913721 batch mAP 0.555206299 batch PCKh 0.25\n",
      "Trained batch 2191 batch loss 0.615422904 batch mAP 0.589538574 batch PCKh 0.375\n",
      "Trained batch 2192 batch loss 0.510600686 batch mAP 0.552612305 batch PCKh 0.5\n",
      "Trained batch 2193 batch loss 0.596100926 batch mAP 0.459442139 batch PCKh 0.125\n",
      "Trained batch 2194 batch loss 0.674276 batch mAP 0.438995361 batch PCKh 0.625\n",
      "Trained batch 2195 batch loss 0.52400142 batch mAP 0.438110352 batch PCKh 0\n",
      "Trained batch 2196 batch loss 0.546823442 batch mAP 0.527069092 batch PCKh 0.25\n",
      "Trained batch 2197 batch loss 0.592029452 batch mAP 0.535247803 batch PCKh 0.8125\n",
      "Trained batch 2198 batch loss 0.580407262 batch mAP 0.520782471 batch PCKh 0.625\n",
      "Trained batch 2199 batch loss 0.610601544 batch mAP 0.547454834 batch PCKh 0.6875\n",
      "Trained batch 2200 batch loss 0.548149228 batch mAP 0.54019165 batch PCKh 0.75\n",
      "Trained batch 2201 batch loss 0.581548 batch mAP 0.520629883 batch PCKh 0.8125\n",
      "Trained batch 2202 batch loss 0.576773643 batch mAP 0.564056396 batch PCKh 0.6875\n",
      "Trained batch 2203 batch loss 0.576488495 batch mAP 0.548217773 batch PCKh 0.3125\n",
      "Trained batch 2204 batch loss 0.63088274 batch mAP 0.520446777 batch PCKh 0.5625\n",
      "Trained batch 2205 batch loss 0.602341354 batch mAP 0.577392578 batch PCKh 0.625\n",
      "Trained batch 2206 batch loss 0.597759783 batch mAP 0.554443359 batch PCKh 0.625\n",
      "Trained batch 2207 batch loss 0.587135792 batch mAP 0.54876709 batch PCKh 0.3125\n",
      "Trained batch 2208 batch loss 0.6116786 batch mAP 0.523132324 batch PCKh 0.25\n",
      "Trained batch 2209 batch loss 0.674168289 batch mAP 0.498657227 batch PCKh 0.125\n",
      "Trained batch 2210 batch loss 0.583932102 batch mAP 0.597961426 batch PCKh 0.6875\n",
      "Trained batch 2211 batch loss 0.574153721 batch mAP 0.61126709 batch PCKh 0.3125\n",
      "Trained batch 2212 batch loss 0.604011416 batch mAP 0.557647705 batch PCKh 0.4375\n",
      "Trained batch 2213 batch loss 0.588264227 batch mAP 0.584960938 batch PCKh 0.3125\n",
      "Trained batch 2214 batch loss 0.608293533 batch mAP 0.560394287 batch PCKh 0.25\n",
      "Trained batch 2215 batch loss 0.590566814 batch mAP 0.564361572 batch PCKh 0.25\n",
      "Trained batch 2216 batch loss 0.584824324 batch mAP 0.536376953 batch PCKh 0.625\n",
      "Trained batch 2217 batch loss 0.604209185 batch mAP 0.537658691 batch PCKh 0.625\n",
      "Trained batch 2218 batch loss 0.673213959 batch mAP 0.51675415 batch PCKh 0.8125\n",
      "Trained batch 2219 batch loss 0.673042297 batch mAP 0.481384277 batch PCKh 0.5\n",
      "Trained batch 2220 batch loss 0.589477777 batch mAP 0.503631592 batch PCKh 0.75\n",
      "Trained batch 2221 batch loss 0.547482848 batch mAP 0.545562744 batch PCKh 0.75\n",
      "Trained batch 2222 batch loss 0.583586752 batch mAP 0.496917725 batch PCKh 0.5625\n",
      "Trained batch 2223 batch loss 0.537189722 batch mAP 0.515777588 batch PCKh 0.125\n",
      "Trained batch 2224 batch loss 0.65550518 batch mAP 0.503051758 batch PCKh 0.6875\n",
      "Trained batch 2225 batch loss 0.666555166 batch mAP 0.502410889 batch PCKh 0.875\n",
      "Trained batch 2226 batch loss 0.563841462 batch mAP 0.57131958 batch PCKh 0.25\n",
      "Trained batch 2227 batch loss 0.603600502 batch mAP 0.530578613 batch PCKh 0.4375\n",
      "Trained batch 2228 batch loss 0.62464 batch mAP 0.545501709 batch PCKh 0.875\n",
      "Trained batch 2229 batch loss 0.602427602 batch mAP 0.560119629 batch PCKh 0.75\n",
      "Trained batch 2230 batch loss 0.600908399 batch mAP 0.527252197 batch PCKh 0.3125\n",
      "Trained batch 2231 batch loss 0.588612914 batch mAP 0.512420654 batch PCKh 0.75\n",
      "Trained batch 2232 batch loss 0.535582781 batch mAP 0.544799805 batch PCKh 0.0625\n",
      "Trained batch 2233 batch loss 0.61670804 batch mAP 0.521606445 batch PCKh 0.5\n",
      "Trained batch 2234 batch loss 0.550257206 batch mAP 0.549743652 batch PCKh 0.25\n",
      "Trained batch 2235 batch loss 0.672451854 batch mAP 0.540618896 batch PCKh 0.125\n",
      "Trained batch 2236 batch loss 0.668914 batch mAP 0.538299561 batch PCKh 0.5625\n",
      "Trained batch 2237 batch loss 0.556131 batch mAP 0.531951904 batch PCKh 0.6875\n",
      "Trained batch 2238 batch loss 0.660276055 batch mAP 0.523590088 batch PCKh 0.25\n",
      "Trained batch 2239 batch loss 0.65019691 batch mAP 0.524719238 batch PCKh 0.625\n",
      "Trained batch 2240 batch loss 0.583647251 batch mAP 0.540374756 batch PCKh 0.25\n",
      "Trained batch 2241 batch loss 0.622427881 batch mAP 0.630981445 batch PCKh 0.8125\n",
      "Trained batch 2242 batch loss 0.61845392 batch mAP 0.593841553 batch PCKh 0.875\n",
      "Trained batch 2243 batch loss 0.552977204 batch mAP 0.610534668 batch PCKh 0.1875\n",
      "Trained batch 2244 batch loss 0.483379632 batch mAP 0.645904541 batch PCKh 0.375\n",
      "Trained batch 2245 batch loss 0.410770565 batch mAP 0.650756836 batch PCKh 0.1875\n",
      "Trained batch 2246 batch loss 0.434447944 batch mAP 0.679779053 batch PCKh 0.1875\n",
      "Trained batch 2247 batch loss 0.544454813 batch mAP 0.628509521 batch PCKh 0.375\n",
      "Trained batch 2248 batch loss 0.595660448 batch mAP 0.621002197 batch PCKh 0.1875\n",
      "Trained batch 2249 batch loss 0.538325131 batch mAP 0.651519775 batch PCKh 0.5\n",
      "Trained batch 2250 batch loss 0.526396871 batch mAP 0.566711426 batch PCKh 0.375\n",
      "Trained batch 2251 batch loss 0.573469281 batch mAP 0.42401123 batch PCKh 0.0625\n",
      "Trained batch 2252 batch loss 0.622365832 batch mAP 0.554351807 batch PCKh 0.6875\n",
      "Trained batch 2253 batch loss 0.676345825 batch mAP 0.454223633 batch PCKh 0.125\n",
      "Trained batch 2254 batch loss 0.57661593 batch mAP 0.530426 batch PCKh 0\n",
      "Trained batch 2255 batch loss 0.602834821 batch mAP 0.517730713 batch PCKh 0.3125\n",
      "Trained batch 2256 batch loss 0.620374262 batch mAP 0.488616943 batch PCKh 0.3125\n",
      "Trained batch 2257 batch loss 0.60456866 batch mAP 0.513763428 batch PCKh 0\n",
      "Trained batch 2258 batch loss 0.612746716 batch mAP 0.518981934 batch PCKh 0.75\n",
      "Trained batch 2259 batch loss 0.588258445 batch mAP 0.486785889 batch PCKh 0.75\n",
      "Trained batch 2260 batch loss 0.553651452 batch mAP 0.548675537 batch PCKh 0.25\n",
      "Trained batch 2261 batch loss 0.583879828 batch mAP 0.452423096 batch PCKh 0.0625\n",
      "Trained batch 2262 batch loss 0.65734303 batch mAP 0.462249756 batch PCKh 0.25\n",
      "Trained batch 2263 batch loss 0.56213367 batch mAP 0.559326172 batch PCKh 0.1875\n",
      "Trained batch 2264 batch loss 0.592494845 batch mAP 0.448181152 batch PCKh 0.5\n",
      "Trained batch 2265 batch loss 0.673690915 batch mAP 0.450073242 batch PCKh 0.1875\n",
      "Trained batch 2266 batch loss 0.598155737 batch mAP 0.434570312 batch PCKh 0.1875\n",
      "Trained batch 2267 batch loss 0.586414814 batch mAP 0.421386719 batch PCKh 0.4375\n",
      "Trained batch 2268 batch loss 0.597481489 batch mAP 0.39163208 batch PCKh 0.875\n",
      "Trained batch 2269 batch loss 0.584349096 batch mAP 0.46472168 batch PCKh 0.6875\n",
      "Trained batch 2270 batch loss 0.572210968 batch mAP 0.511169434 batch PCKh 0.1875\n",
      "Trained batch 2271 batch loss 0.510941207 batch mAP 0.493347168 batch PCKh 0.625\n",
      "Trained batch 2272 batch loss 0.537245095 batch mAP 0.512512207 batch PCKh 0.4375\n",
      "Trained batch 2273 batch loss 0.538524449 batch mAP 0.52935791 batch PCKh 0.3125\n",
      "Trained batch 2274 batch loss 0.458113968 batch mAP 0.548431396 batch PCKh 0.625\n",
      "Trained batch 2275 batch loss 0.493162364 batch mAP 0.582885742 batch PCKh 0.75\n",
      "Trained batch 2276 batch loss 0.506753385 batch mAP 0.568359375 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2277 batch loss 0.56012553 batch mAP 0.550231934 batch PCKh 0.75\n",
      "Trained batch 2278 batch loss 0.515278041 batch mAP 0.574829102 batch PCKh 0.75\n",
      "Trained batch 2279 batch loss 0.573418558 batch mAP 0.487426758 batch PCKh 0.75\n",
      "Trained batch 2280 batch loss 0.532985091 batch mAP 0.513519287 batch PCKh 0.6875\n",
      "Trained batch 2281 batch loss 0.581974745 batch mAP 0.537902832 batch PCKh 0.6875\n",
      "Trained batch 2282 batch loss 0.565291762 batch mAP 0.578674316 batch PCKh 0.6875\n",
      "Trained batch 2283 batch loss 0.565529823 batch mAP 0.578338623 batch PCKh 0.75\n",
      "Trained batch 2284 batch loss 0.451556623 batch mAP 0.535369873 batch PCKh 0.0625\n",
      "Trained batch 2285 batch loss 0.555132747 batch mAP 0.61151123 batch PCKh 0.375\n",
      "Trained batch 2286 batch loss 0.559188604 batch mAP 0.575042725 batch PCKh 0.875\n",
      "Trained batch 2287 batch loss 0.565852046 batch mAP 0.603607178 batch PCKh 0.75\n",
      "Trained batch 2288 batch loss 0.550042391 batch mAP 0.586669922 batch PCKh 0.625\n",
      "Trained batch 2289 batch loss 0.515731037 batch mAP 0.598907471 batch PCKh 0.75\n",
      "Trained batch 2290 batch loss 0.505124688 batch mAP 0.626800537 batch PCKh 0.5\n",
      "Trained batch 2291 batch loss 0.563144088 batch mAP 0.53237915 batch PCKh 0.4375\n",
      "Trained batch 2292 batch loss 0.612445474 batch mAP 0.58380127 batch PCKh 0.4375\n",
      "Trained batch 2293 batch loss 0.649913907 batch mAP 0.607605 batch PCKh 0.5\n",
      "Trained batch 2294 batch loss 0.5602597 batch mAP 0.526947 batch PCKh 0.5625\n",
      "Trained batch 2295 batch loss 0.668180585 batch mAP 0.546478271 batch PCKh 0.375\n",
      "Trained batch 2296 batch loss 0.598099232 batch mAP 0.522918701 batch PCKh 0.0625\n",
      "Trained batch 2297 batch loss 0.569183409 batch mAP 0.514282227 batch PCKh 0.6875\n",
      "Trained batch 2298 batch loss 0.648809135 batch mAP 0.456665039 batch PCKh 0.8125\n",
      "Trained batch 2299 batch loss 0.523997128 batch mAP 0.458526611 batch PCKh 0.5\n",
      "Trained batch 2300 batch loss 0.564962327 batch mAP 0.486022949 batch PCKh 0.4375\n",
      "Trained batch 2301 batch loss 0.534257531 batch mAP 0.495391846 batch PCKh 0.4375\n",
      "Trained batch 2302 batch loss 0.46518302 batch mAP 0.577728271 batch PCKh 0.1875\n",
      "Trained batch 2303 batch loss 0.587881744 batch mAP 0.473114 batch PCKh 0.75\n",
      "Trained batch 2304 batch loss 0.541862726 batch mAP 0.507171631 batch PCKh 0.3125\n",
      "Trained batch 2305 batch loss 0.59499079 batch mAP 0.523895264 batch PCKh 0.75\n",
      "Trained batch 2306 batch loss 0.684479833 batch mAP 0.512023926 batch PCKh 0.75\n",
      "Trained batch 2307 batch loss 0.557442307 batch mAP 0.53024292 batch PCKh 0.25\n",
      "Trained batch 2308 batch loss 0.625064492 batch mAP 0.511291504 batch PCKh 0.3125\n",
      "Trained batch 2309 batch loss 0.614183605 batch mAP 0.500488281 batch PCKh 0.3125\n",
      "Trained batch 2310 batch loss 0.528076947 batch mAP 0.586151123 batch PCKh 0.6875\n",
      "Trained batch 2311 batch loss 0.541761398 batch mAP 0.509033203 batch PCKh 0.8125\n",
      "Trained batch 2312 batch loss 0.61638391 batch mAP 0.50692749 batch PCKh 0.375\n",
      "Trained batch 2313 batch loss 0.585162878 batch mAP 0.503112793 batch PCKh 0.5625\n",
      "Trained batch 2314 batch loss 0.492937028 batch mAP 0.472900391 batch PCKh 0.375\n",
      "Trained batch 2315 batch loss 0.558189273 batch mAP 0.405273438 batch PCKh 0.3125\n",
      "Trained batch 2316 batch loss 0.589172482 batch mAP 0.508453369 batch PCKh 0.75\n",
      "Trained batch 2317 batch loss 0.595525265 batch mAP 0.570526123 batch PCKh 0.25\n",
      "Trained batch 2318 batch loss 0.602130234 batch mAP 0.606170654 batch PCKh 0.25\n",
      "Trained batch 2319 batch loss 0.601783037 batch mAP 0.586486816 batch PCKh 0.375\n",
      "Trained batch 2320 batch loss 0.548128486 batch mAP 0.650848389 batch PCKh 0.5625\n",
      "Trained batch 2321 batch loss 0.507154226 batch mAP 0.628356934 batch PCKh 0.5\n",
      "Trained batch 2322 batch loss 0.505732894 batch mAP 0.630493164 batch PCKh 0.6875\n",
      "Trained batch 2323 batch loss 0.540166378 batch mAP 0.588653564 batch PCKh 0.625\n",
      "Trained batch 2324 batch loss 0.486722 batch mAP 0.542327881 batch PCKh 0.4375\n",
      "Trained batch 2325 batch loss 0.561766267 batch mAP 0.564117432 batch PCKh 0.5\n",
      "Trained batch 2326 batch loss 0.567315936 batch mAP 0.541626 batch PCKh 0.4375\n",
      "Trained batch 2327 batch loss 0.613600612 batch mAP 0.458618164 batch PCKh 0.5\n",
      "Trained batch 2328 batch loss 0.603194833 batch mAP 0.499237061 batch PCKh 0.5\n",
      "Trained batch 2329 batch loss 0.626969635 batch mAP 0.534942627 batch PCKh 0.1875\n",
      "Trained batch 2330 batch loss 0.593917787 batch mAP 0.470977783 batch PCKh 0.625\n",
      "Trained batch 2331 batch loss 0.524439096 batch mAP 0.50881958 batch PCKh 0.625\n",
      "Trained batch 2332 batch loss 0.548075795 batch mAP 0.499450684 batch PCKh 0.75\n",
      "Trained batch 2333 batch loss 0.573828101 batch mAP 0.477844238 batch PCKh 0.875\n",
      "Trained batch 2334 batch loss 0.399499923 batch mAP 0.686462402 batch PCKh 0.5625\n",
      "Trained batch 2335 batch loss 0.440109611 batch mAP 0.630645752 batch PCKh 0.625\n",
      "Trained batch 2336 batch loss 0.486602545 batch mAP 0.570587158 batch PCKh 0.75\n",
      "Trained batch 2337 batch loss 0.486108899 batch mAP 0.594299316 batch PCKh 0.875\n",
      "Trained batch 2338 batch loss 0.456678778 batch mAP 0.600708 batch PCKh 0.875\n",
      "Trained batch 2339 batch loss 0.400417447 batch mAP 0.662414551 batch PCKh 0.5625\n",
      "Trained batch 2340 batch loss 0.439955115 batch mAP 0.625213623 batch PCKh 0.875\n",
      "Trained batch 2341 batch loss 0.52321583 batch mAP 0.500366211 batch PCKh 0.5\n",
      "Trained batch 2342 batch loss 0.564210534 batch mAP 0.5730896 batch PCKh 0.375\n",
      "Trained batch 2343 batch loss 0.527016699 batch mAP 0.647491455 batch PCKh 0.4375\n",
      "Trained batch 2344 batch loss 0.562212765 batch mAP 0.511505127 batch PCKh 0.5625\n",
      "Trained batch 2345 batch loss 0.69037 batch mAP 0.609069824 batch PCKh 0.1875\n",
      "Trained batch 2346 batch loss 0.53428 batch mAP 0.585845947 batch PCKh 0.5\n",
      "Trained batch 2347 batch loss 0.489434779 batch mAP 0.615020752 batch PCKh 0.625\n",
      "Trained batch 2348 batch loss 0.515144765 batch mAP 0.621795654 batch PCKh 0.3125\n",
      "Trained batch 2349 batch loss 0.612332344 batch mAP 0.595947266 batch PCKh 0.125\n",
      "Trained batch 2350 batch loss 0.61801821 batch mAP 0.545593262 batch PCKh 0.6875\n",
      "Trained batch 2351 batch loss 0.634641171 batch mAP 0.50894165 batch PCKh 0.625\n",
      "Trained batch 2352 batch loss 0.584477067 batch mAP 0.537231445 batch PCKh 0.75\n",
      "Trained batch 2353 batch loss 0.517797828 batch mAP 0.518798828 batch PCKh 0.25\n",
      "Trained batch 2354 batch loss 0.568922341 batch mAP 0.521820068 batch PCKh 0.5625\n",
      "Trained batch 2355 batch loss 0.510076 batch mAP 0.523376465 batch PCKh 0.6875\n",
      "Trained batch 2356 batch loss 0.457481056 batch mAP 0.513977051 batch PCKh 0.5625\n",
      "Trained batch 2357 batch loss 0.505166292 batch mAP 0.478851318 batch PCKh 0.5625\n",
      "Trained batch 2358 batch loss 0.603490472 batch mAP 0.515380859 batch PCKh 0.25\n",
      "Trained batch 2359 batch loss 0.615938306 batch mAP 0.56350708 batch PCKh 0.1875\n",
      "Trained batch 2360 batch loss 0.544170082 batch mAP 0.532959 batch PCKh 0.75\n",
      "Trained batch 2361 batch loss 0.567744553 batch mAP 0.504638672 batch PCKh 0.6875\n",
      "Trained batch 2362 batch loss 0.65174973 batch mAP 0.494293213 batch PCKh 0.25\n",
      "Trained batch 2363 batch loss 0.664334238 batch mAP 0.479003906 batch PCKh 0.5625\n",
      "Trained batch 2364 batch loss 0.547018588 batch mAP 0.54006958 batch PCKh 0.1875\n",
      "Trained batch 2365 batch loss 0.567957222 batch mAP 0.597167969 batch PCKh 0.5\n",
      "Trained batch 2366 batch loss 0.540585458 batch mAP 0.611938477 batch PCKh 0.3125\n",
      "Trained batch 2367 batch loss 0.506705403 batch mAP 0.562835693 batch PCKh 0.5\n",
      "Trained batch 2368 batch loss 0.493652195 batch mAP 0.540435791 batch PCKh 0.6875\n",
      "Trained batch 2369 batch loss 0.586439371 batch mAP 0.583129883 batch PCKh 0.375\n",
      "Trained batch 2370 batch loss 0.570014238 batch mAP 0.580413818 batch PCKh 0.1875\n",
      "Trained batch 2371 batch loss 0.574152946 batch mAP 0.561462402 batch PCKh 0.375\n",
      "Trained batch 2372 batch loss 0.629404128 batch mAP 0.575683594 batch PCKh 0.4375\n",
      "Trained batch 2373 batch loss 0.559366703 batch mAP 0.575714111 batch PCKh 0.4375\n",
      "Trained batch 2374 batch loss 0.558003664 batch mAP 0.558319092 batch PCKh 0.625\n",
      "Trained batch 2375 batch loss 0.580120862 batch mAP 0.558349609 batch PCKh 0.375\n",
      "Trained batch 2376 batch loss 0.648622334 batch mAP 0.525817871 batch PCKh 0.3125\n",
      "Trained batch 2377 batch loss 0.601598859 batch mAP 0.540344238 batch PCKh 0.125\n",
      "Trained batch 2378 batch loss 0.539401472 batch mAP 0.543792725 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2379 batch loss 0.481692374 batch mAP 0.59564209 batch PCKh 0.5625\n",
      "Trained batch 2380 batch loss 0.513471961 batch mAP 0.545593262 batch PCKh 0.3125\n",
      "Trained batch 2381 batch loss 0.520017505 batch mAP 0.540374756 batch PCKh 0.75\n",
      "Trained batch 2382 batch loss 0.541306853 batch mAP 0.52948 batch PCKh 0.625\n",
      "Trained batch 2383 batch loss 0.581022203 batch mAP 0.594635 batch PCKh 0.625\n",
      "Trained batch 2384 batch loss 0.493067294 batch mAP 0.610443115 batch PCKh 0.4375\n",
      "Trained batch 2385 batch loss 0.504579306 batch mAP 0.537780762 batch PCKh 0.25\n",
      "Trained batch 2386 batch loss 0.491367847 batch mAP 0.607940674 batch PCKh 0.625\n",
      "Trained batch 2387 batch loss 0.627985895 batch mAP 0.493103027 batch PCKh 0.875\n",
      "Trained batch 2388 batch loss 0.553171635 batch mAP 0.466308594 batch PCKh 0.75\n",
      "Trained batch 2389 batch loss 0.628924966 batch mAP 0.48538208 batch PCKh 0.75\n",
      "Trained batch 2390 batch loss 0.572632 batch mAP 0.587463379 batch PCKh 0.5625\n",
      "Trained batch 2391 batch loss 0.56133455 batch mAP 0.59552 batch PCKh 0.25\n",
      "Trained batch 2392 batch loss 0.501052856 batch mAP 0.655365 batch PCKh 0.5625\n",
      "Trained batch 2393 batch loss 0.532547295 batch mAP 0.653289795 batch PCKh 0.3125\n",
      "Trained batch 2394 batch loss 0.542832 batch mAP 0.616088867 batch PCKh 0.25\n",
      "Trained batch 2395 batch loss 0.513289869 batch mAP 0.642578125 batch PCKh 0.4375\n",
      "Trained batch 2396 batch loss 0.572197795 batch mAP 0.584869385 batch PCKh 0.5\n",
      "Trained batch 2397 batch loss 0.606784582 batch mAP 0.578521729 batch PCKh 0.625\n",
      "Trained batch 2398 batch loss 0.606877089 batch mAP 0.581329346 batch PCKh 0.375\n",
      "Trained batch 2399 batch loss 0.595672667 batch mAP 0.5887146 batch PCKh 0.8125\n",
      "Trained batch 2400 batch loss 0.575091302 batch mAP 0.570098877 batch PCKh 0.5\n",
      "Trained batch 2401 batch loss 0.626637161 batch mAP 0.546844482 batch PCKh 0.625\n",
      "Trained batch 2402 batch loss 0.547397435 batch mAP 0.479278564 batch PCKh 0.625\n",
      "Trained batch 2403 batch loss 0.560654759 batch mAP 0.606201172 batch PCKh 0.5625\n",
      "Trained batch 2404 batch loss 0.600969672 batch mAP 0.557434082 batch PCKh 0.625\n",
      "Trained batch 2405 batch loss 0.613471 batch mAP 0.480743408 batch PCKh 0.125\n",
      "Trained batch 2406 batch loss 0.620062709 batch mAP 0.525604248 batch PCKh 0.75\n",
      "Trained batch 2407 batch loss 0.599169552 batch mAP 0.56652832 batch PCKh 0.5625\n",
      "Trained batch 2408 batch loss 0.664561093 batch mAP 0.523864746 batch PCKh 0.1875\n",
      "Trained batch 2409 batch loss 0.647388577 batch mAP 0.573455811 batch PCKh 0.1875\n",
      "Trained batch 2410 batch loss 0.587014318 batch mAP 0.53302 batch PCKh 0.4375\n",
      "Trained batch 2411 batch loss 0.629947782 batch mAP 0.492828369 batch PCKh 0.5625\n",
      "Trained batch 2412 batch loss 0.690562308 batch mAP 0.473266602 batch PCKh 0.1875\n",
      "Trained batch 2413 batch loss 0.585892916 batch mAP 0.542022705 batch PCKh 0.5625\n",
      "Trained batch 2414 batch loss 0.68637991 batch mAP 0.462432861 batch PCKh 0.0625\n",
      "Trained batch 2415 batch loss 0.717357159 batch mAP 0.461242676 batch PCKh 0.125\n",
      "Trained batch 2416 batch loss 0.581344604 batch mAP 0.399292 batch PCKh 0\n",
      "Trained batch 2417 batch loss 0.470352173 batch mAP 0.418457031 batch PCKh 0.125\n",
      "Trained batch 2418 batch loss 0.512537122 batch mAP 0.476348877 batch PCKh 0.4375\n",
      "Trained batch 2419 batch loss 0.565398693 batch mAP 0.466888428 batch PCKh 0.4375\n",
      "Trained batch 2420 batch loss 0.576615334 batch mAP 0.487213135 batch PCKh 0.5\n",
      "Trained batch 2421 batch loss 0.616369724 batch mAP 0.494567871 batch PCKh 0.25\n",
      "Trained batch 2422 batch loss 0.541074753 batch mAP 0.511199951 batch PCKh 0.375\n",
      "Trained batch 2423 batch loss 0.428220689 batch mAP 0.616577148 batch PCKh 0.625\n",
      "Trained batch 2424 batch loss 0.541997194 batch mAP 0.563873291 batch PCKh 0.1875\n",
      "Trained batch 2425 batch loss 0.522223949 batch mAP 0.591247559 batch PCKh 0.25\n",
      "Trained batch 2426 batch loss 0.526812196 batch mAP 0.555053711 batch PCKh 0.5625\n",
      "Trained batch 2427 batch loss 0.640009642 batch mAP 0.594543457 batch PCKh 0.6875\n",
      "Trained batch 2428 batch loss 0.571899176 batch mAP 0.50567627 batch PCKh 0.625\n",
      "Trained batch 2429 batch loss 0.580424786 batch mAP 0.44631958 batch PCKh 0.25\n",
      "Trained batch 2430 batch loss 0.559211254 batch mAP 0.520751953 batch PCKh 0.25\n",
      "Trained batch 2431 batch loss 0.606870651 batch mAP 0.538879395 batch PCKh 0.5\n",
      "Trained batch 2432 batch loss 0.596725702 batch mAP 0.555786133 batch PCKh 0.375\n",
      "Trained batch 2433 batch loss 0.575131655 batch mAP 0.617858887 batch PCKh 0.8125\n",
      "Trained batch 2434 batch loss 0.58387059 batch mAP 0.612365723 batch PCKh 0.125\n",
      "Trained batch 2435 batch loss 0.587557077 batch mAP 0.621429443 batch PCKh 0.25\n",
      "Trained batch 2436 batch loss 0.598093152 batch mAP 0.606567383 batch PCKh 0.25\n",
      "Trained batch 2437 batch loss 0.531838596 batch mAP 0.617340088 batch PCKh 0.3125\n",
      "Trained batch 2438 batch loss 0.615708947 batch mAP 0.546936035 batch PCKh 0.3125\n",
      "Trained batch 2439 batch loss 0.739059508 batch mAP 0.491241455 batch PCKh 0\n",
      "Trained batch 2440 batch loss 0.661556304 batch mAP 0.533203125 batch PCKh 0.125\n",
      "Trained batch 2441 batch loss 0.570697844 batch mAP 0.494842529 batch PCKh 0.5625\n",
      "Trained batch 2442 batch loss 0.576269686 batch mAP 0.494995117 batch PCKh 0.5\n",
      "Trained batch 2443 batch loss 0.535212517 batch mAP 0.436889648 batch PCKh 0.3125\n",
      "Trained batch 2444 batch loss 0.590404153 batch mAP 0.431060791 batch PCKh 0.125\n",
      "Trained batch 2445 batch loss 0.677448928 batch mAP 0.433654785 batch PCKh 0.1875\n",
      "Trained batch 2446 batch loss 0.603456199 batch mAP 0.441894531 batch PCKh 0.3125\n",
      "Trained batch 2447 batch loss 0.688924491 batch mAP 0.459899902 batch PCKh 0.125\n",
      "Trained batch 2448 batch loss 0.635523379 batch mAP 0.481109619 batch PCKh 0.6875\n",
      "Trained batch 2449 batch loss 0.591496229 batch mAP 0.50668335 batch PCKh 0.3125\n",
      "Trained batch 2450 batch loss 0.539129376 batch mAP 0.523620605 batch PCKh 0.1875\n",
      "Trained batch 2451 batch loss 0.587922096 batch mAP 0.533569336 batch PCKh 0.375\n",
      "Trained batch 2452 batch loss 0.611789525 batch mAP 0.533416748 batch PCKh 0.6875\n",
      "Trained batch 2453 batch loss 0.553582072 batch mAP 0.555236816 batch PCKh 0.25\n",
      "Trained batch 2454 batch loss 0.514435232 batch mAP 0.542999268 batch PCKh 0.4375\n",
      "Trained batch 2455 batch loss 0.526881814 batch mAP 0.524993896 batch PCKh 0.625\n",
      "Trained batch 2456 batch loss 0.527351141 batch mAP 0.639953613 batch PCKh 0.375\n",
      "Trained batch 2457 batch loss 0.573021293 batch mAP 0.557647705 batch PCKh 0.75\n",
      "Trained batch 2458 batch loss 0.58547765 batch mAP 0.50378418 batch PCKh 0.625\n",
      "Trained batch 2459 batch loss 0.596253753 batch mAP 0.603820801 batch PCKh 0.375\n",
      "Trained batch 2460 batch loss 0.588092 batch mAP 0.570220947 batch PCKh 0.375\n",
      "Trained batch 2461 batch loss 0.537243485 batch mAP 0.592681885 batch PCKh 0.5\n",
      "Trained batch 2462 batch loss 0.581446052 batch mAP 0.52142334 batch PCKh 0.1875\n",
      "Trained batch 2463 batch loss 0.613056362 batch mAP 0.586456299 batch PCKh 0.4375\n",
      "Trained batch 2464 batch loss 0.528137267 batch mAP 0.601898193 batch PCKh 0.5\n",
      "Trained batch 2465 batch loss 0.567905605 batch mAP 0.621459961 batch PCKh 0.25\n",
      "Trained batch 2466 batch loss 0.472949088 batch mAP 0.640045166 batch PCKh 0.3125\n",
      "Trained batch 2467 batch loss 0.630529404 batch mAP 0.639465332 batch PCKh 0.875\n",
      "Trained batch 2468 batch loss 0.562700391 batch mAP 0.67401123 batch PCKh 0.3125\n",
      "Trained batch 2469 batch loss 0.580331564 batch mAP 0.621948242 batch PCKh 0.625\n",
      "Trained batch 2470 batch loss 0.540648937 batch mAP 0.577819824 batch PCKh 0.3125\n",
      "Trained batch 2471 batch loss 0.600307465 batch mAP 0.527160645 batch PCKh 0.3125\n",
      "Trained batch 2472 batch loss 0.518838286 batch mAP 0.575561523 batch PCKh 0.1875\n",
      "Trained batch 2473 batch loss 0.539779842 batch mAP 0.515228271 batch PCKh 0.6875\n",
      "Trained batch 2474 batch loss 0.550913811 batch mAP 0.511108398 batch PCKh 0\n",
      "Trained batch 2475 batch loss 0.580860257 batch mAP 0.490081787 batch PCKh 0.75\n",
      "Trained batch 2476 batch loss 0.639821172 batch mAP 0.50378418 batch PCKh 0.375\n",
      "Trained batch 2477 batch loss 0.70108515 batch mAP 0.541412354 batch PCKh 0.125\n",
      "Trained batch 2478 batch loss 0.632526636 batch mAP 0.57711792 batch PCKh 0.5625\n",
      "Trained batch 2479 batch loss 0.524263203 batch mAP 0.570343 batch PCKh 0.4375\n",
      "Trained batch 2480 batch loss 0.57450062 batch mAP 0.614349365 batch PCKh 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2481 batch loss 0.543631911 batch mAP 0.54586792 batch PCKh 0.6875\n",
      "Trained batch 2482 batch loss 0.667630851 batch mAP 0.586181641 batch PCKh 0.625\n",
      "Trained batch 2483 batch loss 0.673889518 batch mAP 0.594390869 batch PCKh 0.25\n",
      "Trained batch 2484 batch loss 0.573073328 batch mAP 0.59854126 batch PCKh 0.6875\n",
      "Trained batch 2485 batch loss 0.593417645 batch mAP 0.588195801 batch PCKh 0.5625\n",
      "Trained batch 2486 batch loss 0.601212561 batch mAP 0.6043396 batch PCKh 0.1875\n",
      "Trained batch 2487 batch loss 0.638799429 batch mAP 0.555236816 batch PCKh 0.5625\n",
      "Trained batch 2488 batch loss 0.56755656 batch mAP 0.55871582 batch PCKh 0.5\n",
      "Trained batch 2489 batch loss 0.601233363 batch mAP 0.535430908 batch PCKh 0.6875\n",
      "Trained batch 2490 batch loss 0.652485 batch mAP 0.497558594 batch PCKh 0.375\n",
      "Trained batch 2491 batch loss 0.702637911 batch mAP 0.514129639 batch PCKh 0.3125\n",
      "Trained batch 2492 batch loss 0.646200657 batch mAP 0.555633545 batch PCKh 0.3125\n",
      "Trained batch 2493 batch loss 0.643779635 batch mAP 0.523132324 batch PCKh 0.5\n",
      "Trained batch 2494 batch loss 0.498707414 batch mAP 0.620239258 batch PCKh 0.6875\n",
      "Trained batch 2495 batch loss 0.494936049 batch mAP 0.604431152 batch PCKh 0.4375\n",
      "Trained batch 2496 batch loss 0.592309833 batch mAP 0.501220703 batch PCKh 0.3125\n",
      "Trained batch 2497 batch loss 0.679410696 batch mAP 0.52355957 batch PCKh 0.6875\n",
      "Trained batch 2498 batch loss 0.604850411 batch mAP 0.592163086 batch PCKh 0.4375\n",
      "Trained batch 2499 batch loss 0.598668814 batch mAP 0.605896 batch PCKh 0.625\n",
      "Trained batch 2500 batch loss 0.500172257 batch mAP 0.614074707 batch PCKh 0.1875\n",
      "Trained batch 2501 batch loss 0.716663361 batch mAP 0.590057373 batch PCKh 0.125\n",
      "Trained batch 2502 batch loss 0.570520699 batch mAP 0.593170166 batch PCKh 0.1875\n",
      "Trained batch 2503 batch loss 0.552817 batch mAP 0.629425049 batch PCKh 0.75\n",
      "Trained batch 2504 batch loss 0.576561213 batch mAP 0.598419189 batch PCKh 0.125\n",
      "Trained batch 2505 batch loss 0.613561869 batch mAP 0.530487061 batch PCKh 0.5625\n",
      "Trained batch 2506 batch loss 0.535225809 batch mAP 0.552093506 batch PCKh 0.6875\n",
      "Trained batch 2507 batch loss 0.608611345 batch mAP 0.517883301 batch PCKh 0.75\n",
      "Trained batch 2508 batch loss 0.623325229 batch mAP 0.487518311 batch PCKh 0.6875\n",
      "Trained batch 2509 batch loss 0.497542739 batch mAP 0.567565918 batch PCKh 0.4375\n",
      "Trained batch 2510 batch loss 0.552440643 batch mAP 0.532012939 batch PCKh 0.6875\n",
      "Trained batch 2511 batch loss 0.650953174 batch mAP 0.497772217 batch PCKh 0.125\n",
      "Trained batch 2512 batch loss 0.635683179 batch mAP 0.512207031 batch PCKh 0.3125\n",
      "Trained batch 2513 batch loss 0.572243 batch mAP 0.626678467 batch PCKh 0.4375\n",
      "Trained batch 2514 batch loss 0.64523834 batch mAP 0.560241699 batch PCKh 0.3125\n",
      "Trained batch 2515 batch loss 0.504677117 batch mAP 0.579650879 batch PCKh 0.4375\n",
      "Trained batch 2516 batch loss 0.525858641 batch mAP 0.52645874 batch PCKh 0.5\n",
      "Trained batch 2517 batch loss 0.6635921 batch mAP 0.511322 batch PCKh 0.875\n",
      "Trained batch 2518 batch loss 0.530939341 batch mAP 0.510986328 batch PCKh 0.6875\n",
      "Trained batch 2519 batch loss 0.601338 batch mAP 0.482971191 batch PCKh 0.875\n",
      "Trained batch 2520 batch loss 0.593102098 batch mAP 0.522583 batch PCKh 0.6875\n",
      "Trained batch 2521 batch loss 0.668441713 batch mAP 0.515472412 batch PCKh 0.5625\n",
      "Trained batch 2522 batch loss 0.635702968 batch mAP 0.526641846 batch PCKh 0.8125\n",
      "Trained batch 2523 batch loss 0.571665406 batch mAP 0.512786865 batch PCKh 0.25\n",
      "Trained batch 2524 batch loss 0.580373764 batch mAP 0.54473877 batch PCKh 0.375\n",
      "Trained batch 2525 batch loss 0.569368422 batch mAP 0.512908936 batch PCKh 0.25\n",
      "Trained batch 2526 batch loss 0.592314065 batch mAP 0.562438965 batch PCKh 0.6875\n",
      "Trained batch 2527 batch loss 0.706406355 batch mAP 0.549469 batch PCKh 0.125\n",
      "Trained batch 2528 batch loss 0.661872447 batch mAP 0.542572 batch PCKh 0.5\n",
      "Trained batch 2529 batch loss 0.545720756 batch mAP 0.603637695 batch PCKh 0.8125\n",
      "Trained batch 2530 batch loss 0.560932 batch mAP 0.571136475 batch PCKh 0.75\n",
      "Trained batch 2531 batch loss 0.527124465 batch mAP 0.565887451 batch PCKh 0.6875\n",
      "Trained batch 2532 batch loss 0.560931087 batch mAP 0.567749 batch PCKh 0.3125\n",
      "Trained batch 2533 batch loss 0.54856658 batch mAP 0.598114 batch PCKh 0.6875\n",
      "Trained batch 2534 batch loss 0.568616 batch mAP 0.602020264 batch PCKh 0.375\n",
      "Trained batch 2535 batch loss 0.679040432 batch mAP 0.549346924 batch PCKh 0.375\n",
      "Trained batch 2536 batch loss 0.672584534 batch mAP 0.494293213 batch PCKh 0.1875\n",
      "Trained batch 2537 batch loss 0.670461595 batch mAP 0.541778564 batch PCKh 0.25\n",
      "Trained batch 2538 batch loss 0.638393581 batch mAP 0.559783936 batch PCKh 0.375\n",
      "Trained batch 2539 batch loss 0.603305578 batch mAP 0.583709717 batch PCKh 0.125\n",
      "Trained batch 2540 batch loss 0.630093873 batch mAP 0.598632812 batch PCKh 0.25\n",
      "Trained batch 2541 batch loss 0.548820794 batch mAP 0.58392334 batch PCKh 0.5\n",
      "Trained batch 2542 batch loss 0.505586207 batch mAP 0.633392334 batch PCKh 0.3125\n",
      "Trained batch 2543 batch loss 0.584039569 batch mAP 0.614624 batch PCKh 0.3125\n",
      "Trained batch 2544 batch loss 0.638141334 batch mAP 0.601165771 batch PCKh 0.25\n",
      "Trained batch 2545 batch loss 0.600918174 batch mAP 0.602996826 batch PCKh 0.3125\n",
      "Trained batch 2546 batch loss 0.594721675 batch mAP 0.579406738 batch PCKh 0.375\n",
      "Trained batch 2547 batch loss 0.575458586 batch mAP 0.518920898 batch PCKh 0.4375\n",
      "Trained batch 2548 batch loss 0.613667428 batch mAP 0.561859131 batch PCKh 0.8125\n",
      "Trained batch 2549 batch loss 0.653313398 batch mAP 0.481903076 batch PCKh 0.75\n",
      "Trained batch 2550 batch loss 0.691033602 batch mAP 0.449584961 batch PCKh 0.6875\n",
      "Trained batch 2551 batch loss 0.643442392 batch mAP 0.431732178 batch PCKh 0.4375\n",
      "Trained batch 2552 batch loss 0.723958135 batch mAP 0.461608887 batch PCKh 0.1875\n",
      "Trained batch 2553 batch loss 0.598364413 batch mAP 0.467987061 batch PCKh 0.6875\n",
      "Trained batch 2554 batch loss 0.581262 batch mAP 0.503875732 batch PCKh 0.0625\n",
      "Trained batch 2555 batch loss 0.588718712 batch mAP 0.488861084 batch PCKh 0.25\n",
      "Trained batch 2556 batch loss 0.462979496 batch mAP 0.385772705 batch PCKh 0.3125\n",
      "Trained batch 2557 batch loss 0.528946877 batch mAP 0.468139648 batch PCKh 0.25\n",
      "Trained batch 2558 batch loss 0.453841716 batch mAP 0.402618408 batch PCKh 0.25\n",
      "Trained batch 2559 batch loss 0.472375035 batch mAP 0.45526123 batch PCKh 0\n",
      "Trained batch 2560 batch loss 0.468389809 batch mAP 0.51550293 batch PCKh 0.5\n",
      "Trained batch 2561 batch loss 0.461534 batch mAP 0.55859375 batch PCKh 0.4375\n",
      "Trained batch 2562 batch loss 0.405746341 batch mAP 0.613311768 batch PCKh 0.3125\n",
      "Trained batch 2563 batch loss 0.550548792 batch mAP 0.554290771 batch PCKh 0.6875\n",
      "Trained batch 2564 batch loss 0.570607424 batch mAP 0.565368652 batch PCKh 0.375\n",
      "Trained batch 2565 batch loss 0.617679119 batch mAP 0.557647705 batch PCKh 0.1875\n",
      "Trained batch 2566 batch loss 0.662926 batch mAP 0.537353516 batch PCKh 0.3125\n",
      "Trained batch 2567 batch loss 0.649448454 batch mAP 0.498016357 batch PCKh 0.8125\n",
      "Trained batch 2568 batch loss 0.605578184 batch mAP 0.556243896 batch PCKh 0.625\n",
      "Trained batch 2569 batch loss 0.621381462 batch mAP 0.509490967 batch PCKh 0.0625\n",
      "Trained batch 2570 batch loss 0.609833241 batch mAP 0.550323486 batch PCKh 0.6875\n",
      "Trained batch 2571 batch loss 0.611389637 batch mAP 0.532043457 batch PCKh 0.875\n",
      "Trained batch 2572 batch loss 0.602660537 batch mAP 0.587860107 batch PCKh 0.5\n",
      "Trained batch 2573 batch loss 0.69829 batch mAP 0.552856445 batch PCKh 0.125\n",
      "Trained batch 2574 batch loss 0.683373 batch mAP 0.551727295 batch PCKh 0.4375\n",
      "Trained batch 2575 batch loss 0.575297475 batch mAP 0.587280273 batch PCKh 0.375\n",
      "Trained batch 2576 batch loss 0.609603941 batch mAP 0.554351807 batch PCKh 0.3125\n",
      "Trained batch 2577 batch loss 0.565549493 batch mAP 0.53326416 batch PCKh 0.5625\n",
      "Trained batch 2578 batch loss 0.598657846 batch mAP 0.507751465 batch PCKh 0.75\n",
      "Trained batch 2579 batch loss 0.672917783 batch mAP 0.527679443 batch PCKh 0.3125\n",
      "Trained batch 2580 batch loss 0.626368701 batch mAP 0.474121094 batch PCKh 0.625\n",
      "Trained batch 2581 batch loss 0.675880909 batch mAP 0.486419678 batch PCKh 0\n",
      "Trained batch 2582 batch loss 0.54180789 batch mAP 0.591400146 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2583 batch loss 0.562328935 batch mAP 0.528533936 batch PCKh 0.375\n",
      "Trained batch 2584 batch loss 0.545408845 batch mAP 0.518676758 batch PCKh 0.3125\n",
      "Trained batch 2585 batch loss 0.530565321 batch mAP 0.507995605 batch PCKh 0.125\n",
      "Trained batch 2586 batch loss 0.503606677 batch mAP 0.605926514 batch PCKh 0.375\n",
      "Trained batch 2587 batch loss 0.5906201 batch mAP 0.547546387 batch PCKh 0.3125\n",
      "Trained batch 2588 batch loss 0.400268644 batch mAP 0.602050781 batch PCKh 0.4375\n",
      "Trained batch 2589 batch loss 0.457588941 batch mAP 0.536132812 batch PCKh 0.1875\n",
      "Trained batch 2590 batch loss 0.424292564 batch mAP 0.523620605 batch PCKh 0\n",
      "Trained batch 2591 batch loss 0.546850085 batch mAP 0.604919434 batch PCKh 0.25\n",
      "Trained batch 2592 batch loss 0.53683126 batch mAP 0.560272217 batch PCKh 0.0625\n",
      "Trained batch 2593 batch loss 0.574586868 batch mAP 0.597808838 batch PCKh 0.25\n",
      "Trained batch 2594 batch loss 0.595348 batch mAP 0.529205322 batch PCKh 0.1875\n",
      "Trained batch 2595 batch loss 0.631524205 batch mAP 0.429534912 batch PCKh 0.4375\n",
      "Trained batch 2596 batch loss 0.605146229 batch mAP 0.453063965 batch PCKh 0.25\n",
      "Trained batch 2597 batch loss 0.563940585 batch mAP 0.47177124 batch PCKh 0.5625\n",
      "Trained batch 2598 batch loss 0.565121 batch mAP 0.55871582 batch PCKh 0.6875\n",
      "Trained batch 2599 batch loss 0.660322368 batch mAP 0.452087402 batch PCKh 0.8125\n",
      "Trained batch 2600 batch loss 0.568864584 batch mAP 0.535095215 batch PCKh 0.6875\n",
      "Trained batch 2601 batch loss 0.608588278 batch mAP 0.52255249 batch PCKh 0.4375\n",
      "Trained batch 2602 batch loss 0.553869665 batch mAP 0.545379639 batch PCKh 0.75\n",
      "Trained batch 2603 batch loss 0.627047658 batch mAP 0.562530518 batch PCKh 0.125\n",
      "Trained batch 2604 batch loss 0.62069279 batch mAP 0.545288086 batch PCKh 0.75\n",
      "Trained batch 2605 batch loss 0.651242733 batch mAP 0.540893555 batch PCKh 0.875\n",
      "Trained batch 2606 batch loss 0.603035092 batch mAP 0.56439209 batch PCKh 0.6875\n",
      "Trained batch 2607 batch loss 0.609369397 batch mAP 0.610687256 batch PCKh 0.8125\n",
      "Trained batch 2608 batch loss 0.548897386 batch mAP 0.558868408 batch PCKh 0.625\n",
      "Trained batch 2609 batch loss 0.513669908 batch mAP 0.587982178 batch PCKh 0.625\n",
      "Trained batch 2610 batch loss 0.554383755 batch mAP 0.615661621 batch PCKh 0.5625\n",
      "Trained batch 2611 batch loss 0.585517526 batch mAP 0.596405 batch PCKh 0.375\n",
      "Trained batch 2612 batch loss 0.527506948 batch mAP 0.609466553 batch PCKh 0.5\n",
      "Trained batch 2613 batch loss 0.50042522 batch mAP 0.538604736 batch PCKh 0.75\n",
      "Trained batch 2614 batch loss 0.51377 batch mAP 0.583587646 batch PCKh 0.75\n",
      "Trained batch 2615 batch loss 0.562465906 batch mAP 0.532318115 batch PCKh 0.625\n",
      "Trained batch 2616 batch loss 0.508129716 batch mAP 0.576721191 batch PCKh 0.5625\n",
      "Trained batch 2617 batch loss 0.489442 batch mAP 0.63848877 batch PCKh 0.625\n",
      "Trained batch 2618 batch loss 0.56038785 batch mAP 0.642181396 batch PCKh 0.25\n",
      "Trained batch 2619 batch loss 0.523241282 batch mAP 0.578308105 batch PCKh 0.8125\n",
      "Trained batch 2620 batch loss 0.505597293 batch mAP 0.611236572 batch PCKh 0.375\n",
      "Trained batch 2621 batch loss 0.548304141 batch mAP 0.59979248 batch PCKh 0.3125\n",
      "Trained batch 2622 batch loss 0.506946385 batch mAP 0.605651855 batch PCKh 0.3125\n",
      "Trained batch 2623 batch loss 0.485791624 batch mAP 0.603851318 batch PCKh 0.375\n",
      "Trained batch 2624 batch loss 0.550691724 batch mAP 0.578094482 batch PCKh 0.75\n",
      "Trained batch 2625 batch loss 0.577693939 batch mAP 0.614563 batch PCKh 0\n",
      "Trained batch 2626 batch loss 0.472185463 batch mAP 0.622283936 batch PCKh 0.0625\n",
      "Trained batch 2627 batch loss 0.487499565 batch mAP 0.625305176 batch PCKh 0.625\n",
      "Trained batch 2628 batch loss 0.497396052 batch mAP 0.646789551 batch PCKh 0.375\n",
      "Trained batch 2629 batch loss 0.582168818 batch mAP 0.580749512 batch PCKh 0.125\n",
      "Trained batch 2630 batch loss 0.678509712 batch mAP 0.47833252 batch PCKh 0\n",
      "Trained batch 2631 batch loss 0.597100437 batch mAP 0.624511719 batch PCKh 0.6875\n",
      "Trained batch 2632 batch loss 0.583501041 batch mAP 0.515350342 batch PCKh 0.375\n",
      "Trained batch 2633 batch loss 0.561540723 batch mAP 0.546600342 batch PCKh 0.3125\n",
      "Trained batch 2634 batch loss 0.616056442 batch mAP 0.54586792 batch PCKh 0.125\n",
      "Trained batch 2635 batch loss 0.497332275 batch mAP 0.548614502 batch PCKh 0.1875\n",
      "Trained batch 2636 batch loss 0.616162539 batch mAP 0.510559082 batch PCKh 0.125\n",
      "Trained batch 2637 batch loss 0.527049482 batch mAP 0.486297607 batch PCKh 0.25\n",
      "Trained batch 2638 batch loss 0.591102839 batch mAP 0.459320068 batch PCKh 0\n",
      "Trained batch 2639 batch loss 0.558077574 batch mAP 0.564117432 batch PCKh 0.3125\n",
      "Trained batch 2640 batch loss 0.532280803 batch mAP 0.588806152 batch PCKh 0.3125\n",
      "Trained batch 2641 batch loss 0.47661972 batch mAP 0.582183838 batch PCKh 0.5625\n",
      "Trained batch 2642 batch loss 0.575052738 batch mAP 0.541748047 batch PCKh 0.5\n",
      "Trained batch 2643 batch loss 0.629556596 batch mAP 0.560180664 batch PCKh 0.125\n",
      "Trained batch 2644 batch loss 0.61706686 batch mAP 0.495727539 batch PCKh 0\n",
      "Trained batch 2645 batch loss 0.554339886 batch mAP 0.551361084 batch PCKh 0.5625\n",
      "Trained batch 2646 batch loss 0.647692323 batch mAP 0.570800781 batch PCKh 0.75\n",
      "Trained batch 2647 batch loss 0.688632429 batch mAP 0.547515869 batch PCKh 0.25\n",
      "Trained batch 2648 batch loss 0.736943126 batch mAP 0.536651611 batch PCKh 0\n",
      "Trained batch 2649 batch loss 0.739142895 batch mAP 0.584899902 batch PCKh 0.1875\n",
      "Trained batch 2650 batch loss 0.698619545 batch mAP 0.549926758 batch PCKh 0.0625\n",
      "Trained batch 2651 batch loss 0.627665 batch mAP 0.549987793 batch PCKh 0.25\n",
      "Trained batch 2652 batch loss 0.441088557 batch mAP 0.578979492 batch PCKh 0.75\n",
      "Trained batch 2653 batch loss 0.625136793 batch mAP 0.57901 batch PCKh 0.1875\n",
      "Trained batch 2654 batch loss 0.577605665 batch mAP 0.540283203 batch PCKh 0.75\n",
      "Trained batch 2655 batch loss 0.55381 batch mAP 0.550720215 batch PCKh 0.5\n",
      "Trained batch 2656 batch loss 0.531461954 batch mAP 0.543792725 batch PCKh 0.75\n",
      "Trained batch 2657 batch loss 0.562479734 batch mAP 0.560119629 batch PCKh 0.5625\n",
      "Trained batch 2658 batch loss 0.630300462 batch mAP 0.539032 batch PCKh 0.6875\n",
      "Trained batch 2659 batch loss 0.563935816 batch mAP 0.527374268 batch PCKh 0.5\n",
      "Trained batch 2660 batch loss 0.630819917 batch mAP 0.49395752 batch PCKh 0.25\n",
      "Trained batch 2661 batch loss 0.51284337 batch mAP 0.533599854 batch PCKh 0\n",
      "Trained batch 2662 batch loss 0.632935405 batch mAP 0.52557373 batch PCKh 0.5\n",
      "Trained batch 2663 batch loss 0.56113255 batch mAP 0.483123779 batch PCKh 0.625\n",
      "Trained batch 2664 batch loss 0.624299765 batch mAP 0.424804688 batch PCKh 0.25\n",
      "Trained batch 2665 batch loss 0.669818163 batch mAP 0.500030518 batch PCKh 0.5625\n",
      "Trained batch 2666 batch loss 0.64166826 batch mAP 0.490966797 batch PCKh 0.1875\n",
      "Trained batch 2667 batch loss 0.63470912 batch mAP 0.492462158 batch PCKh 0.125\n",
      "Trained batch 2668 batch loss 0.667604685 batch mAP 0.503417969 batch PCKh 0.25\n",
      "Trained batch 2669 batch loss 0.678015292 batch mAP 0.492218018 batch PCKh 0.75\n",
      "Trained batch 2670 batch loss 0.690360129 batch mAP 0.498809814 batch PCKh 0.4375\n",
      "Trained batch 2671 batch loss 0.600683749 batch mAP 0.582366943 batch PCKh 0.4375\n",
      "Trained batch 2672 batch loss 0.615115523 batch mAP 0.549377441 batch PCKh 0.375\n",
      "Trained batch 2673 batch loss 0.559773564 batch mAP 0.562957764 batch PCKh 0.3125\n",
      "Trained batch 2674 batch loss 0.569189548 batch mAP 0.565246582 batch PCKh 0.5\n",
      "Trained batch 2675 batch loss 0.540526152 batch mAP 0.566009521 batch PCKh 0.125\n",
      "Trained batch 2676 batch loss 0.523564219 batch mAP 0.569885254 batch PCKh 0.5625\n",
      "Trained batch 2677 batch loss 0.583959877 batch mAP 0.580505371 batch PCKh 0.75\n",
      "Trained batch 2678 batch loss 0.608225644 batch mAP 0.558990479 batch PCKh 0.3125\n",
      "Trained batch 2679 batch loss 0.674533 batch mAP 0.575653076 batch PCKh 0.25\n",
      "Trained batch 2680 batch loss 0.587048233 batch mAP 0.50213623 batch PCKh 0\n",
      "Trained batch 2681 batch loss 0.560291 batch mAP 0.578643799 batch PCKh 0.375\n",
      "Trained batch 2682 batch loss 0.484107316 batch mAP 0.659729 batch PCKh 0.3125\n",
      "Trained batch 2683 batch loss 0.580364943 batch mAP 0.605224609 batch PCKh 0.3125\n",
      "Trained batch 2684 batch loss 0.581415832 batch mAP 0.653717041 batch PCKh 0.8125\n",
      "Trained batch 2685 batch loss 0.545490801 batch mAP 0.632354736 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2686 batch loss 0.582234085 batch mAP 0.623168945 batch PCKh 0.3125\n",
      "Trained batch 2687 batch loss 0.540351033 batch mAP 0.613098145 batch PCKh 0.6875\n",
      "Trained batch 2688 batch loss 0.524057269 batch mAP 0.613006592 batch PCKh 0.625\n",
      "Trained batch 2689 batch loss 0.574713767 batch mAP 0.561615 batch PCKh 0.3125\n",
      "Trained batch 2690 batch loss 0.520507395 batch mAP 0.532592773 batch PCKh 0.6875\n",
      "Trained batch 2691 batch loss 0.622649908 batch mAP 0.498809814 batch PCKh 0.875\n",
      "Trained batch 2692 batch loss 0.550859451 batch mAP 0.519744873 batch PCKh 0.875\n",
      "Trained batch 2693 batch loss 0.653046668 batch mAP 0.452728271 batch PCKh 0.625\n",
      "Trained batch 2694 batch loss 0.514071763 batch mAP 0.552734375 batch PCKh 0.5625\n",
      "Trained batch 2695 batch loss 0.567734063 batch mAP 0.548522949 batch PCKh 0.5\n",
      "Trained batch 2696 batch loss 0.459412336 batch mAP 0.633270264 batch PCKh 0.5625\n",
      "Trained batch 2697 batch loss 0.408532649 batch mAP 0.643676758 batch PCKh 0.4375\n",
      "Trained batch 2698 batch loss 0.461935878 batch mAP 0.599700928 batch PCKh 0.3125\n",
      "Trained batch 2699 batch loss 0.50061512 batch mAP 0.610199 batch PCKh 0.5\n",
      "Trained batch 2700 batch loss 0.524674714 batch mAP 0.590179443 batch PCKh 0.375\n",
      "Trained batch 2701 batch loss 0.533733666 batch mAP 0.610595703 batch PCKh 0.3125\n",
      "Trained batch 2702 batch loss 0.488698334 batch mAP 0.632995605 batch PCKh 0.4375\n",
      "Trained batch 2703 batch loss 0.584038198 batch mAP 0.619934082 batch PCKh 0.3125\n",
      "Trained batch 2704 batch loss 0.517788172 batch mAP 0.658752441 batch PCKh 0.75\n",
      "Trained batch 2705 batch loss 0.559853375 batch mAP 0.570983887 batch PCKh 0.5\n",
      "Trained batch 2706 batch loss 0.568048477 batch mAP 0.517242432 batch PCKh 0.3125\n",
      "Trained batch 2707 batch loss 0.497085631 batch mAP 0.594696045 batch PCKh 0.375\n",
      "Trained batch 2708 batch loss 0.576009512 batch mAP 0.635620117 batch PCKh 0.375\n",
      "Trained batch 2709 batch loss 0.604949176 batch mAP 0.608703613 batch PCKh 0.8125\n",
      "Trained batch 2710 batch loss 0.675596893 batch mAP 0.586883545 batch PCKh 0.6875\n",
      "Trained batch 2711 batch loss 0.585655153 batch mAP 0.566864 batch PCKh 0.125\n",
      "Trained batch 2712 batch loss 0.623709142 batch mAP 0.554290771 batch PCKh 0.1875\n",
      "Trained batch 2713 batch loss 0.61533469 batch mAP 0.476989746 batch PCKh 0.375\n",
      "Trained batch 2714 batch loss 0.631085455 batch mAP 0.507110596 batch PCKh 0.5\n",
      "Trained batch 2715 batch loss 0.66193068 batch mAP 0.47644043 batch PCKh 0.625\n",
      "Trained batch 2716 batch loss 0.540619373 batch mAP 0.504150391 batch PCKh 0.3125\n",
      "Trained batch 2717 batch loss 0.459076285 batch mAP 0.477539062 batch PCKh 0\n",
      "Trained batch 2718 batch loss 0.535939 batch mAP 0.608093262 batch PCKh 0.3125\n",
      "Trained batch 2719 batch loss 0.545807 batch mAP 0.571258545 batch PCKh 0.375\n",
      "Trained batch 2720 batch loss 0.517450631 batch mAP 0.501709 batch PCKh 0.75\n",
      "Trained batch 2721 batch loss 0.54962945 batch mAP 0.512359619 batch PCKh 0.8125\n",
      "Trained batch 2722 batch loss 0.593346119 batch mAP 0.533996582 batch PCKh 0.6875\n",
      "Trained batch 2723 batch loss 0.628378391 batch mAP 0.520385742 batch PCKh 0.5\n",
      "Trained batch 2724 batch loss 0.630638 batch mAP 0.534973145 batch PCKh 0.3125\n",
      "Trained batch 2725 batch loss 0.682101 batch mAP 0.510009766 batch PCKh 0.4375\n",
      "Trained batch 2726 batch loss 0.662320197 batch mAP 0.547363281 batch PCKh 0.3125\n",
      "Trained batch 2727 batch loss 0.545559525 batch mAP 0.623016357 batch PCKh 0.3125\n",
      "Trained batch 2728 batch loss 0.593947589 batch mAP 0.637817383 batch PCKh 0.375\n",
      "Trained batch 2729 batch loss 0.618827343 batch mAP 0.585083 batch PCKh 0.25\n",
      "Trained batch 2730 batch loss 0.581884265 batch mAP 0.56539917 batch PCKh 0.1875\n",
      "Trained batch 2731 batch loss 0.52334857 batch mAP 0.528991699 batch PCKh 0.5625\n",
      "Trained batch 2732 batch loss 0.523691773 batch mAP 0.547180176 batch PCKh 0.125\n",
      "Trained batch 2733 batch loss 0.650084 batch mAP 0.464691162 batch PCKh 0.4375\n",
      "Trained batch 2734 batch loss 0.645406306 batch mAP 0.518310547 batch PCKh 0.25\n",
      "Trained batch 2735 batch loss 0.716566801 batch mAP 0.492584229 batch PCKh 0.5625\n",
      "Trained batch 2736 batch loss 0.519601345 batch mAP 0.619140625 batch PCKh 0.125\n",
      "Trained batch 2737 batch loss 0.674111903 batch mAP 0.533050537 batch PCKh 0.0625\n",
      "Trained batch 2738 batch loss 0.627796769 batch mAP 0.515991211 batch PCKh 0.75\n",
      "Trained batch 2739 batch loss 0.633337498 batch mAP 0.522613525 batch PCKh 0.75\n",
      "Trained batch 2740 batch loss 0.505168259 batch mAP 0.537261963 batch PCKh 0.625\n",
      "Trained batch 2741 batch loss 0.489806563 batch mAP 0.528656 batch PCKh 0.3125\n",
      "Trained batch 2742 batch loss 0.51793015 batch mAP 0.489898682 batch PCKh 0.6875\n",
      "Trained batch 2743 batch loss 0.52268517 batch mAP 0.41192627 batch PCKh 0.375\n",
      "Trained batch 2744 batch loss 0.507302 batch mAP 0.420166016 batch PCKh 0.4375\n",
      "Trained batch 2745 batch loss 0.571956575 batch mAP 0.408355713 batch PCKh 0.625\n",
      "Trained batch 2746 batch loss 0.527553082 batch mAP 0.459716797 batch PCKh 0.125\n",
      "Trained batch 2747 batch loss 0.593828499 batch mAP 0.448242188 batch PCKh 0.6875\n",
      "Trained batch 2748 batch loss 0.607594669 batch mAP 0.458862305 batch PCKh 0.625\n",
      "Trained batch 2749 batch loss 0.638142526 batch mAP 0.516876221 batch PCKh 0.5625\n",
      "Trained batch 2750 batch loss 0.594271183 batch mAP 0.579803467 batch PCKh 0.6875\n",
      "Trained batch 2751 batch loss 0.602722883 batch mAP 0.527130127 batch PCKh 0.25\n",
      "Trained batch 2752 batch loss 0.600526512 batch mAP 0.528961182 batch PCKh 0.3125\n",
      "Trained batch 2753 batch loss 0.593511939 batch mAP 0.546417236 batch PCKh 0.3125\n",
      "Trained batch 2754 batch loss 0.579619765 batch mAP 0.569274902 batch PCKh 0.5\n",
      "Trained batch 2755 batch loss 0.528879881 batch mAP 0.589874268 batch PCKh 0.5\n",
      "Trained batch 2756 batch loss 0.553942323 batch mAP 0.576873779 batch PCKh 0.5\n",
      "Trained batch 2757 batch loss 0.55491662 batch mAP 0.546234131 batch PCKh 0.4375\n",
      "Trained batch 2758 batch loss 0.527108431 batch mAP 0.623077393 batch PCKh 0.5\n",
      "Trained batch 2759 batch loss 0.528974593 batch mAP 0.61428833 batch PCKh 0.6875\n",
      "Trained batch 2760 batch loss 0.637533069 batch mAP 0.584838867 batch PCKh 0.4375\n",
      "Trained batch 2761 batch loss 0.595168948 batch mAP 0.524017334 batch PCKh 0.5625\n",
      "Trained batch 2762 batch loss 0.576582909 batch mAP 0.526947 batch PCKh 0.1875\n",
      "Trained batch 2763 batch loss 0.442178071 batch mAP 0.513336182 batch PCKh 0.75\n",
      "Trained batch 2764 batch loss 0.586755753 batch mAP 0.495147705 batch PCKh 0.75\n",
      "Trained batch 2765 batch loss 0.583843 batch mAP 0.507232666 batch PCKh 0.625\n",
      "Trained batch 2766 batch loss 0.634637475 batch mAP 0.575134277 batch PCKh 0.3125\n",
      "Trained batch 2767 batch loss 0.558955431 batch mAP 0.578582764 batch PCKh 0.625\n",
      "Trained batch 2768 batch loss 0.518513441 batch mAP 0.631622314 batch PCKh 0.4375\n",
      "Trained batch 2769 batch loss 0.60264504 batch mAP 0.63772583 batch PCKh 0.5\n",
      "Trained batch 2770 batch loss 0.555259049 batch mAP 0.61151123 batch PCKh 0.1875\n",
      "Trained batch 2771 batch loss 0.560407698 batch mAP 0.633239746 batch PCKh 0.375\n",
      "Trained batch 2772 batch loss 0.541461229 batch mAP 0.646636963 batch PCKh 0.375\n",
      "Trained batch 2773 batch loss 0.539296627 batch mAP 0.641784668 batch PCKh 0.25\n",
      "Trained batch 2774 batch loss 0.508864224 batch mAP 0.685333252 batch PCKh 0.25\n",
      "Trained batch 2775 batch loss 0.530619264 batch mAP 0.65448 batch PCKh 0.3125\n",
      "Trained batch 2776 batch loss 0.450061 batch mAP 0.68460083 batch PCKh 0.25\n",
      "Epoch 4 train loss 0.5737546682357788 train mAP 0.527790367603302 train PCKh\n",
      "Validated batch 1 batch loss 0.566156864 batch mAP 0.51297 batch PCKh 0.5\n",
      "Validated batch 2 batch loss 0.519771516 batch mAP 0.536499 batch PCKh 0.75\n",
      "Validated batch 3 batch loss 0.5873456 batch mAP 0.52935791 batch PCKh 0.1875\n",
      "Validated batch 4 batch loss 0.572613358 batch mAP 0.5887146 batch PCKh 0.625\n",
      "Validated batch 5 batch loss 0.639160454 batch mAP 0.473266602 batch PCKh 0.625\n",
      "Validated batch 6 batch loss 0.574970782 batch mAP 0.571228 batch PCKh 0.625\n",
      "Validated batch 7 batch loss 0.58446449 batch mAP 0.574401855 batch PCKh 0.0625\n",
      "Validated batch 8 batch loss 0.534267545 batch mAP 0.670105 batch PCKh 0.5\n",
      "Validated batch 9 batch loss 0.591035247 batch mAP 0.55380249 batch PCKh 0.625\n",
      "Validated batch 10 batch loss 0.642939687 batch mAP 0.501403809 batch PCKh 0.1875\n",
      "Validated batch 11 batch loss 0.707876205 batch mAP 0.492523193 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 12 batch loss 0.607999444 batch mAP 0.580810547 batch PCKh 0.5625\n",
      "Validated batch 13 batch loss 0.468316764 batch mAP 0.593139648 batch PCKh 0.875\n",
      "Validated batch 14 batch loss 0.58219552 batch mAP 0.597900391 batch PCKh 0.75\n",
      "Validated batch 15 batch loss 0.576989889 batch mAP 0.644348145 batch PCKh 0.125\n",
      "Validated batch 16 batch loss 0.606819034 batch mAP 0.538482666 batch PCKh 0.25\n",
      "Validated batch 17 batch loss 0.567532599 batch mAP 0.50201416 batch PCKh 0.1875\n",
      "Validated batch 18 batch loss 0.554982245 batch mAP 0.4871521 batch PCKh 0.625\n",
      "Validated batch 19 batch loss 0.665353417 batch mAP 0.555755615 batch PCKh 0.5\n",
      "Validated batch 20 batch loss 0.61526674 batch mAP 0.562774658 batch PCKh 0.3125\n",
      "Validated batch 21 batch loss 0.585713565 batch mAP 0.57434082 batch PCKh 0.4375\n",
      "Validated batch 22 batch loss 0.569451928 batch mAP 0.571105957 batch PCKh 0.5625\n",
      "Validated batch 23 batch loss 0.580093861 batch mAP 0.532775879 batch PCKh 0.75\n",
      "Validated batch 24 batch loss 0.410702735 batch mAP 0.555175781 batch PCKh 0.5625\n",
      "Validated batch 25 batch loss 0.417616487 batch mAP 0.65246582 batch PCKh 0.3125\n",
      "Validated batch 26 batch loss 0.626549423 batch mAP 0.558502197 batch PCKh 0.8125\n",
      "Validated batch 27 batch loss 0.57689929 batch mAP 0.59954834 batch PCKh 0.6875\n",
      "Validated batch 28 batch loss 0.520804405 batch mAP 0.629333496 batch PCKh 0.375\n",
      "Validated batch 29 batch loss 0.558469 batch mAP 0.638397217 batch PCKh 0.25\n",
      "Validated batch 30 batch loss 0.537304044 batch mAP 0.580963135 batch PCKh 0.5625\n",
      "Validated batch 31 batch loss 0.550840259 batch mAP 0.541015625 batch PCKh 0.5\n",
      "Validated batch 32 batch loss 0.626586437 batch mAP 0.464294434 batch PCKh 0.4375\n",
      "Validated batch 33 batch loss 0.6189996 batch mAP 0.605987549 batch PCKh 0.6875\n",
      "Validated batch 34 batch loss 0.502753377 batch mAP 0.640594482 batch PCKh 0.625\n",
      "Validated batch 35 batch loss 0.503641188 batch mAP 0.631195068 batch PCKh 0.5625\n",
      "Validated batch 36 batch loss 0.531722426 batch mAP 0.586456299 batch PCKh 0.125\n",
      "Validated batch 37 batch loss 0.636242568 batch mAP 0.597290039 batch PCKh 0.125\n",
      "Validated batch 38 batch loss 0.556955934 batch mAP 0.516449 batch PCKh 0\n",
      "Validated batch 39 batch loss 0.689541936 batch mAP 0.481903076 batch PCKh 0\n",
      "Validated batch 40 batch loss 0.564953148 batch mAP 0.617431641 batch PCKh 0.25\n",
      "Validated batch 41 batch loss 0.625453711 batch mAP 0.591644287 batch PCKh 0.625\n",
      "Validated batch 42 batch loss 0.640357375 batch mAP 0.664093 batch PCKh 0.5\n",
      "Validated batch 43 batch loss 0.594762921 batch mAP 0.557342529 batch PCKh 0.5625\n",
      "Validated batch 44 batch loss 0.581835866 batch mAP 0.545471191 batch PCKh 0.75\n",
      "Validated batch 45 batch loss 0.502781689 batch mAP 0.523681641 batch PCKh 0.3125\n",
      "Validated batch 46 batch loss 0.595826745 batch mAP 0.582946777 batch PCKh 0.875\n",
      "Validated batch 47 batch loss 0.543500185 batch mAP 0.614959717 batch PCKh 0.375\n",
      "Validated batch 48 batch loss 0.574082851 batch mAP 0.482116699 batch PCKh 0.125\n",
      "Validated batch 49 batch loss 0.532920837 batch mAP 0.559234619 batch PCKh 0.4375\n",
      "Validated batch 50 batch loss 0.559975266 batch mAP 0.525848389 batch PCKh 0.4375\n",
      "Validated batch 51 batch loss 0.580548406 batch mAP 0.446594238 batch PCKh 0.5625\n",
      "Validated batch 52 batch loss 0.532675147 batch mAP 0.447570801 batch PCKh 0.125\n",
      "Validated batch 53 batch loss 0.60042417 batch mAP 0.552490234 batch PCKh 0.625\n",
      "Validated batch 54 batch loss 0.497563899 batch mAP 0.516815186 batch PCKh 0.6875\n",
      "Validated batch 55 batch loss 0.545920372 batch mAP 0.563262939 batch PCKh 0.75\n",
      "Validated batch 56 batch loss 0.603203535 batch mAP 0.496246338 batch PCKh 0.625\n",
      "Validated batch 57 batch loss 0.585596442 batch mAP 0.506011963 batch PCKh 0.5\n",
      "Validated batch 58 batch loss 0.633127093 batch mAP 0.530059814 batch PCKh 0.4375\n",
      "Validated batch 59 batch loss 0.531009 batch mAP 0.540863037 batch PCKh 0.4375\n",
      "Validated batch 60 batch loss 0.672336161 batch mAP 0.380859375 batch PCKh 0\n",
      "Validated batch 61 batch loss 0.558994 batch mAP 0.56463623 batch PCKh 0.3125\n",
      "Validated batch 62 batch loss 0.583176494 batch mAP 0.56628418 batch PCKh 0.5625\n",
      "Validated batch 63 batch loss 0.637727797 batch mAP 0.459899902 batch PCKh 0.625\n",
      "Validated batch 64 batch loss 0.488460183 batch mAP 0.611022949 batch PCKh 0.3125\n",
      "Validated batch 65 batch loss 0.549027622 batch mAP 0.677063 batch PCKh 0.75\n",
      "Validated batch 66 batch loss 0.646905124 batch mAP 0.573883057 batch PCKh 0.1875\n",
      "Validated batch 67 batch loss 0.628486335 batch mAP 0.51373291 batch PCKh 0.5\n",
      "Validated batch 68 batch loss 0.549593031 batch mAP 0.571167 batch PCKh 0.125\n",
      "Validated batch 69 batch loss 0.641061902 batch mAP 0.490478516 batch PCKh 0.125\n",
      "Validated batch 70 batch loss 0.550769687 batch mAP 0.543792725 batch PCKh 0.25\n",
      "Validated batch 71 batch loss 0.508545339 batch mAP 0.649230957 batch PCKh 0.1875\n",
      "Validated batch 72 batch loss 0.613755107 batch mAP 0.527923584 batch PCKh 0.3125\n",
      "Validated batch 73 batch loss 0.567650676 batch mAP 0.526947 batch PCKh 0.375\n",
      "Validated batch 74 batch loss 0.597537041 batch mAP 0.580169678 batch PCKh 0.1875\n",
      "Validated batch 75 batch loss 0.589498758 batch mAP 0.512359619 batch PCKh 0.6875\n",
      "Validated batch 76 batch loss 0.584168673 batch mAP 0.53326416 batch PCKh 0.1875\n",
      "Validated batch 77 batch loss 0.521801531 batch mAP 0.575683594 batch PCKh 0.5\n",
      "Validated batch 78 batch loss 0.608933568 batch mAP 0.55859375 batch PCKh 0.125\n",
      "Validated batch 79 batch loss 0.650925875 batch mAP 0.449890137 batch PCKh 0.3125\n",
      "Validated batch 80 batch loss 0.647710502 batch mAP 0.495513916 batch PCKh 0.3125\n",
      "Validated batch 81 batch loss 0.535028338 batch mAP 0.549804688 batch PCKh 0.1875\n",
      "Validated batch 82 batch loss 0.69081533 batch mAP 0.456726074 batch PCKh 0.3125\n",
      "Validated batch 83 batch loss 0.58616513 batch mAP 0.595489502 batch PCKh 0.5\n",
      "Validated batch 84 batch loss 0.441691339 batch mAP 0.632232666 batch PCKh 0.375\n",
      "Validated batch 85 batch loss 0.589238048 batch mAP 0.469451904 batch PCKh 0.625\n",
      "Validated batch 86 batch loss 0.677042 batch mAP 0.411865234 batch PCKh 0.0625\n",
      "Validated batch 87 batch loss 0.595473886 batch mAP 0.546112061 batch PCKh 0.5\n",
      "Validated batch 88 batch loss 0.564276099 batch mAP 0.556030273 batch PCKh 0.1875\n",
      "Validated batch 89 batch loss 0.612445474 batch mAP 0.444366455 batch PCKh 0.125\n",
      "Validated batch 90 batch loss 0.457190633 batch mAP 0.593475342 batch PCKh 0.6875\n",
      "Validated batch 91 batch loss 0.5861094 batch mAP 0.514312744 batch PCKh 0.0625\n",
      "Validated batch 92 batch loss 0.628155112 batch mAP 0.54901123 batch PCKh 0.5\n",
      "Validated batch 93 batch loss 0.550843596 batch mAP 0.620239258 batch PCKh 0.6875\n",
      "Validated batch 94 batch loss 0.604922175 batch mAP 0.571655273 batch PCKh 0.4375\n",
      "Validated batch 95 batch loss 0.598047912 batch mAP 0.566314697 batch PCKh 0.5\n",
      "Validated batch 96 batch loss 0.531689167 batch mAP 0.592346191 batch PCKh 0.5\n",
      "Validated batch 97 batch loss 0.632536769 batch mAP 0.405090332 batch PCKh 0.1875\n",
      "Validated batch 98 batch loss 0.494378775 batch mAP 0.613616943 batch PCKh 0.1875\n",
      "Validated batch 99 batch loss 0.483844548 batch mAP 0.565185547 batch PCKh 0.125\n",
      "Validated batch 100 batch loss 0.619232714 batch mAP 0.572937 batch PCKh 0.4375\n",
      "Validated batch 101 batch loss 0.607207477 batch mAP 0.620819092 batch PCKh 0.75\n",
      "Validated batch 102 batch loss 0.502845585 batch mAP 0.61706543 batch PCKh 0.3125\n",
      "Validated batch 103 batch loss 0.727509618 batch mAP 0.543701172 batch PCKh 0\n",
      "Validated batch 104 batch loss 0.559261382 batch mAP 0.61730957 batch PCKh 0.6875\n",
      "Validated batch 105 batch loss 0.570962071 batch mAP 0.584564209 batch PCKh 0.1875\n",
      "Validated batch 106 batch loss 0.569491744 batch mAP 0.497894287 batch PCKh 0.5\n",
      "Validated batch 107 batch loss 0.528374135 batch mAP 0.523681641 batch PCKh 0.4375\n",
      "Validated batch 108 batch loss 0.472391605 batch mAP 0.647216797 batch PCKh 0.1875\n",
      "Validated batch 109 batch loss 0.536946952 batch mAP 0.655670166 batch PCKh 0.6875\n",
      "Validated batch 110 batch loss 0.553901672 batch mAP 0.575042725 batch PCKh 0.3125\n",
      "Validated batch 111 batch loss 0.604440808 batch mAP 0.539428711 batch PCKh 0\n",
      "Validated batch 112 batch loss 0.510707736 batch mAP 0.641571045 batch PCKh 0.25\n",
      "Validated batch 113 batch loss 0.584318 batch mAP 0.591308594 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 114 batch loss 0.496508867 batch mAP 0.616088867 batch PCKh 0.5\n",
      "Validated batch 115 batch loss 0.695557058 batch mAP 0.512146 batch PCKh 0\n",
      "Validated batch 116 batch loss 0.490506619 batch mAP 0.4765625 batch PCKh 0\n",
      "Validated batch 117 batch loss 0.555765748 batch mAP 0.542236328 batch PCKh 0.5\n",
      "Validated batch 118 batch loss 0.589151263 batch mAP 0.542053223 batch PCKh 0.375\n",
      "Validated batch 119 batch loss 0.540792346 batch mAP 0.611053467 batch PCKh 0.25\n",
      "Validated batch 120 batch loss 0.601427913 batch mAP 0.561615 batch PCKh 0.8125\n",
      "Validated batch 121 batch loss 0.626749754 batch mAP 0.584381104 batch PCKh 0.75\n",
      "Validated batch 122 batch loss 0.628063321 batch mAP 0.5027771 batch PCKh 0.4375\n",
      "Validated batch 123 batch loss 0.599348307 batch mAP 0.477172852 batch PCKh 0.125\n",
      "Validated batch 124 batch loss 0.645169795 batch mAP 0.58001709 batch PCKh 0.5\n",
      "Validated batch 125 batch loss 0.561118126 batch mAP 0.570556641 batch PCKh 0.625\n",
      "Validated batch 126 batch loss 0.548568 batch mAP 0.542205811 batch PCKh 0.6875\n",
      "Validated batch 127 batch loss 0.654100299 batch mAP 0.469573975 batch PCKh 0.0625\n",
      "Validated batch 128 batch loss 0.637799501 batch mAP 0.529418945 batch PCKh 0.1875\n",
      "Validated batch 129 batch loss 0.477843732 batch mAP 0.64453125 batch PCKh 0.3125\n",
      "Validated batch 130 batch loss 0.476826102 batch mAP 0.606506348 batch PCKh 0\n",
      "Validated batch 131 batch loss 0.526409388 batch mAP 0.549713135 batch PCKh 0.1875\n",
      "Validated batch 132 batch loss 0.600134492 batch mAP 0.492431641 batch PCKh 0.25\n",
      "Validated batch 133 batch loss 0.534114361 batch mAP 0.555664062 batch PCKh 0.6875\n",
      "Validated batch 134 batch loss 0.52633059 batch mAP 0.576019287 batch PCKh 0.6875\n",
      "Validated batch 135 batch loss 0.529264629 batch mAP 0.633483887 batch PCKh 0.375\n",
      "Validated batch 136 batch loss 0.612370074 batch mAP 0.503997803 batch PCKh 0.4375\n",
      "Validated batch 137 batch loss 0.714471519 batch mAP 0.507446289 batch PCKh 0.125\n",
      "Validated batch 138 batch loss 0.632093251 batch mAP 0.485534668 batch PCKh 0.5\n",
      "Validated batch 139 batch loss 0.58470875 batch mAP 0.514282227 batch PCKh 0.375\n",
      "Validated batch 140 batch loss 0.548416376 batch mAP 0.486572266 batch PCKh 0\n",
      "Validated batch 141 batch loss 0.532422543 batch mAP 0.555023193 batch PCKh 0.25\n",
      "Validated batch 142 batch loss 0.520874381 batch mAP 0.550720215 batch PCKh 0.25\n",
      "Validated batch 143 batch loss 0.538252473 batch mAP 0.596069336 batch PCKh 0.75\n",
      "Validated batch 144 batch loss 0.617183208 batch mAP 0.486785889 batch PCKh 0.5625\n",
      "Validated batch 145 batch loss 0.513222 batch mAP 0.681671143 batch PCKh 0.3125\n",
      "Validated batch 146 batch loss 0.564876497 batch mAP 0.613006592 batch PCKh 0.625\n",
      "Validated batch 147 batch loss 0.595529914 batch mAP 0.548187256 batch PCKh 0.6875\n",
      "Validated batch 148 batch loss 0.541590214 batch mAP 0.562561035 batch PCKh 0.6875\n",
      "Validated batch 149 batch loss 0.542972684 batch mAP 0.591064453 batch PCKh 0.125\n",
      "Validated batch 150 batch loss 0.662280083 batch mAP 0.462921143 batch PCKh 0.6875\n",
      "Validated batch 151 batch loss 0.534290671 batch mAP 0.58026123 batch PCKh 0.4375\n",
      "Validated batch 152 batch loss 0.640837193 batch mAP 0.464691162 batch PCKh 0.3125\n",
      "Validated batch 153 batch loss 0.588155448 batch mAP 0.575256348 batch PCKh 0.3125\n",
      "Validated batch 154 batch loss 0.684299 batch mAP 0.498687744 batch PCKh 0.125\n",
      "Validated batch 155 batch loss 0.624818265 batch mAP 0.425689697 batch PCKh 0.625\n",
      "Validated batch 156 batch loss 0.605804265 batch mAP 0.577697754 batch PCKh 0.5625\n",
      "Validated batch 157 batch loss 0.633137465 batch mAP 0.540557861 batch PCKh 0.3125\n",
      "Validated batch 158 batch loss 0.55096966 batch mAP 0.447235107 batch PCKh 0.5\n",
      "Validated batch 159 batch loss 0.592205644 batch mAP 0.576049805 batch PCKh 0.375\n",
      "Validated batch 160 batch loss 0.628896475 batch mAP 0.544891357 batch PCKh 0.6875\n",
      "Validated batch 161 batch loss 0.666937053 batch mAP 0.509338379 batch PCKh 0.75\n",
      "Validated batch 162 batch loss 0.686959 batch mAP 0.492858887 batch PCKh 0.5\n",
      "Validated batch 163 batch loss 0.625628 batch mAP 0.556488037 batch PCKh 0.5625\n",
      "Validated batch 164 batch loss 0.613909304 batch mAP 0.391723633 batch PCKh 0.375\n",
      "Validated batch 165 batch loss 0.652369201 batch mAP 0.610656738 batch PCKh 0.375\n",
      "Validated batch 166 batch loss 0.618590713 batch mAP 0.54574585 batch PCKh 0.5\n",
      "Validated batch 167 batch loss 0.595953584 batch mAP 0.56817627 batch PCKh 0.75\n",
      "Validated batch 168 batch loss 0.640108764 batch mAP 0.536102295 batch PCKh 0.75\n",
      "Validated batch 169 batch loss 0.656245 batch mAP 0.502532959 batch PCKh 0.6875\n",
      "Validated batch 170 batch loss 0.617368221 batch mAP 0.58581543 batch PCKh 0.75\n",
      "Validated batch 171 batch loss 0.687019825 batch mAP 0.488891602 batch PCKh 0.5\n",
      "Validated batch 172 batch loss 0.576796293 batch mAP 0.600372314 batch PCKh 0.1875\n",
      "Validated batch 173 batch loss 0.616952062 batch mAP 0.590667725 batch PCKh 0.6875\n",
      "Validated batch 174 batch loss 0.506112397 batch mAP 0.545898438 batch PCKh 0.125\n",
      "Validated batch 175 batch loss 0.577055 batch mAP 0.608551 batch PCKh 0.625\n",
      "Validated batch 176 batch loss 0.598250091 batch mAP 0.59185791 batch PCKh 0.375\n",
      "Validated batch 177 batch loss 0.655989945 batch mAP 0.468048096 batch PCKh 0.3125\n",
      "Validated batch 178 batch loss 0.612470627 batch mAP 0.602233887 batch PCKh 0.375\n",
      "Validated batch 179 batch loss 0.66110605 batch mAP 0.548980713 batch PCKh 0.5625\n",
      "Validated batch 180 batch loss 0.582458258 batch mAP 0.522155762 batch PCKh 0.5\n",
      "Validated batch 181 batch loss 0.526050925 batch mAP 0.639953613 batch PCKh 0.1875\n",
      "Validated batch 182 batch loss 0.620372295 batch mAP 0.531738281 batch PCKh 0.375\n",
      "Validated batch 183 batch loss 0.656538606 batch mAP 0.487792969 batch PCKh 0.4375\n",
      "Validated batch 184 batch loss 0.547050178 batch mAP 0.528961182 batch PCKh 0.5\n",
      "Validated batch 185 batch loss 0.615272343 batch mAP 0.543182373 batch PCKh 0.75\n",
      "Validated batch 186 batch loss 0.624955773 batch mAP 0.503143311 batch PCKh 0.4375\n",
      "Validated batch 187 batch loss 0.621857762 batch mAP 0.520843506 batch PCKh 0.6875\n",
      "Validated batch 188 batch loss 0.618633091 batch mAP 0.56036377 batch PCKh 0.5625\n",
      "Validated batch 189 batch loss 0.585719287 batch mAP 0.524047852 batch PCKh 0.4375\n",
      "Validated batch 190 batch loss 0.561489463 batch mAP 0.587921143 batch PCKh 0.5\n",
      "Validated batch 191 batch loss 0.591926873 batch mAP 0.550354 batch PCKh 0.6875\n",
      "Validated batch 192 batch loss 0.592834711 batch mAP 0.543487549 batch PCKh 0.375\n",
      "Validated batch 193 batch loss 0.6251809 batch mAP 0.516021729 batch PCKh 0.125\n",
      "Validated batch 194 batch loss 0.609423637 batch mAP 0.497680664 batch PCKh 0.5\n",
      "Validated batch 195 batch loss 0.739206493 batch mAP 0.442443848 batch PCKh 0.1875\n",
      "Validated batch 196 batch loss 0.664554477 batch mAP 0.527832031 batch PCKh 0.5\n",
      "Validated batch 197 batch loss 0.53329277 batch mAP 0.571899414 batch PCKh 0.0625\n",
      "Validated batch 198 batch loss 0.61408627 batch mAP 0.618530273 batch PCKh 0.3125\n",
      "Validated batch 199 batch loss 0.567201614 batch mAP 0.587310791 batch PCKh 0.8125\n",
      "Validated batch 200 batch loss 0.645526767 batch mAP 0.496521 batch PCKh 0.6875\n",
      "Validated batch 201 batch loss 0.565057814 batch mAP 0.566040039 batch PCKh 0.25\n",
      "Validated batch 202 batch loss 0.585881174 batch mAP 0.621704102 batch PCKh 0.4375\n",
      "Validated batch 203 batch loss 0.674219728 batch mAP 0.523223877 batch PCKh 0.1875\n",
      "Validated batch 204 batch loss 0.587039232 batch mAP 0.544250488 batch PCKh 0.4375\n",
      "Validated batch 205 batch loss 0.594223797 batch mAP 0.646240234 batch PCKh 0.5625\n",
      "Validated batch 206 batch loss 0.573947608 batch mAP 0.55645752 batch PCKh 0.8125\n",
      "Validated batch 207 batch loss 0.595869064 batch mAP 0.551025391 batch PCKh 0.625\n",
      "Validated batch 208 batch loss 0.535904408 batch mAP 0.624267578 batch PCKh 0.4375\n",
      "Validated batch 209 batch loss 0.583768606 batch mAP 0.568359375 batch PCKh 0.6875\n",
      "Validated batch 210 batch loss 0.708504438 batch mAP 0.520233154 batch PCKh 0.4375\n",
      "Validated batch 211 batch loss 0.711749673 batch mAP 0.523193359 batch PCKh 0\n",
      "Validated batch 212 batch loss 0.557916403 batch mAP 0.57800293 batch PCKh 0.4375\n",
      "Validated batch 213 batch loss 0.589872241 batch mAP 0.479156494 batch PCKh 0.5\n",
      "Validated batch 214 batch loss 0.654021621 batch mAP 0.536346436 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 215 batch loss 0.554451346 batch mAP 0.610778809 batch PCKh 0\n",
      "Validated batch 216 batch loss 0.630204082 batch mAP 0.655090332 batch PCKh 0.6875\n",
      "Validated batch 217 batch loss 0.580203176 batch mAP 0.623291 batch PCKh 0.75\n",
      "Validated batch 218 batch loss 0.644272685 batch mAP 0.44140625 batch PCKh 0.375\n",
      "Validated batch 219 batch loss 0.63620013 batch mAP 0.552398682 batch PCKh 0.75\n",
      "Validated batch 220 batch loss 0.429973781 batch mAP 0.591308594 batch PCKh 0.5\n",
      "Validated batch 221 batch loss 0.565776765 batch mAP 0.600067139 batch PCKh 0.6875\n",
      "Validated batch 222 batch loss 0.611114383 batch mAP 0.416809082 batch PCKh 0.75\n",
      "Validated batch 223 batch loss 0.63628763 batch mAP 0.554077148 batch PCKh 0.5\n",
      "Validated batch 224 batch loss 0.581806302 batch mAP 0.556915283 batch PCKh 0.375\n",
      "Validated batch 225 batch loss 0.543610334 batch mAP 0.656524658 batch PCKh 0.5625\n",
      "Validated batch 226 batch loss 0.565545499 batch mAP 0.555450439 batch PCKh 0.5\n",
      "Validated batch 227 batch loss 0.654144347 batch mAP 0.536987305 batch PCKh 0.25\n",
      "Validated batch 228 batch loss 0.56997335 batch mAP 0.588317871 batch PCKh 0.25\n",
      "Validated batch 229 batch loss 0.588002503 batch mAP 0.560638428 batch PCKh 0.5625\n",
      "Validated batch 230 batch loss 0.58724916 batch mAP 0.533294678 batch PCKh 0.3125\n",
      "Validated batch 231 batch loss 0.659246743 batch mAP 0.539764404 batch PCKh 0.4375\n",
      "Validated batch 232 batch loss 0.571712494 batch mAP 0.598968506 batch PCKh 0.8125\n",
      "Validated batch 233 batch loss 0.620864749 batch mAP 0.500518799 batch PCKh 0.625\n",
      "Validated batch 234 batch loss 0.57770592 batch mAP 0.546813965 batch PCKh 0.75\n",
      "Validated batch 235 batch loss 0.558779716 batch mAP 0.594696045 batch PCKh 0.5625\n",
      "Validated batch 236 batch loss 0.584655881 batch mAP 0.52355957 batch PCKh 0.375\n",
      "Validated batch 237 batch loss 0.486602962 batch mAP 0.657074 batch PCKh 0.6875\n",
      "Validated batch 238 batch loss 0.617913187 batch mAP 0.555114746 batch PCKh 0.5625\n",
      "Validated batch 239 batch loss 0.619131327 batch mAP 0.552032471 batch PCKh 0.1875\n",
      "Validated batch 240 batch loss 0.587401092 batch mAP 0.597290039 batch PCKh 0.3125\n",
      "Validated batch 241 batch loss 0.627863765 batch mAP 0.549163818 batch PCKh 0.625\n",
      "Validated batch 242 batch loss 0.646213 batch mAP 0.714111328 batch PCKh 0.8125\n",
      "Validated batch 243 batch loss 0.63457036 batch mAP 0.558654785 batch PCKh 0.3125\n",
      "Validated batch 244 batch loss 0.619908452 batch mAP 0.5546875 batch PCKh 0.5\n",
      "Validated batch 245 batch loss 0.618485391 batch mAP 0.532562256 batch PCKh 0.375\n",
      "Validated batch 246 batch loss 0.624487 batch mAP 0.50769043 batch PCKh 0.0625\n",
      "Validated batch 247 batch loss 0.622793734 batch mAP 0.49319458 batch PCKh 0.5\n",
      "Validated batch 248 batch loss 0.622580171 batch mAP 0.53414917 batch PCKh 0\n",
      "Validated batch 249 batch loss 0.555832 batch mAP 0.545318604 batch PCKh 0.625\n",
      "Validated batch 250 batch loss 0.568571091 batch mAP 0.632293701 batch PCKh 0.25\n",
      "Validated batch 251 batch loss 0.60340333 batch mAP 0.602142334 batch PCKh 0.5\n",
      "Validated batch 252 batch loss 0.617207944 batch mAP 0.587036133 batch PCKh 0.3125\n",
      "Validated batch 253 batch loss 0.622697532 batch mAP 0.532989502 batch PCKh 0.0625\n",
      "Validated batch 254 batch loss 0.565467715 batch mAP 0.568511963 batch PCKh 0.6875\n",
      "Validated batch 255 batch loss 0.548858106 batch mAP 0.412902832 batch PCKh 0.1875\n",
      "Validated batch 256 batch loss 0.643465757 batch mAP 0.493255615 batch PCKh 0.4375\n",
      "Validated batch 257 batch loss 0.654883444 batch mAP 0.547149658 batch PCKh 0.5\n",
      "Validated batch 258 batch loss 0.652895212 batch mAP 0.458892822 batch PCKh 0\n",
      "Validated batch 259 batch loss 0.690273404 batch mAP 0.479095459 batch PCKh 0.125\n",
      "Validated batch 260 batch loss 0.570092678 batch mAP 0.484222412 batch PCKh 0\n",
      "Validated batch 261 batch loss 0.601976514 batch mAP 0.535186768 batch PCKh 0.5625\n",
      "Validated batch 262 batch loss 0.635613203 batch mAP 0.545410156 batch PCKh 0.1875\n",
      "Validated batch 263 batch loss 0.55376 batch mAP 0.621429443 batch PCKh 0.5625\n",
      "Validated batch 264 batch loss 0.764454 batch mAP 0.350402832 batch PCKh 0\n",
      "Validated batch 265 batch loss 0.577480257 batch mAP 0.501800537 batch PCKh 0.4375\n",
      "Validated batch 266 batch loss 0.570038 batch mAP 0.536956787 batch PCKh 0.25\n",
      "Validated batch 267 batch loss 0.60412848 batch mAP 0.552520752 batch PCKh 0.125\n",
      "Validated batch 268 batch loss 0.549257815 batch mAP 0.52243042 batch PCKh 0.1875\n",
      "Validated batch 269 batch loss 0.634889841 batch mAP 0.555450439 batch PCKh 0.5625\n",
      "Validated batch 270 batch loss 0.475389481 batch mAP 0.595459 batch PCKh 0.4375\n",
      "Validated batch 271 batch loss 0.495448917 batch mAP 0.606933594 batch PCKh 0.5625\n",
      "Validated batch 272 batch loss 0.550040603 batch mAP 0.508911133 batch PCKh 0.5\n",
      "Validated batch 273 batch loss 0.668291211 batch mAP 0.529388428 batch PCKh 0.0625\n",
      "Validated batch 274 batch loss 0.584240556 batch mAP 0.623687744 batch PCKh 0.75\n",
      "Validated batch 275 batch loss 0.609272599 batch mAP 0.515045166 batch PCKh 0.5625\n",
      "Validated batch 276 batch loss 0.526861 batch mAP 0.614318848 batch PCKh 0.6875\n",
      "Validated batch 277 batch loss 0.640175581 batch mAP 0.507629395 batch PCKh 0.3125\n",
      "Validated batch 278 batch loss 0.646139383 batch mAP 0.505065918 batch PCKh 0.0625\n",
      "Validated batch 279 batch loss 0.606469274 batch mAP 0.550628662 batch PCKh 0.3125\n",
      "Validated batch 280 batch loss 0.562740684 batch mAP 0.605255127 batch PCKh 0.5625\n",
      "Validated batch 281 batch loss 0.642895579 batch mAP 0.61605835 batch PCKh 0.375\n",
      "Validated batch 282 batch loss 0.526888132 batch mAP 0.597045898 batch PCKh 0.625\n",
      "Validated batch 283 batch loss 0.591197 batch mAP 0.62399292 batch PCKh 0.4375\n",
      "Validated batch 284 batch loss 0.572688 batch mAP 0.650817871 batch PCKh 0.5625\n",
      "Validated batch 285 batch loss 0.553820074 batch mAP 0.610809326 batch PCKh 0.875\n",
      "Validated batch 286 batch loss 0.500391603 batch mAP 0.626342773 batch PCKh 0.5625\n",
      "Validated batch 287 batch loss 0.62355274 batch mAP 0.49105835 batch PCKh 0.1875\n",
      "Validated batch 288 batch loss 0.610750318 batch mAP 0.62121582 batch PCKh 0.5625\n",
      "Validated batch 289 batch loss 0.58082962 batch mAP 0.609588623 batch PCKh 0.4375\n",
      "Validated batch 290 batch loss 0.613997 batch mAP 0.539154053 batch PCKh 0.625\n",
      "Validated batch 291 batch loss 0.590282321 batch mAP 0.623809814 batch PCKh 0\n",
      "Validated batch 292 batch loss 0.517637968 batch mAP 0.545928955 batch PCKh 0.5\n",
      "Validated batch 293 batch loss 0.554106891 batch mAP 0.561767578 batch PCKh 0.5625\n",
      "Validated batch 294 batch loss 0.586874485 batch mAP 0.652679443 batch PCKh 0.25\n",
      "Validated batch 295 batch loss 0.606854737 batch mAP 0.58581543 batch PCKh 0.625\n",
      "Validated batch 296 batch loss 0.590208292 batch mAP 0.601226807 batch PCKh 0.4375\n",
      "Validated batch 297 batch loss 0.591914415 batch mAP 0.642028809 batch PCKh 0.3125\n",
      "Validated batch 298 batch loss 0.609246969 batch mAP 0.534759521 batch PCKh 0\n",
      "Validated batch 299 batch loss 0.637925446 batch mAP 0.461883545 batch PCKh 0.25\n",
      "Validated batch 300 batch loss 0.582660377 batch mAP 0.542236328 batch PCKh 0.625\n",
      "Validated batch 301 batch loss 0.591648817 batch mAP 0.551513672 batch PCKh 0.5\n",
      "Validated batch 302 batch loss 0.619288802 batch mAP 0.451629639 batch PCKh 0.4375\n",
      "Validated batch 303 batch loss 0.496824563 batch mAP 0.548553467 batch PCKh 0.4375\n",
      "Validated batch 304 batch loss 0.509998858 batch mAP 0.541046143 batch PCKh 0.625\n",
      "Validated batch 305 batch loss 0.647861 batch mAP 0.4871521 batch PCKh 0.1875\n",
      "Validated batch 306 batch loss 0.506825924 batch mAP 0.643585205 batch PCKh 0.1875\n",
      "Validated batch 307 batch loss 0.60303688 batch mAP 0.491973877 batch PCKh 0.5625\n",
      "Validated batch 308 batch loss 0.562902033 batch mAP 0.567718506 batch PCKh 0.625\n",
      "Validated batch 309 batch loss 0.529012382 batch mAP 0.577667236 batch PCKh 0.125\n",
      "Validated batch 310 batch loss 0.599694848 batch mAP 0.5625 batch PCKh 0.125\n",
      "Validated batch 311 batch loss 0.571937203 batch mAP 0.453826904 batch PCKh 0.1875\n",
      "Validated batch 312 batch loss 0.628711 batch mAP 0.518737793 batch PCKh 0.25\n",
      "Validated batch 313 batch loss 0.561727285 batch mAP 0.607147217 batch PCKh 0.25\n",
      "Validated batch 314 batch loss 0.606566787 batch mAP 0.556945801 batch PCKh 0.375\n",
      "Validated batch 315 batch loss 0.672499239 batch mAP 0.535919189 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 316 batch loss 0.700567961 batch mAP 0.525482178 batch PCKh 0.3125\n",
      "Validated batch 317 batch loss 0.721449614 batch mAP 0.513763428 batch PCKh 0\n",
      "Validated batch 318 batch loss 0.61070174 batch mAP 0.570373535 batch PCKh 0.3125\n",
      "Validated batch 319 batch loss 0.553636372 batch mAP 0.620544434 batch PCKh 0.625\n",
      "Validated batch 320 batch loss 0.627334297 batch mAP 0.534301758 batch PCKh 0.4375\n",
      "Validated batch 321 batch loss 0.529726505 batch mAP 0.45690918 batch PCKh 0.25\n",
      "Validated batch 322 batch loss 0.507906735 batch mAP 0.631286621 batch PCKh 0.5\n",
      "Validated batch 323 batch loss 0.531349719 batch mAP 0.575805664 batch PCKh 0.5625\n",
      "Validated batch 324 batch loss 0.566270232 batch mAP 0.558227539 batch PCKh 0.1875\n",
      "Validated batch 325 batch loss 0.704472065 batch mAP 0.531463623 batch PCKh 0.5\n",
      "Validated batch 326 batch loss 0.549021423 batch mAP 0.623413086 batch PCKh 0.5\n",
      "Validated batch 327 batch loss 0.558035493 batch mAP 0.582519531 batch PCKh 0.5\n",
      "Validated batch 328 batch loss 0.546990573 batch mAP 0.675842285 batch PCKh 0.375\n",
      "Validated batch 329 batch loss 0.61408633 batch mAP 0.467681885 batch PCKh 0.375\n",
      "Validated batch 330 batch loss 0.513711691 batch mAP 0.54699707 batch PCKh 0.25\n",
      "Validated batch 331 batch loss 0.57837975 batch mAP 0.483032227 batch PCKh 0.4375\n",
      "Validated batch 332 batch loss 0.613167942 batch mAP 0.568939209 batch PCKh 0.75\n",
      "Validated batch 333 batch loss 0.526066065 batch mAP 0.566131592 batch PCKh 0.0625\n",
      "Validated batch 334 batch loss 0.662289262 batch mAP 0.549224854 batch PCKh 0.75\n",
      "Validated batch 335 batch loss 0.546261668 batch mAP 0.609985352 batch PCKh 0.25\n",
      "Validated batch 336 batch loss 0.612075686 batch mAP 0.580810547 batch PCKh 0.5625\n",
      "Validated batch 337 batch loss 0.565723896 batch mAP 0.568237305 batch PCKh 0.1875\n",
      "Validated batch 338 batch loss 0.597075582 batch mAP 0.463867188 batch PCKh 0.4375\n",
      "Validated batch 339 batch loss 0.523188591 batch mAP 0.59161377 batch PCKh 0.5\n",
      "Validated batch 340 batch loss 0.576251745 batch mAP 0.557647705 batch PCKh 0.3125\n",
      "Validated batch 341 batch loss 0.756985784 batch mAP 0.566253662 batch PCKh 0\n",
      "Validated batch 342 batch loss 0.607630372 batch mAP 0.569335938 batch PCKh 0.25\n",
      "Validated batch 343 batch loss 0.588523686 batch mAP 0.582855225 batch PCKh 0.3125\n",
      "Validated batch 344 batch loss 0.564948559 batch mAP 0.571380615 batch PCKh 0.0625\n",
      "Validated batch 345 batch loss 0.53272742 batch mAP 0.565856934 batch PCKh 0.5625\n",
      "Validated batch 346 batch loss 0.483087569 batch mAP 0.628601074 batch PCKh 0\n",
      "Validated batch 347 batch loss 0.562653601 batch mAP 0.433441162 batch PCKh 0.4375\n",
      "Validated batch 348 batch loss 0.566516161 batch mAP 0.560791 batch PCKh 0.375\n",
      "Validated batch 349 batch loss 0.665556192 batch mAP 0.57611084 batch PCKh 0.6875\n",
      "Validated batch 350 batch loss 0.655221939 batch mAP 0.543914795 batch PCKh 0.5\n",
      "Validated batch 351 batch loss 0.561123133 batch mAP 0.477996826 batch PCKh 0.1875\n",
      "Validated batch 352 batch loss 0.705324233 batch mAP 0.398162842 batch PCKh 0.125\n",
      "Validated batch 353 batch loss 0.635373 batch mAP 0.379547119 batch PCKh 0.5\n",
      "Validated batch 354 batch loss 0.674143195 batch mAP 0.397766113 batch PCKh 0.25\n",
      "Validated batch 355 batch loss 0.628662229 batch mAP 0.518432617 batch PCKh 0.75\n",
      "Validated batch 356 batch loss 0.533696771 batch mAP 0.609985352 batch PCKh 0.5\n",
      "Validated batch 357 batch loss 0.672526419 batch mAP 0.539520264 batch PCKh 0.125\n",
      "Validated batch 358 batch loss 0.675852418 batch mAP 0.565124512 batch PCKh 0.375\n",
      "Validated batch 359 batch loss 0.606236577 batch mAP 0.46762085 batch PCKh 0.1875\n",
      "Validated batch 360 batch loss 0.576552272 batch mAP 0.574676514 batch PCKh 0.5625\n",
      "Validated batch 361 batch loss 0.693989635 batch mAP 0.553985596 batch PCKh 0\n",
      "Validated batch 362 batch loss 0.556137204 batch mAP 0.526763916 batch PCKh 0.4375\n",
      "Validated batch 363 batch loss 0.551762283 batch mAP 0.503448486 batch PCKh 0.5\n",
      "Validated batch 364 batch loss 0.660663366 batch mAP 0.537261963 batch PCKh 0.625\n",
      "Validated batch 365 batch loss 0.543615758 batch mAP 0.595153809 batch PCKh 0.6875\n",
      "Validated batch 366 batch loss 0.447764903 batch mAP 0.606445312 batch PCKh 0.1875\n",
      "Validated batch 367 batch loss 0.493065566 batch mAP 0.580596924 batch PCKh 0.625\n",
      "Validated batch 368 batch loss 0.559204698 batch mAP 0.522064209 batch PCKh 0.5\n",
      "Validated batch 369 batch loss 0.666835666 batch mAP 0.584198 batch PCKh 0.375\n",
      "Epoch 4 val loss 0.5890436172485352 val mAP 0.5514048933982849 val PCKh\n",
      "Epoch 4 completed in 767.03 seconds\n",
      "Model /aiffel/aiffel/model_weight/GD08/y_model-epoch-4-loss-0.5890.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.481522888 batch mAP 0.502929688 batch PCKh 0.1875\n",
      "Trained batch 2 batch loss 0.529851496 batch mAP 0.522155762 batch PCKh 0\n",
      "Trained batch 3 batch loss 0.581101358 batch mAP 0.506011963 batch PCKh 0.6875\n",
      "Trained batch 4 batch loss 0.652347147 batch mAP 0.507476807 batch PCKh 0.8125\n",
      "Trained batch 5 batch loss 0.653336227 batch mAP 0.535888672 batch PCKh 0.5\n",
      "Trained batch 6 batch loss 0.546268702 batch mAP 0.589416504 batch PCKh 0.25\n",
      "Trained batch 7 batch loss 0.48742 batch mAP 0.561004639 batch PCKh 0\n",
      "Trained batch 8 batch loss 0.526546359 batch mAP 0.650299072 batch PCKh 0.75\n",
      "Trained batch 9 batch loss 0.608036518 batch mAP 0.629882812 batch PCKh 0.75\n",
      "Trained batch 10 batch loss 0.591725 batch mAP 0.619659424 batch PCKh 0.625\n",
      "Trained batch 11 batch loss 0.636115074 batch mAP 0.59765625 batch PCKh 0.6875\n",
      "Trained batch 12 batch loss 0.467252374 batch mAP 0.668762207 batch PCKh 0.5625\n",
      "Trained batch 13 batch loss 0.560680091 batch mAP 0.654052734 batch PCKh 0.625\n",
      "Trained batch 14 batch loss 0.539024353 batch mAP 0.63104248 batch PCKh 0.625\n",
      "Trained batch 15 batch loss 0.523614049 batch mAP 0.604553223 batch PCKh 0.25\n",
      "Trained batch 16 batch loss 0.564620733 batch mAP 0.618896484 batch PCKh 0.6875\n",
      "Trained batch 17 batch loss 0.617780805 batch mAP 0.576812744 batch PCKh 0.3125\n",
      "Trained batch 18 batch loss 0.647535443 batch mAP 0.572387695 batch PCKh 0.1875\n",
      "Trained batch 19 batch loss 0.675141275 batch mAP 0.523773193 batch PCKh 0.4375\n",
      "Trained batch 20 batch loss 0.594635844 batch mAP 0.614501953 batch PCKh 0.6875\n",
      "Trained batch 21 batch loss 0.49244234 batch mAP 0.613983154 batch PCKh 0.5\n",
      "Trained batch 22 batch loss 0.507490337 batch mAP 0.617950439 batch PCKh 0.4375\n",
      "Trained batch 23 batch loss 0.557361603 batch mAP 0.566833496 batch PCKh 0.3125\n",
      "Trained batch 24 batch loss 0.626307964 batch mAP 0.541534424 batch PCKh 0.625\n",
      "Trained batch 25 batch loss 0.605836034 batch mAP 0.635345459 batch PCKh 0.625\n",
      "Trained batch 26 batch loss 0.591043174 batch mAP 0.57611084 batch PCKh 0.5\n",
      "Trained batch 27 batch loss 0.499174893 batch mAP 0.601837158 batch PCKh 0.1875\n",
      "Trained batch 28 batch loss 0.560156405 batch mAP 0.611053467 batch PCKh 0.3125\n",
      "Trained batch 29 batch loss 0.585640311 batch mAP 0.628387451 batch PCKh 0.25\n",
      "Trained batch 30 batch loss 0.55757 batch mAP 0.660797119 batch PCKh 0.125\n",
      "Trained batch 31 batch loss 0.603534 batch mAP 0.646331787 batch PCKh 0.6875\n",
      "Trained batch 32 batch loss 0.569995821 batch mAP 0.601196289 batch PCKh 0.625\n",
      "Trained batch 33 batch loss 0.571285605 batch mAP 0.512298584 batch PCKh 0.5625\n",
      "Trained batch 34 batch loss 0.487775326 batch mAP 0.534057617 batch PCKh 0.75\n",
      "Trained batch 35 batch loss 0.576740503 batch mAP 0.546173096 batch PCKh 0.6875\n",
      "Trained batch 36 batch loss 0.546832 batch mAP 0.575195312 batch PCKh 0.75\n",
      "Trained batch 37 batch loss 0.4618783 batch mAP 0.558868408 batch PCKh 0.5625\n",
      "Trained batch 38 batch loss 0.619713068 batch mAP 0.541931152 batch PCKh 0.25\n",
      "Trained batch 39 batch loss 0.620422542 batch mAP 0.560272217 batch PCKh 0.1875\n",
      "Trained batch 40 batch loss 0.579296649 batch mAP 0.554931641 batch PCKh 0.6875\n",
      "Trained batch 41 batch loss 0.475380689 batch mAP 0.692199707 batch PCKh 0.1875\n",
      "Trained batch 42 batch loss 0.572317 batch mAP 0.604644775 batch PCKh 0.625\n",
      "Trained batch 43 batch loss 0.443546236 batch mAP 0.627044678 batch PCKh 0.25\n",
      "Trained batch 44 batch loss 0.556292057 batch mAP 0.523681641 batch PCKh 0.125\n",
      "Trained batch 45 batch loss 0.489247501 batch mAP 0.567016602 batch PCKh 0\n",
      "Trained batch 46 batch loss 0.586536169 batch mAP 0.543945312 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 47 batch loss 0.739011526 batch mAP 0.60144043 batch PCKh 0.25\n",
      "Trained batch 48 batch loss 0.739640236 batch mAP 0.605041504 batch PCKh 0\n",
      "Trained batch 49 batch loss 0.593646884 batch mAP 0.618225098 batch PCKh 0.4375\n",
      "Trained batch 50 batch loss 0.646230757 batch mAP 0.683959961 batch PCKh 0.1875\n",
      "Trained batch 51 batch loss 0.624372 batch mAP 0.524993896 batch PCKh 0.125\n",
      "Trained batch 52 batch loss 0.662915111 batch mAP 0.526977539 batch PCKh 0.8125\n",
      "Trained batch 53 batch loss 0.654972255 batch mAP 0.42956543 batch PCKh 0.5625\n",
      "Trained batch 54 batch loss 0.576113164 batch mAP 0.28427124 batch PCKh 0.4375\n",
      "Trained batch 55 batch loss 0.605124474 batch mAP 0.16784668 batch PCKh 0.75\n",
      "Trained batch 56 batch loss 0.63713032 batch mAP 0.158172607 batch PCKh 0.5625\n",
      "Trained batch 57 batch loss 0.609946251 batch mAP 0.190216064 batch PCKh 0.5\n",
      "Trained batch 58 batch loss 0.574332476 batch mAP 0.177642822 batch PCKh 0.75\n",
      "Trained batch 59 batch loss 0.555317283 batch mAP 0.368743896 batch PCKh 0.1875\n",
      "Trained batch 60 batch loss 0.576793611 batch mAP 0.319641113 batch PCKh 0.5\n",
      "Trained batch 61 batch loss 0.579929829 batch mAP 0.47454834 batch PCKh 0.25\n",
      "Trained batch 62 batch loss 0.631734431 batch mAP 0.501739502 batch PCKh 0.125\n",
      "Trained batch 63 batch loss 0.559931695 batch mAP 0.557556152 batch PCKh 0.5\n",
      "Trained batch 64 batch loss 0.6044119 batch mAP 0.542327881 batch PCKh 0.1875\n",
      "Trained batch 65 batch loss 0.562932611 batch mAP 0.541748047 batch PCKh 0.5\n",
      "Trained batch 66 batch loss 0.55935359 batch mAP 0.57724 batch PCKh 0.375\n",
      "Trained batch 67 batch loss 0.566113651 batch mAP 0.52722168 batch PCKh 0.4375\n",
      "Trained batch 68 batch loss 0.588115931 batch mAP 0.590820312 batch PCKh 0.6875\n",
      "Trained batch 69 batch loss 0.568118274 batch mAP 0.569366455 batch PCKh 0.8125\n",
      "Trained batch 70 batch loss 0.585602 batch mAP 0.519073486 batch PCKh 0.625\n",
      "Trained batch 71 batch loss 0.518466532 batch mAP 0.456054688 batch PCKh 0.375\n",
      "Trained batch 72 batch loss 0.536557794 batch mAP 0.533355713 batch PCKh 0.1875\n",
      "Trained batch 73 batch loss 0.533216834 batch mAP 0.608154297 batch PCKh 0.5\n",
      "Trained batch 74 batch loss 0.501158237 batch mAP 0.550445557 batch PCKh 0.5\n",
      "Trained batch 75 batch loss 0.448370367 batch mAP 0.52645874 batch PCKh 0.6875\n",
      "Trained batch 76 batch loss 0.420454443 batch mAP 0.47052002 batch PCKh 0.6875\n",
      "Trained batch 77 batch loss 0.457073122 batch mAP 0.518981934 batch PCKh 0.75\n",
      "Trained batch 78 batch loss 0.446613044 batch mAP 0.525543213 batch PCKh 0.75\n",
      "Trained batch 79 batch loss 0.503348172 batch mAP 0.509185791 batch PCKh 0.6875\n",
      "Trained batch 80 batch loss 0.670036435 batch mAP 0.390106201 batch PCKh 0\n",
      "Trained batch 81 batch loss 0.585348308 batch mAP 0.47644043 batch PCKh 0.5\n",
      "Trained batch 82 batch loss 0.580275834 batch mAP 0.50289917 batch PCKh 0.875\n",
      "Trained batch 83 batch loss 0.648749352 batch mAP 0.467071533 batch PCKh 0.6875\n",
      "Trained batch 84 batch loss 0.658080816 batch mAP 0.511291504 batch PCKh 0.625\n",
      "Trained batch 85 batch loss 0.656982481 batch mAP 0.539245605 batch PCKh 0.5625\n",
      "Trained batch 86 batch loss 0.581172466 batch mAP 0.514923096 batch PCKh 0.4375\n",
      "Trained batch 87 batch loss 0.538760781 batch mAP 0.545501709 batch PCKh 0.375\n",
      "Trained batch 88 batch loss 0.60981524 batch mAP 0.605133057 batch PCKh 0.375\n",
      "Trained batch 89 batch loss 0.59232372 batch mAP 0.59274292 batch PCKh 0.4375\n",
      "Trained batch 90 batch loss 0.567853332 batch mAP 0.584655762 batch PCKh 0.25\n",
      "Trained batch 91 batch loss 0.603934884 batch mAP 0.50289917 batch PCKh 0.5\n",
      "Trained batch 92 batch loss 0.559390664 batch mAP 0.516326904 batch PCKh 0.25\n",
      "Trained batch 93 batch loss 0.597594261 batch mAP 0.546142578 batch PCKh 0.375\n",
      "Trained batch 94 batch loss 0.581236 batch mAP 0.595397949 batch PCKh 0.375\n",
      "Trained batch 95 batch loss 0.616342247 batch mAP 0.48727417 batch PCKh 0.625\n",
      "Trained batch 96 batch loss 0.602800548 batch mAP 0.509490967 batch PCKh 0.75\n",
      "Trained batch 97 batch loss 0.676853597 batch mAP 0.550720215 batch PCKh 0\n",
      "Trained batch 98 batch loss 0.504946589 batch mAP 0.563659668 batch PCKh 0.5\n",
      "Trained batch 99 batch loss 0.584065139 batch mAP 0.532012939 batch PCKh 0.3125\n",
      "Trained batch 100 batch loss 0.589379609 batch mAP 0.559326172 batch PCKh 0.875\n",
      "Trained batch 101 batch loss 0.546960294 batch mAP 0.559967041 batch PCKh 0.8125\n",
      "Trained batch 102 batch loss 0.641535461 batch mAP 0.527862549 batch PCKh 0.25\n",
      "Trained batch 103 batch loss 0.631469 batch mAP 0.565429688 batch PCKh 0.0625\n",
      "Trained batch 104 batch loss 0.590846181 batch mAP 0.564331055 batch PCKh 0.625\n",
      "Trained batch 105 batch loss 0.503537416 batch mAP 0.683837891 batch PCKh 0.875\n",
      "Trained batch 106 batch loss 0.606494069 batch mAP 0.569274902 batch PCKh 0.75\n",
      "Trained batch 107 batch loss 0.634566247 batch mAP 0.494140625 batch PCKh 0.125\n",
      "Trained batch 108 batch loss 0.579619169 batch mAP 0.563995361 batch PCKh 0.3125\n",
      "Trained batch 109 batch loss 0.608605742 batch mAP 0.519989 batch PCKh 0.0625\n",
      "Trained batch 110 batch loss 0.600714803 batch mAP 0.545440674 batch PCKh 0.3125\n",
      "Trained batch 111 batch loss 0.559877932 batch mAP 0.596893311 batch PCKh 0.8125\n",
      "Trained batch 112 batch loss 0.58810389 batch mAP 0.579345703 batch PCKh 0.5625\n",
      "Trained batch 113 batch loss 0.596001089 batch mAP 0.581512451 batch PCKh 0.5625\n",
      "Trained batch 114 batch loss 0.604477644 batch mAP 0.56036377 batch PCKh 0.5625\n",
      "Trained batch 115 batch loss 0.553247809 batch mAP 0.567474365 batch PCKh 0.625\n",
      "Trained batch 116 batch loss 0.51739043 batch mAP 0.571868896 batch PCKh 0.625\n",
      "Trained batch 117 batch loss 0.578687608 batch mAP 0.579101562 batch PCKh 0.6875\n",
      "Trained batch 118 batch loss 0.575933 batch mAP 0.578155518 batch PCKh 0.6875\n",
      "Trained batch 119 batch loss 0.571095884 batch mAP 0.522949219 batch PCKh 0.375\n",
      "Trained batch 120 batch loss 0.495847344 batch mAP 0.507415771 batch PCKh 0.125\n",
      "Trained batch 121 batch loss 0.642307162 batch mAP 0.483551025 batch PCKh 0.3125\n",
      "Trained batch 122 batch loss 0.50884068 batch mAP 0.541778564 batch PCKh 0.75\n",
      "Trained batch 123 batch loss 0.640143275 batch mAP 0.486969 batch PCKh 0.6875\n",
      "Trained batch 124 batch loss 0.516670525 batch mAP 0.55960083 batch PCKh 0.6875\n",
      "Trained batch 125 batch loss 0.540433407 batch mAP 0.544921875 batch PCKh 0.1875\n",
      "Trained batch 126 batch loss 0.538238347 batch mAP 0.540802 batch PCKh 0.8125\n",
      "Trained batch 127 batch loss 0.679336369 batch mAP 0.525085449 batch PCKh 0.0625\n",
      "Trained batch 128 batch loss 0.48123163 batch mAP 0.591125488 batch PCKh 0.75\n",
      "Trained batch 129 batch loss 0.502872 batch mAP 0.595367432 batch PCKh 0.3125\n",
      "Trained batch 130 batch loss 0.505269766 batch mAP 0.593688965 batch PCKh 0.0625\n",
      "Trained batch 131 batch loss 0.524388611 batch mAP 0.580505371 batch PCKh 0.375\n",
      "Trained batch 132 batch loss 0.518777728 batch mAP 0.587768555 batch PCKh 0.1875\n",
      "Trained batch 133 batch loss 0.538589239 batch mAP 0.624450684 batch PCKh 0.5\n",
      "Trained batch 134 batch loss 0.57704854 batch mAP 0.580108643 batch PCKh 0.6875\n",
      "Trained batch 135 batch loss 0.546423197 batch mAP 0.591949463 batch PCKh 0.3125\n",
      "Trained batch 136 batch loss 0.586846054 batch mAP 0.563262939 batch PCKh 0.3125\n",
      "Trained batch 137 batch loss 0.520058632 batch mAP 0.566375732 batch PCKh 0.3125\n",
      "Trained batch 138 batch loss 0.550353587 batch mAP 0.516296387 batch PCKh 0.3125\n",
      "Trained batch 139 batch loss 0.574844837 batch mAP 0.528961182 batch PCKh 0.375\n",
      "Trained batch 140 batch loss 0.569860578 batch mAP 0.521728516 batch PCKh 0.375\n",
      "Trained batch 141 batch loss 0.580815911 batch mAP 0.542114258 batch PCKh 0.375\n",
      "Trained batch 142 batch loss 0.531813264 batch mAP 0.578949 batch PCKh 0.3125\n",
      "Trained batch 143 batch loss 0.50752157 batch mAP 0.581573486 batch PCKh 0.8125\n",
      "Trained batch 144 batch loss 0.531259537 batch mAP 0.595153809 batch PCKh 0.1875\n",
      "Trained batch 145 batch loss 0.556472 batch mAP 0.638641357 batch PCKh 0.625\n",
      "Trained batch 146 batch loss 0.550622702 batch mAP 0.615936279 batch PCKh 0.375\n",
      "Trained batch 147 batch loss 0.583547175 batch mAP 0.60559082 batch PCKh 0.5\n",
      "Trained batch 148 batch loss 0.617667258 batch mAP 0.564697266 batch PCKh 0.375\n",
      "Trained batch 149 batch loss 0.565935731 batch mAP 0.612762451 batch PCKh 0.8125\n",
      "Trained batch 150 batch loss 0.567448735 batch mAP 0.605377197 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 151 batch loss 0.633990765 batch mAP 0.538513184 batch PCKh 0.5\n",
      "Trained batch 152 batch loss 0.511994481 batch mAP 0.497528076 batch PCKh 0.6875\n",
      "Trained batch 153 batch loss 0.533708334 batch mAP 0.575531 batch PCKh 0.5\n",
      "Trained batch 154 batch loss 0.492451519 batch mAP 0.546600342 batch PCKh 0.5\n",
      "Trained batch 155 batch loss 0.52392447 batch mAP 0.572235107 batch PCKh 0.5\n",
      "Trained batch 156 batch loss 0.482222974 batch mAP 0.526306152 batch PCKh 0.25\n",
      "Trained batch 157 batch loss 0.491391331 batch mAP 0.609436035 batch PCKh 0.5625\n",
      "Trained batch 158 batch loss 0.564555764 batch mAP 0.610839844 batch PCKh 0.4375\n",
      "Trained batch 159 batch loss 0.593158603 batch mAP 0.600830078 batch PCKh 0.6875\n",
      "Trained batch 160 batch loss 0.564086676 batch mAP 0.589904785 batch PCKh 0.4375\n",
      "Trained batch 161 batch loss 0.624471784 batch mAP 0.510040283 batch PCKh 0.6875\n",
      "Trained batch 162 batch loss 0.585283041 batch mAP 0.527862549 batch PCKh 0.3125\n",
      "Trained batch 163 batch loss 0.541688681 batch mAP 0.547576904 batch PCKh 0.125\n",
      "Trained batch 164 batch loss 0.635565519 batch mAP 0.558807373 batch PCKh 0.625\n",
      "Trained batch 165 batch loss 0.622821033 batch mAP 0.52545166 batch PCKh 0.625\n",
      "Trained batch 166 batch loss 0.60998714 batch mAP 0.533508301 batch PCKh 0.625\n",
      "Trained batch 167 batch loss 0.594433 batch mAP 0.577972412 batch PCKh 0.8125\n",
      "Trained batch 168 batch loss 0.648277164 batch mAP 0.535125732 batch PCKh 0.4375\n",
      "Trained batch 169 batch loss 0.54957974 batch mAP 0.567138672 batch PCKh 0.6875\n",
      "Trained batch 170 batch loss 0.595100403 batch mAP 0.553192139 batch PCKh 0.75\n",
      "Trained batch 171 batch loss 0.537514865 batch mAP 0.567688 batch PCKh 0.1875\n",
      "Trained batch 172 batch loss 0.601823747 batch mAP 0.561462402 batch PCKh 0.1875\n",
      "Trained batch 173 batch loss 0.518728733 batch mAP 0.463348389 batch PCKh 0.625\n",
      "Trained batch 174 batch loss 0.548547506 batch mAP 0.547485352 batch PCKh 0.25\n",
      "Trained batch 175 batch loss 0.54175216 batch mAP 0.551513672 batch PCKh 0.25\n",
      "Trained batch 176 batch loss 0.588448524 batch mAP 0.535827637 batch PCKh 0.25\n",
      "Trained batch 177 batch loss 0.573781431 batch mAP 0.577636719 batch PCKh 0.0625\n",
      "Trained batch 178 batch loss 0.549928188 batch mAP 0.533325195 batch PCKh 0.4375\n",
      "Trained batch 179 batch loss 0.621533275 batch mAP 0.474609375 batch PCKh 0.25\n",
      "Trained batch 180 batch loss 0.579527795 batch mAP 0.482940674 batch PCKh 0.75\n",
      "Trained batch 181 batch loss 0.512023628 batch mAP 0.662109375 batch PCKh 0.6875\n",
      "Trained batch 182 batch loss 0.609096408 batch mAP 0.551300049 batch PCKh 0.8125\n",
      "Trained batch 183 batch loss 0.634282708 batch mAP 0.571258545 batch PCKh 0.75\n",
      "Trained batch 184 batch loss 0.742704868 batch mAP 0.537109375 batch PCKh 0\n",
      "Trained batch 185 batch loss 0.686030567 batch mAP 0.516143799 batch PCKh 0.25\n",
      "Trained batch 186 batch loss 0.691637933 batch mAP 0.478179932 batch PCKh 0.3125\n",
      "Trained batch 187 batch loss 0.543915868 batch mAP 0.467010498 batch PCKh 0.375\n",
      "Trained batch 188 batch loss 0.574526429 batch mAP 0.508483887 batch PCKh 0.75\n",
      "Trained batch 189 batch loss 0.543701291 batch mAP 0.549163818 batch PCKh 0.75\n",
      "Trained batch 190 batch loss 0.542206526 batch mAP 0.579895 batch PCKh 0.5625\n",
      "Trained batch 191 batch loss 0.516051412 batch mAP 0.562927246 batch PCKh 0.5\n",
      "Trained batch 192 batch loss 0.454060525 batch mAP 0.596679688 batch PCKh 0.4375\n",
      "Trained batch 193 batch loss 0.385346472 batch mAP 0.611633301 batch PCKh 0.125\n",
      "Trained batch 194 batch loss 0.416307747 batch mAP 0.551269531 batch PCKh 0.0625\n",
      "Trained batch 195 batch loss 0.510108829 batch mAP 0.589813232 batch PCKh 0.5\n",
      "Trained batch 196 batch loss 0.540129542 batch mAP 0.573638916 batch PCKh 0.6875\n",
      "Trained batch 197 batch loss 0.574119568 batch mAP 0.566803 batch PCKh 0.5625\n",
      "Trained batch 198 batch loss 0.469991237 batch mAP 0.53503418 batch PCKh 0.0625\n",
      "Trained batch 199 batch loss 0.375195742 batch mAP 0.564849854 batch PCKh 0\n",
      "Trained batch 200 batch loss 0.363402724 batch mAP 0.595733643 batch PCKh 0\n",
      "Trained batch 201 batch loss 0.411142796 batch mAP 0.574127197 batch PCKh 0.25\n",
      "Trained batch 202 batch loss 0.395881057 batch mAP 0.622650146 batch PCKh 0\n",
      "Trained batch 203 batch loss 0.439732403 batch mAP 0.548553467 batch PCKh 0.375\n",
      "Trained batch 204 batch loss 0.408487111 batch mAP 0.556304932 batch PCKh 0.5\n",
      "Trained batch 205 batch loss 0.533032894 batch mAP 0.57623291 batch PCKh 0.3125\n",
      "Trained batch 206 batch loss 0.482002318 batch mAP 0.569122314 batch PCKh 0.6875\n",
      "Trained batch 207 batch loss 0.534076 batch mAP 0.530883789 batch PCKh 0.75\n",
      "Trained batch 208 batch loss 0.466376901 batch mAP 0.530975342 batch PCKh 0.8125\n",
      "Trained batch 209 batch loss 0.505537748 batch mAP 0.536376953 batch PCKh 0.4375\n",
      "Trained batch 210 batch loss 0.566367805 batch mAP 0.554138184 batch PCKh 0.25\n",
      "Trained batch 211 batch loss 0.622714162 batch mAP 0.535339355 batch PCKh 0\n",
      "Trained batch 212 batch loss 0.47709012 batch mAP 0.593109131 batch PCKh 0.625\n",
      "Trained batch 213 batch loss 0.500700235 batch mAP 0.575042725 batch PCKh 0.4375\n",
      "Trained batch 214 batch loss 0.569779634 batch mAP 0.566009521 batch PCKh 0.5\n",
      "Trained batch 215 batch loss 0.611341178 batch mAP 0.540466309 batch PCKh 0.75\n",
      "Trained batch 216 batch loss 0.568160176 batch mAP 0.569702148 batch PCKh 0.3125\n",
      "Trained batch 217 batch loss 0.611718178 batch mAP 0.560424805 batch PCKh 0.3125\n",
      "Trained batch 218 batch loss 0.5997262 batch mAP 0.487854 batch PCKh 0.5625\n",
      "Trained batch 219 batch loss 0.588077426 batch mAP 0.539520264 batch PCKh 0.75\n",
      "Trained batch 220 batch loss 0.56342876 batch mAP 0.511749268 batch PCKh 0.4375\n",
      "Trained batch 221 batch loss 0.605098248 batch mAP 0.517456055 batch PCKh 0.5\n",
      "Trained batch 222 batch loss 0.584463775 batch mAP 0.52557373 batch PCKh 0.3125\n",
      "Trained batch 223 batch loss 0.498242617 batch mAP 0.583496094 batch PCKh 0.5625\n",
      "Trained batch 224 batch loss 0.451389313 batch mAP 0.586547852 batch PCKh 0.5\n",
      "Trained batch 225 batch loss 0.504063904 batch mAP 0.559753418 batch PCKh 0.375\n",
      "Trained batch 226 batch loss 0.448050201 batch mAP 0.609466553 batch PCKh 0.5625\n",
      "Trained batch 227 batch loss 0.437811971 batch mAP 0.643920898 batch PCKh 0.5\n",
      "Trained batch 228 batch loss 0.40752244 batch mAP 0.685455322 batch PCKh 0.5625\n",
      "Trained batch 229 batch loss 0.486979187 batch mAP 0.647460938 batch PCKh 0.1875\n",
      "Trained batch 230 batch loss 0.596908331 batch mAP 0.590637207 batch PCKh 0.1875\n",
      "Trained batch 231 batch loss 0.596591234 batch mAP 0.56930542 batch PCKh 0.1875\n",
      "Trained batch 232 batch loss 0.631940186 batch mAP 0.583892822 batch PCKh 0.25\n",
      "Trained batch 233 batch loss 0.608445883 batch mAP 0.53918457 batch PCKh 0.625\n",
      "Trained batch 234 batch loss 0.596056581 batch mAP 0.556121826 batch PCKh 0.625\n",
      "Trained batch 235 batch loss 0.685278416 batch mAP 0.516052246 batch PCKh 0.0625\n",
      "Trained batch 236 batch loss 0.696610153 batch mAP 0.477478027 batch PCKh 0.625\n",
      "Trained batch 237 batch loss 0.655988574 batch mAP 0.430053711 batch PCKh 0\n",
      "Trained batch 238 batch loss 0.633332431 batch mAP 0.49609375 batch PCKh 0.25\n",
      "Trained batch 239 batch loss 0.575211167 batch mAP 0.431274414 batch PCKh 0.6875\n",
      "Trained batch 240 batch loss 0.586131096 batch mAP 0.486785889 batch PCKh 0.25\n",
      "Trained batch 241 batch loss 0.601657927 batch mAP 0.472717285 batch PCKh 0.3125\n",
      "Trained batch 242 batch loss 0.618653417 batch mAP 0.458740234 batch PCKh 0.25\n",
      "Trained batch 243 batch loss 0.525761366 batch mAP 0.569519043 batch PCKh 0.4375\n",
      "Trained batch 244 batch loss 0.544459879 batch mAP 0.582214355 batch PCKh 0.5625\n",
      "Trained batch 245 batch loss 0.478224754 batch mAP 0.547546387 batch PCKh 0.75\n",
      "Trained batch 246 batch loss 0.532437563 batch mAP 0.616912842 batch PCKh 0.25\n",
      "Trained batch 247 batch loss 0.552651107 batch mAP 0.532043457 batch PCKh 0.625\n",
      "Trained batch 248 batch loss 0.528318405 batch mAP 0.544250488 batch PCKh 0\n",
      "Trained batch 249 batch loss 0.599919081 batch mAP 0.563537598 batch PCKh 0.375\n",
      "Trained batch 250 batch loss 0.561983764 batch mAP 0.563873291 batch PCKh 0.25\n",
      "Trained batch 251 batch loss 0.53745991 batch mAP 0.583862305 batch PCKh 0.625\n",
      "Trained batch 252 batch loss 0.592393339 batch mAP 0.557434082 batch PCKh 0.25\n",
      "Trained batch 253 batch loss 0.615026653 batch mAP 0.56930542 batch PCKh 0.5\n",
      "Trained batch 254 batch loss 0.586632371 batch mAP 0.629333496 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 255 batch loss 0.633790195 batch mAP 0.614349365 batch PCKh 0.4375\n",
      "Trained batch 256 batch loss 0.587078273 batch mAP 0.581665039 batch PCKh 0.8125\n",
      "Trained batch 257 batch loss 0.565270245 batch mAP 0.563324 batch PCKh 0.1875\n",
      "Trained batch 258 batch loss 0.563127875 batch mAP 0.597717285 batch PCKh 0.875\n",
      "Trained batch 259 batch loss 0.536482692 batch mAP 0.615142822 batch PCKh 0.4375\n",
      "Trained batch 260 batch loss 0.638016582 batch mAP 0.547363281 batch PCKh 0.75\n",
      "Trained batch 261 batch loss 0.644631624 batch mAP 0.555511475 batch PCKh 0.5625\n",
      "Trained batch 262 batch loss 0.632057548 batch mAP 0.607666 batch PCKh 0.625\n",
      "Trained batch 263 batch loss 0.642359734 batch mAP 0.629974365 batch PCKh 0.625\n",
      "Trained batch 264 batch loss 0.58856988 batch mAP 0.605133057 batch PCKh 0.25\n",
      "Trained batch 265 batch loss 0.573185205 batch mAP 0.660400391 batch PCKh 0.375\n",
      "Trained batch 266 batch loss 0.538447142 batch mAP 0.663513184 batch PCKh 0.375\n",
      "Trained batch 267 batch loss 0.531398177 batch mAP 0.64465332 batch PCKh 0.4375\n",
      "Trained batch 268 batch loss 0.589171469 batch mAP 0.56362915 batch PCKh 0.5625\n",
      "Trained batch 269 batch loss 0.643293738 batch mAP 0.557830811 batch PCKh 0.4375\n",
      "Trained batch 270 batch loss 0.579891205 batch mAP 0.52041626 batch PCKh 0.6875\n",
      "Trained batch 271 batch loss 0.602213621 batch mAP 0.515930176 batch PCKh 0.625\n",
      "Trained batch 272 batch loss 0.557729 batch mAP 0.536529541 batch PCKh 0.6875\n",
      "Trained batch 273 batch loss 0.602074265 batch mAP 0.526550293 batch PCKh 0.6875\n",
      "Trained batch 274 batch loss 0.53173244 batch mAP 0.499481201 batch PCKh 0.5625\n",
      "Trained batch 275 batch loss 0.510841131 batch mAP 0.488983154 batch PCKh 0.4375\n",
      "Trained batch 276 batch loss 0.614226222 batch mAP 0.511444092 batch PCKh 0.75\n",
      "Trained batch 277 batch loss 0.636392474 batch mAP 0.504089355 batch PCKh 0.75\n",
      "Trained batch 278 batch loss 0.568431854 batch mAP 0.484893799 batch PCKh 0.375\n",
      "Trained batch 279 batch loss 0.544582784 batch mAP 0.514892578 batch PCKh 0.3125\n",
      "Trained batch 280 batch loss 0.537392497 batch mAP 0.555725098 batch PCKh 0.75\n",
      "Trained batch 281 batch loss 0.53722918 batch mAP 0.551940918 batch PCKh 0.75\n",
      "Trained batch 282 batch loss 0.565935373 batch mAP 0.599945068 batch PCKh 0.8125\n",
      "Trained batch 283 batch loss 0.54527396 batch mAP 0.549743652 batch PCKh 0.5\n",
      "Trained batch 284 batch loss 0.586316645 batch mAP 0.559021 batch PCKh 0.5625\n",
      "Trained batch 285 batch loss 0.568082213 batch mAP 0.533447266 batch PCKh 0.625\n",
      "Trained batch 286 batch loss 0.597514391 batch mAP 0.540039062 batch PCKh 0.5\n",
      "Trained batch 287 batch loss 0.596913934 batch mAP 0.511993408 batch PCKh 0.25\n",
      "Trained batch 288 batch loss 0.536121607 batch mAP 0.522155762 batch PCKh 0.5625\n",
      "Trained batch 289 batch loss 0.519199729 batch mAP 0.518737793 batch PCKh 0.6875\n",
      "Trained batch 290 batch loss 0.52250737 batch mAP 0.618896484 batch PCKh 0.8125\n",
      "Trained batch 291 batch loss 0.456031203 batch mAP 0.63104248 batch PCKh 0.5625\n",
      "Trained batch 292 batch loss 0.471586496 batch mAP 0.607452393 batch PCKh 0.375\n",
      "Trained batch 293 batch loss 0.449198425 batch mAP 0.644928 batch PCKh 0.8125\n",
      "Trained batch 294 batch loss 0.442052603 batch mAP 0.565246582 batch PCKh 0.6875\n",
      "Trained batch 295 batch loss 0.505400419 batch mAP 0.61605835 batch PCKh 0.5625\n",
      "Trained batch 296 batch loss 0.484002292 batch mAP 0.635406494 batch PCKh 0.625\n",
      "Trained batch 297 batch loss 0.518521 batch mAP 0.617797852 batch PCKh 0.625\n",
      "Trained batch 298 batch loss 0.59410423 batch mAP 0.562042236 batch PCKh 0.4375\n",
      "Trained batch 299 batch loss 0.543933511 batch mAP 0.561309814 batch PCKh 0.5625\n",
      "Trained batch 300 batch loss 0.519144475 batch mAP 0.569061279 batch PCKh 0.5\n",
      "Trained batch 301 batch loss 0.424787164 batch mAP 0.600921631 batch PCKh 0.5\n",
      "Trained batch 302 batch loss 0.436981827 batch mAP 0.582977295 batch PCKh 0.5625\n",
      "Trained batch 303 batch loss 0.442314506 batch mAP 0.601165771 batch PCKh 0.6875\n",
      "Trained batch 304 batch loss 0.537142158 batch mAP 0.548675537 batch PCKh 0.5\n",
      "Trained batch 305 batch loss 0.600918114 batch mAP 0.513366699 batch PCKh 0.1875\n",
      "Trained batch 306 batch loss 0.53644824 batch mAP 0.589752197 batch PCKh 0.1875\n",
      "Trained batch 307 batch loss 0.60921216 batch mAP 0.534820557 batch PCKh 0.75\n",
      "Trained batch 308 batch loss 0.546877801 batch mAP 0.548736572 batch PCKh 0.3125\n",
      "Trained batch 309 batch loss 0.572630644 batch mAP 0.61151123 batch PCKh 0.4375\n",
      "Trained batch 310 batch loss 0.473245263 batch mAP 0.632019043 batch PCKh 0.5625\n",
      "Trained batch 311 batch loss 0.490910947 batch mAP 0.641082764 batch PCKh 0.4375\n",
      "Trained batch 312 batch loss 0.542165101 batch mAP 0.563995361 batch PCKh 0.875\n",
      "Trained batch 313 batch loss 0.493061632 batch mAP 0.635345459 batch PCKh 0.1875\n",
      "Trained batch 314 batch loss 0.444456398 batch mAP 0.62298584 batch PCKh 0.1875\n",
      "Trained batch 315 batch loss 0.516988933 batch mAP 0.648101807 batch PCKh 0.25\n",
      "Trained batch 316 batch loss 0.494743884 batch mAP 0.640869141 batch PCKh 0.3125\n",
      "Trained batch 317 batch loss 0.4765715 batch mAP 0.678100586 batch PCKh 0.5\n",
      "Trained batch 318 batch loss 0.44025898 batch mAP 0.642028809 batch PCKh 0.625\n",
      "Trained batch 319 batch loss 0.484846145 batch mAP 0.654266357 batch PCKh 0.75\n",
      "Trained batch 320 batch loss 0.547588587 batch mAP 0.605743408 batch PCKh 0.375\n",
      "Trained batch 321 batch loss 0.55406183 batch mAP 0.632080078 batch PCKh 0.625\n",
      "Trained batch 322 batch loss 0.51547277 batch mAP 0.592865 batch PCKh 0.375\n",
      "Trained batch 323 batch loss 0.500838816 batch mAP 0.579742432 batch PCKh 0.25\n",
      "Trained batch 324 batch loss 0.527471244 batch mAP 0.577301 batch PCKh 0.75\n",
      "Trained batch 325 batch loss 0.471283078 batch mAP 0.64352417 batch PCKh 0.625\n",
      "Trained batch 326 batch loss 0.59214282 batch mAP 0.565551758 batch PCKh 0.1875\n",
      "Trained batch 327 batch loss 0.501906633 batch mAP 0.596313477 batch PCKh 0.125\n",
      "Trained batch 328 batch loss 0.489147127 batch mAP 0.599884033 batch PCKh 0\n",
      "Trained batch 329 batch loss 0.548960328 batch mAP 0.533447266 batch PCKh 0.6875\n",
      "Trained batch 330 batch loss 0.534679353 batch mAP 0.571136475 batch PCKh 0.1875\n",
      "Trained batch 331 batch loss 0.579904318 batch mAP 0.606445312 batch PCKh 0.3125\n",
      "Trained batch 332 batch loss 0.620676398 batch mAP 0.577362061 batch PCKh 0.125\n",
      "Trained batch 333 batch loss 0.610396683 batch mAP 0.584472656 batch PCKh 0.1875\n",
      "Trained batch 334 batch loss 0.533142328 batch mAP 0.599029541 batch PCKh 0.75\n",
      "Trained batch 335 batch loss 0.642135859 batch mAP 0.531799316 batch PCKh 0.3125\n",
      "Trained batch 336 batch loss 0.583633184 batch mAP 0.580444336 batch PCKh 0.375\n",
      "Trained batch 337 batch loss 0.567130327 batch mAP 0.559631348 batch PCKh 0.1875\n",
      "Trained batch 338 batch loss 0.574796677 batch mAP 0.564453125 batch PCKh 0.4375\n",
      "Trained batch 339 batch loss 0.539120376 batch mAP 0.532135 batch PCKh 0.375\n",
      "Trained batch 340 batch loss 0.562925935 batch mAP 0.495819092 batch PCKh 0.25\n",
      "Trained batch 341 batch loss 0.539641142 batch mAP 0.5340271 batch PCKh 0.625\n",
      "Trained batch 342 batch loss 0.540960073 batch mAP 0.529724121 batch PCKh 0.1875\n",
      "Trained batch 343 batch loss 0.484452248 batch mAP 0.572570801 batch PCKh 0.625\n",
      "Trained batch 344 batch loss 0.599298954 batch mAP 0.577697754 batch PCKh 0.8125\n",
      "Trained batch 345 batch loss 0.528984547 batch mAP 0.574951172 batch PCKh 0.5625\n",
      "Trained batch 346 batch loss 0.515458584 batch mAP 0.584686279 batch PCKh 0.4375\n",
      "Trained batch 347 batch loss 0.516112685 batch mAP 0.586242676 batch PCKh 0.1875\n",
      "Trained batch 348 batch loss 0.511056423 batch mAP 0.592041 batch PCKh 0.75\n",
      "Trained batch 349 batch loss 0.601036906 batch mAP 0.575408936 batch PCKh 0.8125\n",
      "Trained batch 350 batch loss 0.571144462 batch mAP 0.663879395 batch PCKh 0.375\n",
      "Trained batch 351 batch loss 0.470005125 batch mAP 0.641082764 batch PCKh 0.6875\n",
      "Trained batch 352 batch loss 0.50675422 batch mAP 0.663238525 batch PCKh 0.25\n",
      "Trained batch 353 batch loss 0.418185234 batch mAP 0.648162842 batch PCKh 0.5\n",
      "Trained batch 354 batch loss 0.557396591 batch mAP 0.629730225 batch PCKh 0.5625\n",
      "Trained batch 355 batch loss 0.596483231 batch mAP 0.596221924 batch PCKh 0.375\n",
      "Trained batch 356 batch loss 0.580170035 batch mAP 0.529968262 batch PCKh 0.3125\n",
      "Trained batch 357 batch loss 0.585751534 batch mAP 0.554473877 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 358 batch loss 0.613958 batch mAP 0.505188 batch PCKh 0.375\n",
      "Trained batch 359 batch loss 0.591403186 batch mAP 0.514221191 batch PCKh 0.3125\n",
      "Trained batch 360 batch loss 0.640648365 batch mAP 0.519500732 batch PCKh 0.1875\n",
      "Trained batch 361 batch loss 0.537392616 batch mAP 0.558410645 batch PCKh 0.4375\n",
      "Trained batch 362 batch loss 0.538083792 batch mAP 0.585449219 batch PCKh 0.75\n",
      "Trained batch 363 batch loss 0.604488552 batch mAP 0.551391602 batch PCKh 0.875\n",
      "Trained batch 364 batch loss 0.551632464 batch mAP 0.572235107 batch PCKh 0.4375\n",
      "Trained batch 365 batch loss 0.612472355 batch mAP 0.520141602 batch PCKh 0.25\n",
      "Trained batch 366 batch loss 0.616901457 batch mAP 0.549682617 batch PCKh 0.375\n",
      "Trained batch 367 batch loss 0.621574044 batch mAP 0.551544189 batch PCKh 0.375\n",
      "Trained batch 368 batch loss 0.621707439 batch mAP 0.508453369 batch PCKh 0.375\n",
      "Trained batch 369 batch loss 0.581044555 batch mAP 0.527252197 batch PCKh 0.1875\n",
      "Trained batch 370 batch loss 0.509580791 batch mAP 0.541107178 batch PCKh 0.5\n",
      "Trained batch 371 batch loss 0.491309 batch mAP 0.50881958 batch PCKh 0.6875\n",
      "Trained batch 372 batch loss 0.576045871 batch mAP 0.512817383 batch PCKh 0.0625\n",
      "Trained batch 373 batch loss 0.557622373 batch mAP 0.531188965 batch PCKh 0.5625\n",
      "Trained batch 374 batch loss 0.47346586 batch mAP 0.616363525 batch PCKh 0.375\n",
      "Trained batch 375 batch loss 0.534952 batch mAP 0.499542236 batch PCKh 0.5625\n",
      "Trained batch 376 batch loss 0.570100486 batch mAP 0.463348389 batch PCKh 0.375\n",
      "Trained batch 377 batch loss 0.493498743 batch mAP 0.45703125 batch PCKh 0.5625\n",
      "Trained batch 378 batch loss 0.481838971 batch mAP 0.507110596 batch PCKh 0.5\n",
      "Trained batch 379 batch loss 0.451756626 batch mAP 0.543273926 batch PCKh 0.25\n",
      "Trained batch 380 batch loss 0.603334844 batch mAP 0.493774414 batch PCKh 0.3125\n",
      "Trained batch 381 batch loss 0.516985178 batch mAP 0.528808594 batch PCKh 0.25\n",
      "Trained batch 382 batch loss 0.541460872 batch mAP 0.527679443 batch PCKh 0.375\n",
      "Trained batch 383 batch loss 0.439181626 batch mAP 0.625762939 batch PCKh 0.4375\n",
      "Trained batch 384 batch loss 0.454692781 batch mAP 0.603179932 batch PCKh 0.75\n",
      "Trained batch 385 batch loss 0.53311187 batch mAP 0.576171875 batch PCKh 0.5625\n",
      "Trained batch 386 batch loss 0.479649544 batch mAP 0.560852051 batch PCKh 0.8125\n",
      "Trained batch 387 batch loss 0.505025744 batch mAP 0.552032471 batch PCKh 0.625\n",
      "Trained batch 388 batch loss 0.462830245 batch mAP 0.527313232 batch PCKh 0.625\n",
      "Trained batch 389 batch loss 0.493916959 batch mAP 0.54385376 batch PCKh 0.75\n",
      "Trained batch 390 batch loss 0.512993813 batch mAP 0.527984619 batch PCKh 0.1875\n",
      "Trained batch 391 batch loss 0.5751881 batch mAP 0.545013428 batch PCKh 0.5625\n",
      "Trained batch 392 batch loss 0.516340613 batch mAP 0.60836792 batch PCKh 0.375\n",
      "Trained batch 393 batch loss 0.512921035 batch mAP 0.581573486 batch PCKh 0.625\n",
      "Trained batch 394 batch loss 0.568661094 batch mAP 0.5184021 batch PCKh 0.4375\n",
      "Trained batch 395 batch loss 0.531506956 batch mAP 0.596069336 batch PCKh 0.25\n",
      "Trained batch 396 batch loss 0.522542655 batch mAP 0.57220459 batch PCKh 0.4375\n",
      "Trained batch 397 batch loss 0.602034688 batch mAP 0.536132812 batch PCKh 0.25\n",
      "Trained batch 398 batch loss 0.504051089 batch mAP 0.561035156 batch PCKh 0.625\n",
      "Trained batch 399 batch loss 0.505120635 batch mAP 0.529327393 batch PCKh 0.1875\n",
      "Trained batch 400 batch loss 0.562834382 batch mAP 0.600067139 batch PCKh 0.375\n",
      "Trained batch 401 batch loss 0.588134229 batch mAP 0.59072876 batch PCKh 0\n",
      "Trained batch 402 batch loss 0.597854614 batch mAP 0.592712402 batch PCKh 0.1875\n",
      "Trained batch 403 batch loss 0.686460376 batch mAP 0.549163818 batch PCKh 0.0625\n",
      "Trained batch 404 batch loss 0.660644531 batch mAP 0.498535156 batch PCKh 0\n",
      "Trained batch 405 batch loss 0.620639086 batch mAP 0.492675781 batch PCKh 0\n",
      "Trained batch 406 batch loss 0.534672 batch mAP 0.494812 batch PCKh 0.75\n",
      "Trained batch 407 batch loss 0.581978321 batch mAP 0.436706543 batch PCKh 0.8125\n",
      "Trained batch 408 batch loss 0.601713419 batch mAP 0.407104492 batch PCKh 0.375\n",
      "Trained batch 409 batch loss 0.566467881 batch mAP 0.411315918 batch PCKh 0.8125\n",
      "Trained batch 410 batch loss 0.562007129 batch mAP 0.276824951 batch PCKh 0.875\n",
      "Trained batch 411 batch loss 0.591109276 batch mAP 0.354858398 batch PCKh 0.75\n",
      "Trained batch 412 batch loss 0.650922179 batch mAP 0.337127686 batch PCKh 0.75\n",
      "Trained batch 413 batch loss 0.591879368 batch mAP 0.426239 batch PCKh 0.875\n",
      "Trained batch 414 batch loss 0.546275735 batch mAP 0.426300049 batch PCKh 0.5625\n",
      "Trained batch 415 batch loss 0.547933 batch mAP 0.552001953 batch PCKh 0.75\n",
      "Trained batch 416 batch loss 0.493099362 batch mAP 0.520446777 batch PCKh 0.75\n",
      "Trained batch 417 batch loss 0.585606694 batch mAP 0.53793335 batch PCKh 0.5625\n",
      "Trained batch 418 batch loss 0.623608112 batch mAP 0.507598877 batch PCKh 0.8125\n",
      "Trained batch 419 batch loss 0.480859429 batch mAP 0.605651855 batch PCKh 0.3125\n",
      "Trained batch 420 batch loss 0.538562059 batch mAP 0.601989746 batch PCKh 0.5625\n",
      "Trained batch 421 batch loss 0.549300194 batch mAP 0.61517334 batch PCKh 0.25\n",
      "Trained batch 422 batch loss 0.4971264 batch mAP 0.620269775 batch PCKh 0.375\n",
      "Trained batch 423 batch loss 0.57419914 batch mAP 0.613525391 batch PCKh 0.5625\n",
      "Trained batch 424 batch loss 0.578378856 batch mAP 0.546539307 batch PCKh 0.1875\n",
      "Trained batch 425 batch loss 0.554275632 batch mAP 0.632476807 batch PCKh 0.625\n",
      "Trained batch 426 batch loss 0.540489256 batch mAP 0.598510742 batch PCKh 0.1875\n",
      "Trained batch 427 batch loss 0.531564295 batch mAP 0.596038818 batch PCKh 0.6875\n",
      "Trained batch 428 batch loss 0.499814391 batch mAP 0.532165527 batch PCKh 0.4375\n",
      "Trained batch 429 batch loss 0.545885623 batch mAP 0.51574707 batch PCKh 0.5\n",
      "Trained batch 430 batch loss 0.464455098 batch mAP 0.526519775 batch PCKh 0.8125\n",
      "Trained batch 431 batch loss 0.634776294 batch mAP 0.483306885 batch PCKh 0.25\n",
      "Trained batch 432 batch loss 0.522126913 batch mAP 0.54486084 batch PCKh 0.4375\n",
      "Trained batch 433 batch loss 0.431115806 batch mAP 0.564697266 batch PCKh 0.75\n",
      "Trained batch 434 batch loss 0.527905643 batch mAP 0.530944824 batch PCKh 0.5\n",
      "Trained batch 435 batch loss 0.478134364 batch mAP 0.49810791 batch PCKh 0.0625\n",
      "Trained batch 436 batch loss 0.522626042 batch mAP 0.517211914 batch PCKh 0.125\n",
      "Trained batch 437 batch loss 0.62129283 batch mAP 0.552154541 batch PCKh 0.125\n",
      "Trained batch 438 batch loss 0.694089711 batch mAP 0.390686035 batch PCKh 0.0625\n",
      "Trained batch 439 batch loss 0.70622021 batch mAP 0.456420898 batch PCKh 0.25\n",
      "Trained batch 440 batch loss 0.705908656 batch mAP 0.496887207 batch PCKh 0.25\n",
      "Trained batch 441 batch loss 0.559004 batch mAP 0.592865 batch PCKh 0.5\n",
      "Trained batch 442 batch loss 0.496646821 batch mAP 0.570953369 batch PCKh 0\n",
      "Trained batch 443 batch loss 0.534986377 batch mAP 0.506073 batch PCKh 0.125\n",
      "Trained batch 444 batch loss 0.447729677 batch mAP 0.436676025 batch PCKh 0.25\n",
      "Trained batch 445 batch loss 0.43012777 batch mAP 0.373199463 batch PCKh 0.25\n",
      "Trained batch 446 batch loss 0.519575357 batch mAP 0.314453125 batch PCKh 0.625\n",
      "Trained batch 447 batch loss 0.456433 batch mAP 0.349578857 batch PCKh 0.375\n",
      "Trained batch 448 batch loss 0.544624329 batch mAP 0.386474609 batch PCKh 0.1875\n",
      "Trained batch 449 batch loss 0.517288804 batch mAP 0.499328613 batch PCKh 0.0625\n",
      "Trained batch 450 batch loss 0.586259425 batch mAP 0.494110107 batch PCKh 0.1875\n",
      "Trained batch 451 batch loss 0.670486331 batch mAP 0.503448486 batch PCKh 0.8125\n",
      "Trained batch 452 batch loss 0.612105489 batch mAP 0.523132324 batch PCKh 0.0625\n",
      "Trained batch 453 batch loss 0.567771137 batch mAP 0.564025879 batch PCKh 0\n",
      "Trained batch 454 batch loss 0.609059215 batch mAP 0.524230957 batch PCKh 0.3125\n",
      "Trained batch 455 batch loss 0.522643924 batch mAP 0.662475586 batch PCKh 0.3125\n",
      "Trained batch 456 batch loss 0.529357374 batch mAP 0.581054688 batch PCKh 0.1875\n",
      "Trained batch 457 batch loss 0.637433529 batch mAP 0.556518555 batch PCKh 0\n",
      "Trained batch 458 batch loss 0.521081328 batch mAP 0.638214111 batch PCKh 0.6875\n",
      "Trained batch 459 batch loss 0.556676507 batch mAP 0.615478516 batch PCKh 0.375\n",
      "Trained batch 460 batch loss 0.500332594 batch mAP 0.597808838 batch PCKh 0.125\n",
      "Trained batch 461 batch loss 0.56757462 batch mAP 0.485443115 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 462 batch loss 0.515222669 batch mAP 0.556213379 batch PCKh 0.5\n",
      "Trained batch 463 batch loss 0.522074699 batch mAP 0.548614502 batch PCKh 0\n",
      "Trained batch 464 batch loss 0.554868 batch mAP 0.550994873 batch PCKh 0.5\n",
      "Trained batch 465 batch loss 0.658031642 batch mAP 0.501831055 batch PCKh 0.25\n",
      "Trained batch 466 batch loss 0.684291482 batch mAP 0.503173828 batch PCKh 0.3125\n",
      "Trained batch 467 batch loss 0.637205839 batch mAP 0.511383057 batch PCKh 0\n",
      "Trained batch 468 batch loss 0.58799696 batch mAP 0.537780762 batch PCKh 0.875\n",
      "Trained batch 469 batch loss 0.566696346 batch mAP 0.571990967 batch PCKh 0.8125\n",
      "Trained batch 470 batch loss 0.448897421 batch mAP 0.481719971 batch PCKh 0.5625\n",
      "Trained batch 471 batch loss 0.447857499 batch mAP 0.471435547 batch PCKh 0.5625\n",
      "Trained batch 472 batch loss 0.516084194 batch mAP 0.455993652 batch PCKh 0.8125\n",
      "Trained batch 473 batch loss 0.504671097 batch mAP 0.454620361 batch PCKh 0.25\n",
      "Trained batch 474 batch loss 0.476364762 batch mAP 0.485809326 batch PCKh 0.4375\n",
      "Trained batch 475 batch loss 0.430653393 batch mAP 0.531494141 batch PCKh 0.6875\n",
      "Trained batch 476 batch loss 0.548276424 batch mAP 0.567901611 batch PCKh 0.625\n",
      "Trained batch 477 batch loss 0.582803905 batch mAP 0.603973389 batch PCKh 0.25\n",
      "Trained batch 478 batch loss 0.617902 batch mAP 0.564453125 batch PCKh 0.6875\n",
      "Trained batch 479 batch loss 0.605511248 batch mAP 0.598388672 batch PCKh 0.3125\n",
      "Trained batch 480 batch loss 0.623432398 batch mAP 0.593048096 batch PCKh 0.5\n",
      "Trained batch 481 batch loss 0.644296169 batch mAP 0.561431885 batch PCKh 0.6875\n",
      "Trained batch 482 batch loss 0.512471914 batch mAP 0.646881104 batch PCKh 0.5625\n",
      "Trained batch 483 batch loss 0.562766492 batch mAP 0.585388184 batch PCKh 0.5625\n",
      "Trained batch 484 batch loss 0.543616176 batch mAP 0.55078125 batch PCKh 0.4375\n",
      "Trained batch 485 batch loss 0.462233126 batch mAP 0.586303711 batch PCKh 0.3125\n",
      "Trained batch 486 batch loss 0.417666346 batch mAP 0.600891113 batch PCKh 0.25\n",
      "Trained batch 487 batch loss 0.55234468 batch mAP 0.568817139 batch PCKh 0.0625\n",
      "Trained batch 488 batch loss 0.540408373 batch mAP 0.609893799 batch PCKh 0.8125\n",
      "Trained batch 489 batch loss 0.604051352 batch mAP 0.545715332 batch PCKh 0.5625\n",
      "Trained batch 490 batch loss 0.536422193 batch mAP 0.573761 batch PCKh 0.5625\n",
      "Trained batch 491 batch loss 0.607442915 batch mAP 0.512268066 batch PCKh 0.375\n",
      "Trained batch 492 batch loss 0.627565205 batch mAP 0.542358398 batch PCKh 0.1875\n",
      "Trained batch 493 batch loss 0.631771684 batch mAP 0.507293701 batch PCKh 0.5\n",
      "Trained batch 494 batch loss 0.57268095 batch mAP 0.474761963 batch PCKh 0.75\n",
      "Trained batch 495 batch loss 0.575973153 batch mAP 0.511871338 batch PCKh 0.5625\n",
      "Trained batch 496 batch loss 0.551341116 batch mAP 0.529083252 batch PCKh 0.5\n",
      "Trained batch 497 batch loss 0.557516932 batch mAP 0.500885 batch PCKh 0.6875\n",
      "Trained batch 498 batch loss 0.530572593 batch mAP 0.535217285 batch PCKh 0.625\n",
      "Trained batch 499 batch loss 0.500394583 batch mAP 0.548584 batch PCKh 0.375\n",
      "Trained batch 500 batch loss 0.614525199 batch mAP 0.501037598 batch PCKh 0.375\n",
      "Trained batch 501 batch loss 0.590742469 batch mAP 0.518341064 batch PCKh 0.625\n",
      "Trained batch 502 batch loss 0.598341 batch mAP 0.55456543 batch PCKh 0.3125\n",
      "Trained batch 503 batch loss 0.603580415 batch mAP 0.545593262 batch PCKh 0.75\n",
      "Trained batch 504 batch loss 0.572462738 batch mAP 0.564971924 batch PCKh 0.625\n",
      "Trained batch 505 batch loss 0.626971841 batch mAP 0.550170898 batch PCKh 0.5625\n",
      "Trained batch 506 batch loss 0.544738293 batch mAP 0.495056152 batch PCKh 0.5\n",
      "Trained batch 507 batch loss 0.606630445 batch mAP 0.499938965 batch PCKh 0.625\n",
      "Trained batch 508 batch loss 0.564528942 batch mAP 0.510559082 batch PCKh 0.625\n",
      "Trained batch 509 batch loss 0.57346344 batch mAP 0.520355225 batch PCKh 0.5625\n",
      "Trained batch 510 batch loss 0.55877316 batch mAP 0.543823242 batch PCKh 0.1875\n",
      "Trained batch 511 batch loss 0.526890218 batch mAP 0.47869873 batch PCKh 0\n",
      "Trained batch 512 batch loss 0.49834916 batch mAP 0.514282227 batch PCKh 0.5\n",
      "Trained batch 513 batch loss 0.511520505 batch mAP 0.546386719 batch PCKh 0.4375\n",
      "Trained batch 514 batch loss 0.505517602 batch mAP 0.525634766 batch PCKh 0.4375\n",
      "Trained batch 515 batch loss 0.576357722 batch mAP 0.553497314 batch PCKh 0.625\n",
      "Trained batch 516 batch loss 0.640614033 batch mAP 0.50289917 batch PCKh 0.375\n",
      "Trained batch 517 batch loss 0.575167775 batch mAP 0.515960693 batch PCKh 0.75\n",
      "Trained batch 518 batch loss 0.585496843 batch mAP 0.498535156 batch PCKh 0.5625\n",
      "Trained batch 519 batch loss 0.513094902 batch mAP 0.528656 batch PCKh 0.625\n",
      "Trained batch 520 batch loss 0.59164083 batch mAP 0.510040283 batch PCKh 0.625\n",
      "Trained batch 521 batch loss 0.618770242 batch mAP 0.431762695 batch PCKh 0.4375\n",
      "Trained batch 522 batch loss 0.568959773 batch mAP 0.580993652 batch PCKh 0.625\n",
      "Trained batch 523 batch loss 0.603418946 batch mAP 0.560241699 batch PCKh 0.5\n",
      "Trained batch 524 batch loss 0.582649112 batch mAP 0.592895508 batch PCKh 0.25\n",
      "Trained batch 525 batch loss 0.494368613 batch mAP 0.648468 batch PCKh 0.6875\n",
      "Trained batch 526 batch loss 0.572205544 batch mAP 0.628265381 batch PCKh 0.375\n",
      "Trained batch 527 batch loss 0.559563518 batch mAP 0.591918945 batch PCKh 0.625\n",
      "Trained batch 528 batch loss 0.659238636 batch mAP 0.569274902 batch PCKh 0.4375\n",
      "Trained batch 529 batch loss 0.585165262 batch mAP 0.598144531 batch PCKh 0.375\n",
      "Trained batch 530 batch loss 0.521751881 batch mAP 0.628356934 batch PCKh 0.3125\n",
      "Trained batch 556 batch loss 0.586489201 batch mAP 0.552764893 batch PCKh 0.75\n",
      "Trained batch 557 batch loss 0.542268157 batch mAP 0.489196777 batch PCKh 0.125\n",
      "Trained batch 558 batch loss 0.59340477 batch mAP 0.559112549 batch PCKh 0.6875\n",
      "Trained batch 559 batch loss 0.51653 batch mAP 0.582580566 batch PCKh 0.625\n",
      "Trained batch 560 batch loss 0.550834477 batch mAP 0.506378174 batch PCKh 0.5625\n",
      "Trained batch 561 batch loss 0.5890131 batch mAP 0.509674072 batch PCKh 0.3125\n",
      "Trained batch 562 batch loss 0.61344111 batch mAP 0.488647461 batch PCKh 0.625\n",
      "Trained batch 563 batch loss 0.528005421 batch mAP 0.561187744 batch PCKh 0.4375\n",
      "Trained batch 564 batch loss 0.57055819 batch mAP 0.516296387 batch PCKh 0.6875\n",
      "Trained batch 565 batch loss 0.541269779 batch mAP 0.584991455 batch PCKh 0.5625\n",
      "Trained batch 566 batch loss 0.507833719 batch mAP 0.555114746 batch PCKh 0.5\n",
      "Trained batch 567 batch loss 0.550188422 batch mAP 0.552246094 batch PCKh 0.3125\n",
      "Trained batch 568 batch loss 0.505499423 batch mAP 0.590698242 batch PCKh 0.4375\n",
      "Trained batch 569 batch loss 0.499769688 batch mAP 0.635192871 batch PCKh 0.8125\n",
      "Trained batch 570 batch loss 0.54803896 batch mAP 0.56829834 batch PCKh 0.3125\n",
      "Trained batch 571 batch loss 0.561594844 batch mAP 0.59362793 batch PCKh 0.25\n",
      "Trained batch 572 batch loss 0.494873196 batch mAP 0.66506958 batch PCKh 0.3125\n",
      "Trained batch 573 batch loss 0.526708126 batch mAP 0.604309082 batch PCKh 0.4375\n",
      "Trained batch 574 batch loss 0.534102559 batch mAP 0.616973877 batch PCKh 0.75\n",
      "Trained batch 575 batch loss 0.552111864 batch mAP 0.562255859 batch PCKh 0.8125\n",
      "Trained batch 576 batch loss 0.655973911 batch mAP 0.531433105 batch PCKh 0.4375\n",
      "Trained batch 577 batch loss 0.588398814 batch mAP 0.491699219 batch PCKh 0\n",
      "Trained batch 578 batch loss 0.708419681 batch mAP 0.502563477 batch PCKh 0.0625\n",
      "Trained batch 579 batch loss 0.607810855 batch mAP 0.603820801 batch PCKh 0.75\n",
      "Trained batch 580 batch loss 0.544147849 batch mAP 0.625579834 batch PCKh 0.75\n",
      "Trained batch 581 batch loss 0.661742687 batch mAP 0.513580322 batch PCKh 0.5\n",
      "Trained batch 582 batch loss 0.662394404 batch mAP 0.553619385 batch PCKh 0.375\n",
      "Trained batch 583 batch loss 0.596912622 batch mAP 0.569519043 batch PCKh 0.3125\n",
      "Trained batch 584 batch loss 0.586484194 batch mAP 0.607513428 batch PCKh 0.0625\n",
      "Trained batch 585 batch loss 0.574448705 batch mAP 0.577331543 batch PCKh 0.4375\n",
      "Trained batch 586 batch loss 0.557135 batch mAP 0.537231445 batch PCKh 0.5\n",
      "Trained batch 587 batch loss 0.539157689 batch mAP 0.512084961 batch PCKh 0.6875\n",
      "Trained batch 588 batch loss 0.452147305 batch mAP 0.501678467 batch PCKh 0.5\n",
      "Trained batch 589 batch loss 0.448800802 batch mAP 0.528839111 batch PCKh 0.75\n",
      "Trained batch 590 batch loss 0.50004971 batch mAP 0.509124756 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 591 batch loss 0.500757933 batch mAP 0.526763916 batch PCKh 0.75\n",
      "Trained batch 592 batch loss 0.535855889 batch mAP 0.528106689 batch PCKh 0.625\n",
      "Trained batch 593 batch loss 0.604998708 batch mAP 0.477233887 batch PCKh 0.375\n",
      "Trained batch 594 batch loss 0.587920904 batch mAP 0.46585083 batch PCKh 0.75\n",
      "Trained batch 595 batch loss 0.625691891 batch mAP 0.464355469 batch PCKh 0.8125\n",
      "Trained batch 596 batch loss 0.66174221 batch mAP 0.558441162 batch PCKh 0.75\n",
      "Trained batch 597 batch loss 0.585719347 batch mAP 0.521118164 batch PCKh 0.375\n",
      "Trained batch 598 batch loss 0.55317992 batch mAP 0.583709717 batch PCKh 0.625\n",
      "Trained batch 599 batch loss 0.557593524 batch mAP 0.592956543 batch PCKh 0.5625\n",
      "Trained batch 600 batch loss 0.516530633 batch mAP 0.523010254 batch PCKh 0.3125\n",
      "Trained batch 601 batch loss 0.567973673 batch mAP 0.558288574 batch PCKh 0.5\n",
      "Trained batch 602 batch loss 0.519442141 batch mAP 0.511047363 batch PCKh 0.1875\n",
      "Trained batch 603 batch loss 0.606348395 batch mAP 0.635498047 batch PCKh 0.25\n",
      "Trained batch 604 batch loss 0.519571781 batch mAP 0.639434814 batch PCKh 0.875\n",
      "Trained batch 605 batch loss 0.577015102 batch mAP 0.574401855 batch PCKh 0.5\n",
      "Trained batch 606 batch loss 0.448011696 batch mAP 0.56829834 batch PCKh 0\n",
      "Trained batch 607 batch loss 0.435785949 batch mAP 0.600830078 batch PCKh 0.3125\n",
      "Trained batch 608 batch loss 0.55750227 batch mAP 0.548126221 batch PCKh 0.6875\n",
      "Trained batch 609 batch loss 0.612820327 batch mAP 0.501434326 batch PCKh 0.5\n",
      "Trained batch 610 batch loss 0.470234364 batch mAP 0.480621338 batch PCKh 0.3125\n",
      "Trained batch 611 batch loss 0.56295532 batch mAP 0.515258789 batch PCKh 0.125\n",
      "Trained batch 612 batch loss 0.573800743 batch mAP 0.504821777 batch PCKh 0.0625\n",
      "Trained batch 613 batch loss 0.571594715 batch mAP 0.531402588 batch PCKh 0\n",
      "Trained batch 614 batch loss 0.641281 batch mAP 0.507965088 batch PCKh 0.0625\n",
      "Trained batch 615 batch loss 0.590263963 batch mAP 0.497711182 batch PCKh 0.5\n",
      "Trained batch 616 batch loss 0.626594841 batch mAP 0.58215332 batch PCKh 0.5625\n",
      "Trained batch 617 batch loss 0.56880188 batch mAP 0.550628662 batch PCKh 0.375\n",
      "Trained batch 618 batch loss 0.518470287 batch mAP 0.491271973 batch PCKh 0.1875\n",
      "Trained batch 619 batch loss 0.524406195 batch mAP 0.50982666 batch PCKh 0.75\n",
      "Trained batch 620 batch loss 0.586780429 batch mAP 0.496307373 batch PCKh 0.75\n",
      "Trained batch 621 batch loss 0.485722721 batch mAP 0.517608643 batch PCKh 0.625\n",
      "Trained batch 622 batch loss 0.555539489 batch mAP 0.531860352 batch PCKh 0.75\n",
      "Trained batch 623 batch loss 0.46268639 batch mAP 0.506469727 batch PCKh 0.375\n",
      "Trained batch 624 batch loss 0.460888624 batch mAP 0.524841309 batch PCKh 0.5625\n",
      "Trained batch 625 batch loss 0.509661674 batch mAP 0.527374268 batch PCKh 0.75\n",
      "Trained batch 626 batch loss 0.568135679 batch mAP 0.527252197 batch PCKh 0.75\n",
      "Trained batch 627 batch loss 0.55836308 batch mAP 0.538726807 batch PCKh 0.875\n",
      "Trained batch 628 batch loss 0.610623121 batch mAP 0.538543701 batch PCKh 0.375\n",
      "Trained batch 629 batch loss 0.564702213 batch mAP 0.555450439 batch PCKh 0.1875\n",
      "Trained batch 630 batch loss 0.518054962 batch mAP 0.633239746 batch PCKh 0.5625\n",
      "Trained batch 631 batch loss 0.494768739 batch mAP 0.682098389 batch PCKh 0.25\n",
      "Trained batch 632 batch loss 0.530605555 batch mAP 0.567504883 batch PCKh 0.4375\n",
      "Trained batch 633 batch loss 0.487201154 batch mAP 0.60672 batch PCKh 0.875\n",
      "Trained batch 634 batch loss 0.481648773 batch mAP 0.465026855 batch PCKh 0.1875\n",
      "Trained batch 635 batch loss 0.499243915 batch mAP 0.553039551 batch PCKh 0.6875\n",
      "Trained batch 636 batch loss 0.53253746 batch mAP 0.56539917 batch PCKh 0.5625\n",
      "Trained batch 637 batch loss 0.636024356 batch mAP 0.507843 batch PCKh 0.625\n",
      "Trained batch 638 batch loss 0.724174619 batch mAP 0.489776611 batch PCKh 0.25\n",
      "Trained batch 639 batch loss 0.608888149 batch mAP 0.490234375 batch PCKh 0.5\n",
      "Trained batch 640 batch loss 0.484283656 batch mAP 0.58416748 batch PCKh 0.5\n",
      "Trained batch 641 batch loss 0.60648042 batch mAP 0.490875244 batch PCKh 0.8125\n",
      "Trained batch 642 batch loss 0.539973259 batch mAP 0.527984619 batch PCKh 0.625\n",
      "Trained batch 643 batch loss 0.585113347 batch mAP 0.480102539 batch PCKh 0.75\n",
      "Trained batch 644 batch loss 0.423391342 batch mAP 0.576782227 batch PCKh 0.5\n",
      "Trained batch 645 batch loss 0.402854651 batch mAP 0.687896729 batch PCKh 0.5625\n",
      "Trained batch 646 batch loss 0.387348145 batch mAP 0.674346924 batch PCKh 0.375\n",
      "Trained batch 647 batch loss 0.337752819 batch mAP 0.738708496 batch PCKh 0.6875\n",
      "Trained batch 648 batch loss 0.395336807 batch mAP 0.703704834 batch PCKh 0.6875\n",
      "Trained batch 649 batch loss 0.382767051 batch mAP 0.694763184 batch PCKh 0.375\n",
      "Trained batch 650 batch loss 0.45250091 batch mAP 0.673492432 batch PCKh 0.5625\n",
      "Trained batch 651 batch loss 0.592347682 batch mAP 0.597381592 batch PCKh 0.1875\n",
      "Trained batch 652 batch loss 0.551593423 batch mAP 0.563293457 batch PCKh 0.3125\n",
      "Trained batch 653 batch loss 0.581954062 batch mAP 0.623046875 batch PCKh 0\n",
      "Trained batch 654 batch loss 0.575576127 batch mAP 0.605743408 batch PCKh 0.25\n",
      "Trained batch 655 batch loss 0.585955441 batch mAP 0.660675049 batch PCKh 0.3125\n",
      "Trained batch 656 batch loss 0.543920875 batch mAP 0.627624512 batch PCKh 0.0625\n",
      "Trained batch 657 batch loss 0.605955243 batch mAP 0.511535645 batch PCKh 0.6875\n",
      "Trained batch 658 batch loss 0.553663731 batch mAP 0.65246582 batch PCKh 0.4375\n",
      "Trained batch 659 batch loss 0.54691875 batch mAP 0.549133301 batch PCKh 0.3125\n",
      "Trained batch 660 batch loss 0.554655135 batch mAP 0.627960205 batch PCKh 0.5\n",
      "Trained batch 661 batch loss 0.616595268 batch mAP 0.657714844 batch PCKh 0.875\n",
      "Trained batch 662 batch loss 0.586034954 batch mAP 0.607299805 batch PCKh 0.625\n",
      "Trained batch 663 batch loss 0.647088885 batch mAP 0.509521484 batch PCKh 0.4375\n",
      "Trained batch 664 batch loss 0.648089051 batch mAP 0.479705811 batch PCKh 0.5\n",
      "Trained batch 665 batch loss 0.650002182 batch mAP 0.453704834 batch PCKh 0.125\n",
      "Trained batch 666 batch loss 0.635412335 batch mAP 0.441619873 batch PCKh 0.8125\n",
      "Trained batch 667 batch loss 0.573358893 batch mAP 0.542816162 batch PCKh 0.4375\n",
      "Trained batch 668 batch loss 0.490883946 batch mAP 0.525177 batch PCKh 0.1875\n",
      "Trained batch 669 batch loss 0.587093771 batch mAP 0.525177 batch PCKh 0.1875\n",
      "Trained batch 670 batch loss 0.545065701 batch mAP 0.372619629 batch PCKh 0\n",
      "Trained batch 671 batch loss 0.570508778 batch mAP 0.423339844 batch PCKh 0.4375\n",
      "Trained batch 672 batch loss 0.671374321 batch mAP 0.433349609 batch PCKh 0.5625\n",
      "Trained batch 673 batch loss 0.566046953 batch mAP 0.418731689 batch PCKh 0.4375\n",
      "Trained batch 674 batch loss 0.537532 batch mAP 0.429016113 batch PCKh 0.6875\n",
      "Trained batch 675 batch loss 0.600539207 batch mAP 0.442077637 batch PCKh 0.4375\n",
      "Trained batch 676 batch loss 0.604541481 batch mAP 0.423553467 batch PCKh 0.625\n",
      "Trained batch 677 batch loss 0.530936301 batch mAP 0.457214355 batch PCKh 0.4375\n",
      "Trained batch 678 batch loss 0.525792241 batch mAP 0.516296387 batch PCKh 0.625\n",
      "Trained batch 679 batch loss 0.536126 batch mAP 0.541595459 batch PCKh 0.5625\n",
      "Trained batch 680 batch loss 0.58819592 batch mAP 0.567260742 batch PCKh 0.625\n",
      "Trained batch 681 batch loss 0.629977345 batch mAP 0.512817383 batch PCKh 0.75\n",
      "Trained batch 682 batch loss 0.645580947 batch mAP 0.499328613 batch PCKh 0.6875\n",
      "Trained batch 683 batch loss 0.620766878 batch mAP 0.528442383 batch PCKh 0.1875\n",
      "Trained batch 684 batch loss 0.663983226 batch mAP 0.474578857 batch PCKh 0.375\n",
      "Trained batch 685 batch loss 0.59902215 batch mAP 0.513275146 batch PCKh 0.625\n",
      "Trained batch 686 batch loss 0.645552 batch mAP 0.53515625 batch PCKh 0.6875\n",
      "Trained batch 687 batch loss 0.611282468 batch mAP 0.497039795 batch PCKh 0.75\n",
      "Trained batch 688 batch loss 0.555038333 batch mAP 0.483093262 batch PCKh 0.3125\n",
      "Trained batch 689 batch loss 0.659940362 batch mAP 0.541534424 batch PCKh 0.3125\n",
      "Trained batch 690 batch loss 0.644796133 batch mAP 0.457672119 batch PCKh 0\n",
      "Trained batch 691 batch loss 0.594220698 batch mAP 0.467163086 batch PCKh 0.5\n",
      "Trained batch 692 batch loss 0.637884796 batch mAP 0.413269043 batch PCKh 0.0625\n",
      "Trained batch 693 batch loss 0.584882259 batch mAP 0.375732422 batch PCKh 0.6875\n",
      "Trained batch 694 batch loss 0.560976803 batch mAP 0.348907471 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 695 batch loss 0.593473852 batch mAP 0.409301758 batch PCKh 0.5625\n",
      "Trained batch 696 batch loss 0.638150334 batch mAP 0.393096924 batch PCKh 0.375\n",
      "Trained batch 697 batch loss 0.608768225 batch mAP 0.459320068 batch PCKh 0\n",
      "Trained batch 698 batch loss 0.536809862 batch mAP 0.522979736 batch PCKh 0.1875\n",
      "Trained batch 699 batch loss 0.588563204 batch mAP 0.532989502 batch PCKh 0.5625\n",
      "Trained batch 700 batch loss 0.603961766 batch mAP 0.520263672 batch PCKh 0.5\n",
      "Trained batch 701 batch loss 0.577041626 batch mAP 0.554016113 batch PCKh 0.4375\n",
      "Trained batch 702 batch loss 0.555325389 batch mAP 0.585113525 batch PCKh 0.75\n",
      "Trained batch 703 batch loss 0.589833617 batch mAP 0.518676758 batch PCKh 0.3125\n",
      "Trained batch 704 batch loss 0.581606746 batch mAP 0.530426 batch PCKh 0.1875\n",
      "Trained batch 705 batch loss 0.543389142 batch mAP 0.475830078 batch PCKh 0.1875\n",
      "Trained batch 706 batch loss 0.596182168 batch mAP 0.469055176 batch PCKh 0.25\n",
      "Trained batch 707 batch loss 0.607119143 batch mAP 0.416137695 batch PCKh 0\n",
      "Trained batch 708 batch loss 0.60580647 batch mAP 0.512420654 batch PCKh 0.6875\n",
      "Trained batch 709 batch loss 0.550592244 batch mAP 0.601501465 batch PCKh 0.875\n",
      "Trained batch 710 batch loss 0.566332757 batch mAP 0.577301 batch PCKh 0.625\n",
      "Trained batch 711 batch loss 0.603771389 batch mAP 0.572753906 batch PCKh 0.625\n",
      "Trained batch 712 batch loss 0.624242842 batch mAP 0.577392578 batch PCKh 0.25\n",
      "Trained batch 713 batch loss 0.613357365 batch mAP 0.555938721 batch PCKh 0.5\n",
      "Trained batch 714 batch loss 0.608392477 batch mAP 0.601379395 batch PCKh 0.625\n",
      "Trained batch 715 batch loss 0.685447276 batch mAP 0.612670898 batch PCKh 0.625\n",
      "Trained batch 716 batch loss 0.596326768 batch mAP 0.612365723 batch PCKh 0.4375\n",
      "Trained batch 717 batch loss 0.62167 batch mAP 0.533599854 batch PCKh 0.625\n",
      "Trained batch 718 batch loss 0.58160758 batch mAP 0.493621826 batch PCKh 0.5625\n",
      "Trained batch 719 batch loss 0.572501063 batch mAP 0.472717285 batch PCKh 0.6875\n",
      "Trained batch 720 batch loss 0.583259523 batch mAP 0.500457764 batch PCKh 0.75\n",
      "Trained batch 721 batch loss 0.517938614 batch mAP 0.525726318 batch PCKh 0.5625\n",
      "Trained batch 722 batch loss 0.520429 batch mAP 0.575561523 batch PCKh 0.5\n",
      "Trained batch 723 batch loss 0.555910826 batch mAP 0.547424316 batch PCKh 0.75\n",
      "Trained batch 724 batch loss 0.581575871 batch mAP 0.521698 batch PCKh 0.625\n",
      "Trained batch 725 batch loss 0.570529342 batch mAP 0.595947266 batch PCKh 0.3125\n",
      "Trained batch 726 batch loss 0.550082445 batch mAP 0.518798828 batch PCKh 0.1875\n",
      "Trained batch 727 batch loss 0.615426958 batch mAP 0.554840088 batch PCKh 0.8125\n",
      "Trained batch 728 batch loss 0.550525904 batch mAP 0.610717773 batch PCKh 0.75\n",
      "Trained batch 729 batch loss 0.545797348 batch mAP 0.605773926 batch PCKh 0.6875\n",
      "Trained batch 730 batch loss 0.601066947 batch mAP 0.573974609 batch PCKh 0.5\n",
      "Trained batch 731 batch loss 0.544804931 batch mAP 0.547332764 batch PCKh 0.0625\n",
      "Trained batch 732 batch loss 0.563843668 batch mAP 0.647644043 batch PCKh 0.875\n",
      "Trained batch 733 batch loss 0.598362803 batch mAP 0.579223633 batch PCKh 0.4375\n",
      "Trained batch 734 batch loss 0.570582092 batch mAP 0.619751 batch PCKh 0\n",
      "Trained batch 735 batch loss 0.542519391 batch mAP 0.634674072 batch PCKh 0.375\n",
      "Trained batch 736 batch loss 0.574621916 batch mAP 0.657653809 batch PCKh 0.1875\n",
      "Trained batch 737 batch loss 0.569925368 batch mAP 0.629821777 batch PCKh 0.625\n",
      "Trained batch 738 batch loss 0.529974818 batch mAP 0.663726807 batch PCKh 0.25\n",
      "Trained batch 739 batch loss 0.465647459 batch mAP 0.676574707 batch PCKh 0.3125\n",
      "Trained batch 740 batch loss 0.47126621 batch mAP 0.667785645 batch PCKh 0.25\n",
      "Trained batch 741 batch loss 0.512607 batch mAP 0.634216309 batch PCKh 0.25\n",
      "Trained batch 742 batch loss 0.532799602 batch mAP 0.633880615 batch PCKh 0.5625\n",
      "Trained batch 743 batch loss 0.497187138 batch mAP 0.632324219 batch PCKh 0.75\n",
      "Trained batch 744 batch loss 0.503581405 batch mAP 0.601959229 batch PCKh 0.625\n",
      "Trained batch 745 batch loss 0.507365942 batch mAP 0.58706665 batch PCKh 0.5\n",
      "Trained batch 746 batch loss 0.564684272 batch mAP 0.601379395 batch PCKh 0.4375\n",
      "Trained batch 747 batch loss 0.635640919 batch mAP 0.541229248 batch PCKh 0.875\n",
      "Trained batch 748 batch loss 0.538348794 batch mAP 0.629180908 batch PCKh 0.1875\n",
      "Trained batch 749 batch loss 0.518236578 batch mAP 0.57800293 batch PCKh 0.1875\n",
      "Trained batch 750 batch loss 0.622689843 batch mAP 0.559997559 batch PCKh 0.5\n",
      "Trained batch 751 batch loss 0.609484255 batch mAP 0.540771484 batch PCKh 0.4375\n",
      "Trained batch 752 batch loss 0.65833658 batch mAP 0.529846191 batch PCKh 0.875\n",
      "Trained batch 753 batch loss 0.576818049 batch mAP 0.563903809 batch PCKh 0.625\n",
      "Trained batch 754 batch loss 0.675426662 batch mAP 0.532012939 batch PCKh 0.75\n",
      "Trained batch 755 batch loss 0.512996137 batch mAP 0.53302 batch PCKh 0.0625\n",
      "Trained batch 756 batch loss 0.488089621 batch mAP 0.615905762 batch PCKh 0.4375\n",
      "Trained batch 757 batch loss 0.613461256 batch mAP 0.582977295 batch PCKh 0.4375\n",
      "Trained batch 758 batch loss 0.606936038 batch mAP 0.579223633 batch PCKh 0.375\n",
      "Trained batch 759 batch loss 0.54546 batch mAP 0.543731689 batch PCKh 0.375\n",
      "Trained batch 760 batch loss 0.555520654 batch mAP 0.572387695 batch PCKh 0.4375\n",
      "Trained batch 761 batch loss 0.553736031 batch mAP 0.563415527 batch PCKh 0.5625\n",
      "Trained batch 762 batch loss 0.583002388 batch mAP 0.570770264 batch PCKh 0.375\n",
      "Trained batch 763 batch loss 0.580404222 batch mAP 0.569030762 batch PCKh 0.4375\n",
      "Trained batch 764 batch loss 0.618048787 batch mAP 0.553955078 batch PCKh 0.5625\n",
      "Trained batch 765 batch loss 0.572424889 batch mAP 0.594207764 batch PCKh 0.3125\n",
      "Trained batch 766 batch loss 0.619696498 batch mAP 0.521606445 batch PCKh 0.375\n",
      "Trained batch 767 batch loss 0.595992625 batch mAP 0.558563232 batch PCKh 0.1875\n",
      "Trained batch 768 batch loss 0.616065562 batch mAP 0.541290283 batch PCKh 0.4375\n",
      "Trained batch 769 batch loss 0.520293117 batch mAP 0.565612793 batch PCKh 0.375\n",
      "Trained batch 770 batch loss 0.560025334 batch mAP 0.458007812 batch PCKh 0.625\n",
      "Trained batch 771 batch loss 0.56190443 batch mAP 0.488830566 batch PCKh 0.5625\n",
      "Trained batch 772 batch loss 0.503309965 batch mAP 0.520477295 batch PCKh 0.3125\n",
      "Trained batch 773 batch loss 0.64234519 batch mAP 0.515136719 batch PCKh 0.1875\n",
      "Trained batch 774 batch loss 0.616162539 batch mAP 0.472412109 batch PCKh 0.3125\n",
      "Trained batch 775 batch loss 0.676258147 batch mAP 0.48147583 batch PCKh 0.75\n",
      "Trained batch 776 batch loss 0.655143261 batch mAP 0.451873779 batch PCKh 0.5\n",
      "Trained batch 777 batch loss 0.591383874 batch mAP 0.55456543 batch PCKh 0.375\n",
      "Trained batch 778 batch loss 0.469395876 batch mAP 0.61541748 batch PCKh 0.5\n",
      "Trained batch 779 batch loss 0.518157184 batch mAP 0.612915039 batch PCKh 0.6875\n",
      "Trained batch 780 batch loss 0.561465085 batch mAP 0.579925537 batch PCKh 0.75\n",
      "Trained batch 781 batch loss 0.50303781 batch mAP 0.58996582 batch PCKh 0.5\n",
      "Trained batch 782 batch loss 0.414491653 batch mAP 0.553375244 batch PCKh 0.25\n",
      "Trained batch 783 batch loss 0.432347834 batch mAP 0.542480469 batch PCKh 0.1875\n",
      "Trained batch 784 batch loss 0.429084 batch mAP 0.575653076 batch PCKh 0.5\n",
      "Trained batch 785 batch loss 0.503448844 batch mAP 0.583862305 batch PCKh 0.3125\n",
      "Trained batch 786 batch loss 0.50332284 batch mAP 0.619873047 batch PCKh 0.5625\n",
      "Trained batch 787 batch loss 0.58284837 batch mAP 0.630828857 batch PCKh 0.5\n",
      "Trained batch 788 batch loss 0.467092931 batch mAP 0.602111816 batch PCKh 0.1875\n",
      "Trained batch 789 batch loss 0.517291844 batch mAP 0.634735107 batch PCKh 0.75\n",
      "Trained batch 790 batch loss 0.520665288 batch mAP 0.642852783 batch PCKh 0.4375\n",
      "Trained batch 791 batch loss 0.600348949 batch mAP 0.640197754 batch PCKh 0.9375\n",
      "Trained batch 792 batch loss 0.504825652 batch mAP 0.665130615 batch PCKh 0.6875\n",
      "Trained batch 793 batch loss 0.429753721 batch mAP 0.721984863 batch PCKh 0.4375\n",
      "Trained batch 794 batch loss 0.462805837 batch mAP 0.684570312 batch PCKh 0.5625\n",
      "Trained batch 795 batch loss 0.46343559 batch mAP 0.619384766 batch PCKh 0.375\n",
      "Trained batch 796 batch loss 0.477255851 batch mAP 0.602478 batch PCKh 0.4375\n",
      "Trained batch 797 batch loss 0.54821372 batch mAP 0.502563477 batch PCKh 0.4375\n",
      "Trained batch 798 batch loss 0.604117572 batch mAP 0.4815979 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 799 batch loss 0.5195539 batch mAP 0.569671631 batch PCKh 0.375\n",
      "Trained batch 800 batch loss 0.510939181 batch mAP 0.626647949 batch PCKh 0.5\n",
      "Trained batch 801 batch loss 0.633128166 batch mAP 0.524108887 batch PCKh 0\n",
      "Trained batch 802 batch loss 0.618245423 batch mAP 0.46496582 batch PCKh 0.3125\n",
      "Trained batch 803 batch loss 0.497506201 batch mAP 0.49597168 batch PCKh 0.625\n",
      "Trained batch 804 batch loss 0.456926644 batch mAP 0.477935791 batch PCKh 0.3125\n",
      "Trained batch 805 batch loss 0.46354124 batch mAP 0.507965088 batch PCKh 0.0625\n",
      "Trained batch 806 batch loss 0.521818 batch mAP 0.462036133 batch PCKh 0.75\n",
      "Trained batch 807 batch loss 0.470912486 batch mAP 0.526123047 batch PCKh 0.375\n",
      "Trained batch 808 batch loss 0.493463516 batch mAP 0.526611328 batch PCKh 0.5\n",
      "Trained batch 809 batch loss 0.538008 batch mAP 0.498962402 batch PCKh 0.375\n",
      "Trained batch 810 batch loss 0.53355962 batch mAP 0.559753418 batch PCKh 0.4375\n",
      "Trained batch 811 batch loss 0.585932 batch mAP 0.52545166 batch PCKh 0.25\n",
      "Trained batch 812 batch loss 0.569417119 batch mAP 0.551025391 batch PCKh 0.75\n",
      "Trained batch 813 batch loss 0.549744248 batch mAP 0.561096191 batch PCKh 0.4375\n",
      "Trained batch 814 batch loss 0.718248188 batch mAP 0.508544922 batch PCKh 0.5625\n",
      "Trained batch 815 batch loss 0.676893353 batch mAP 0.512756348 batch PCKh 0.1875\n",
      "Trained batch 816 batch loss 0.644964397 batch mAP 0.541687 batch PCKh 0.375\n",
      "Trained batch 817 batch loss 0.666768849 batch mAP 0.47756958 batch PCKh 0.3125\n",
      "Trained batch 818 batch loss 0.612287641 batch mAP 0.439453125 batch PCKh 0.625\n",
      "Trained batch 819 batch loss 0.53032 batch mAP 0.479736328 batch PCKh 0.625\n",
      "Trained batch 820 batch loss 0.487317383 batch mAP 0.459442139 batch PCKh 0.4375\n",
      "Trained batch 821 batch loss 0.563672543 batch mAP 0.422576904 batch PCKh 0.75\n",
      "Trained batch 822 batch loss 0.503532946 batch mAP 0.366455078 batch PCKh 0.5625\n",
      "Trained batch 823 batch loss 0.551624954 batch mAP 0.325439453 batch PCKh 0.375\n",
      "Trained batch 824 batch loss 0.559885263 batch mAP 0.382904053 batch PCKh 0\n",
      "Trained batch 825 batch loss 0.521930099 batch mAP 0.391326904 batch PCKh 0.875\n",
      "Trained batch 826 batch loss 0.501972914 batch mAP 0.435638428 batch PCKh 0.5\n",
      "Trained batch 827 batch loss 0.431964546 batch mAP 0.440460205 batch PCKh 0.375\n",
      "Trained batch 828 batch loss 0.581338167 batch mAP 0.395324707 batch PCKh 0.5625\n",
      "Trained batch 829 batch loss 0.588241816 batch mAP 0.519256592 batch PCKh 0.75\n",
      "Trained batch 830 batch loss 0.583635867 batch mAP 0.573028564 batch PCKh 0.375\n",
      "Trained batch 831 batch loss 0.516268909 batch mAP 0.593719482 batch PCKh 0.1875\n",
      "Trained batch 832 batch loss 0.498366386 batch mAP 0.504211426 batch PCKh 0.6875\n",
      "Trained batch 833 batch loss 0.576477528 batch mAP 0.53994751 batch PCKh 0.4375\n",
      "Trained batch 834 batch loss 0.593851268 batch mAP 0.539794922 batch PCKh 0.5\n",
      "Trained batch 835 batch loss 0.635937333 batch mAP 0.588439941 batch PCKh 0.375\n",
      "Trained batch 836 batch loss 0.57196486 batch mAP 0.551269531 batch PCKh 0.375\n",
      "Trained batch 837 batch loss 0.587246 batch mAP 0.500671387 batch PCKh 0.375\n",
      "Trained batch 838 batch loss 0.549482882 batch mAP 0.555389404 batch PCKh 0.25\n",
      "Trained batch 839 batch loss 0.6141361 batch mAP 0.442108154 batch PCKh 0.375\n",
      "Trained batch 840 batch loss 0.568871319 batch mAP 0.466583252 batch PCKh 0.625\n",
      "Trained batch 841 batch loss 0.642653346 batch mAP 0.442321777 batch PCKh 0.1875\n",
      "Trained batch 842 batch loss 0.670559049 batch mAP 0.417785645 batch PCKh 0.125\n",
      "Trained batch 843 batch loss 0.576473236 batch mAP 0.457855225 batch PCKh 0.125\n",
      "Trained batch 844 batch loss 0.549951732 batch mAP 0.460205078 batch PCKh 0.375\n",
      "Trained batch 845 batch loss 0.565944076 batch mAP 0.424499512 batch PCKh 0.625\n",
      "Trained batch 846 batch loss 0.654309452 batch mAP 0.451141357 batch PCKh 0.6875\n",
      "Trained batch 847 batch loss 0.618418217 batch mAP 0.464324951 batch PCKh 0.625\n",
      "Trained batch 848 batch loss 0.512077093 batch mAP 0.471038818 batch PCKh 0\n",
      "Trained batch 849 batch loss 0.536183417 batch mAP 0.511413574 batch PCKh 0.75\n",
      "Trained batch 850 batch loss 0.510685623 batch mAP 0.549072266 batch PCKh 0.5625\n",
      "Trained batch 851 batch loss 0.541275859 batch mAP 0.554931641 batch PCKh 0.5\n",
      "Trained batch 852 batch loss 0.485619783 batch mAP 0.524597168 batch PCKh 0.625\n",
      "Trained batch 853 batch loss 0.535887659 batch mAP 0.534454346 batch PCKh 0.125\n",
      "Trained batch 854 batch loss 0.570723653 batch mAP 0.58203125 batch PCKh 0.0625\n",
      "Trained batch 855 batch loss 0.641087353 batch mAP 0.550354 batch PCKh 0.5625\n",
      "Trained batch 856 batch loss 0.526938915 batch mAP 0.564086914 batch PCKh 0.1875\n",
      "Trained batch 857 batch loss 0.68242991 batch mAP 0.51474 batch PCKh 0.4375\n",
      "Trained batch 858 batch loss 0.705135465 batch mAP 0.547973633 batch PCKh 0.0625\n",
      "Trained batch 859 batch loss 0.601079226 batch mAP 0.606231689 batch PCKh 0.1875\n",
      "Trained batch 860 batch loss 0.597761095 batch mAP 0.624908447 batch PCKh 0.6875\n",
      "Trained batch 861 batch loss 0.627739906 batch mAP 0.591339111 batch PCKh 0.6875\n",
      "Trained batch 862 batch loss 0.599639654 batch mAP 0.51663208 batch PCKh 0.75\n",
      "Trained batch 863 batch loss 0.517299235 batch mAP 0.515777588 batch PCKh 0.25\n",
      "Trained batch 864 batch loss 0.442970097 batch mAP 0.545074463 batch PCKh 0.1875\n",
      "Trained batch 865 batch loss 0.570473373 batch mAP 0.560516357 batch PCKh 0.5\n",
      "Trained batch 866 batch loss 0.536252797 batch mAP 0.579376221 batch PCKh 0.625\n",
      "Trained batch 867 batch loss 0.544292212 batch mAP 0.579834 batch PCKh 0.6875\n",
      "Trained batch 868 batch loss 0.62204361 batch mAP 0.58694458 batch PCKh 0.3125\n",
      "Trained batch 869 batch loss 0.6311481 batch mAP 0.550415039 batch PCKh 0.0625\n",
      "Trained batch 870 batch loss 0.615016043 batch mAP 0.53793335 batch PCKh 0.375\n",
      "Trained batch 871 batch loss 0.61813271 batch mAP 0.555999756 batch PCKh 0.4375\n",
      "Trained batch 872 batch loss 0.611728907 batch mAP 0.531555176 batch PCKh 0.375\n",
      "Trained batch 873 batch loss 0.510069132 batch mAP 0.559814453 batch PCKh 0.3125\n",
      "Trained batch 874 batch loss 0.502095 batch mAP 0.533111572 batch PCKh 0.125\n",
      "Trained batch 875 batch loss 0.451624513 batch mAP 0.660217285 batch PCKh 0.3125\n",
      "Trained batch 876 batch loss 0.498529524 batch mAP 0.631103516 batch PCKh 0.375\n",
      "Trained batch 877 batch loss 0.645172834 batch mAP 0.577880859 batch PCKh 0.875\n",
      "Trained batch 878 batch loss 0.49557358 batch mAP 0.610992432 batch PCKh 0.0625\n",
      "Trained batch 879 batch loss 0.501709282 batch mAP 0.660888672 batch PCKh 0.4375\n",
      "Trained batch 880 batch loss 0.535517097 batch mAP 0.632232666 batch PCKh 0.8125\n",
      "Trained batch 881 batch loss 0.509547591 batch mAP 0.614563 batch PCKh 0.1875\n",
      "Trained batch 882 batch loss 0.482676566 batch mAP 0.551727295 batch PCKh 0.0625\n",
      "Trained batch 883 batch loss 0.453500897 batch mAP 0.673553467 batch PCKh 0.5625\n",
      "Trained batch 884 batch loss 0.554361582 batch mAP 0.565582275 batch PCKh 0.5625\n",
      "Trained batch 885 batch loss 0.583872616 batch mAP 0.512268066 batch PCKh 0.375\n",
      "Trained batch 886 batch loss 0.611010253 batch mAP 0.531188965 batch PCKh 0.125\n",
      "Trained batch 887 batch loss 0.586399794 batch mAP 0.523742676 batch PCKh 0.25\n",
      "Trained batch 888 batch loss 0.457482755 batch mAP 0.597961426 batch PCKh 0\n",
      "Trained batch 889 batch loss 0.532505572 batch mAP 0.639984131 batch PCKh 0.5625\n",
      "Trained batch 890 batch loss 0.621751308 batch mAP 0.564361572 batch PCKh 0.1875\n",
      "Trained batch 891 batch loss 0.595270038 batch mAP 0.52911377 batch PCKh 0.375\n",
      "Trained batch 892 batch loss 0.582120836 batch mAP 0.544128418 batch PCKh 0.4375\n",
      "Trained batch 893 batch loss 0.612843394 batch mAP 0.521118164 batch PCKh 0.375\n",
      "Trained batch 894 batch loss 0.603921294 batch mAP 0.541503906 batch PCKh 0.25\n",
      "Trained batch 895 batch loss 0.681178212 batch mAP 0.543396 batch PCKh 0.125\n",
      "Trained batch 896 batch loss 0.795220196 batch mAP 0.486999512 batch PCKh 0\n",
      "Trained batch 897 batch loss 0.613212585 batch mAP 0.490783691 batch PCKh 0.0625\n",
      "Trained batch 898 batch loss 0.704525471 batch mAP 0.418945312 batch PCKh 0.4375\n",
      "Trained batch 899 batch loss 0.670356512 batch mAP 0.41015625 batch PCKh 0.1875\n",
      "Trained batch 900 batch loss 0.595315099 batch mAP 0.397125244 batch PCKh 0.375\n",
      "Trained batch 901 batch loss 0.602903306 batch mAP 0.337493896 batch PCKh 0.125\n",
      "Trained batch 902 batch loss 0.517345369 batch mAP 0.373413086 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 903 batch loss 0.400933087 batch mAP 0.295806885 batch PCKh 0\n",
      "Trained batch 904 batch loss 0.597593069 batch mAP 0.386444092 batch PCKh 0.125\n",
      "Trained batch 905 batch loss 0.570494652 batch mAP 0.434692383 batch PCKh 0.1875\n",
      "Trained batch 906 batch loss 0.65372026 batch mAP 0.393341064 batch PCKh 0.375\n",
      "Trained batch 907 batch loss 0.658531964 batch mAP 0.358398438 batch PCKh 0.125\n",
      "Trained batch 908 batch loss 0.630736828 batch mAP 0.386230469 batch PCKh 0.125\n",
      "Trained batch 909 batch loss 0.670460761 batch mAP 0.417144775 batch PCKh 0\n",
      "Trained batch 910 batch loss 0.591119587 batch mAP 0.430847168 batch PCKh 0.8125\n",
      "Trained batch 911 batch loss 0.596882105 batch mAP 0.483734131 batch PCKh 0.75\n",
      "Trained batch 912 batch loss 0.586468399 batch mAP 0.516204834 batch PCKh 0.8125\n",
      "Trained batch 913 batch loss 0.520121157 batch mAP 0.590759277 batch PCKh 0.8125\n",
      "Trained batch 914 batch loss 0.586535 batch mAP 0.569732666 batch PCKh 0.625\n",
      "Trained batch 915 batch loss 0.592550397 batch mAP 0.580383301 batch PCKh 0.625\n",
      "Trained batch 916 batch loss 0.602725446 batch mAP 0.553833 batch PCKh 0.625\n",
      "Trained batch 917 batch loss 0.537818432 batch mAP 0.566497803 batch PCKh 0.75\n",
      "Trained batch 918 batch loss 0.569685936 batch mAP 0.545074463 batch PCKh 0.625\n",
      "Trained batch 919 batch loss 0.558186829 batch mAP 0.672271729 batch PCKh 0.3125\n",
      "Trained batch 920 batch loss 0.508619785 batch mAP 0.633850098 batch PCKh 0.4375\n",
      "Trained batch 921 batch loss 0.495207578 batch mAP 0.629852295 batch PCKh 0.5625\n",
      "Trained batch 922 batch loss 0.466082931 batch mAP 0.643035889 batch PCKh 0.75\n",
      "Trained batch 923 batch loss 0.472228885 batch mAP 0.625610352 batch PCKh 0.75\n",
      "Trained batch 924 batch loss 0.52546978 batch mAP 0.592773438 batch PCKh 0.625\n",
      "Trained batch 925 batch loss 0.534518838 batch mAP 0.613128662 batch PCKh 0.5625\n",
      "Trained batch 926 batch loss 0.461888015 batch mAP 0.618560791 batch PCKh 0.4375\n",
      "Trained batch 927 batch loss 0.521821737 batch mAP 0.644958496 batch PCKh 0.5625\n",
      "Trained batch 928 batch loss 0.461687744 batch mAP 0.660369873 batch PCKh 0.5\n",
      "Trained batch 929 batch loss 0.484914064 batch mAP 0.631744385 batch PCKh 0.0625\n",
      "Trained batch 930 batch loss 0.54790175 batch mAP 0.6456604 batch PCKh 0.5625\n",
      "Trained batch 931 batch loss 0.508801579 batch mAP 0.576568604 batch PCKh 0.6875\n",
      "Trained batch 932 batch loss 0.431089818 batch mAP 0.660766602 batch PCKh 0.625\n",
      "Trained batch 933 batch loss 0.554095149 batch mAP 0.615081787 batch PCKh 0.25\n",
      "Trained batch 934 batch loss 0.510586 batch mAP 0.646484375 batch PCKh 0.3125\n",
      "Trained batch 935 batch loss 0.465730309 batch mAP 0.645477295 batch PCKh 0.5\n",
      "Trained batch 936 batch loss 0.516868 batch mAP 0.596313477 batch PCKh 0.625\n",
      "Trained batch 937 batch loss 0.585537 batch mAP 0.629821777 batch PCKh 0.5625\n",
      "Trained batch 938 batch loss 0.572807312 batch mAP 0.592376709 batch PCKh 0.0625\n",
      "Trained batch 939 batch loss 0.535951316 batch mAP 0.499847412 batch PCKh 0\n",
      "Trained batch 940 batch loss 0.524235606 batch mAP 0.579437256 batch PCKh 0.625\n",
      "Trained batch 941 batch loss 0.58107233 batch mAP 0.566223145 batch PCKh 0.75\n",
      "Trained batch 942 batch loss 0.52212888 batch mAP 0.506652832 batch PCKh 0\n",
      "Trained batch 943 batch loss 0.554115891 batch mAP 0.537719727 batch PCKh 0.125\n",
      "Trained batch 944 batch loss 0.544301033 batch mAP 0.473144531 batch PCKh 0.0625\n",
      "Trained batch 945 batch loss 0.535325825 batch mAP 0.538085938 batch PCKh 0.125\n",
      "Trained batch 946 batch loss 0.545162678 batch mAP 0.456787109 batch PCKh 0.5\n",
      "Trained batch 947 batch loss 0.491721451 batch mAP 0.548980713 batch PCKh 0.1875\n",
      "Trained batch 948 batch loss 0.545874 batch mAP 0.540344238 batch PCKh 0.4375\n",
      "Trained batch 949 batch loss 0.507692456 batch mAP 0.611084 batch PCKh 0.25\n",
      "Trained batch 950 batch loss 0.519671202 batch mAP 0.584289551 batch PCKh 0.6875\n",
      "Trained batch 951 batch loss 0.586962 batch mAP 0.536499 batch PCKh 0.125\n",
      "Trained batch 952 batch loss 0.60130626 batch mAP 0.542327881 batch PCKh 0.0625\n",
      "Trained batch 953 batch loss 0.520142555 batch mAP 0.555633545 batch PCKh 0.625\n",
      "Trained batch 954 batch loss 0.514849305 batch mAP 0.573577881 batch PCKh 0.8125\n",
      "Trained batch 955 batch loss 0.60840416 batch mAP 0.4715271 batch PCKh 0.875\n",
      "Trained batch 956 batch loss 0.616596818 batch mAP 0.48324585 batch PCKh 0.5625\n",
      "Trained batch 957 batch loss 0.516957521 batch mAP 0.537628174 batch PCKh 0.6875\n",
      "Trained batch 958 batch loss 0.585151196 batch mAP 0.519897461 batch PCKh 0.6875\n",
      "Trained batch 959 batch loss 0.503706694 batch mAP 0.568054199 batch PCKh 0.875\n",
      "Trained batch 960 batch loss 0.66448307 batch mAP 0.504425049 batch PCKh 0.5625\n",
      "Trained batch 961 batch loss 0.55798918 batch mAP 0.5703125 batch PCKh 0.5625\n",
      "Trained batch 962 batch loss 0.562907577 batch mAP 0.568023682 batch PCKh 0.375\n",
      "Trained batch 963 batch loss 0.517607927 batch mAP 0.565856934 batch PCKh 0.375\n",
      "Trained batch 964 batch loss 0.528280854 batch mAP 0.631500244 batch PCKh 0.375\n",
      "Trained batch 965 batch loss 0.680321693 batch mAP 0.551696777 batch PCKh 0.125\n",
      "Trained batch 966 batch loss 0.64847368 batch mAP 0.574707031 batch PCKh 0.1875\n",
      "Trained batch 967 batch loss 0.545705676 batch mAP 0.615386963 batch PCKh 0.625\n",
      "Trained batch 968 batch loss 0.503633201 batch mAP 0.586425781 batch PCKh 0.875\n",
      "Trained batch 969 batch loss 0.5079391 batch mAP 0.574462891 batch PCKh 0.875\n",
      "Trained batch 970 batch loss 0.469181538 batch mAP 0.571105957 batch PCKh 0.5625\n",
      "Trained batch 971 batch loss 0.536412 batch mAP 0.591033936 batch PCKh 0.8125\n",
      "Trained batch 972 batch loss 0.510373592 batch mAP 0.611999512 batch PCKh 0.5\n",
      "Trained batch 973 batch loss 0.578148127 batch mAP 0.574829102 batch PCKh 0.3125\n",
      "Trained batch 974 batch loss 0.664374232 batch mAP 0.518951416 batch PCKh 0.4375\n",
      "Trained batch 975 batch loss 0.624312282 batch mAP 0.513092041 batch PCKh 0.4375\n",
      "Trained batch 976 batch loss 0.630512 batch mAP 0.555175781 batch PCKh 0.0625\n",
      "Trained batch 977 batch loss 0.614585221 batch mAP 0.574768066 batch PCKh 0.375\n",
      "Trained batch 978 batch loss 0.548422217 batch mAP 0.607147217 batch PCKh 0.25\n",
      "Trained batch 979 batch loss 0.503554642 batch mAP 0.642669678 batch PCKh 0.3125\n",
      "Trained batch 980 batch loss 0.56614 batch mAP 0.597625732 batch PCKh 0.375\n",
      "Trained batch 981 batch loss 0.539509237 batch mAP 0.585784912 batch PCKh 0.5\n",
      "Trained batch 982 batch loss 0.56950289 batch mAP 0.579803467 batch PCKh 0.5\n",
      "Trained batch 983 batch loss 0.575864911 batch mAP 0.609191895 batch PCKh 0.25\n",
      "Trained batch 984 batch loss 0.610846281 batch mAP 0.607635498 batch PCKh 0.25\n",
      "Trained batch 985 batch loss 0.54932642 batch mAP 0.563537598 batch PCKh 0.5625\n",
      "Trained batch 986 batch loss 0.55205214 batch mAP 0.50680542 batch PCKh 0.25\n",
      "Trained batch 987 batch loss 0.616986394 batch mAP 0.482666016 batch PCKh 0.75\n",
      "Trained batch 988 batch loss 0.628656745 batch mAP 0.446777344 batch PCKh 0.875\n",
      "Trained batch 989 batch loss 0.636369646 batch mAP 0.430725098 batch PCKh 0.625\n",
      "Trained batch 990 batch loss 0.605765402 batch mAP 0.405517578 batch PCKh 0.25\n",
      "Trained batch 991 batch loss 0.623353 batch mAP 0.453643799 batch PCKh 0\n",
      "Trained batch 992 batch loss 0.573651195 batch mAP 0.516235352 batch PCKh 0.8125\n",
      "Trained batch 993 batch loss 0.535408556 batch mAP 0.550567627 batch PCKh 0.0625\n",
      "Trained batch 994 batch loss 0.515032768 batch mAP 0.557159424 batch PCKh 0.1875\n",
      "Trained batch 995 batch loss 0.458979726 batch mAP 0.477508545 batch PCKh 0.1875\n",
      "Trained batch 996 batch loss 0.505470395 batch mAP 0.484130859 batch PCKh 0.1875\n",
      "Trained batch 997 batch loss 0.476377785 batch mAP 0.520324707 batch PCKh 0.1875\n",
      "Trained batch 998 batch loss 0.527451336 batch mAP 0.554016113 batch PCKh 0.5\n",
      "Trained batch 999 batch loss 0.498116672 batch mAP 0.640838623 batch PCKh 0.6875\n",
      "Trained batch 1000 batch loss 0.475299329 batch mAP 0.603607178 batch PCKh 0.5\n",
      "Trained batch 1001 batch loss 0.544062197 batch mAP 0.600982666 batch PCKh 0.625\n",
      "Trained batch 1002 batch loss 0.478465319 batch mAP 0.618408203 batch PCKh 0.375\n",
      "Trained batch 1003 batch loss 0.608403087 batch mAP 0.646820068 batch PCKh 0.3125\n",
      "Trained batch 1004 batch loss 0.583980858 batch mAP 0.580169678 batch PCKh 0.25\n",
      "Trained batch 1005 batch loss 0.589975595 batch mAP 0.53805542 batch PCKh 0.4375\n",
      "Trained batch 1006 batch loss 0.486671031 batch mAP 0.612548828 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1007 batch loss 0.43895036 batch mAP 0.628692627 batch PCKh 0.375\n",
      "Trained batch 1008 batch loss 0.557871342 batch mAP 0.578460693 batch PCKh 0.375\n",
      "Trained batch 1009 batch loss 0.534214199 batch mAP 0.605072 batch PCKh 0.25\n",
      "Trained batch 1010 batch loss 0.562973 batch mAP 0.573394775 batch PCKh 0.4375\n",
      "Trained batch 1011 batch loss 0.512127042 batch mAP 0.595581055 batch PCKh 0.5\n",
      "Trained batch 1012 batch loss 0.487824231 batch mAP 0.540710449 batch PCKh 0.25\n",
      "Trained batch 1013 batch loss 0.608744502 batch mAP 0.502594 batch PCKh 0.8125\n",
      "Trained batch 1014 batch loss 0.521128058 batch mAP 0.569976807 batch PCKh 0.8125\n",
      "Trained batch 1015 batch loss 0.563099742 batch mAP 0.552124 batch PCKh 0.625\n",
      "Trained batch 1016 batch loss 0.45685941 batch mAP 0.583740234 batch PCKh 0.5625\n",
      "Trained batch 1017 batch loss 0.560768962 batch mAP 0.55670166 batch PCKh 0.5625\n",
      "Trained batch 1018 batch loss 0.533478677 batch mAP 0.590271 batch PCKh 0.375\n",
      "Trained batch 1019 batch loss 0.57632792 batch mAP 0.611938477 batch PCKh 0.3125\n",
      "Trained batch 1020 batch loss 0.537081659 batch mAP 0.607910156 batch PCKh 0.1875\n",
      "Trained batch 1021 batch loss 0.525522709 batch mAP 0.60546875 batch PCKh 0\n",
      "Trained batch 1022 batch loss 0.475899518 batch mAP 0.602325439 batch PCKh 0.3125\n",
      "Trained batch 1023 batch loss 0.609286666 batch mAP 0.452850342 batch PCKh 0.1875\n",
      "Trained batch 1024 batch loss 0.563628912 batch mAP 0.506469727 batch PCKh 0.5\n",
      "Trained batch 1025 batch loss 0.566045344 batch mAP 0.588409424 batch PCKh 0.4375\n",
      "Trained batch 1026 batch loss 0.504210472 batch mAP 0.598175049 batch PCKh 0.625\n",
      "Trained batch 1027 batch loss 0.511577129 batch mAP 0.583404541 batch PCKh 0.5\n",
      "Trained batch 1028 batch loss 0.519697845 batch mAP 0.570251465 batch PCKh 0.625\n",
      "Trained batch 1029 batch loss 0.560236633 batch mAP 0.612640381 batch PCKh 0.375\n",
      "Trained batch 1030 batch loss 0.692049086 batch mAP 0.49420166 batch PCKh 0.0625\n",
      "Trained batch 1031 batch loss 0.593886 batch mAP 0.49331665 batch PCKh 0.5\n",
      "Trained batch 1032 batch loss 0.545059681 batch mAP 0.480072021 batch PCKh 0.75\n",
      "Trained batch 1033 batch loss 0.48012203 batch mAP 0.528808594 batch PCKh 0.125\n",
      "Trained batch 1034 batch loss 0.520075679 batch mAP 0.481719971 batch PCKh 0.5625\n",
      "Trained batch 1035 batch loss 0.587113202 batch mAP 0.472808838 batch PCKh 0.1875\n",
      "Trained batch 1036 batch loss 0.592698336 batch mAP 0.512054443 batch PCKh 0.375\n",
      "Trained batch 1037 batch loss 0.478705615 batch mAP 0.49710083 batch PCKh 0.3125\n",
      "Trained batch 1038 batch loss 0.491255939 batch mAP 0.556640625 batch PCKh 0\n",
      "Trained batch 1039 batch loss 0.515297413 batch mAP 0.55770874 batch PCKh 0.875\n",
      "Trained batch 1040 batch loss 0.643800318 batch mAP 0.553405762 batch PCKh 0.4375\n",
      "Trained batch 1041 batch loss 0.553145349 batch mAP 0.553985596 batch PCKh 0.5\n",
      "Trained batch 1042 batch loss 0.481227458 batch mAP 0.609802246 batch PCKh 0.25\n",
      "Trained batch 1043 batch loss 0.380815595 batch mAP 0.591674805 batch PCKh 0\n",
      "Trained batch 1044 batch loss 0.434833795 batch mAP 0.519348145 batch PCKh 0\n",
      "Trained batch 1045 batch loss 0.447213113 batch mAP 0.563324 batch PCKh 0.25\n",
      "Trained batch 1046 batch loss 0.460076958 batch mAP 0.5597229 batch PCKh 0\n",
      "Trained batch 1047 batch loss 0.564383447 batch mAP 0.559997559 batch PCKh 0.1875\n",
      "Trained batch 1048 batch loss 0.629764318 batch mAP 0.522674561 batch PCKh 0.75\n",
      "Trained batch 1049 batch loss 0.666769385 batch mAP 0.438171387 batch PCKh 0.75\n",
      "Trained batch 1050 batch loss 0.638759136 batch mAP 0.391693115 batch PCKh 0.1875\n",
      "Trained batch 1051 batch loss 0.71200043 batch mAP 0.37802124 batch PCKh 0.3125\n",
      "Trained batch 1052 batch loss 0.770196795 batch mAP 0.383728027 batch PCKh 0.125\n",
      "Trained batch 1053 batch loss 0.718153536 batch mAP 0.411682129 batch PCKh 0\n",
      "Trained batch 1054 batch loss 0.461859465 batch mAP 0.567443848 batch PCKh 0.1875\n",
      "Trained batch 1055 batch loss 0.589008451 batch mAP 0.391235352 batch PCKh 0.3125\n",
      "Trained batch 1056 batch loss 0.502329826 batch mAP 0.469207764 batch PCKh 0.625\n",
      "Trained batch 1057 batch loss 0.50643 batch mAP 0.465179443 batch PCKh 0.5\n",
      "Trained batch 1058 batch loss 0.56570375 batch mAP 0.45803833 batch PCKh 0.4375\n",
      "Trained batch 1059 batch loss 0.583697915 batch mAP 0.515625 batch PCKh 0.3125\n",
      "Trained batch 1060 batch loss 0.58504647 batch mAP 0.536254883 batch PCKh 0.5625\n",
      "Trained batch 1061 batch loss 0.653587699 batch mAP 0.561981201 batch PCKh 0.25\n",
      "Trained batch 1062 batch loss 0.540557146 batch mAP 0.580596924 batch PCKh 0.3125\n",
      "Trained batch 1063 batch loss 0.570285082 batch mAP 0.625274658 batch PCKh 0.5\n",
      "Trained batch 1064 batch loss 0.595068812 batch mAP 0.542205811 batch PCKh 0.375\n",
      "Trained batch 1065 batch loss 0.627733231 batch mAP 0.588806152 batch PCKh 0.3125\n",
      "Trained batch 1066 batch loss 0.518817246 batch mAP 0.588348389 batch PCKh 0.6875\n",
      "Trained batch 1067 batch loss 0.57210803 batch mAP 0.572937 batch PCKh 0.8125\n",
      "Trained batch 1068 batch loss 0.574006438 batch mAP 0.547485352 batch PCKh 0.625\n",
      "Trained batch 1069 batch loss 0.599021912 batch mAP 0.472473145 batch PCKh 0.4375\n",
      "Trained batch 1070 batch loss 0.596525311 batch mAP 0.523895264 batch PCKh 0.8125\n",
      "Trained batch 1071 batch loss 0.549656868 batch mAP 0.567108154 batch PCKh 0.5\n",
      "Trained batch 1072 batch loss 0.548107743 batch mAP 0.555236816 batch PCKh 0.25\n",
      "Trained batch 1073 batch loss 0.499250174 batch mAP 0.578674316 batch PCKh 0.4375\n",
      "Trained batch 1074 batch loss 0.559316635 batch mAP 0.581115723 batch PCKh 0.5625\n",
      "Trained batch 1075 batch loss 0.473783076 batch mAP 0.57421875 batch PCKh 0.4375\n",
      "Trained batch 1076 batch loss 0.533602834 batch mAP 0.538574219 batch PCKh 0.5\n",
      "Trained batch 1077 batch loss 0.493769288 batch mAP 0.625244141 batch PCKh 0.375\n",
      "Trained batch 1078 batch loss 0.516423106 batch mAP 0.674560547 batch PCKh 0.3125\n",
      "Trained batch 1079 batch loss 0.572527707 batch mAP 0.625091553 batch PCKh 0.5625\n",
      "Trained batch 1080 batch loss 0.640158892 batch mAP 0.54498291 batch PCKh 0.375\n",
      "Trained batch 1081 batch loss 0.552497864 batch mAP 0.584503174 batch PCKh 0.6875\n",
      "Trained batch 1082 batch loss 0.535000682 batch mAP 0.564025879 batch PCKh 0.625\n",
      "Trained batch 1083 batch loss 0.429639846 batch mAP 0.539733887 batch PCKh 0.1875\n",
      "Trained batch 1084 batch loss 0.446793795 batch mAP 0.494476318 batch PCKh 0.125\n",
      "Trained batch 1085 batch loss 0.561852574 batch mAP 0.512451172 batch PCKh 0.5625\n",
      "Trained batch 1086 batch loss 0.695310414 batch mAP 0.50668335 batch PCKh 0.625\n",
      "Trained batch 1087 batch loss 0.610382318 batch mAP 0.563324 batch PCKh 0.625\n",
      "Trained batch 1088 batch loss 0.56541729 batch mAP 0.567596436 batch PCKh 0.1875\n",
      "Trained batch 1089 batch loss 0.543914914 batch mAP 0.570343 batch PCKh 0.5625\n",
      "Trained batch 1090 batch loss 0.646407366 batch mAP 0.554748535 batch PCKh 0.6875\n",
      "Trained batch 1091 batch loss 0.547758758 batch mAP 0.61541748 batch PCKh 0.75\n",
      "Trained batch 1092 batch loss 0.530730605 batch mAP 0.562744141 batch PCKh 0.5625\n",
      "Trained batch 1093 batch loss 0.49083221 batch mAP 0.570159912 batch PCKh 0.1875\n",
      "Trained batch 1094 batch loss 0.522785842 batch mAP 0.52355957 batch PCKh 0.75\n",
      "Trained batch 1095 batch loss 0.560615778 batch mAP 0.494415283 batch PCKh 0.625\n",
      "Trained batch 1096 batch loss 0.511075616 batch mAP 0.501495361 batch PCKh 0.5\n",
      "Trained batch 1097 batch loss 0.571899414 batch mAP 0.482635498 batch PCKh 0.25\n",
      "Trained batch 1098 batch loss 0.504843414 batch mAP 0.527709961 batch PCKh 0.625\n",
      "Trained batch 1099 batch loss 0.539481759 batch mAP 0.574249268 batch PCKh 0.625\n",
      "Trained batch 1100 batch loss 0.600329876 batch mAP 0.527923584 batch PCKh 0.3125\n",
      "Trained batch 1101 batch loss 0.564043045 batch mAP 0.571350098 batch PCKh 0.6875\n",
      "Trained batch 1102 batch loss 0.526123047 batch mAP 0.607513428 batch PCKh 0.3125\n",
      "Trained batch 1103 batch loss 0.582821667 batch mAP 0.611541748 batch PCKh 0.5\n",
      "Trained batch 1104 batch loss 0.55697608 batch mAP 0.577758789 batch PCKh 0.4375\n",
      "Trained batch 1105 batch loss 0.542799592 batch mAP 0.614685059 batch PCKh 0.6875\n",
      "Trained batch 1106 batch loss 0.612226963 batch mAP 0.603759766 batch PCKh 0.0625\n",
      "Trained batch 1107 batch loss 0.493576497 batch mAP 0.618042 batch PCKh 0.5625\n",
      "Trained batch 1108 batch loss 0.517446399 batch mAP 0.573974609 batch PCKh 0.3125\n",
      "Trained batch 1109 batch loss 0.587408543 batch mAP 0.564575195 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1110 batch loss 0.567100227 batch mAP 0.618377686 batch PCKh 0.625\n",
      "Trained batch 1111 batch loss 0.57596606 batch mAP 0.641387939 batch PCKh 0.375\n",
      "Trained batch 1112 batch loss 0.516472161 batch mAP 0.579925537 batch PCKh 0.5\n",
      "Trained batch 1113 batch loss 0.588633776 batch mAP 0.558807373 batch PCKh 0.75\n",
      "Trained batch 1114 batch loss 0.521020889 batch mAP 0.523468 batch PCKh 0.6875\n",
      "Trained batch 1115 batch loss 0.507231295 batch mAP 0.537139893 batch PCKh 0.75\n",
      "Trained batch 1116 batch loss 0.537835717 batch mAP 0.495788574 batch PCKh 0.6875\n",
      "Trained batch 1117 batch loss 0.603025615 batch mAP 0.51297 batch PCKh 0.6875\n",
      "Trained batch 1118 batch loss 0.585181355 batch mAP 0.579681396 batch PCKh 0.4375\n",
      "Trained batch 1119 batch loss 0.536791086 batch mAP 0.606994629 batch PCKh 0.5625\n",
      "Trained batch 1120 batch loss 0.536498487 batch mAP 0.60824585 batch PCKh 0.25\n",
      "Trained batch 1121 batch loss 0.550153077 batch mAP 0.647888184 batch PCKh 0.3125\n",
      "Trained batch 1122 batch loss 0.593664527 batch mAP 0.640136719 batch PCKh 0.375\n",
      "Trained batch 1123 batch loss 0.46207419 batch mAP 0.682251 batch PCKh 0.6875\n",
      "Trained batch 1124 batch loss 0.563619554 batch mAP 0.617828369 batch PCKh 0.3125\n",
      "Trained batch 1125 batch loss 0.544430494 batch mAP 0.637146 batch PCKh 0.3125\n",
      "Trained batch 1126 batch loss 0.463436842 batch mAP 0.684936523 batch PCKh 0.25\n",
      "Trained batch 1127 batch loss 0.428308904 batch mAP 0.663604736 batch PCKh 0.3125\n",
      "Trained batch 1128 batch loss 0.403254479 batch mAP 0.71282959 batch PCKh 0.3125\n",
      "Trained batch 1129 batch loss 0.522721231 batch mAP 0.593688965 batch PCKh 0.375\n",
      "Trained batch 1130 batch loss 0.531888664 batch mAP 0.581970215 batch PCKh 0.4375\n",
      "Trained batch 1131 batch loss 0.528369784 batch mAP 0.575775146 batch PCKh 0.4375\n",
      "Trained batch 1132 batch loss 0.520873904 batch mAP 0.637512207 batch PCKh 0.4375\n",
      "Trained batch 1133 batch loss 0.438269436 batch mAP 0.642730713 batch PCKh 0.5625\n",
      "Trained batch 1134 batch loss 0.532776058 batch mAP 0.599975586 batch PCKh 0.375\n",
      "Trained batch 1135 batch loss 0.528495312 batch mAP 0.575714111 batch PCKh 0.1875\n",
      "Trained batch 1136 batch loss 0.496309191 batch mAP 0.678833 batch PCKh 0.375\n",
      "Trained batch 1137 batch loss 0.505683839 batch mAP 0.657043457 batch PCKh 0.1875\n",
      "Trained batch 1138 batch loss 0.5503124 batch mAP 0.685211182 batch PCKh 0.3125\n",
      "Trained batch 1139 batch loss 0.398478717 batch mAP 0.713317871 batch PCKh 0.375\n",
      "Trained batch 1140 batch loss 0.461615175 batch mAP 0.682556152 batch PCKh 0.25\n",
      "Trained batch 1141 batch loss 0.454192817 batch mAP 0.697052 batch PCKh 0.25\n",
      "Trained batch 1142 batch loss 0.54988569 batch mAP 0.521881104 batch PCKh 0.25\n",
      "Trained batch 1143 batch loss 0.474715233 batch mAP 0.589660645 batch PCKh 0.4375\n",
      "Trained batch 1144 batch loss 0.558088064 batch mAP 0.581756592 batch PCKh 0.6875\n",
      "Trained batch 1145 batch loss 0.476726562 batch mAP 0.638977051 batch PCKh 0.5\n",
      "Trained batch 1146 batch loss 0.50213927 batch mAP 0.620025635 batch PCKh 0.5\n",
      "Trained batch 1147 batch loss 0.555780351 batch mAP 0.648345947 batch PCKh 0.4375\n",
      "Trained batch 1148 batch loss 0.497565389 batch mAP 0.597930908 batch PCKh 0.4375\n",
      "Trained batch 1149 batch loss 0.453174084 batch mAP 0.648468 batch PCKh 0.25\n",
      "Trained batch 1150 batch loss 0.540376663 batch mAP 0.630310059 batch PCKh 0.375\n",
      "Trained batch 1151 batch loss 0.542386293 batch mAP 0.646850586 batch PCKh 0.375\n",
      "Trained batch 1152 batch loss 0.481016129 batch mAP 0.625183105 batch PCKh 0.25\n",
      "Trained batch 1153 batch loss 0.541011751 batch mAP 0.604431152 batch PCKh 0.625\n",
      "Trained batch 1154 batch loss 0.513788819 batch mAP 0.533599854 batch PCKh 0\n",
      "Trained batch 1155 batch loss 0.638315737 batch mAP 0.519683838 batch PCKh 0.6875\n",
      "Trained batch 1156 batch loss 0.740550399 batch mAP 0.445556641 batch PCKh 0.0625\n",
      "Trained batch 1157 batch loss 0.741884232 batch mAP 0.491638184 batch PCKh 0.8125\n",
      "Trained batch 1158 batch loss 0.57321167 batch mAP 0.577392578 batch PCKh 0.1875\n",
      "Trained batch 1159 batch loss 0.499489307 batch mAP 0.592681885 batch PCKh 0.5625\n",
      "Trained batch 1160 batch loss 0.506105959 batch mAP 0.639465332 batch PCKh 0.25\n",
      "Trained batch 1161 batch loss 0.562541723 batch mAP 0.570465088 batch PCKh 0.3125\n",
      "Trained batch 1162 batch loss 0.621139646 batch mAP 0.524017334 batch PCKh 0\n",
      "Trained batch 1163 batch loss 0.649882913 batch mAP 0.505737305 batch PCKh 0.1875\n",
      "Trained batch 1164 batch loss 0.615978837 batch mAP 0.486022949 batch PCKh 0\n",
      "Trained batch 1165 batch loss 0.525200367 batch mAP 0.463745117 batch PCKh 0.4375\n",
      "Trained batch 1166 batch loss 0.544948697 batch mAP 0.550140381 batch PCKh 0.1875\n",
      "Trained batch 1167 batch loss 0.608924866 batch mAP 0.499176025 batch PCKh 0\n",
      "Trained batch 1168 batch loss 0.613672256 batch mAP 0.512420654 batch PCKh 0.3125\n",
      "Trained batch 1169 batch loss 0.567309141 batch mAP 0.589416504 batch PCKh 0.5\n",
      "Trained batch 1170 batch loss 0.501508653 batch mAP 0.533233643 batch PCKh 0.75\n",
      "Trained batch 1171 batch loss 0.557924449 batch mAP 0.541412354 batch PCKh 0.875\n",
      "Trained batch 1172 batch loss 0.504005909 batch mAP 0.618011475 batch PCKh 0.875\n",
      "Trained batch 1173 batch loss 0.651579797 batch mAP 0.582214355 batch PCKh 0.5\n",
      "Trained batch 1174 batch loss 0.655060649 batch mAP 0.54397583 batch PCKh 0.4375\n",
      "Trained batch 1175 batch loss 0.524257839 batch mAP 0.602508545 batch PCKh 0.5\n",
      "Trained batch 1176 batch loss 0.551474929 batch mAP 0.573608398 batch PCKh 0.75\n",
      "Trained batch 1177 batch loss 0.622475564 batch mAP 0.605957031 batch PCKh 0.625\n",
      "Trained batch 1178 batch loss 0.594686508 batch mAP 0.528991699 batch PCKh 0.4375\n",
      "Trained batch 1179 batch loss 0.623028 batch mAP 0.566803 batch PCKh 0.6875\n",
      "Trained batch 1180 batch loss 0.586494386 batch mAP 0.571563721 batch PCKh 0.875\n",
      "Trained batch 1181 batch loss 0.565278947 batch mAP 0.651428223 batch PCKh 0.75\n",
      "Trained batch 1182 batch loss 0.616645515 batch mAP 0.575592041 batch PCKh 0.5625\n",
      "Trained batch 1183 batch loss 0.611762285 batch mAP 0.56427 batch PCKh 0.625\n",
      "Trained batch 1184 batch loss 0.554513 batch mAP 0.573974609 batch PCKh 0.8125\n",
      "Trained batch 1185 batch loss 0.591021657 batch mAP 0.534881592 batch PCKh 0.8125\n",
      "Trained batch 1186 batch loss 0.54041791 batch mAP 0.522796631 batch PCKh 0.5\n",
      "Trained batch 1187 batch loss 0.48510462 batch mAP 0.591003418 batch PCKh 0.25\n",
      "Trained batch 1188 batch loss 0.543481588 batch mAP 0.563140869 batch PCKh 0.625\n",
      "Trained batch 1189 batch loss 0.503008485 batch mAP 0.557647705 batch PCKh 0.75\n",
      "Trained batch 1190 batch loss 0.536697507 batch mAP 0.516326904 batch PCKh 0.4375\n",
      "Trained batch 1191 batch loss 0.577694774 batch mAP 0.532867432 batch PCKh 0.625\n",
      "Trained batch 1192 batch loss 0.547516 batch mAP 0.529174805 batch PCKh 0.75\n",
      "Trained batch 1193 batch loss 0.634997 batch mAP 0.474182129 batch PCKh 0.75\n",
      "Trained batch 1194 batch loss 0.565761387 batch mAP 0.546112061 batch PCKh 0.75\n",
      "Trained batch 1195 batch loss 0.620921552 batch mAP 0.564544678 batch PCKh 0.4375\n",
      "Trained batch 1196 batch loss 0.63126719 batch mAP 0.501586914 batch PCKh 0.125\n",
      "Trained batch 1197 batch loss 0.609155834 batch mAP 0.484741211 batch PCKh 0.375\n",
      "Trained batch 1198 batch loss 0.549226403 batch mAP 0.611694336 batch PCKh 0.75\n",
      "Trained batch 1199 batch loss 0.517952323 batch mAP 0.629638672 batch PCKh 0.3125\n",
      "Trained batch 1200 batch loss 0.556830287 batch mAP 0.609130859 batch PCKh 0.375\n",
      "Trained batch 1201 batch loss 0.56261611 batch mAP 0.541656494 batch PCKh 0.6875\n",
      "Trained batch 1202 batch loss 0.592490256 batch mAP 0.589813232 batch PCKh 0.625\n",
      "Trained batch 1203 batch loss 0.482455075 batch mAP 0.583251953 batch PCKh 0.375\n",
      "Trained batch 1204 batch loss 0.606034517 batch mAP 0.516326904 batch PCKh 0.6875\n",
      "Trained batch 1205 batch loss 0.491429478 batch mAP 0.562225342 batch PCKh 0.375\n",
      "Trained batch 1206 batch loss 0.454125971 batch mAP 0.568206787 batch PCKh 0.1875\n",
      "Trained batch 1207 batch loss 0.498632848 batch mAP 0.561248779 batch PCKh 0.625\n",
      "Trained batch 1208 batch loss 0.542207241 batch mAP 0.579376221 batch PCKh 0.625\n",
      "Trained batch 1209 batch loss 0.606558919 batch mAP 0.525115967 batch PCKh 0.5625\n",
      "Trained batch 1210 batch loss 0.622139156 batch mAP 0.50201416 batch PCKh 0.75\n",
      "Trained batch 1211 batch loss 0.631237268 batch mAP 0.477386475 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1212 batch loss 0.61417377 batch mAP 0.547119141 batch PCKh 0.125\n",
      "Trained batch 1213 batch loss 0.5995785 batch mAP 0.538818359 batch PCKh 0.625\n",
      "Trained batch 1214 batch loss 0.607665896 batch mAP 0.540557861 batch PCKh 0.3125\n",
      "Trained batch 1215 batch loss 0.64225471 batch mAP 0.603118896 batch PCKh 0.0625\n",
      "Trained batch 1216 batch loss 0.640789 batch mAP 0.608520508 batch PCKh 0.375\n",
      "Trained batch 1217 batch loss 0.66989857 batch mAP 0.566803 batch PCKh 0.0625\n",
      "Trained batch 1218 batch loss 0.687659383 batch mAP 0.607818604 batch PCKh 0.1875\n",
      "Trained batch 1219 batch loss 0.696638048 batch mAP 0.530456543 batch PCKh 0.1875\n",
      "Trained batch 1220 batch loss 0.629111826 batch mAP 0.644317627 batch PCKh 0.5625\n",
      "Trained batch 1221 batch loss 0.484285653 batch mAP 0.614532471 batch PCKh 0.5625\n",
      "Trained batch 1222 batch loss 0.532192349 batch mAP 0.593902588 batch PCKh 0.625\n",
      "Trained batch 1223 batch loss 0.511546 batch mAP 0.577423096 batch PCKh 0.4375\n",
      "Trained batch 1224 batch loss 0.567810893 batch mAP 0.55847168 batch PCKh 0.5625\n",
      "Trained batch 1225 batch loss 0.437136352 batch mAP 0.59487915 batch PCKh 0.625\n",
      "Trained batch 1226 batch loss 0.564889252 batch mAP 0.544769287 batch PCKh 0.75\n",
      "Trained batch 1227 batch loss 0.615194917 batch mAP 0.547393799 batch PCKh 0.625\n",
      "Trained batch 1228 batch loss 0.59035635 batch mAP 0.512481689 batch PCKh 0.5625\n",
      "Trained batch 1229 batch loss 0.56802243 batch mAP 0.552490234 batch PCKh 0.1875\n",
      "Trained batch 1230 batch loss 0.563123763 batch mAP 0.582214355 batch PCKh 0.1875\n",
      "Trained batch 1231 batch loss 0.610070467 batch mAP 0.477722168 batch PCKh 0.6875\n",
      "Trained batch 1232 batch loss 0.507310033 batch mAP 0.511016846 batch PCKh 0.3125\n",
      "Trained batch 1233 batch loss 0.612484217 batch mAP 0.440368652 batch PCKh 0\n",
      "Trained batch 1234 batch loss 0.596510351 batch mAP 0.565185547 batch PCKh 0.25\n",
      "Trained batch 1235 batch loss 0.661720634 batch mAP 0.524230957 batch PCKh 0.0625\n",
      "Trained batch 1236 batch loss 0.638199687 batch mAP 0.539917 batch PCKh 0.3125\n",
      "Trained batch 1237 batch loss 0.671280742 batch mAP 0.517669678 batch PCKh 0.1875\n",
      "Trained batch 1238 batch loss 0.66251564 batch mAP 0.523407 batch PCKh 0.6875\n",
      "Trained batch 1239 batch loss 0.653869033 batch mAP 0.53326416 batch PCKh 0.625\n",
      "Trained batch 1240 batch loss 0.532845318 batch mAP 0.643096924 batch PCKh 0.375\n",
      "Trained batch 1241 batch loss 0.584654 batch mAP 0.625183105 batch PCKh 0.4375\n",
      "Trained batch 1242 batch loss 0.58144933 batch mAP 0.546478271 batch PCKh 0.4375\n",
      "Trained batch 1243 batch loss 0.55723983 batch mAP 0.551147461 batch PCKh 0.3125\n",
      "Trained batch 1244 batch loss 0.462251723 batch mAP 0.565673828 batch PCKh 0.5625\n",
      "Trained batch 1245 batch loss 0.554282367 batch mAP 0.580627441 batch PCKh 0.3125\n",
      "Trained batch 1246 batch loss 0.547193229 batch mAP 0.596313477 batch PCKh 0.4375\n",
      "Trained batch 1247 batch loss 0.639189 batch mAP 0.602294922 batch PCKh 0.3125\n",
      "Trained batch 1248 batch loss 0.5834288 batch mAP 0.59185791 batch PCKh 0.375\n",
      "Trained batch 1249 batch loss 0.562346816 batch mAP 0.590454102 batch PCKh 0.25\n",
      "Trained batch 1250 batch loss 0.541412473 batch mAP 0.598999 batch PCKh 0.25\n",
      "Trained batch 1251 batch loss 0.5958969 batch mAP 0.585845947 batch PCKh 0.3125\n",
      "Trained batch 1252 batch loss 0.547756314 batch mAP 0.643951416 batch PCKh 0.375\n",
      "Trained batch 1253 batch loss 0.554166853 batch mAP 0.680786133 batch PCKh 0.8125\n",
      "Trained batch 1254 batch loss 0.585117817 batch mAP 0.657562256 batch PCKh 0.6875\n",
      "Trained batch 1255 batch loss 0.530856848 batch mAP 0.647216797 batch PCKh 0.625\n",
      "Trained batch 1256 batch loss 0.471792 batch mAP 0.614257812 batch PCKh 0.5625\n",
      "Trained batch 1257 batch loss 0.562863469 batch mAP 0.571289062 batch PCKh 0.125\n",
      "Trained batch 1258 batch loss 0.5589028 batch mAP 0.56149292 batch PCKh 0.4375\n",
      "Trained batch 1259 batch loss 0.570216835 batch mAP 0.530700684 batch PCKh 0.6875\n",
      "Trained batch 1260 batch loss 0.537263334 batch mAP 0.520599365 batch PCKh 0.6875\n",
      "Trained batch 1261 batch loss 0.505391181 batch mAP 0.547454834 batch PCKh 0.75\n",
      "Trained batch 1262 batch loss 0.50174284 batch mAP 0.54473877 batch PCKh 0.5\n",
      "Trained batch 1263 batch loss 0.546024621 batch mAP 0.552337646 batch PCKh 0.625\n",
      "Trained batch 1264 batch loss 0.609687924 batch mAP 0.512023926 batch PCKh 0.375\n",
      "Trained batch 1265 batch loss 0.608504295 batch mAP 0.474914551 batch PCKh 0.5625\n",
      "Trained batch 1266 batch loss 0.578182876 batch mAP 0.586273193 batch PCKh 0.625\n",
      "Trained batch 1267 batch loss 0.577965617 batch mAP 0.528900146 batch PCKh 0.125\n",
      "Trained batch 1268 batch loss 0.567414761 batch mAP 0.540588379 batch PCKh 0.5\n",
      "Trained batch 1269 batch loss 0.578299105 batch mAP 0.536529541 batch PCKh 0.4375\n",
      "Trained batch 1270 batch loss 0.543126345 batch mAP 0.513061523 batch PCKh 0.5\n",
      "Trained batch 1271 batch loss 0.574539125 batch mAP 0.498260498 batch PCKh 0.75\n",
      "Trained batch 1272 batch loss 0.550269246 batch mAP 0.464324951 batch PCKh 0.4375\n",
      "Trained batch 1273 batch loss 0.494835645 batch mAP 0.458007812 batch PCKh 0.6875\n",
      "Trained batch 1274 batch loss 0.579277277 batch mAP 0.419952393 batch PCKh 0.5\n",
      "Trained batch 1275 batch loss 0.570916414 batch mAP 0.475402832 batch PCKh 0.5625\n",
      "Trained batch 1276 batch loss 0.529880404 batch mAP 0.541046143 batch PCKh 0.3125\n",
      "Trained batch 1277 batch loss 0.578703523 batch mAP 0.607177734 batch PCKh 0.6875\n",
      "Trained batch 1278 batch loss 0.557740331 batch mAP 0.599121094 batch PCKh 0.5\n",
      "Trained batch 1279 batch loss 0.50960809 batch mAP 0.663970947 batch PCKh 0.6875\n",
      "Trained batch 1280 batch loss 0.488450676 batch mAP 0.660400391 batch PCKh 0.375\n",
      "Trained batch 1281 batch loss 0.500702143 batch mAP 0.632904053 batch PCKh 0.6875\n",
      "Trained batch 1282 batch loss 0.503226936 batch mAP 0.613525391 batch PCKh 0.75\n",
      "Trained batch 1283 batch loss 0.469314754 batch mAP 0.556976318 batch PCKh 0.1875\n",
      "Trained batch 1284 batch loss 0.570835948 batch mAP 0.607910156 batch PCKh 0.5\n",
      "Trained batch 1285 batch loss 0.560130894 batch mAP 0.53918457 batch PCKh 0.4375\n",
      "Trained batch 1286 batch loss 0.650846839 batch mAP 0.502075195 batch PCKh 0.4375\n",
      "Trained batch 1287 batch loss 0.529838741 batch mAP 0.568054199 batch PCKh 0.25\n",
      "Trained batch 1288 batch loss 0.594962835 batch mAP 0.500061035 batch PCKh 0.3125\n",
      "Trained batch 1289 batch loss 0.569238484 batch mAP 0.536590576 batch PCKh 0.75\n",
      "Trained batch 1290 batch loss 0.527709484 batch mAP 0.475799561 batch PCKh 0.8125\n",
      "Trained batch 1291 batch loss 0.516409039 batch mAP 0.513336182 batch PCKh 0.875\n",
      "Trained batch 1292 batch loss 0.544369459 batch mAP 0.521942139 batch PCKh 0.1875\n",
      "Trained batch 1293 batch loss 0.423517883 batch mAP 0.650604248 batch PCKh 0.8125\n",
      "Trained batch 1294 batch loss 0.444149524 batch mAP 0.615356445 batch PCKh 0.375\n",
      "Trained batch 1295 batch loss 0.434719771 batch mAP 0.594268799 batch PCKh 0.8125\n",
      "Trained batch 1296 batch loss 0.403098226 batch mAP 0.624053955 batch PCKh 0.875\n",
      "Trained batch 1297 batch loss 0.440527678 batch mAP 0.610534668 batch PCKh 0.8125\n",
      "Trained batch 1298 batch loss 0.44532752 batch mAP 0.622955322 batch PCKh 0.5\n",
      "Trained batch 1299 batch loss 0.418650866 batch mAP 0.60055542 batch PCKh 0.6875\n",
      "Trained batch 1300 batch loss 0.527369082 batch mAP 0.549072266 batch PCKh 0.25\n",
      "Trained batch 1301 batch loss 0.46291548 batch mAP 0.634490967 batch PCKh 0.375\n",
      "Trained batch 1302 batch loss 0.567504168 batch mAP 0.582366943 batch PCKh 0.4375\n",
      "Trained batch 1303 batch loss 0.559666514 batch mAP 0.567779541 batch PCKh 0.75\n",
      "Trained batch 1304 batch loss 0.558988094 batch mAP 0.643920898 batch PCKh 0.5625\n",
      "Trained batch 1305 batch loss 0.621844709 batch mAP 0.614471436 batch PCKh 0.75\n",
      "Trained batch 1306 batch loss 0.520522 batch mAP 0.618896484 batch PCKh 0.375\n",
      "Trained batch 1307 batch loss 0.635971069 batch mAP 0.647186279 batch PCKh 0.3125\n",
      "Trained batch 1308 batch loss 0.52933079 batch mAP 0.660644531 batch PCKh 0.375\n",
      "Trained batch 1309 batch loss 0.507306218 batch mAP 0.648284912 batch PCKh 0.375\n",
      "Trained batch 1310 batch loss 0.680749893 batch mAP 0.483154297 batch PCKh 0.25\n",
      "Trained batch 1311 batch loss 0.610057592 batch mAP 0.503265381 batch PCKh 0.4375\n",
      "Trained batch 1312 batch loss 0.666078866 batch mAP 0.434417725 batch PCKh 0.25\n",
      "Trained batch 1313 batch loss 0.636364 batch mAP 0.501159668 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1314 batch loss 0.662478685 batch mAP 0.511047363 batch PCKh 0.5625\n",
      "Trained batch 1315 batch loss 0.60507381 batch mAP 0.548034668 batch PCKh 0.625\n",
      "Trained batch 1316 batch loss 0.540430069 batch mAP 0.523345947 batch PCKh 0.875\n",
      "Trained batch 1317 batch loss 0.551132 batch mAP 0.524230957 batch PCKh 0.75\n",
      "Trained batch 1318 batch loss 0.534368932 batch mAP 0.487976074 batch PCKh 0.875\n",
      "Trained batch 1319 batch loss 0.536383808 batch mAP 0.521118164 batch PCKh 0.875\n",
      "Trained batch 1320 batch loss 0.526928544 batch mAP 0.524871826 batch PCKh 0.375\n",
      "Trained batch 1321 batch loss 0.580438316 batch mAP 0.593109131 batch PCKh 0.25\n",
      "Trained batch 1322 batch loss 0.505960166 batch mAP 0.513275146 batch PCKh 0.75\n",
      "Trained batch 1323 batch loss 0.648749471 batch mAP 0.535217285 batch PCKh 0\n",
      "Trained batch 1324 batch loss 0.674221754 batch mAP 0.507995605 batch PCKh 0.4375\n",
      "Trained batch 1325 batch loss 0.614706337 batch mAP 0.533081055 batch PCKh 0.625\n",
      "Trained batch 1326 batch loss 0.637715 batch mAP 0.549407959 batch PCKh 0.6875\n",
      "Trained batch 1327 batch loss 0.72104919 batch mAP 0.457611084 batch PCKh 0.5625\n",
      "Trained batch 1328 batch loss 0.620399296 batch mAP 0.469177246 batch PCKh 0.375\n",
      "Trained batch 1329 batch loss 0.68182379 batch mAP 0.444000244 batch PCKh 0.125\n",
      "Trained batch 1330 batch loss 0.674443364 batch mAP 0.495300293 batch PCKh 0.5625\n",
      "Trained batch 1331 batch loss 0.533240736 batch mAP 0.59475708 batch PCKh 0.0625\n",
      "Trained batch 1332 batch loss 0.525004148 batch mAP 0.585205078 batch PCKh 0.375\n",
      "Trained batch 1333 batch loss 0.573499084 batch mAP 0.513702393 batch PCKh 0.1875\n",
      "Trained batch 1334 batch loss 0.659165621 batch mAP 0.487823486 batch PCKh 0.3125\n",
      "Trained batch 1335 batch loss 0.568876 batch mAP 0.593597412 batch PCKh 0.25\n",
      "Trained batch 1336 batch loss 0.545975566 batch mAP 0.543212891 batch PCKh 0.3125\n",
      "Trained batch 1337 batch loss 0.633013129 batch mAP 0.478912354 batch PCKh 0.4375\n",
      "Trained batch 1338 batch loss 0.636891127 batch mAP 0.496307373 batch PCKh 0.125\n",
      "Trained batch 1339 batch loss 0.68798542 batch mAP 0.512542725 batch PCKh 0.125\n",
      "Trained batch 1340 batch loss 0.637626529 batch mAP 0.516082764 batch PCKh 0.1875\n",
      "Trained batch 1341 batch loss 0.60210979 batch mAP 0.518310547 batch PCKh 0.4375\n",
      "Trained batch 1342 batch loss 0.501397669 batch mAP 0.627441406 batch PCKh 0.3125\n",
      "Trained batch 1343 batch loss 0.442097515 batch mAP 0.581542969 batch PCKh 0.3125\n",
      "Trained batch 1344 batch loss 0.552199304 batch mAP 0.553344727 batch PCKh 0.375\n",
      "Trained batch 1345 batch loss 0.558487713 batch mAP 0.542999268 batch PCKh 0.25\n",
      "Trained batch 1346 batch loss 0.5152089 batch mAP 0.50769043 batch PCKh 0.5625\n",
      "Trained batch 1347 batch loss 0.582612157 batch mAP 0.502532959 batch PCKh 0.5\n",
      "Trained batch 1348 batch loss 0.584794521 batch mAP 0.498168945 batch PCKh 0.1875\n",
      "Trained batch 1349 batch loss 0.520814598 batch mAP 0.526123047 batch PCKh 0.3125\n",
      "Trained batch 1350 batch loss 0.464031249 batch mAP 0.565032959 batch PCKh 0.6875\n",
      "Trained batch 1351 batch loss 0.606014609 batch mAP 0.537841797 batch PCKh 0.75\n",
      "Trained batch 1352 batch loss 0.564143658 batch mAP 0.535430908 batch PCKh 0.5\n",
      "Trained batch 1353 batch loss 0.587576151 batch mAP 0.482727051 batch PCKh 0.3125\n",
      "Trained batch 1354 batch loss 0.636540651 batch mAP 0.466491699 batch PCKh 0.5625\n",
      "Trained batch 1355 batch loss 0.563286901 batch mAP 0.523407 batch PCKh 0.6875\n",
      "Trained batch 1356 batch loss 0.542735696 batch mAP 0.536468506 batch PCKh 0.4375\n",
      "Trained batch 1357 batch loss 0.540783882 batch mAP 0.507110596 batch PCKh 0.4375\n",
      "Trained batch 1358 batch loss 0.451743186 batch mAP 0.590820312 batch PCKh 0.6875\n",
      "Trained batch 1359 batch loss 0.54381007 batch mAP 0.550201416 batch PCKh 0.4375\n",
      "Trained batch 1360 batch loss 0.555440426 batch mAP 0.579864502 batch PCKh 0.5\n",
      "Trained batch 1361 batch loss 0.574289799 batch mAP 0.59173584 batch PCKh 0.375\n",
      "Trained batch 1362 batch loss 0.530999422 batch mAP 0.552978516 batch PCKh 0.75\n",
      "Trained batch 1363 batch loss 0.540355265 batch mAP 0.578125 batch PCKh 0.6875\n",
      "Trained batch 1364 batch loss 0.596816421 batch mAP 0.528778076 batch PCKh 0.75\n",
      "Trained batch 1365 batch loss 0.65157032 batch mAP 0.417511 batch PCKh 0.75\n",
      "Trained batch 1366 batch loss 0.558654368 batch mAP 0.551422119 batch PCKh 0.125\n",
      "Trained batch 1367 batch loss 0.561370075 batch mAP 0.558868408 batch PCKh 0.5\n",
      "Trained batch 1368 batch loss 0.513206363 batch mAP 0.619781494 batch PCKh 0.625\n",
      "Trained batch 1369 batch loss 0.524577498 batch mAP 0.589752197 batch PCKh 0.4375\n",
      "Trained batch 1370 batch loss 0.51262486 batch mAP 0.540771484 batch PCKh 0.5\n",
      "Trained batch 1371 batch loss 0.531073451 batch mAP 0.575378418 batch PCKh 0.6875\n",
      "Trained batch 1372 batch loss 0.532702088 batch mAP 0.591278076 batch PCKh 0.4375\n",
      "Trained batch 1373 batch loss 0.546337366 batch mAP 0.61416626 batch PCKh 0.375\n",
      "Trained batch 1374 batch loss 0.573547721 batch mAP 0.581756592 batch PCKh 0.375\n",
      "Trained batch 1375 batch loss 0.563210547 batch mAP 0.609161377 batch PCKh 0.5\n",
      "Trained batch 1376 batch loss 0.543902218 batch mAP 0.603424072 batch PCKh 0.5\n",
      "Trained batch 1377 batch loss 0.551668882 batch mAP 0.565948486 batch PCKh 0.375\n",
      "Trained batch 1378 batch loss 0.549296498 batch mAP 0.55480957 batch PCKh 0.75\n",
      "Trained batch 1379 batch loss 0.598265767 batch mAP 0.528686523 batch PCKh 0.25\n",
      "Trained batch 1380 batch loss 0.546427429 batch mAP 0.532135 batch PCKh 0.3125\n",
      "Trained batch 1381 batch loss 0.485811204 batch mAP 0.573272705 batch PCKh 0.5625\n",
      "Trained batch 1382 batch loss 0.500898838 batch mAP 0.564422607 batch PCKh 0.3125\n",
      "Trained batch 1383 batch loss 0.462739766 batch mAP 0.54095459 batch PCKh 0.6875\n",
      "Trained batch 1384 batch loss 0.503026247 batch mAP 0.560089111 batch PCKh 0.4375\n",
      "Trained batch 1385 batch loss 0.539025605 batch mAP 0.583068848 batch PCKh 0.625\n",
      "Trained batch 1386 batch loss 0.554685831 batch mAP 0.54888916 batch PCKh 0.75\n",
      "Trained batch 1387 batch loss 0.453248352 batch mAP 0.57144165 batch PCKh 0.4375\n",
      "Trained batch 1388 batch loss 0.532732725 batch mAP 0.566223145 batch PCKh 0.4375\n",
      "Trained batch 1389 batch loss 0.590678215 batch mAP 0.478942871 batch PCKh 0.25\n",
      "Trained batch 1390 batch loss 0.527430177 batch mAP 0.558532715 batch PCKh 0\n",
      "Trained batch 1391 batch loss 0.463534862 batch mAP 0.626861572 batch PCKh 0.3125\n",
      "Trained batch 1392 batch loss 0.575870335 batch mAP 0.512054443 batch PCKh 0.0625\n",
      "Trained batch 1393 batch loss 0.605315208 batch mAP 0.477417 batch PCKh 0.0625\n",
      "Trained batch 1394 batch loss 0.54682976 batch mAP 0.445129395 batch PCKh 0.5\n",
      "Trained batch 1395 batch loss 0.54154706 batch mAP 0.45224 batch PCKh 0.1875\n",
      "Trained batch 1396 batch loss 0.600405455 batch mAP 0.491210938 batch PCKh 0.75\n",
      "Trained batch 1397 batch loss 0.622486949 batch mAP 0.50012207 batch PCKh 0.1875\n",
      "Trained batch 1398 batch loss 0.664290309 batch mAP 0.550079346 batch PCKh 0.4375\n",
      "Trained batch 1399 batch loss 0.657047451 batch mAP 0.491271973 batch PCKh 0.75\n",
      "Trained batch 1400 batch loss 0.683063626 batch mAP 0.470977783 batch PCKh 0.8125\n",
      "Trained batch 1401 batch loss 0.664704323 batch mAP 0.532531738 batch PCKh 0.1875\n",
      "Trained batch 1402 batch loss 0.632195175 batch mAP 0.467865 batch PCKh 0.6875\n",
      "Trained batch 1403 batch loss 0.495831192 batch mAP 0.582794189 batch PCKh 0.75\n",
      "Trained batch 1404 batch loss 0.542277753 batch mAP 0.498321533 batch PCKh 0.4375\n",
      "Trained batch 1405 batch loss 0.580149829 batch mAP 0.508911133 batch PCKh 0.75\n",
      "Trained batch 1406 batch loss 0.580804825 batch mAP 0.521362305 batch PCKh 0.75\n",
      "Trained batch 1407 batch loss 0.568990946 batch mAP 0.525787354 batch PCKh 0.5\n",
      "Trained batch 1408 batch loss 0.545073032 batch mAP 0.493988037 batch PCKh 0.5625\n",
      "Trained batch 1409 batch loss 0.565286636 batch mAP 0.557434082 batch PCKh 0.6875\n",
      "Trained batch 1410 batch loss 0.600043416 batch mAP 0.522247314 batch PCKh 0.0625\n",
      "Trained batch 1411 batch loss 0.554285884 batch mAP 0.57232666 batch PCKh 0.5625\n",
      "Trained batch 1412 batch loss 0.618280172 batch mAP 0.576904297 batch PCKh 0.25\n",
      "Trained batch 1413 batch loss 0.635681033 batch mAP 0.554077148 batch PCKh 0.8125\n",
      "Trained batch 1414 batch loss 0.573765814 batch mAP 0.545410156 batch PCKh 0.125\n",
      "Trained batch 1415 batch loss 0.671911538 batch mAP 0.547607422 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1416 batch loss 0.643520653 batch mAP 0.595977783 batch PCKh 0.1875\n",
      "Trained batch 1417 batch loss 0.582096756 batch mAP 0.630645752 batch PCKh 0.4375\n",
      "Trained batch 1418 batch loss 0.691194773 batch mAP 0.571838379 batch PCKh 0.125\n",
      "Trained batch 1419 batch loss 0.608814061 batch mAP 0.573272705 batch PCKh 0.3125\n",
      "Trained batch 1420 batch loss 0.705489278 batch mAP 0.483947754 batch PCKh 0.625\n",
      "Trained batch 1421 batch loss 0.637201667 batch mAP 0.538818359 batch PCKh 0.625\n",
      "Trained batch 1422 batch loss 0.689004481 batch mAP 0.542053223 batch PCKh 0.5625\n",
      "Trained batch 1423 batch loss 0.618961394 batch mAP 0.580719 batch PCKh 0.6875\n",
      "Trained batch 1424 batch loss 0.646598101 batch mAP 0.537261963 batch PCKh 0.625\n",
      "Trained batch 1425 batch loss 0.613882303 batch mAP 0.523132324 batch PCKh 0.4375\n",
      "Trained batch 1426 batch loss 0.639749765 batch mAP 0.496582031 batch PCKh 0.5625\n",
      "Trained batch 1427 batch loss 0.632090747 batch mAP 0.515289307 batch PCKh 0.4375\n",
      "Trained batch 1428 batch loss 0.586856127 batch mAP 0.519348145 batch PCKh 0.625\n",
      "Trained batch 1429 batch loss 0.590272307 batch mAP 0.528656 batch PCKh 0.625\n",
      "Trained batch 1430 batch loss 0.56421721 batch mAP 0.529052734 batch PCKh 0.5625\n",
      "Trained batch 1431 batch loss 0.60496372 batch mAP 0.490966797 batch PCKh 0.375\n",
      "Trained batch 1432 batch loss 0.620145559 batch mAP 0.563568115 batch PCKh 0.1875\n",
      "Trained batch 1433 batch loss 0.563245773 batch mAP 0.592193604 batch PCKh 0.5625\n",
      "Trained batch 1434 batch loss 0.632501066 batch mAP 0.58215332 batch PCKh 0.5\n",
      "Trained batch 1435 batch loss 0.577607512 batch mAP 0.595397949 batch PCKh 0.1875\n",
      "Trained batch 1436 batch loss 0.633440137 batch mAP 0.574615479 batch PCKh 0.25\n",
      "Trained batch 1437 batch loss 0.641840577 batch mAP 0.56060791 batch PCKh 0.1875\n",
      "Trained batch 1438 batch loss 0.591501713 batch mAP 0.579956055 batch PCKh 0.3125\n",
      "Trained batch 1439 batch loss 0.609338105 batch mAP 0.535125732 batch PCKh 0.375\n",
      "Trained batch 1440 batch loss 0.622676849 batch mAP 0.519195557 batch PCKh 0.25\n",
      "Trained batch 1441 batch loss 0.604880393 batch mAP 0.544372559 batch PCKh 0\n",
      "Trained batch 1442 batch loss 0.725150883 batch mAP 0.504608154 batch PCKh 0\n",
      "Trained batch 1443 batch loss 0.631835461 batch mAP 0.522216797 batch PCKh 0.125\n",
      "Trained batch 1444 batch loss 0.592380047 batch mAP 0.504455566 batch PCKh 0.375\n",
      "Trained batch 1445 batch loss 0.4729909 batch mAP 0.499450684 batch PCKh 0.5\n",
      "Trained batch 1446 batch loss 0.55310142 batch mAP 0.474304199 batch PCKh 0.3125\n",
      "Trained batch 1447 batch loss 0.67583096 batch mAP 0.427612305 batch PCKh 0.3125\n",
      "Trained batch 1448 batch loss 0.559321284 batch mAP 0.43270874 batch PCKh 0.1875\n",
      "Trained batch 1449 batch loss 0.691517234 batch mAP 0.447418213 batch PCKh 0.875\n",
      "Trained batch 1450 batch loss 0.576597691 batch mAP 0.455627441 batch PCKh 0.5625\n",
      "Trained batch 1451 batch loss 0.644349933 batch mAP 0.463592529 batch PCKh 0.25\n",
      "Trained batch 1452 batch loss 0.589227796 batch mAP 0.558044434 batch PCKh 0.25\n",
      "Trained batch 1453 batch loss 0.548950076 batch mAP 0.553863525 batch PCKh 0.75\n",
      "Trained batch 1454 batch loss 0.531563 batch mAP 0.498901367 batch PCKh 0.1875\n",
      "Trained batch 1455 batch loss 0.564945698 batch mAP 0.54019165 batch PCKh 0.375\n",
      "Trained batch 1456 batch loss 0.520338 batch mAP 0.54397583 batch PCKh 0.375\n",
      "Trained batch 1457 batch loss 0.536014676 batch mAP 0.602630615 batch PCKh 0.25\n",
      "Trained batch 1458 batch loss 0.529416919 batch mAP 0.53616333 batch PCKh 0.75\n",
      "Trained batch 1459 batch loss 0.521254838 batch mAP 0.56652832 batch PCKh 0.1875\n",
      "Trained batch 1460 batch loss 0.569668293 batch mAP 0.519714355 batch PCKh 0.4375\n",
      "Trained batch 1461 batch loss 0.587884307 batch mAP 0.577148438 batch PCKh 0.75\n",
      "Trained batch 1462 batch loss 0.559567571 batch mAP 0.623901367 batch PCKh 0.5625\n",
      "Trained batch 1463 batch loss 0.534495354 batch mAP 0.623504639 batch PCKh 0.1875\n",
      "Trained batch 1464 batch loss 0.618802547 batch mAP 0.573516846 batch PCKh 0.4375\n",
      "Trained batch 1465 batch loss 0.609593093 batch mAP 0.581207275 batch PCKh 0.4375\n",
      "Trained batch 1466 batch loss 0.552536 batch mAP 0.596740723 batch PCKh 0.3125\n",
      "Trained batch 1467 batch loss 0.505117536 batch mAP 0.608093262 batch PCKh 0.25\n",
      "Trained batch 1468 batch loss 0.461633027 batch mAP 0.600006104 batch PCKh 0.125\n",
      "Trained batch 1469 batch loss 0.552209139 batch mAP 0.635406494 batch PCKh 0.5\n",
      "Trained batch 1470 batch loss 0.601714909 batch mAP 0.67199707 batch PCKh 0.375\n",
      "Trained batch 1471 batch loss 0.582990289 batch mAP 0.644958496 batch PCKh 0.5\n",
      "Trained batch 1472 batch loss 0.573380411 batch mAP 0.664733887 batch PCKh 0.5\n",
      "Trained batch 1473 batch loss 0.519895136 batch mAP 0.524963379 batch PCKh 0.125\n",
      "Trained batch 1474 batch loss 0.496400148 batch mAP 0.585022 batch PCKh 0.75\n",
      "Trained batch 1475 batch loss 0.624321222 batch mAP 0.503417969 batch PCKh 0.5625\n",
      "Trained batch 1476 batch loss 0.614243269 batch mAP 0.507293701 batch PCKh 0.8125\n",
      "Trained batch 1477 batch loss 0.599412262 batch mAP 0.54083252 batch PCKh 0.25\n",
      "Trained batch 1478 batch loss 0.544752657 batch mAP 0.507080078 batch PCKh 0.6875\n",
      "Trained batch 1479 batch loss 0.561395884 batch mAP 0.479980469 batch PCKh 0.3125\n",
      "Trained batch 1480 batch loss 0.555627167 batch mAP 0.608856201 batch PCKh 0.375\n",
      "Trained batch 1481 batch loss 0.583769679 batch mAP 0.538421631 batch PCKh 0.0625\n",
      "Trained batch 1482 batch loss 0.60739696 batch mAP 0.532592773 batch PCKh 0.4375\n",
      "Trained batch 1483 batch loss 0.573664546 batch mAP 0.560913086 batch PCKh 0.625\n",
      "Trained batch 1484 batch loss 0.584016144 batch mAP 0.51260376 batch PCKh 0.1875\n",
      "Trained batch 1485 batch loss 0.579853415 batch mAP 0.575378418 batch PCKh 0.375\n",
      "Trained batch 1486 batch loss 0.562506497 batch mAP 0.592559814 batch PCKh 0.875\n",
      "Trained batch 1487 batch loss 0.576504827 batch mAP 0.599487305 batch PCKh 0.8125\n",
      "Trained batch 1488 batch loss 0.516902685 batch mAP 0.660553 batch PCKh 0.125\n",
      "Trained batch 1489 batch loss 0.482914358 batch mAP 0.67401123 batch PCKh 0.4375\n",
      "Trained batch 1490 batch loss 0.546362102 batch mAP 0.618225098 batch PCKh 0.6875\n",
      "Trained batch 1491 batch loss 0.536619186 batch mAP 0.570953369 batch PCKh 0.75\n",
      "Trained batch 1492 batch loss 0.541783 batch mAP 0.654022217 batch PCKh 0.5\n",
      "Trained batch 1493 batch loss 0.49512887 batch mAP 0.67477417 batch PCKh 0.25\n",
      "Trained batch 1494 batch loss 0.513120949 batch mAP 0.651947 batch PCKh 0.5\n",
      "Trained batch 1495 batch loss 0.542661488 batch mAP 0.652801514 batch PCKh 0.5\n",
      "Trained batch 1496 batch loss 0.602190256 batch mAP 0.610626221 batch PCKh 0.1875\n",
      "Trained batch 1497 batch loss 0.540390432 batch mAP 0.625793457 batch PCKh 0.4375\n",
      "Trained batch 1498 batch loss 0.635924935 batch mAP 0.544403076 batch PCKh 0.75\n",
      "Trained batch 1499 batch loss 0.582839727 batch mAP 0.625885 batch PCKh 0.375\n",
      "Trained batch 1500 batch loss 0.581204057 batch mAP 0.593414307 batch PCKh 0.0625\n",
      "Trained batch 1501 batch loss 0.643943667 batch mAP 0.593353271 batch PCKh 0.3125\n",
      "Trained batch 1502 batch loss 0.536627352 batch mAP 0.60043335 batch PCKh 0.4375\n",
      "Trained batch 1503 batch loss 0.558015704 batch mAP 0.594909668 batch PCKh 0.4375\n",
      "Trained batch 1504 batch loss 0.583365917 batch mAP 0.601867676 batch PCKh 0.75\n",
      "Trained batch 1505 batch loss 0.603803039 batch mAP 0.598815918 batch PCKh 0.3125\n",
      "Trained batch 1506 batch loss 0.587468505 batch mAP 0.540557861 batch PCKh 0.375\n",
      "Trained batch 1507 batch loss 0.58928 batch mAP 0.530365 batch PCKh 0.5\n",
      "Trained batch 1508 batch loss 0.568984687 batch mAP 0.586090088 batch PCKh 0.625\n",
      "Trained batch 1509 batch loss 0.494318902 batch mAP 0.625335693 batch PCKh 0.625\n",
      "Trained batch 1510 batch loss 0.563350797 batch mAP 0.52243042 batch PCKh 0.25\n",
      "Trained batch 1511 batch loss 0.585810661 batch mAP 0.589416504 batch PCKh 0.625\n",
      "Trained batch 1512 batch loss 0.508670628 batch mAP 0.61340332 batch PCKh 0.375\n",
      "Trained batch 1513 batch loss 0.541547179 batch mAP 0.601928711 batch PCKh 0.3125\n",
      "Trained batch 1514 batch loss 0.540463 batch mAP 0.587646484 batch PCKh 0.8125\n",
      "Trained batch 1515 batch loss 0.459613681 batch mAP 0.655822754 batch PCKh 0.5625\n",
      "Trained batch 1516 batch loss 0.502111733 batch mAP 0.639801 batch PCKh 0.625\n",
      "Trained batch 1517 batch loss 0.579842567 batch mAP 0.51828 batch PCKh 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1518 batch loss 0.564336061 batch mAP 0.528930664 batch PCKh 0.375\n",
      "Trained batch 1519 batch loss 0.620057404 batch mAP 0.527771 batch PCKh 0.375\n",
      "Trained batch 1520 batch loss 0.57594192 batch mAP 0.532440186 batch PCKh 0.5\n",
      "Trained batch 1521 batch loss 0.603992701 batch mAP 0.540435791 batch PCKh 0.1875\n",
      "Trained batch 1522 batch loss 0.522302568 batch mAP 0.548614502 batch PCKh 0.25\n",
      "Trained batch 1523 batch loss 0.42435655 batch mAP 0.602386475 batch PCKh 0.25\n",
      "Trained batch 1524 batch loss 0.606025219 batch mAP 0.599243164 batch PCKh 0.375\n",
      "Trained batch 1525 batch loss 0.570245147 batch mAP 0.563385 batch PCKh 0.3125\n",
      "Trained batch 1526 batch loss 0.612354636 batch mAP 0.569763184 batch PCKh 0.25\n",
      "Trained batch 1527 batch loss 0.596188426 batch mAP 0.545135498 batch PCKh 0.1875\n",
      "Trained batch 1528 batch loss 0.568840623 batch mAP 0.475921631 batch PCKh 0.125\n",
      "Trained batch 1529 batch loss 0.438752592 batch mAP 0.459991455 batch PCKh 0.125\n",
      "Trained batch 1530 batch loss 0.471131921 batch mAP 0.449981689 batch PCKh 0.125\n",
      "Trained batch 1531 batch loss 0.668536484 batch mAP 0.451843262 batch PCKh 0.25\n",
      "Trained batch 1532 batch loss 0.577160478 batch mAP 0.49319458 batch PCKh 0.3125\n",
      "Trained batch 1533 batch loss 0.672041059 batch mAP 0.490783691 batch PCKh 0.625\n",
      "Trained batch 1534 batch loss 0.68935746 batch mAP 0.494689941 batch PCKh 0.625\n",
      "Trained batch 1535 batch loss 0.653315425 batch mAP 0.525665283 batch PCKh 0.1875\n",
      "Trained batch 1536 batch loss 0.557426333 batch mAP 0.649414062 batch PCKh 0.375\n",
      "Trained batch 1537 batch loss 0.511372209 batch mAP 0.655334473 batch PCKh 0.375\n",
      "Trained batch 1538 batch loss 0.48606497 batch mAP 0.685516357 batch PCKh 0.4375\n",
      "Trained batch 1539 batch loss 0.424125731 batch mAP 0.689209 batch PCKh 0.375\n",
      "Trained batch 1540 batch loss 0.425761819 batch mAP 0.644805908 batch PCKh 0.5\n",
      "Trained batch 1541 batch loss 0.507406414 batch mAP 0.632659912 batch PCKh 0.375\n",
      "Trained batch 1542 batch loss 0.51249069 batch mAP 0.508361816 batch PCKh 0.6875\n",
      "Trained batch 1543 batch loss 0.493471831 batch mAP 0.589111328 batch PCKh 0.4375\n",
      "Trained batch 1544 batch loss 0.4750655 batch mAP 0.592681885 batch PCKh 0.5\n",
      "Trained batch 1545 batch loss 0.475240946 batch mAP 0.626617432 batch PCKh 0.4375\n",
      "Trained batch 1546 batch loss 0.487935305 batch mAP 0.639678955 batch PCKh 0.5625\n",
      "Trained batch 1547 batch loss 0.526077032 batch mAP 0.666534424 batch PCKh 0.4375\n",
      "Trained batch 1548 batch loss 0.507675827 batch mAP 0.616241455 batch PCKh 0.75\n",
      "Trained batch 1549 batch loss 0.430117965 batch mAP 0.575286865 batch PCKh 0\n",
      "Trained batch 1550 batch loss 0.504124641 batch mAP 0.583648682 batch PCKh 0.4375\n",
      "Trained batch 1551 batch loss 0.485144734 batch mAP 0.570404053 batch PCKh 0.875\n",
      "Trained batch 1552 batch loss 0.564199567 batch mAP 0.52532959 batch PCKh 0.75\n",
      "Trained batch 1553 batch loss 0.53475 batch mAP 0.565612793 batch PCKh 0.875\n",
      "Trained batch 1554 batch loss 0.506871462 batch mAP 0.594177246 batch PCKh 0.6875\n",
      "Trained batch 1555 batch loss 0.568968117 batch mAP 0.535919189 batch PCKh 0.875\n",
      "Trained batch 1556 batch loss 0.590904117 batch mAP 0.51751709 batch PCKh 0.875\n",
      "Trained batch 1557 batch loss 0.582935333 batch mAP 0.547027588 batch PCKh 0.875\n",
      "Trained batch 1558 batch loss 0.568786442 batch mAP 0.528533936 batch PCKh 0.75\n",
      "Trained batch 1559 batch loss 0.60823369 batch mAP 0.46673584 batch PCKh 0.3125\n",
      "Trained batch 1560 batch loss 0.604345083 batch mAP 0.471557617 batch PCKh 0.4375\n",
      "Trained batch 1561 batch loss 0.529068828 batch mAP 0.479370117 batch PCKh 0.4375\n",
      "Trained batch 1562 batch loss 0.607362211 batch mAP 0.528839111 batch PCKh 0.3125\n",
      "Trained batch 1563 batch loss 0.739329576 batch mAP 0.44442749 batch PCKh 0.25\n",
      "Trained batch 1564 batch loss 0.604750216 batch mAP 0.541626 batch PCKh 0.25\n",
      "Trained batch 1565 batch loss 0.604600549 batch mAP 0.525177 batch PCKh 0.375\n",
      "Trained batch 1566 batch loss 0.574581 batch mAP 0.444244385 batch PCKh 0.75\n",
      "Trained batch 1567 batch loss 0.610634387 batch mAP 0.407623291 batch PCKh 0.5\n",
      "Trained batch 1568 batch loss 0.582739472 batch mAP 0.359527588 batch PCKh 0.625\n",
      "Trained batch 1569 batch loss 0.641247571 batch mAP 0.341369629 batch PCKh 0.125\n",
      "Trained batch 1570 batch loss 0.618649483 batch mAP 0.324981689 batch PCKh 0.3125\n",
      "Trained batch 1571 batch loss 0.703615308 batch mAP 0.344238281 batch PCKh 0.375\n",
      "Trained batch 1572 batch loss 0.6001845 batch mAP 0.375152588 batch PCKh 0.6875\n",
      "Trained batch 1573 batch loss 0.691416264 batch mAP 0.419036865 batch PCKh 0.25\n",
      "Trained batch 1574 batch loss 0.565208554 batch mAP 0.413726807 batch PCKh 0.625\n",
      "Trained batch 1575 batch loss 0.504172921 batch mAP 0.501464844 batch PCKh 0.25\n",
      "Trained batch 1576 batch loss 0.536289573 batch mAP 0.496459961 batch PCKh 0.125\n",
      "Trained batch 1577 batch loss 0.51407 batch mAP 0.602355957 batch PCKh 0.25\n",
      "Trained batch 1578 batch loss 0.53802228 batch mAP 0.627716064 batch PCKh 0.25\n",
      "Trained batch 1579 batch loss 0.591836929 batch mAP 0.547332764 batch PCKh 0.3125\n",
      "Trained batch 1580 batch loss 0.559849858 batch mAP 0.573761 batch PCKh 0.6875\n",
      "Trained batch 1581 batch loss 0.512295723 batch mAP 0.564453125 batch PCKh 0.1875\n",
      "Trained batch 1582 batch loss 0.633356512 batch mAP 0.53125 batch PCKh 0.6875\n",
      "Trained batch 1583 batch loss 0.609821558 batch mAP 0.561645508 batch PCKh 0.625\n",
      "Trained batch 1584 batch loss 0.586108 batch mAP 0.562408447 batch PCKh 0.875\n",
      "Trained batch 1585 batch loss 0.601694405 batch mAP 0.573028564 batch PCKh 0.375\n",
      "Trained batch 1586 batch loss 0.650482714 batch mAP 0.520294189 batch PCKh 0.6875\n",
      "Trained batch 1587 batch loss 0.591700554 batch mAP 0.55758667 batch PCKh 0.375\n",
      "Trained batch 1588 batch loss 0.631705284 batch mAP 0.501617432 batch PCKh 0.4375\n",
      "Trained batch 1589 batch loss 0.569820821 batch mAP 0.531158447 batch PCKh 0.4375\n",
      "Trained batch 1590 batch loss 0.588296115 batch mAP 0.554016113 batch PCKh 0.75\n",
      "Trained batch 1591 batch loss 0.588852048 batch mAP 0.569549561 batch PCKh 0.1875\n",
      "Trained batch 1592 batch loss 0.741502 batch mAP 0.43862915 batch PCKh 0.125\n",
      "Trained batch 1593 batch loss 0.559554219 batch mAP 0.613311768 batch PCKh 0.375\n",
      "Trained batch 1594 batch loss 0.461405218 batch mAP 0.628967285 batch PCKh 0.5625\n",
      "Trained batch 1595 batch loss 0.514802456 batch mAP 0.613555908 batch PCKh 0.375\n",
      "Trained batch 1596 batch loss 0.477169096 batch mAP 0.601501465 batch PCKh 0.5\n",
      "Trained batch 1597 batch loss 0.530726373 batch mAP 0.57119751 batch PCKh 0.625\n",
      "Trained batch 1598 batch loss 0.504576266 batch mAP 0.551452637 batch PCKh 0.1875\n",
      "Trained batch 1599 batch loss 0.560478806 batch mAP 0.549865723 batch PCKh 0.3125\n",
      "Trained batch 1600 batch loss 0.513682902 batch mAP 0.491210938 batch PCKh 0.3125\n",
      "Trained batch 1601 batch loss 0.657736719 batch mAP 0.495422363 batch PCKh 0.375\n",
      "Trained batch 1602 batch loss 0.567618668 batch mAP 0.481811523 batch PCKh 0.375\n",
      "Trained batch 1603 batch loss 0.537868619 batch mAP 0.617767334 batch PCKh 0.25\n",
      "Trained batch 1604 batch loss 0.537977099 batch mAP 0.624938965 batch PCKh 0.6875\n",
      "Trained batch 1605 batch loss 0.537550271 batch mAP 0.648254395 batch PCKh 0.25\n",
      "Trained batch 1606 batch loss 0.628447771 batch mAP 0.461792 batch PCKh 0\n",
      "Trained batch 1607 batch loss 0.602450132 batch mAP 0.574035645 batch PCKh 0.875\n",
      "Trained batch 1608 batch loss 0.538839817 batch mAP 0.51965332 batch PCKh 0.125\n",
      "Trained batch 1609 batch loss 0.514465392 batch mAP 0.547241211 batch PCKh 0.625\n",
      "Trained batch 1610 batch loss 0.560637832 batch mAP 0.510925293 batch PCKh 0.4375\n",
      "Trained batch 1611 batch loss 0.587385714 batch mAP 0.562438965 batch PCKh 0.25\n",
      "Trained batch 1612 batch loss 0.608932912 batch mAP 0.524108887 batch PCKh 0.125\n",
      "Trained batch 1613 batch loss 0.499739051 batch mAP 0.534118652 batch PCKh 0.3125\n",
      "Trained batch 1614 batch loss 0.563235939 batch mAP 0.518981934 batch PCKh 0.6875\n",
      "Trained batch 1615 batch loss 0.538925946 batch mAP 0.512542725 batch PCKh 0.5625\n",
      "Trained batch 1616 batch loss 0.540691733 batch mAP 0.530273438 batch PCKh 0.5\n",
      "Trained batch 1617 batch loss 0.440741479 batch mAP 0.564178467 batch PCKh 0.1875\n",
      "Trained batch 1618 batch loss 0.509580731 batch mAP 0.574035645 batch PCKh 0.1875\n",
      "Trained batch 1619 batch loss 0.56575644 batch mAP 0.585083 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1620 batch loss 0.573238492 batch mAP 0.590423584 batch PCKh 0.625\n",
      "Trained batch 1621 batch loss 0.617864 batch mAP 0.533233643 batch PCKh 0.5\n",
      "Trained batch 1622 batch loss 0.552430809 batch mAP 0.552063 batch PCKh 0.125\n",
      "Trained batch 1623 batch loss 0.50512284 batch mAP 0.607116699 batch PCKh 0.0625\n",
      "Trained batch 1624 batch loss 0.558129847 batch mAP 0.54977417 batch PCKh 0.125\n",
      "Trained batch 1625 batch loss 0.547000587 batch mAP 0.550506592 batch PCKh 0.8125\n",
      "Trained batch 1626 batch loss 0.580052 batch mAP 0.560394287 batch PCKh 0.75\n",
      "Trained batch 1627 batch loss 0.590109706 batch mAP 0.615692139 batch PCKh 0.75\n",
      "Trained batch 1628 batch loss 0.581461549 batch mAP 0.602355957 batch PCKh 0.5625\n",
      "Trained batch 1629 batch loss 0.635974586 batch mAP 0.603118896 batch PCKh 0.25\n",
      "Trained batch 1630 batch loss 0.561543 batch mAP 0.467346191 batch PCKh 0.5625\n",
      "Trained batch 1631 batch loss 0.528254867 batch mAP 0.486969 batch PCKh 0.6875\n",
      "Trained batch 1632 batch loss 0.505795598 batch mAP 0.539032 batch PCKh 0.75\n",
      "Trained batch 1633 batch loss 0.515903354 batch mAP 0.531799316 batch PCKh 0.25\n",
      "Trained batch 1634 batch loss 0.537983 batch mAP 0.525848389 batch PCKh 0.5625\n",
      "Trained batch 1635 batch loss 0.613657713 batch mAP 0.473052979 batch PCKh 0.5\n",
      "Trained batch 1636 batch loss 0.576667726 batch mAP 0.455413818 batch PCKh 0.625\n",
      "Trained batch 1637 batch loss 0.548666477 batch mAP 0.556182861 batch PCKh 0.5625\n",
      "Trained batch 1638 batch loss 0.536582768 batch mAP 0.562774658 batch PCKh 0.1875\n",
      "Trained batch 1639 batch loss 0.61552453 batch mAP 0.538452148 batch PCKh 0.5\n",
      "Trained batch 1640 batch loss 0.575345 batch mAP 0.523468 batch PCKh 0.3125\n",
      "Trained batch 1641 batch loss 0.586038589 batch mAP 0.543060303 batch PCKh 0.375\n",
      "Trained batch 1642 batch loss 0.482849181 batch mAP 0.603210449 batch PCKh 0.625\n",
      "Trained batch 1643 batch loss 0.454362392 batch mAP 0.618560791 batch PCKh 0.875\n",
      "Trained batch 1644 batch loss 0.436948478 batch mAP 0.585510254 batch PCKh 0.3125\n",
      "Trained batch 1645 batch loss 0.465627283 batch mAP 0.541687 batch PCKh 0.375\n",
      "Trained batch 1646 batch loss 0.647261441 batch mAP 0.481506348 batch PCKh 0.375\n",
      "Trained batch 1647 batch loss 0.613697767 batch mAP 0.488433838 batch PCKh 0.1875\n",
      "Trained batch 1648 batch loss 0.635645628 batch mAP 0.504364 batch PCKh 0.3125\n",
      "Trained batch 1649 batch loss 0.558087111 batch mAP 0.603027344 batch PCKh 0.3125\n",
      "Trained batch 1650 batch loss 0.568683803 batch mAP 0.584503174 batch PCKh 0.1875\n",
      "Trained batch 1651 batch loss 0.546986103 batch mAP 0.636169434 batch PCKh 0.5625\n",
      "Trained batch 1652 batch loss 0.532387435 batch mAP 0.625335693 batch PCKh 0.5\n",
      "Trained batch 1653 batch loss 0.54970789 batch mAP 0.60144043 batch PCKh 0.625\n",
      "Trained batch 1654 batch loss 0.612698317 batch mAP 0.575164795 batch PCKh 0.3125\n",
      "Trained batch 1655 batch loss 0.560126126 batch mAP 0.60824585 batch PCKh 0.3125\n",
      "Trained batch 1656 batch loss 0.475366533 batch mAP 0.616577148 batch PCKh 0.0625\n",
      "Trained batch 1657 batch loss 0.536765277 batch mAP 0.572753906 batch PCKh 0.5\n",
      "Trained batch 1658 batch loss 0.539067864 batch mAP 0.562774658 batch PCKh 0.5625\n",
      "Trained batch 1659 batch loss 0.51659739 batch mAP 0.495300293 batch PCKh 0.25\n",
      "Trained batch 1660 batch loss 0.622543275 batch mAP 0.494171143 batch PCKh 0.625\n",
      "Trained batch 1661 batch loss 0.563580096 batch mAP 0.529846191 batch PCKh 0.3125\n",
      "Trained batch 1662 batch loss 0.492042452 batch mAP 0.594726562 batch PCKh 0.6875\n",
      "Trained batch 1663 batch loss 0.530590773 batch mAP 0.581054688 batch PCKh 0.25\n",
      "Trained batch 1664 batch loss 0.436732739 batch mAP 0.604217529 batch PCKh 0.3125\n",
      "Trained batch 1665 batch loss 0.579192162 batch mAP 0.578033447 batch PCKh 0.3125\n",
      "Trained batch 1666 batch loss 0.512292504 batch mAP 0.566619873 batch PCKh 0.0625\n",
      "Trained batch 1667 batch loss 0.489617556 batch mAP 0.482849121 batch PCKh 0.0625\n",
      "Trained batch 1668 batch loss 0.544732928 batch mAP 0.516937256 batch PCKh 0.4375\n",
      "Trained batch 1669 batch loss 0.558821917 batch mAP 0.55947876 batch PCKh 0.5\n",
      "Trained batch 1670 batch loss 0.447917372 batch mAP 0.627044678 batch PCKh 0.25\n",
      "Trained batch 1671 batch loss 0.51819557 batch mAP 0.560974121 batch PCKh 0.3125\n",
      "Trained batch 1672 batch loss 0.528090179 batch mAP 0.575500488 batch PCKh 0.1875\n",
      "Trained batch 1673 batch loss 0.485462695 batch mAP 0.527923584 batch PCKh 0.0625\n",
      "Trained batch 1674 batch loss 0.516923308 batch mAP 0.584838867 batch PCKh 0.125\n",
      "Trained batch 1675 batch loss 0.574495435 batch mAP 0.58505249 batch PCKh 0.1875\n",
      "Trained batch 1676 batch loss 0.573056 batch mAP 0.592437744 batch PCKh 0\n",
      "Trained batch 1677 batch loss 0.565429449 batch mAP 0.602355957 batch PCKh 0.5625\n",
      "Trained batch 1678 batch loss 0.625564277 batch mAP 0.584350586 batch PCKh 0.375\n",
      "Trained batch 1679 batch loss 0.551096201 batch mAP 0.599914551 batch PCKh 0.1875\n",
      "Trained batch 1680 batch loss 0.52266556 batch mAP 0.659179688 batch PCKh 0.75\n",
      "Trained batch 1681 batch loss 0.550812066 batch mAP 0.657409668 batch PCKh 0.25\n",
      "Trained batch 1682 batch loss 0.580518365 batch mAP 0.656738281 batch PCKh 0.25\n",
      "Trained batch 1683 batch loss 0.58161664 batch mAP 0.694854736 batch PCKh 0.6875\n",
      "Trained batch 1684 batch loss 0.580548882 batch mAP 0.663482666 batch PCKh 0.3125\n",
      "Trained batch 1685 batch loss 0.56943512 batch mAP 0.656677246 batch PCKh 0.3125\n",
      "Trained batch 1686 batch loss 0.598872781 batch mAP 0.616729736 batch PCKh 0.5\n",
      "Trained batch 1687 batch loss 0.628540277 batch mAP 0.538574219 batch PCKh 0.1875\n",
      "Trained batch 1688 batch loss 0.623880744 batch mAP 0.572021484 batch PCKh 0\n",
      "Trained batch 1689 batch loss 0.587091446 batch mAP 0.596801758 batch PCKh 0.5\n",
      "Trained batch 1690 batch loss 0.585646391 batch mAP 0.639007568 batch PCKh 0.3125\n",
      "Trained batch 1691 batch loss 0.552541494 batch mAP 0.583313 batch PCKh 0.5\n",
      "Trained batch 1692 batch loss 0.474285841 batch mAP 0.52230835 batch PCKh 0.5625\n",
      "Trained batch 1693 batch loss 0.57999742 batch mAP 0.478973389 batch PCKh 0.8125\n",
      "Trained batch 1694 batch loss 0.536479414 batch mAP 0.489440918 batch PCKh 0.75\n",
      "Trained batch 1695 batch loss 0.480688453 batch mAP 0.478149414 batch PCKh 0.4375\n",
      "Trained batch 1696 batch loss 0.498286337 batch mAP 0.559661865 batch PCKh 0.8125\n",
      "Trained batch 1697 batch loss 0.516733348 batch mAP 0.597564697 batch PCKh 0.6875\n",
      "Trained batch 1698 batch loss 0.628921092 batch mAP 0.526672363 batch PCKh 0\n",
      "Trained batch 1699 batch loss 0.607364655 batch mAP 0.575134277 batch PCKh 0.375\n",
      "Trained batch 1700 batch loss 0.561743438 batch mAP 0.543487549 batch PCKh 0.625\n",
      "Trained batch 1701 batch loss 0.576524258 batch mAP 0.621826172 batch PCKh 0.1875\n",
      "Trained batch 1702 batch loss 0.549562097 batch mAP 0.54107666 batch PCKh 0.0625\n",
      "Trained batch 1703 batch loss 0.513337493 batch mAP 0.585571289 batch PCKh 0.625\n",
      "Trained batch 1704 batch loss 0.600612819 batch mAP 0.563934326 batch PCKh 0.5\n",
      "Trained batch 1705 batch loss 0.605189383 batch mAP 0.585571289 batch PCKh 0.5625\n",
      "Trained batch 1706 batch loss 0.56516844 batch mAP 0.559143066 batch PCKh 0.1875\n",
      "Trained batch 1707 batch loss 0.626857877 batch mAP 0.488891602 batch PCKh 0\n",
      "Trained batch 1708 batch loss 0.744542897 batch mAP 0.490356445 batch PCKh 0\n",
      "Trained batch 1709 batch loss 0.683622241 batch mAP 0.487487793 batch PCKh 0\n",
      "Trained batch 1710 batch loss 0.632412255 batch mAP 0.531463623 batch PCKh 0.1875\n",
      "Trained batch 1711 batch loss 0.614923596 batch mAP 0.471679688 batch PCKh 0.125\n",
      "Trained batch 1712 batch loss 0.638250232 batch mAP 0.482971191 batch PCKh 0.1875\n",
      "Trained batch 1713 batch loss 0.609999716 batch mAP 0.532562256 batch PCKh 0.3125\n",
      "Trained batch 1714 batch loss 0.50271219 batch mAP 0.614959717 batch PCKh 0.375\n",
      "Trained batch 1715 batch loss 0.532035053 batch mAP 0.597747803 batch PCKh 0.375\n",
      "Trained batch 1716 batch loss 0.506472647 batch mAP 0.574951172 batch PCKh 0.3125\n",
      "Trained batch 1717 batch loss 0.541833818 batch mAP 0.567688 batch PCKh 0.6875\n",
      "Trained batch 1718 batch loss 0.645261765 batch mAP 0.492156982 batch PCKh 0.625\n",
      "Trained batch 1719 batch loss 0.600297272 batch mAP 0.506073 batch PCKh 0.375\n",
      "Trained batch 1720 batch loss 0.697557628 batch mAP 0.498687744 batch PCKh 0.125\n",
      "Trained batch 1721 batch loss 0.536144435 batch mAP 0.644714355 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1722 batch loss 0.512074292 batch mAP 0.671447754 batch PCKh 0.4375\n",
      "Trained batch 1723 batch loss 0.543345034 batch mAP 0.688690186 batch PCKh 0.375\n",
      "Trained batch 1724 batch loss 0.51173991 batch mAP 0.652008057 batch PCKh 0.625\n",
      "Trained batch 1725 batch loss 0.49268961 batch mAP 0.613861084 batch PCKh 0.8125\n",
      "Trained batch 1726 batch loss 0.523706794 batch mAP 0.626586914 batch PCKh 0.6875\n",
      "Trained batch 1727 batch loss 0.582942367 batch mAP 0.613830566 batch PCKh 0.5\n",
      "Trained batch 1728 batch loss 0.522629499 batch mAP 0.610900879 batch PCKh 0.4375\n",
      "Trained batch 1729 batch loss 0.532518685 batch mAP 0.611206055 batch PCKh 0.5\n",
      "Trained batch 1730 batch loss 0.515729666 batch mAP 0.615509033 batch PCKh 0.3125\n",
      "Trained batch 1731 batch loss 0.516907 batch mAP 0.675354 batch PCKh 0.25\n",
      "Trained batch 1732 batch loss 0.535324812 batch mAP 0.643554688 batch PCKh 0.3125\n",
      "Trained batch 1733 batch loss 0.574497283 batch mAP 0.641449 batch PCKh 0.25\n",
      "Trained batch 1734 batch loss 0.510172963 batch mAP 0.642578125 batch PCKh 0.125\n",
      "Trained batch 1735 batch loss 0.577059865 batch mAP 0.64364624 batch PCKh 0.4375\n",
      "Trained batch 1736 batch loss 0.567734718 batch mAP 0.537506104 batch PCKh 0.0625\n",
      "Trained batch 1737 batch loss 0.557435095 batch mAP 0.549133301 batch PCKh 0.4375\n",
      "Trained batch 1738 batch loss 0.599213362 batch mAP 0.595031738 batch PCKh 0.3125\n",
      "Trained batch 1739 batch loss 0.57686609 batch mAP 0.547424316 batch PCKh 0.375\n",
      "Trained batch 1740 batch loss 0.528740764 batch mAP 0.553894043 batch PCKh 0.1875\n",
      "Trained batch 1741 batch loss 0.557005048 batch mAP 0.497772217 batch PCKh 0.6875\n",
      "Trained batch 1742 batch loss 0.521451 batch mAP 0.602325439 batch PCKh 0.75\n",
      "Trained batch 1743 batch loss 0.600940645 batch mAP 0.490509033 batch PCKh 0.6875\n",
      "Trained batch 1744 batch loss 0.599425197 batch mAP 0.596374512 batch PCKh 0.8125\n",
      "Trained batch 1745 batch loss 0.644647598 batch mAP 0.541564941 batch PCKh 0.5\n",
      "Trained batch 1746 batch loss 0.567055404 batch mAP 0.596435547 batch PCKh 0.5625\n",
      "Trained batch 1747 batch loss 0.50062871 batch mAP 0.62210083 batch PCKh 0.5\n",
      "Trained batch 1748 batch loss 0.593088388 batch mAP 0.579925537 batch PCKh 0.6875\n",
      "Trained batch 1749 batch loss 0.526660681 batch mAP 0.567749 batch PCKh 0.4375\n",
      "Trained batch 1750 batch loss 0.525746703 batch mAP 0.58026123 batch PCKh 0.375\n",
      "Trained batch 1751 batch loss 0.580548584 batch mAP 0.581207275 batch PCKh 0.75\n",
      "Trained batch 1752 batch loss 0.620171785 batch mAP 0.514465332 batch PCKh 0.5\n",
      "Trained batch 1753 batch loss 0.511322856 batch mAP 0.575439453 batch PCKh 0.3125\n",
      "Trained batch 1754 batch loss 0.575792 batch mAP 0.522766113 batch PCKh 0.6875\n",
      "Trained batch 1755 batch loss 0.477678716 batch mAP 0.60458374 batch PCKh 0.1875\n",
      "Trained batch 1756 batch loss 0.501359344 batch mAP 0.656738281 batch PCKh 0.25\n",
      "Trained batch 1757 batch loss 0.523913622 batch mAP 0.634857178 batch PCKh 0.375\n",
      "Trained batch 1758 batch loss 0.523594081 batch mAP 0.62689209 batch PCKh 0.75\n",
      "Trained batch 1759 batch loss 0.552484751 batch mAP 0.605255127 batch PCKh 0.625\n",
      "Trained batch 1760 batch loss 0.618753433 batch mAP 0.540740967 batch PCKh 0.375\n",
      "Trained batch 1761 batch loss 0.642764807 batch mAP 0.537689209 batch PCKh 0.625\n",
      "Trained batch 1762 batch loss 0.618500948 batch mAP 0.483551025 batch PCKh 0.75\n",
      "Trained batch 1763 batch loss 0.645949125 batch mAP 0.498443604 batch PCKh 0.625\n",
      "Trained batch 1764 batch loss 0.716908038 batch mAP 0.523895264 batch PCKh 0.3125\n",
      "Trained batch 1765 batch loss 0.659816623 batch mAP 0.520202637 batch PCKh 0.3125\n",
      "Trained batch 1766 batch loss 0.633430839 batch mAP 0.536346436 batch PCKh 0.3125\n",
      "Trained batch 1767 batch loss 0.61043793 batch mAP 0.522003174 batch PCKh 0.625\n",
      "Trained batch 1768 batch loss 0.591904 batch mAP 0.554595947 batch PCKh 0.625\n",
      "Trained batch 1769 batch loss 0.545746624 batch mAP 0.487335205 batch PCKh 0.5625\n",
      "Trained batch 1770 batch loss 0.54402411 batch mAP 0.471984863 batch PCKh 0\n",
      "Trained batch 1771 batch loss 0.516887069 batch mAP 0.464660645 batch PCKh 0.5625\n",
      "Trained batch 1772 batch loss 0.465031654 batch mAP 0.55267334 batch PCKh 0.375\n",
      "Trained batch 1773 batch loss 0.536301494 batch mAP 0.49432373 batch PCKh 0.5\n",
      "Trained batch 1774 batch loss 0.430737555 batch mAP 0.510498047 batch PCKh 0\n",
      "Trained batch 1775 batch loss 0.417547524 batch mAP 0.555999756 batch PCKh 0.5\n",
      "Trained batch 1776 batch loss 0.428535163 batch mAP 0.56036377 batch PCKh 0.75\n",
      "Trained batch 1777 batch loss 0.412153721 batch mAP 0.597106934 batch PCKh 0.75\n",
      "Trained batch 1778 batch loss 0.460886538 batch mAP 0.585876465 batch PCKh 0\n",
      "Trained batch 1779 batch loss 0.468308687 batch mAP 0.610015869 batch PCKh 0.3125\n",
      "Trained batch 1780 batch loss 0.438253134 batch mAP 0.732299805 batch PCKh 0.375\n",
      "Trained batch 1781 batch loss 0.372421086 batch mAP 0.688201904 batch PCKh 0.3125\n",
      "Trained batch 1782 batch loss 0.541014135 batch mAP 0.620636 batch PCKh 0.25\n",
      "Trained batch 1783 batch loss 0.60517472 batch mAP 0.620880127 batch PCKh 0.625\n",
      "Trained batch 1784 batch loss 0.640078366 batch mAP 0.560119629 batch PCKh 0.375\n",
      "Trained batch 1785 batch loss 0.59999 batch mAP 0.570404053 batch PCKh 0.625\n",
      "Trained batch 1786 batch loss 0.591368437 batch mAP 0.566650391 batch PCKh 0.5\n",
      "Trained batch 1787 batch loss 0.603291512 batch mAP 0.589386 batch PCKh 0.3125\n",
      "Trained batch 1788 batch loss 0.563522 batch mAP 0.595794678 batch PCKh 0.5625\n",
      "Trained batch 1789 batch loss 0.588249922 batch mAP 0.597442627 batch PCKh 0.5\n",
      "Trained batch 1790 batch loss 0.602860928 batch mAP 0.598724365 batch PCKh 0.625\n",
      "Trained batch 1791 batch loss 0.603621602 batch mAP 0.6065979 batch PCKh 0.3125\n",
      "Trained batch 1792 batch loss 0.530969679 batch mAP 0.558807373 batch PCKh 0.4375\n",
      "Trained batch 1793 batch loss 0.579628229 batch mAP 0.584503174 batch PCKh 0.8125\n",
      "Trained batch 1794 batch loss 0.499612391 batch mAP 0.592193604 batch PCKh 0.8125\n",
      "Trained batch 1795 batch loss 0.492559463 batch mAP 0.551849365 batch PCKh 0.75\n",
      "Trained batch 1796 batch loss 0.539770246 batch mAP 0.552978516 batch PCKh 0.875\n",
      "Trained batch 1797 batch loss 0.525208354 batch mAP 0.598083496 batch PCKh 0.8125\n",
      "Trained batch 1798 batch loss 0.486258298 batch mAP 0.568939209 batch PCKh 0.875\n",
      "Trained batch 1799 batch loss 0.529148519 batch mAP 0.567199707 batch PCKh 0.625\n",
      "Trained batch 1800 batch loss 0.621705592 batch mAP 0.600830078 batch PCKh 0.4375\n",
      "Trained batch 1801 batch loss 0.534972608 batch mAP 0.580566406 batch PCKh 0.6875\n",
      "Trained batch 1802 batch loss 0.544130206 batch mAP 0.60144043 batch PCKh 0.375\n",
      "Trained batch 1803 batch loss 0.589204073 batch mAP 0.574951172 batch PCKh 0.25\n",
      "Trained batch 1804 batch loss 0.609276533 batch mAP 0.558136 batch PCKh 0.625\n",
      "Trained batch 1805 batch loss 0.589077592 batch mAP 0.560821533 batch PCKh 0.6875\n",
      "Trained batch 1806 batch loss 0.612831 batch mAP 0.551055908 batch PCKh 0.125\n",
      "Trained batch 1807 batch loss 0.603279948 batch mAP 0.583099365 batch PCKh 0.5625\n",
      "Trained batch 1808 batch loss 0.528443456 batch mAP 0.526733398 batch PCKh 0.625\n",
      "Trained batch 1809 batch loss 0.606248736 batch mAP 0.500762939 batch PCKh 0.5\n",
      "Trained batch 1810 batch loss 0.619630098 batch mAP 0.514801 batch PCKh 0.1875\n",
      "Trained batch 1811 batch loss 0.629018605 batch mAP 0.442169189 batch PCKh 0.75\n",
      "Trained batch 1812 batch loss 0.577613413 batch mAP 0.436401367 batch PCKh 0.4375\n",
      "Trained batch 1813 batch loss 0.644954205 batch mAP 0.408355713 batch PCKh 0.6875\n",
      "Trained batch 1814 batch loss 0.661906481 batch mAP 0.477539062 batch PCKh 0.6875\n",
      "Trained batch 1815 batch loss 0.6603 batch mAP 0.461914062 batch PCKh 0.5\n",
      "Trained batch 1816 batch loss 0.587675571 batch mAP 0.44430542 batch PCKh 0.75\n",
      "Trained batch 1817 batch loss 0.609526336 batch mAP 0.492614746 batch PCKh 0.625\n",
      "Trained batch 1818 batch loss 0.680628181 batch mAP 0.520690918 batch PCKh 0.1875\n",
      "Trained batch 1819 batch loss 0.651775837 batch mAP 0.486755371 batch PCKh 0.5\n",
      "Trained batch 1820 batch loss 0.585004389 batch mAP 0.51675415 batch PCKh 0.5\n",
      "Trained batch 1821 batch loss 0.567194 batch mAP 0.500366211 batch PCKh 0.6875\n",
      "Trained batch 1822 batch loss 0.503819585 batch mAP 0.529052734 batch PCKh 0.4375\n",
      "Trained batch 1823 batch loss 0.631510794 batch mAP 0.481140137 batch PCKh 0.25\n",
      "Trained batch 1824 batch loss 0.616081655 batch mAP 0.488708496 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1825 batch loss 0.566884041 batch mAP 0.536804199 batch PCKh 0.6875\n",
      "Trained batch 1826 batch loss 0.583225131 batch mAP 0.529144287 batch PCKh 0.6875\n",
      "Trained batch 1827 batch loss 0.615483046 batch mAP 0.497009277 batch PCKh 0.625\n",
      "Trained batch 1828 batch loss 0.66438067 batch mAP 0.505554199 batch PCKh 0.875\n",
      "Trained batch 1829 batch loss 0.586750269 batch mAP 0.533172607 batch PCKh 0.5\n",
      "Trained batch 1830 batch loss 0.558274865 batch mAP 0.518341064 batch PCKh 0.75\n",
      "Trained batch 1831 batch loss 0.575618625 batch mAP 0.519714355 batch PCKh 0.625\n",
      "Trained batch 1832 batch loss 0.606798947 batch mAP 0.482757568 batch PCKh 0.6875\n",
      "Trained batch 1833 batch loss 0.584451556 batch mAP 0.500610352 batch PCKh 0.5625\n",
      "Trained batch 1834 batch loss 0.610288322 batch mAP 0.516021729 batch PCKh 0.875\n",
      "Trained batch 1835 batch loss 0.585015476 batch mAP 0.495117188 batch PCKh 0.6875\n",
      "Trained batch 1836 batch loss 0.536032915 batch mAP 0.582336426 batch PCKh 0.625\n",
      "Trained batch 1837 batch loss 0.634743094 batch mAP 0.514251709 batch PCKh 0.8125\n",
      "Trained batch 1838 batch loss 0.564842105 batch mAP 0.581329346 batch PCKh 0.75\n",
      "Trained batch 1839 batch loss 0.583750784 batch mAP 0.52243042 batch PCKh 0.6875\n",
      "Trained batch 1840 batch loss 0.605291486 batch mAP 0.542694092 batch PCKh 0.3125\n",
      "Trained batch 1841 batch loss 0.591838121 batch mAP 0.576843262 batch PCKh 0.75\n",
      "Trained batch 1842 batch loss 0.551995516 batch mAP 0.576202393 batch PCKh 0.1875\n",
      "Trained batch 1843 batch loss 0.541241646 batch mAP 0.556304932 batch PCKh 0.25\n",
      "Trained batch 1844 batch loss 0.572992623 batch mAP 0.582702637 batch PCKh 0.375\n",
      "Trained batch 1845 batch loss 0.559040964 batch mAP 0.552490234 batch PCKh 0.75\n",
      "Trained batch 1846 batch loss 0.60226506 batch mAP 0.565643311 batch PCKh 0.25\n",
      "Trained batch 1847 batch loss 0.649197221 batch mAP 0.584777832 batch PCKh 0.1875\n",
      "Trained batch 1848 batch loss 0.609843612 batch mAP 0.571594238 batch PCKh 0.8125\n",
      "Trained batch 1849 batch loss 0.596068382 batch mAP 0.533477783 batch PCKh 0.125\n",
      "Trained batch 1850 batch loss 0.543032 batch mAP 0.6277771 batch PCKh 0.5\n",
      "Trained batch 1851 batch loss 0.622138619 batch mAP 0.598876953 batch PCKh 0.875\n",
      "Trained batch 1852 batch loss 0.572394252 batch mAP 0.641693115 batch PCKh 0.75\n",
      "Trained batch 1853 batch loss 0.466218263 batch mAP 0.667816162 batch PCKh 0.25\n",
      "Trained batch 1854 batch loss 0.519842923 batch mAP 0.642181396 batch PCKh 0.375\n",
      "Trained batch 1855 batch loss 0.441537946 batch mAP 0.705413818 batch PCKh 0.4375\n",
      "Trained batch 1856 batch loss 0.447404087 batch mAP 0.682830811 batch PCKh 0.1875\n",
      "Trained batch 1857 batch loss 0.530752599 batch mAP 0.674621582 batch PCKh 0.375\n",
      "Trained batch 1858 batch loss 0.547707 batch mAP 0.654907227 batch PCKh 0.3125\n",
      "Trained batch 1859 batch loss 0.511756659 batch mAP 0.643035889 batch PCKh 0.125\n",
      "Trained batch 1860 batch loss 0.499663234 batch mAP 0.604949951 batch PCKh 0.125\n",
      "Trained batch 1861 batch loss 0.550003827 batch mAP 0.531341553 batch PCKh 0.5625\n",
      "Trained batch 1862 batch loss 0.618037343 batch mAP 0.550048828 batch PCKh 0.5\n",
      "Trained batch 1863 batch loss 0.616583169 batch mAP 0.517364502 batch PCKh 0.3125\n",
      "Trained batch 1864 batch loss 0.640626132 batch mAP 0.515808105 batch PCKh 0.0625\n",
      "Trained batch 1865 batch loss 0.588088453 batch mAP 0.574829102 batch PCKh 0.5\n",
      "Trained batch 1866 batch loss 0.653537631 batch mAP 0.557678223 batch PCKh 0.3125\n",
      "Trained batch 1867 batch loss 0.720579505 batch mAP 0.532074 batch PCKh 0.75\n",
      "Trained batch 1868 batch loss 0.650126 batch mAP 0.572143555 batch PCKh 0.1875\n",
      "Trained batch 1869 batch loss 0.552129865 batch mAP 0.571136475 batch PCKh 0.5625\n",
      "Trained batch 1870 batch loss 0.61837244 batch mAP 0.507782 batch PCKh 0.6875\n",
      "Trained batch 1871 batch loss 0.605496 batch mAP 0.507782 batch PCKh 0.25\n",
      "Trained batch 1872 batch loss 0.58748281 batch mAP 0.558898926 batch PCKh 0.875\n",
      "Trained batch 1873 batch loss 0.617070496 batch mAP 0.483398438 batch PCKh 0.5625\n",
      "Trained batch 1874 batch loss 0.62144345 batch mAP 0.529388428 batch PCKh 0.3125\n",
      "Trained batch 1875 batch loss 0.626456738 batch mAP 0.449768066 batch PCKh 0.625\n",
      "Trained batch 1876 batch loss 0.6952613 batch mAP 0.485076904 batch PCKh 0.25\n",
      "Trained batch 1877 batch loss 0.622721791 batch mAP 0.494110107 batch PCKh 0.75\n",
      "Trained batch 1878 batch loss 0.57025528 batch mAP 0.508544922 batch PCKh 0.5625\n",
      "Trained batch 1879 batch loss 0.542625904 batch mAP 0.575958252 batch PCKh 0.375\n",
      "Trained batch 1880 batch loss 0.515521169 batch mAP 0.543670654 batch PCKh 0.375\n",
      "Trained batch 1881 batch loss 0.627662897 batch mAP 0.489440918 batch PCKh 0.3125\n",
      "Trained batch 1882 batch loss 0.529646277 batch mAP 0.500274658 batch PCKh 0.3125\n",
      "Trained batch 1883 batch loss 0.471499681 batch mAP 0.518615723 batch PCKh 0.25\n",
      "Trained batch 1884 batch loss 0.489622355 batch mAP 0.509429932 batch PCKh 0.625\n",
      "Trained batch 1885 batch loss 0.473788 batch mAP 0.500396729 batch PCKh 0.5625\n",
      "Trained batch 1886 batch loss 0.39947778 batch mAP 0.587860107 batch PCKh 0.5\n",
      "Trained batch 1887 batch loss 0.511749804 batch mAP 0.565612793 batch PCKh 0.3125\n",
      "Trained batch 1888 batch loss 0.502011 batch mAP 0.5625 batch PCKh 0.75\n",
      "Trained batch 1889 batch loss 0.58048445 batch mAP 0.509368896 batch PCKh 0.8125\n",
      "Trained batch 1890 batch loss 0.564909697 batch mAP 0.516601562 batch PCKh 0.75\n",
      "Trained batch 1891 batch loss 0.47955 batch mAP 0.527557373 batch PCKh 0.5625\n",
      "Trained batch 1892 batch loss 0.393500417 batch mAP 0.604156494 batch PCKh 0.5625\n",
      "Trained batch 1893 batch loss 0.359615088 batch mAP 0.625671387 batch PCKh 0.5\n",
      "Trained batch 1894 batch loss 0.405735224 batch mAP 0.606170654 batch PCKh 0.6875\n",
      "Trained batch 1895 batch loss 0.432055384 batch mAP 0.571868896 batch PCKh 0\n",
      "Trained batch 1896 batch loss 0.405835152 batch mAP 0.584655762 batch PCKh 0\n",
      "Trained batch 1897 batch loss 0.39971751 batch mAP 0.605011 batch PCKh 0\n",
      "Trained batch 1898 batch loss 0.478683531 batch mAP 0.570037842 batch PCKh 0.5\n",
      "Trained batch 1899 batch loss 0.491549134 batch mAP 0.566833496 batch PCKh 0.5\n",
      "Trained batch 1900 batch loss 0.536346793 batch mAP 0.579406738 batch PCKh 0.1875\n",
      "Trained batch 1901 batch loss 0.576588511 batch mAP 0.589294434 batch PCKh 0.6875\n",
      "Trained batch 1902 batch loss 0.501670063 batch mAP 0.648132324 batch PCKh 0.4375\n",
      "Trained batch 1903 batch loss 0.592472434 batch mAP 0.600921631 batch PCKh 0.3125\n",
      "Trained batch 1904 batch loss 0.527934849 batch mAP 0.620391846 batch PCKh 0.3125\n",
      "Trained batch 1905 batch loss 0.580245852 batch mAP 0.626037598 batch PCKh 0.25\n",
      "Trained batch 1906 batch loss 0.511268377 batch mAP 0.583892822 batch PCKh 0.5\n",
      "Trained batch 1907 batch loss 0.478546262 batch mAP 0.632751465 batch PCKh 0.625\n",
      "Trained batch 1908 batch loss 0.582131624 batch mAP 0.608215332 batch PCKh 0.375\n",
      "Trained batch 1909 batch loss 0.599384665 batch mAP 0.563964844 batch PCKh 0.3125\n",
      "Trained batch 1910 batch loss 0.64577651 batch mAP 0.527740479 batch PCKh 0.1875\n",
      "Trained batch 1911 batch loss 0.648253202 batch mAP 0.495605469 batch PCKh 0.25\n",
      "Trained batch 1912 batch loss 0.665649116 batch mAP 0.540893555 batch PCKh 0.375\n",
      "Trained batch 1913 batch loss 0.64229548 batch mAP 0.519470215 batch PCKh 0.4375\n",
      "Trained batch 1914 batch loss 0.615943789 batch mAP 0.503082275 batch PCKh 0.25\n",
      "Trained batch 1915 batch loss 0.619821131 batch mAP 0.502807617 batch PCKh 0.75\n",
      "Trained batch 1916 batch loss 0.504279137 batch mAP 0.521179199 batch PCKh 0.6875\n",
      "Trained batch 1917 batch loss 0.532684565 batch mAP 0.395690918 batch PCKh 0.1875\n",
      "Trained batch 1918 batch loss 0.578884244 batch mAP 0.395263672 batch PCKh 0.1875\n",
      "Trained batch 1919 batch loss 0.606122851 batch mAP 0.389862061 batch PCKh 0.3125\n",
      "Trained batch 1920 batch loss 0.560870171 batch mAP 0.474853516 batch PCKh 0.5625\n",
      "Trained batch 1921 batch loss 0.581170678 batch mAP 0.519104 batch PCKh 0.8125\n",
      "Trained batch 1922 batch loss 0.528571308 batch mAP 0.587188721 batch PCKh 0.5\n",
      "Trained batch 1923 batch loss 0.527299285 batch mAP 0.629669189 batch PCKh 0.5\n",
      "Trained batch 1924 batch loss 0.570052266 batch mAP 0.631530762 batch PCKh 0.1875\n",
      "Trained batch 1925 batch loss 0.579031289 batch mAP 0.580413818 batch PCKh 0.5\n",
      "Trained batch 1926 batch loss 0.518040061 batch mAP 0.626556396 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1927 batch loss 0.630881846 batch mAP 0.565032959 batch PCKh 0.3125\n",
      "Trained batch 1928 batch loss 0.624299467 batch mAP 0.543823242 batch PCKh 0.5\n",
      "Trained batch 1929 batch loss 0.625713468 batch mAP 0.463439941 batch PCKh 0.125\n",
      "Trained batch 1930 batch loss 0.54979974 batch mAP 0.533935547 batch PCKh 0.25\n",
      "Trained batch 1931 batch loss 0.587342381 batch mAP 0.540893555 batch PCKh 0.5625\n",
      "Trained batch 1932 batch loss 0.527344942 batch mAP 0.568756104 batch PCKh 0\n",
      "Trained batch 1933 batch loss 0.633763433 batch mAP 0.55645752 batch PCKh 0.6875\n",
      "Trained batch 1934 batch loss 0.603360593 batch mAP 0.62286377 batch PCKh 0.375\n",
      "Trained batch 1935 batch loss 0.613192439 batch mAP 0.590148926 batch PCKh 0.3125\n",
      "Trained batch 1936 batch loss 0.517533779 batch mAP 0.594665527 batch PCKh 0.5625\n",
      "Trained batch 1937 batch loss 0.469698966 batch mAP 0.67099 batch PCKh 0.5625\n",
      "Trained batch 1938 batch loss 0.504902 batch mAP 0.634765625 batch PCKh 0.4375\n",
      "Trained batch 1939 batch loss 0.524405539 batch mAP 0.651489258 batch PCKh 0.5\n",
      "Trained batch 1940 batch loss 0.5457443 batch mAP 0.621032715 batch PCKh 0.5625\n",
      "Trained batch 1941 batch loss 0.521947265 batch mAP 0.660644531 batch PCKh 0.375\n",
      "Trained batch 1942 batch loss 0.491761684 batch mAP 0.690582275 batch PCKh 0.8125\n",
      "Trained batch 1943 batch loss 0.513731599 batch mAP 0.649749756 batch PCKh 0.5625\n",
      "Trained batch 1944 batch loss 0.494574964 batch mAP 0.718505859 batch PCKh 0.5625\n",
      "Trained batch 1945 batch loss 0.494520068 batch mAP 0.681274414 batch PCKh 0.375\n",
      "Trained batch 1946 batch loss 0.544213355 batch mAP 0.667053223 batch PCKh 0.25\n",
      "Trained batch 1947 batch loss 0.509407043 batch mAP 0.692840576 batch PCKh 0.5\n",
      "Trained batch 1948 batch loss 0.459214061 batch mAP 0.657226562 batch PCKh 0.8125\n",
      "Trained batch 1949 batch loss 0.545137703 batch mAP 0.673126221 batch PCKh 0.5\n",
      "Trained batch 1950 batch loss 0.586619258 batch mAP 0.602722168 batch PCKh 0.5\n",
      "Trained batch 1951 batch loss 0.612444341 batch mAP 0.538543701 batch PCKh 0.875\n",
      "Trained batch 1952 batch loss 0.599432826 batch mAP 0.510864258 batch PCKh 0.3125\n",
      "Trained batch 1953 batch loss 0.528827906 batch mAP 0.526153564 batch PCKh 0.875\n",
      "Trained batch 1954 batch loss 0.581470191 batch mAP 0.485839844 batch PCKh 0.6875\n",
      "Trained batch 1955 batch loss 0.581864178 batch mAP 0.497924805 batch PCKh 0.625\n",
      "Trained batch 1956 batch loss 0.564527512 batch mAP 0.595031738 batch PCKh 0.6875\n",
      "Trained batch 1957 batch loss 0.611348867 batch mAP 0.557312 batch PCKh 0.5\n",
      "Trained batch 1958 batch loss 0.513985515 batch mAP 0.665008545 batch PCKh 0.25\n",
      "Trained batch 1959 batch loss 0.485134393 batch mAP 0.675201416 batch PCKh 0.375\n",
      "Trained batch 1960 batch loss 0.537795126 batch mAP 0.632019043 batch PCKh 0.25\n",
      "Trained batch 1961 batch loss 0.512641907 batch mAP 0.653869629 batch PCKh 0.5\n",
      "Trained batch 1962 batch loss 0.561516225 batch mAP 0.565002441 batch PCKh 0.4375\n",
      "Trained batch 1963 batch loss 0.547835708 batch mAP 0.634490967 batch PCKh 0.8125\n",
      "Trained batch 1964 batch loss 0.576373219 batch mAP 0.612854 batch PCKh 0.5625\n",
      "Trained batch 1965 batch loss 0.569258511 batch mAP 0.614074707 batch PCKh 0.625\n",
      "Trained batch 1966 batch loss 0.538085461 batch mAP 0.545806885 batch PCKh 0.5\n",
      "Trained batch 1967 batch loss 0.592054069 batch mAP 0.526489258 batch PCKh 0.625\n",
      "Trained batch 1968 batch loss 0.546969414 batch mAP 0.529876709 batch PCKh 0.5\n",
      "Trained batch 1969 batch loss 0.550011 batch mAP 0.593017578 batch PCKh 0.5625\n",
      "Trained batch 1970 batch loss 0.554574966 batch mAP 0.544067383 batch PCKh 0.375\n",
      "Trained batch 1971 batch loss 0.651902616 batch mAP 0.522491455 batch PCKh 0.625\n",
      "Trained batch 1972 batch loss 0.594660938 batch mAP 0.533966064 batch PCKh 0.75\n",
      "Trained batch 1973 batch loss 0.586257935 batch mAP 0.543792725 batch PCKh 0.5625\n",
      "Trained batch 1974 batch loss 0.627485514 batch mAP 0.572509766 batch PCKh 0.1875\n",
      "Trained batch 1975 batch loss 0.595491886 batch mAP 0.585174561 batch PCKh 0.25\n",
      "Trained batch 1976 batch loss 0.554432 batch mAP 0.566619873 batch PCKh 0.3125\n",
      "Trained batch 1977 batch loss 0.583434224 batch mAP 0.536346436 batch PCKh 0.5625\n",
      "Trained batch 1978 batch loss 0.591636896 batch mAP 0.553833 batch PCKh 0.75\n",
      "Trained batch 1979 batch loss 0.635672 batch mAP 0.541717529 batch PCKh 0.1875\n",
      "Trained batch 1980 batch loss 0.704623461 batch mAP 0.515289307 batch PCKh 0.1875\n",
      "Trained batch 1981 batch loss 0.589626 batch mAP 0.52722168 batch PCKh 0.4375\n",
      "Trained batch 1982 batch loss 0.541320801 batch mAP 0.465026855 batch PCKh 0\n",
      "Trained batch 1983 batch loss 0.510501146 batch mAP 0.464874268 batch PCKh 0.3125\n",
      "Trained batch 1984 batch loss 0.544361055 batch mAP 0.559570312 batch PCKh 0.5\n",
      "Trained batch 1985 batch loss 0.510454476 batch mAP 0.617889404 batch PCKh 0.3125\n",
      "Trained batch 1986 batch loss 0.537505269 batch mAP 0.529388428 batch PCKh 0.5625\n",
      "Trained batch 1987 batch loss 0.527592182 batch mAP 0.565032959 batch PCKh 0.25\n",
      "Trained batch 1988 batch loss 0.557318687 batch mAP 0.531738281 batch PCKh 0.3125\n",
      "Trained batch 1989 batch loss 0.482187927 batch mAP 0.571533203 batch PCKh 0.5\n",
      "Trained batch 1990 batch loss 0.52554661 batch mAP 0.611145 batch PCKh 0.25\n",
      "Trained batch 1991 batch loss 0.476556778 batch mAP 0.582336426 batch PCKh 0.1875\n",
      "Trained batch 1992 batch loss 0.55361557 batch mAP 0.562591553 batch PCKh 0.5\n",
      "Trained batch 1993 batch loss 0.610969365 batch mAP 0.57434082 batch PCKh 0.6875\n",
      "Trained batch 1994 batch loss 0.502141416 batch mAP 0.600280762 batch PCKh 0.5\n",
      "Trained batch 1995 batch loss 0.647573113 batch mAP 0.526733398 batch PCKh 0.375\n",
      "Trained batch 1996 batch loss 0.596212149 batch mAP 0.568389893 batch PCKh 0.25\n",
      "Trained batch 1997 batch loss 0.68368566 batch mAP 0.501251221 batch PCKh 0.0625\n",
      "Trained batch 1998 batch loss 0.670407593 batch mAP 0.543396 batch PCKh 0.375\n",
      "Trained batch 1999 batch loss 0.532071888 batch mAP 0.56942749 batch PCKh 0.6875\n",
      "Trained batch 2000 batch loss 0.54037118 batch mAP 0.524292 batch PCKh 0.1875\n",
      "Trained batch 2001 batch loss 0.534281492 batch mAP 0.558898926 batch PCKh 0.125\n",
      "Trained batch 2002 batch loss 0.490577698 batch mAP 0.584869385 batch PCKh 0.5\n",
      "Trained batch 2003 batch loss 0.422235608 batch mAP 0.564025879 batch PCKh 0\n",
      "Trained batch 2004 batch loss 0.546152115 batch mAP 0.575378418 batch PCKh 0.125\n",
      "Trained batch 2005 batch loss 0.56320107 batch mAP 0.526031494 batch PCKh 0.1875\n",
      "Trained batch 2006 batch loss 0.593194604 batch mAP 0.580719 batch PCKh 0.125\n",
      "Trained batch 2007 batch loss 0.657784522 batch mAP 0.517974854 batch PCKh 0.125\n",
      "Trained batch 2008 batch loss 0.563936353 batch mAP 0.571594238 batch PCKh 0.625\n",
      "Trained batch 2009 batch loss 0.615687251 batch mAP 0.563018799 batch PCKh 0.5625\n",
      "Trained batch 2010 batch loss 0.668714643 batch mAP 0.529968262 batch PCKh 0.125\n",
      "Trained batch 2011 batch loss 0.578058958 batch mAP 0.521179199 batch PCKh 0.5\n",
      "Trained batch 2012 batch loss 0.585389 batch mAP 0.528076172 batch PCKh 0.1875\n",
      "Trained batch 2013 batch loss 0.540280223 batch mAP 0.511169434 batch PCKh 0.125\n",
      "Trained batch 2014 batch loss 0.549682617 batch mAP 0.619689941 batch PCKh 0.3125\n",
      "Trained batch 2015 batch loss 0.599635959 batch mAP 0.61505127 batch PCKh 0.3125\n",
      "Trained batch 2016 batch loss 0.541680813 batch mAP 0.615112305 batch PCKh 0.0625\n",
      "Trained batch 2017 batch loss 0.544660449 batch mAP 0.632049561 batch PCKh 0.6875\n",
      "Trained batch 2018 batch loss 0.608707309 batch mAP 0.57131958 batch PCKh 0.3125\n",
      "Trained batch 2019 batch loss 0.634878635 batch mAP 0.46987915 batch PCKh 0.25\n",
      "Trained batch 2020 batch loss 0.514785171 batch mAP 0.471160889 batch PCKh 0.375\n",
      "Trained batch 2021 batch loss 0.565232456 batch mAP 0.448516846 batch PCKh 0.125\n",
      "Trained batch 2022 batch loss 0.554195583 batch mAP 0.489898682 batch PCKh 0.1875\n",
      "Trained batch 2023 batch loss 0.590183616 batch mAP 0.499603271 batch PCKh 0.625\n",
      "Trained batch 2024 batch loss 0.558942437 batch mAP 0.574462891 batch PCKh 0.6875\n",
      "Trained batch 2025 batch loss 0.583230197 batch mAP 0.567688 batch PCKh 0.5\n",
      "Trained batch 2026 batch loss 0.570161343 batch mAP 0.563812256 batch PCKh 0.875\n",
      "Trained batch 2027 batch loss 0.535258532 batch mAP 0.542938232 batch PCKh 0.5\n",
      "Trained batch 2028 batch loss 0.605419397 batch mAP 0.54006958 batch PCKh 0.75\n",
      "Trained batch 2029 batch loss 0.606811821 batch mAP 0.545623779 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2030 batch loss 0.556812 batch mAP 0.602996826 batch PCKh 0.5\n",
      "Trained batch 2031 batch loss 0.567252636 batch mAP 0.594055176 batch PCKh 0.5625\n",
      "Trained batch 2032 batch loss 0.654756606 batch mAP 0.553131104 batch PCKh 0.75\n",
      "Trained batch 2033 batch loss 0.566281676 batch mAP 0.604797363 batch PCKh 0.625\n",
      "Trained batch 2034 batch loss 0.626994133 batch mAP 0.536071777 batch PCKh 0.125\n",
      "Trained batch 2035 batch loss 0.655007064 batch mAP 0.562072754 batch PCKh 0.3125\n",
      "Trained batch 2036 batch loss 0.61330837 batch mAP 0.594787598 batch PCKh 0.1875\n",
      "Trained batch 2037 batch loss 0.59663403 batch mAP 0.571258545 batch PCKh 0.75\n",
      "Trained batch 2038 batch loss 0.602892518 batch mAP 0.599823 batch PCKh 0.25\n",
      "Trained batch 2039 batch loss 0.623378098 batch mAP 0.532653809 batch PCKh 0.375\n",
      "Trained batch 2040 batch loss 0.52873826 batch mAP 0.565856934 batch PCKh 0.5\n",
      "Trained batch 2041 batch loss 0.553203583 batch mAP 0.547943115 batch PCKh 0.875\n",
      "Trained batch 2042 batch loss 0.501827717 batch mAP 0.599731445 batch PCKh 0.375\n",
      "Trained batch 2043 batch loss 0.556893706 batch mAP 0.576324463 batch PCKh 0.5625\n",
      "Trained batch 2044 batch loss 0.527943313 batch mAP 0.634735107 batch PCKh 0.625\n",
      "Trained batch 2045 batch loss 0.576347649 batch mAP 0.548522949 batch PCKh 0.25\n",
      "Trained batch 2046 batch loss 0.531092405 batch mAP 0.5965271 batch PCKh 0.6875\n",
      "Trained batch 2047 batch loss 0.476517648 batch mAP 0.618560791 batch PCKh 0.625\n",
      "Trained batch 2048 batch loss 0.529556 batch mAP 0.64364624 batch PCKh 0.75\n",
      "Trained batch 2049 batch loss 0.480645418 batch mAP 0.645263672 batch PCKh 0.25\n",
      "Trained batch 2050 batch loss 0.51752758 batch mAP 0.592865 batch PCKh 0.3125\n",
      "Trained batch 2051 batch loss 0.593450248 batch mAP 0.473175049 batch PCKh 0\n",
      "Trained batch 2052 batch loss 0.587999225 batch mAP 0.561981201 batch PCKh 0.5\n",
      "Trained batch 2053 batch loss 0.534429789 batch mAP 0.613555908 batch PCKh 0.125\n",
      "Trained batch 2054 batch loss 0.542588711 batch mAP 0.573181152 batch PCKh 0.375\n",
      "Trained batch 2055 batch loss 0.531774879 batch mAP 0.429656982 batch PCKh 0.3125\n",
      "Trained batch 2056 batch loss 0.493353069 batch mAP 0.496276855 batch PCKh 0.3125\n",
      "Trained batch 2057 batch loss 0.422014058 batch mAP 0.493743896 batch PCKh 0\n",
      "Trained batch 2058 batch loss 0.408144891 batch mAP 0.549102783 batch PCKh 0.3125\n",
      "Trained batch 2059 batch loss 0.385989249 batch mAP 0.55847168 batch PCKh 0\n",
      "Trained batch 2060 batch loss 0.476353854 batch mAP 0.517730713 batch PCKh 0\n",
      "Trained batch 2061 batch loss 0.479332358 batch mAP 0.567047119 batch PCKh 0.1875\n",
      "Trained batch 2062 batch loss 0.673962653 batch mAP 0.491210938 batch PCKh 0.0625\n",
      "Trained batch 2063 batch loss 0.615094781 batch mAP 0.477050781 batch PCKh 0.5\n",
      "Trained batch 2064 batch loss 0.550612867 batch mAP 0.556030273 batch PCKh 0.6875\n",
      "Trained batch 2065 batch loss 0.526239932 batch mAP 0.562652588 batch PCKh 0.6875\n",
      "Trained batch 2066 batch loss 0.530033529 batch mAP 0.575439453 batch PCKh 0.5\n",
      "Trained batch 2067 batch loss 0.536846519 batch mAP 0.448120117 batch PCKh 0\n",
      "Trained batch 2068 batch loss 0.528879166 batch mAP 0.490600586 batch PCKh 0.375\n",
      "Trained batch 2069 batch loss 0.567757249 batch mAP 0.532653809 batch PCKh 0.8125\n",
      "Trained batch 2070 batch loss 0.532431 batch mAP 0.59475708 batch PCKh 0.75\n",
      "Trained batch 2071 batch loss 0.502200127 batch mAP 0.58416748 batch PCKh 0.625\n",
      "Trained batch 2072 batch loss 0.449086607 batch mAP 0.562561035 batch PCKh 0.5625\n",
      "Trained batch 2073 batch loss 0.496351659 batch mAP 0.477935791 batch PCKh 0.5\n",
      "Trained batch 2074 batch loss 0.450260609 batch mAP 0.499572754 batch PCKh 0.3125\n",
      "Trained batch 2075 batch loss 0.550318122 batch mAP 0.425292969 batch PCKh 0.5625\n",
      "Trained batch 2076 batch loss 0.460623652 batch mAP 0.441650391 batch PCKh 0.375\n",
      "Trained batch 2077 batch loss 0.603404462 batch mAP 0.463806152 batch PCKh 0.0625\n",
      "Trained batch 2078 batch loss 0.557149053 batch mAP 0.483673096 batch PCKh 0\n",
      "Trained batch 2079 batch loss 0.532441378 batch mAP 0.412139893 batch PCKh 0.625\n",
      "Trained batch 2080 batch loss 0.639423609 batch mAP 0.444702148 batch PCKh 0.6875\n",
      "Trained batch 2081 batch loss 0.61416465 batch mAP 0.492095947 batch PCKh 0.375\n",
      "Trained batch 2082 batch loss 0.499570727 batch mAP 0.526886 batch PCKh 0.4375\n",
      "Trained batch 2083 batch loss 0.570060492 batch mAP 0.557067871 batch PCKh 0.375\n",
      "Trained batch 2084 batch loss 0.568497479 batch mAP 0.522155762 batch PCKh 0.5\n",
      "Trained batch 2085 batch loss 0.600475669 batch mAP 0.515319824 batch PCKh 0.5625\n",
      "Trained batch 2086 batch loss 0.573873758 batch mAP 0.520965576 batch PCKh 0.875\n",
      "Trained batch 2087 batch loss 0.564975858 batch mAP 0.505249 batch PCKh 0.75\n",
      "Trained batch 2088 batch loss 0.560650647 batch mAP 0.517150879 batch PCKh 0.6875\n",
      "Trained batch 2089 batch loss 0.646630883 batch mAP 0.463989258 batch PCKh 0.125\n",
      "Trained batch 2090 batch loss 0.610809922 batch mAP 0.472412109 batch PCKh 0.375\n",
      "Trained batch 2091 batch loss 0.560896158 batch mAP 0.503356934 batch PCKh 0.75\n",
      "Trained batch 2092 batch loss 0.533682823 batch mAP 0.480804443 batch PCKh 0.3125\n",
      "Trained batch 2093 batch loss 0.449599862 batch mAP 0.466827393 batch PCKh 0.5625\n",
      "Trained batch 2094 batch loss 0.462900519 batch mAP 0.547668457 batch PCKh 0.5\n",
      "Trained batch 2095 batch loss 0.471497208 batch mAP 0.608215332 batch PCKh 0.6875\n",
      "Trained batch 2096 batch loss 0.536979795 batch mAP 0.595367432 batch PCKh 0.5\n",
      "Trained batch 2097 batch loss 0.559960783 batch mAP 0.636077881 batch PCKh 0.375\n",
      "Trained batch 2098 batch loss 0.601890326 batch mAP 0.570404053 batch PCKh 0.5625\n",
      "Trained batch 2099 batch loss 0.577511311 batch mAP 0.561004639 batch PCKh 0.25\n",
      "Trained batch 2100 batch loss 0.6333915 batch mAP 0.501159668 batch PCKh 0.75\n",
      "Trained batch 2101 batch loss 0.608947754 batch mAP 0.467712402 batch PCKh 0.3125\n",
      "Trained batch 2102 batch loss 0.596612573 batch mAP 0.432830811 batch PCKh 0.625\n",
      "Trained batch 2103 batch loss 0.556252599 batch mAP 0.484802246 batch PCKh 0.3125\n",
      "Trained batch 2104 batch loss 0.607468307 batch mAP 0.52255249 batch PCKh 0.1875\n",
      "Trained batch 2105 batch loss 0.679077148 batch mAP 0.494720459 batch PCKh 0.5\n",
      "Trained batch 2106 batch loss 0.667759717 batch mAP 0.528869629 batch PCKh 0\n",
      "Trained batch 2107 batch loss 0.618180037 batch mAP 0.543365479 batch PCKh 0.25\n",
      "Trained batch 2108 batch loss 0.570302844 batch mAP 0.598693848 batch PCKh 0.3125\n",
      "Trained batch 2109 batch loss 0.611082554 batch mAP 0.588745117 batch PCKh 0.25\n",
      "Trained batch 2110 batch loss 0.574836195 batch mAP 0.535644531 batch PCKh 0.6875\n",
      "Trained batch 2111 batch loss 0.663398862 batch mAP 0.497131348 batch PCKh 0.0625\n",
      "Trained batch 2112 batch loss 0.546545565 batch mAP 0.513549805 batch PCKh 0.4375\n",
      "Trained batch 2113 batch loss 0.603363156 batch mAP 0.526855469 batch PCKh 0.8125\n",
      "Trained batch 2114 batch loss 0.517019272 batch mAP 0.552001953 batch PCKh 0.6875\n",
      "Trained batch 2115 batch loss 0.612203 batch mAP 0.566619873 batch PCKh 0.625\n",
      "Trained batch 2116 batch loss 0.651040912 batch mAP 0.468109131 batch PCKh 0.6875\n",
      "Trained batch 2117 batch loss 0.555631101 batch mAP 0.546508789 batch PCKh 0.5625\n",
      "Trained batch 2118 batch loss 0.622520685 batch mAP 0.476898193 batch PCKh 0.3125\n",
      "Trained batch 2119 batch loss 0.60612154 batch mAP 0.447235107 batch PCKh 0.6875\n",
      "Trained batch 2120 batch loss 0.64204067 batch mAP 0.441619873 batch PCKh 0.25\n",
      "Trained batch 2121 batch loss 0.641485 batch mAP 0.493988037 batch PCKh 0.3125\n",
      "Trained batch 2122 batch loss 0.568181396 batch mAP 0.550720215 batch PCKh 0.3125\n",
      "Trained batch 2123 batch loss 0.567107201 batch mAP 0.463012695 batch PCKh 0.6875\n",
      "Trained batch 2124 batch loss 0.617580891 batch mAP 0.474212646 batch PCKh 0.125\n",
      "Trained batch 2125 batch loss 0.567097068 batch mAP 0.443908691 batch PCKh 0.5\n",
      "Trained batch 2126 batch loss 0.583259344 batch mAP 0.472320557 batch PCKh 0.1875\n",
      "Trained batch 2127 batch loss 0.527345657 batch mAP 0.466400146 batch PCKh 0\n",
      "Trained batch 2128 batch loss 0.599882 batch mAP 0.444885254 batch PCKh 0.375\n",
      "Trained batch 2129 batch loss 0.540378451 batch mAP 0.423706055 batch PCKh 0.6875\n",
      "Trained batch 2130 batch loss 0.557702482 batch mAP 0.430358887 batch PCKh 0.4375\n",
      "Trained batch 2131 batch loss 0.583470762 batch mAP 0.532959 batch PCKh 0.75\n",
      "Trained batch 2132 batch loss 0.602843583 batch mAP 0.494018555 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2133 batch loss 0.553551316 batch mAP 0.513336182 batch PCKh 0.1875\n",
      "Trained batch 2134 batch loss 0.602037668 batch mAP 0.516967773 batch PCKh 0.25\n",
      "Trained batch 2135 batch loss 0.583801389 batch mAP 0.520324707 batch PCKh 0.3125\n",
      "Trained batch 2136 batch loss 0.625324607 batch mAP 0.478210449 batch PCKh 0.4375\n",
      "Trained batch 2137 batch loss 0.562530398 batch mAP 0.495758057 batch PCKh 0.875\n",
      "Trained batch 2138 batch loss 0.593980491 batch mAP 0.430664062 batch PCKh 0.5625\n",
      "Trained batch 2139 batch loss 0.467568815 batch mAP 0.553955078 batch PCKh 0.6875\n",
      "Trained batch 2140 batch loss 0.491083175 batch mAP 0.562957764 batch PCKh 0.3125\n",
      "Trained batch 2141 batch loss 0.599205315 batch mAP 0.553344727 batch PCKh 0.625\n",
      "Trained batch 2142 batch loss 0.490345597 batch mAP 0.536102295 batch PCKh 0.5625\n",
      "Trained batch 2143 batch loss 0.490312427 batch mAP 0.589355469 batch PCKh 0.3125\n",
      "Trained batch 2144 batch loss 0.484744668 batch mAP 0.553894043 batch PCKh 0.6875\n",
      "Trained batch 2145 batch loss 0.455920219 batch mAP 0.582672119 batch PCKh 0.625\n",
      "Trained batch 2146 batch loss 0.461058676 batch mAP 0.587097168 batch PCKh 0.75\n",
      "Trained batch 2147 batch loss 0.473436981 batch mAP 0.592865 batch PCKh 0.75\n",
      "Trained batch 2148 batch loss 0.558465123 batch mAP 0.550018311 batch PCKh 0.8125\n",
      "Trained batch 2149 batch loss 0.49521479 batch mAP 0.473999023 batch PCKh 0.75\n",
      "Trained batch 2150 batch loss 0.504527688 batch mAP 0.548614502 batch PCKh 0.8125\n",
      "Trained batch 2151 batch loss 0.636676729 batch mAP 0.553619385 batch PCKh 0.5625\n",
      "Trained batch 2152 batch loss 0.547913194 batch mAP 0.62789917 batch PCKh 0.75\n",
      "Trained batch 2153 batch loss 0.479871929 batch mAP 0.577728271 batch PCKh 0.625\n",
      "Trained batch 2154 batch loss 0.458629787 batch mAP 0.576507568 batch PCKh 0.4375\n",
      "Trained batch 2155 batch loss 0.506343901 batch mAP 0.595855713 batch PCKh 0.6875\n",
      "Trained batch 2156 batch loss 0.463639855 batch mAP 0.638000488 batch PCKh 0.6875\n",
      "Trained batch 2157 batch loss 0.577674627 batch mAP 0.582428 batch PCKh 0.4375\n",
      "Trained batch 2158 batch loss 0.466872156 batch mAP 0.650848389 batch PCKh 0.6875\n",
      "Trained batch 2159 batch loss 0.498771906 batch mAP 0.609802246 batch PCKh 0.75\n",
      "Trained batch 2160 batch loss 0.526849508 batch mAP 0.593841553 batch PCKh 0.625\n",
      "Trained batch 2161 batch loss 0.547659516 batch mAP 0.574798584 batch PCKh 0.4375\n",
      "Trained batch 2162 batch loss 0.63121897 batch mAP 0.557098389 batch PCKh 0.3125\n",
      "Trained batch 2163 batch loss 0.640930533 batch mAP 0.576873779 batch PCKh 0.625\n",
      "Trained batch 2164 batch loss 0.613955617 batch mAP 0.553405762 batch PCKh 0.5625\n",
      "Trained batch 2165 batch loss 0.610275388 batch mAP 0.57901 batch PCKh 0.5\n",
      "Trained batch 2166 batch loss 0.564531207 batch mAP 0.580627441 batch PCKh 0.375\n",
      "Trained batch 2167 batch loss 0.576398849 batch mAP 0.567016602 batch PCKh 0.6875\n",
      "Trained batch 2168 batch loss 0.568766177 batch mAP 0.507843 batch PCKh 0.5\n",
      "Trained batch 2169 batch loss 0.589793205 batch mAP 0.578552246 batch PCKh 0.5\n",
      "Trained batch 2170 batch loss 0.508367658 batch mAP 0.694061279 batch PCKh 0.3125\n",
      "Trained batch 2171 batch loss 0.607567906 batch mAP 0.583831787 batch PCKh 0.375\n",
      "Trained batch 2172 batch loss 0.539621472 batch mAP 0.673492432 batch PCKh 0.375\n",
      "Trained batch 2173 batch loss 0.509082794 batch mAP 0.673370361 batch PCKh 0.375\n",
      "Trained batch 2174 batch loss 0.451190174 batch mAP 0.574401855 batch PCKh 0.3125\n",
      "Trained batch 2175 batch loss 0.501276553 batch mAP 0.537475586 batch PCKh 0.6875\n",
      "Trained batch 2176 batch loss 0.494200826 batch mAP 0.55078125 batch PCKh 0.0625\n",
      "Trained batch 2177 batch loss 0.498106122 batch mAP 0.596557617 batch PCKh 0.1875\n",
      "Trained batch 2178 batch loss 0.509064436 batch mAP 0.605834961 batch PCKh 0.1875\n",
      "Trained batch 2179 batch loss 0.560529292 batch mAP 0.58770752 batch PCKh 0.5625\n",
      "Trained batch 2180 batch loss 0.640859187 batch mAP 0.599090576 batch PCKh 0.375\n",
      "Trained batch 2181 batch loss 0.589291573 batch mAP 0.576812744 batch PCKh 0.5625\n",
      "Trained batch 2182 batch loss 0.604089797 batch mAP 0.615539551 batch PCKh 0.25\n",
      "Trained batch 2183 batch loss 0.461744606 batch mAP 0.60546875 batch PCKh 0.375\n",
      "Trained batch 2184 batch loss 0.611178696 batch mAP 0.630828857 batch PCKh 0.1875\n",
      "Trained batch 2185 batch loss 0.628411651 batch mAP 0.571167 batch PCKh 0\n",
      "Trained batch 2186 batch loss 0.58339119 batch mAP 0.539306641 batch PCKh 0.0625\n",
      "Trained batch 2187 batch loss 0.698277473 batch mAP 0.469360352 batch PCKh 0.3125\n",
      "Trained batch 2188 batch loss 0.564525485 batch mAP 0.479309082 batch PCKh 0.25\n",
      "Trained batch 2189 batch loss 0.593433678 batch mAP 0.573364258 batch PCKh 0.1875\n",
      "Trained batch 2190 batch loss 0.51082325 batch mAP 0.60333252 batch PCKh 0.4375\n",
      "Trained batch 2191 batch loss 0.580169201 batch mAP 0.598419189 batch PCKh 0.125\n",
      "Trained batch 2192 batch loss 0.541903377 batch mAP 0.541137695 batch PCKh 0.1875\n",
      "Trained batch 2193 batch loss 0.587145925 batch mAP 0.553009033 batch PCKh 0.4375\n",
      "Trained batch 2194 batch loss 0.528104484 batch mAP 0.584014893 batch PCKh 0.25\n",
      "Trained batch 2195 batch loss 0.623828828 batch mAP 0.541015625 batch PCKh 0.5\n",
      "Trained batch 2196 batch loss 0.584294438 batch mAP 0.571258545 batch PCKh 0.4375\n",
      "Trained batch 2197 batch loss 0.591844 batch mAP 0.565246582 batch PCKh 0.125\n",
      "Trained batch 2198 batch loss 0.507602692 batch mAP 0.562286377 batch PCKh 0.0625\n",
      "Trained batch 2199 batch loss 0.455186486 batch mAP 0.566223145 batch PCKh 0.1875\n",
      "Trained batch 2200 batch loss 0.418559045 batch mAP 0.590240479 batch PCKh 0.1875\n",
      "Trained batch 2201 batch loss 0.400639981 batch mAP 0.525665283 batch PCKh 0\n",
      "Trained batch 2202 batch loss 0.493863881 batch mAP 0.539672852 batch PCKh 0.1875\n",
      "Trained batch 2203 batch loss 0.477449596 batch mAP 0.623657227 batch PCKh 0.5625\n",
      "Trained batch 2204 batch loss 0.592955947 batch mAP 0.652740479 batch PCKh 0.3125\n",
      "Trained batch 2205 batch loss 0.547798574 batch mAP 0.695556641 batch PCKh 0.5625\n",
      "Trained batch 2206 batch loss 0.601892352 batch mAP 0.645233154 batch PCKh 0.125\n",
      "Trained batch 2207 batch loss 0.655441165 batch mAP 0.651702881 batch PCKh 0.8125\n",
      "Trained batch 2208 batch loss 0.60228312 batch mAP 0.661743164 batch PCKh 0.625\n",
      "Trained batch 2209 batch loss 0.675309837 batch mAP 0.577453613 batch PCKh 0\n",
      "Trained batch 2210 batch loss 0.63062036 batch mAP 0.599975586 batch PCKh 0.3125\n",
      "Trained batch 2211 batch loss 0.624537587 batch mAP 0.565460205 batch PCKh 0.5\n",
      "Trained batch 2212 batch loss 0.54054141 batch mAP 0.606384277 batch PCKh 0.25\n",
      "Trained batch 2213 batch loss 0.505559087 batch mAP 0.554473877 batch PCKh 0.5625\n",
      "Trained batch 2214 batch loss 0.47933346 batch mAP 0.547576904 batch PCKh 0.5\n",
      "Trained batch 2215 batch loss 0.385813951 batch mAP 0.640655518 batch PCKh 0\n",
      "Trained batch 2216 batch loss 0.481499791 batch mAP 0.567474365 batch PCKh 0.3125\n",
      "Trained batch 2217 batch loss 0.564817429 batch mAP 0.599456787 batch PCKh 0.75\n",
      "Trained batch 2218 batch loss 0.59822464 batch mAP 0.578552246 batch PCKh 0.875\n",
      "Trained batch 2219 batch loss 0.646272898 batch mAP 0.52545166 batch PCKh 0.625\n",
      "Trained batch 2220 batch loss 0.622460306 batch mAP 0.504394531 batch PCKh 0.75\n",
      "Trained batch 2221 batch loss 0.620286 batch mAP 0.547332764 batch PCKh 0.8125\n",
      "Trained batch 2222 batch loss 0.63048625 batch mAP 0.519866943 batch PCKh 0.4375\n",
      "Trained batch 2223 batch loss 0.581880689 batch mAP 0.580200195 batch PCKh 0.5625\n",
      "Trained batch 2224 batch loss 0.583278894 batch mAP 0.573577881 batch PCKh 0.8125\n",
      "Trained batch 2225 batch loss 0.582271 batch mAP 0.586730957 batch PCKh 0.75\n",
      "Trained batch 2226 batch loss 0.704784632 batch mAP 0.544952393 batch PCKh 0.0625\n",
      "Trained batch 2227 batch loss 0.605055928 batch mAP 0.597076416 batch PCKh 0.375\n",
      "Trained batch 2228 batch loss 0.575517297 batch mAP 0.64944458 batch PCKh 0.6875\n",
      "Trained batch 2229 batch loss 0.601612628 batch mAP 0.604217529 batch PCKh 0.4375\n",
      "Trained batch 2230 batch loss 0.545454085 batch mAP 0.632751465 batch PCKh 0.375\n",
      "Trained batch 2231 batch loss 0.562203 batch mAP 0.65133667 batch PCKh 0.6875\n",
      "Trained batch 2232 batch loss 0.578983903 batch mAP 0.6199646 batch PCKh 0.625\n",
      "Trained batch 2233 batch loss 0.662503302 batch mAP 0.540344238 batch PCKh 0.75\n",
      "Trained batch 2234 batch loss 0.598561525 batch mAP 0.577392578 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2235 batch loss 0.561036229 batch mAP 0.618988037 batch PCKh 0\n",
      "Trained batch 2236 batch loss 0.580996692 batch mAP 0.601013184 batch PCKh 0.3125\n",
      "Trained batch 2237 batch loss 0.557163179 batch mAP 0.585540771 batch PCKh 0.4375\n",
      "Trained batch 2238 batch loss 0.496087074 batch mAP 0.598846436 batch PCKh 0.1875\n",
      "Trained batch 2239 batch loss 0.481158614 batch mAP 0.60055542 batch PCKh 0.375\n",
      "Trained batch 2240 batch loss 0.513704836 batch mAP 0.571502686 batch PCKh 0.3125\n",
      "Trained batch 2241 batch loss 0.460088 batch mAP 0.56350708 batch PCKh 0.5625\n",
      "Trained batch 2242 batch loss 0.44843483 batch mAP 0.555725098 batch PCKh 0.125\n",
      "Trained batch 2243 batch loss 0.398446232 batch mAP 0.595611572 batch PCKh 0.3125\n",
      "Trained batch 2244 batch loss 0.458561063 batch mAP 0.593200684 batch PCKh 0.1875\n",
      "Trained batch 2245 batch loss 0.593380332 batch mAP 0.563262939 batch PCKh 0.1875\n",
      "Trained batch 2246 batch loss 0.536948323 batch mAP 0.538330078 batch PCKh 0.0625\n",
      "Trained batch 2247 batch loss 0.599261522 batch mAP 0.562957764 batch PCKh 0.6875\n",
      "Trained batch 2248 batch loss 0.623791516 batch mAP 0.593353271 batch PCKh 0.4375\n",
      "Trained batch 2249 batch loss 0.561332047 batch mAP 0.593994141 batch PCKh 0.6875\n",
      "Trained batch 2250 batch loss 0.591503561 batch mAP 0.499389648 batch PCKh 0.625\n",
      "Trained batch 2251 batch loss 0.552308083 batch mAP 0.578094482 batch PCKh 0.75\n",
      "Trained batch 2252 batch loss 0.622658193 batch mAP 0.531280518 batch PCKh 0.75\n",
      "Trained batch 2253 batch loss 0.637272954 batch mAP 0.538726807 batch PCKh 0.4375\n",
      "Trained batch 2254 batch loss 0.561851 batch mAP 0.539794922 batch PCKh 0.6875\n",
      "Trained batch 2255 batch loss 0.528820157 batch mAP 0.535125732 batch PCKh 0.1875\n",
      "Trained batch 2256 batch loss 0.543365061 batch mAP 0.570251465 batch PCKh 0.75\n",
      "Trained batch 2257 batch loss 0.541021407 batch mAP 0.648193359 batch PCKh 0.75\n",
      "Trained batch 2258 batch loss 0.495666146 batch mAP 0.625274658 batch PCKh 0.375\n",
      "Trained batch 2259 batch loss 0.507962 batch mAP 0.638397217 batch PCKh 0.4375\n",
      "Trained batch 2260 batch loss 0.52287817 batch mAP 0.587371826 batch PCKh 0.6875\n",
      "Trained batch 2261 batch loss 0.489294738 batch mAP 0.629394531 batch PCKh 0.375\n",
      "Trained batch 2262 batch loss 0.499054432 batch mAP 0.611694336 batch PCKh 0.375\n",
      "Trained batch 2263 batch loss 0.541631937 batch mAP 0.602111816 batch PCKh 0.75\n",
      "Trained batch 2264 batch loss 0.554065108 batch mAP 0.601776123 batch PCKh 0.75\n",
      "Trained batch 2265 batch loss 0.557698548 batch mAP 0.594574 batch PCKh 0.5\n",
      "Trained batch 2266 batch loss 0.523902655 batch mAP 0.538421631 batch PCKh 0.375\n",
      "Trained batch 2267 batch loss 0.586051702 batch mAP 0.633605957 batch PCKh 0.5\n",
      "Trained batch 2268 batch loss 0.65817374 batch mAP 0.517334 batch PCKh 0.8125\n",
      "Trained batch 2269 batch loss 0.522897959 batch mAP 0.577728271 batch PCKh 0.75\n",
      "Trained batch 2270 batch loss 0.545204937 batch mAP 0.57800293 batch PCKh 0.75\n",
      "Trained batch 2271 batch loss 0.538643956 batch mAP 0.571594238 batch PCKh 0\n",
      "Trained batch 2272 batch loss 0.515032351 batch mAP 0.585418701 batch PCKh 0.3125\n",
      "Trained batch 2273 batch loss 0.428827435 batch mAP 0.536438 batch PCKh 0.1875\n",
      "Trained batch 2274 batch loss 0.500706911 batch mAP 0.579437256 batch PCKh 0.5\n",
      "Trained batch 2275 batch loss 0.477945149 batch mAP 0.519256592 batch PCKh 0\n",
      "Trained batch 2276 batch loss 0.520227432 batch mAP 0.567596436 batch PCKh 0.5625\n",
      "Trained batch 2277 batch loss 0.567413807 batch mAP 0.572296143 batch PCKh 0.4375\n",
      "Trained batch 2278 batch loss 0.552171111 batch mAP 0.611755371 batch PCKh 0.5625\n",
      "Trained batch 2279 batch loss 0.565131903 batch mAP 0.574554443 batch PCKh 0.25\n",
      "Trained batch 2280 batch loss 0.550689936 batch mAP 0.611724854 batch PCKh 0.375\n",
      "Trained batch 2281 batch loss 0.59299171 batch mAP 0.510131836 batch PCKh 0.25\n",
      "Trained batch 2282 batch loss 0.645422101 batch mAP 0.576324463 batch PCKh 0.375\n",
      "Trained batch 2283 batch loss 0.644438267 batch mAP 0.620941162 batch PCKh 0.25\n",
      "Trained batch 2284 batch loss 0.576350689 batch mAP 0.607574463 batch PCKh 0.6875\n",
      "Trained batch 2285 batch loss 0.587389231 batch mAP 0.577911377 batch PCKh 0.6875\n",
      "Trained batch 2286 batch loss 0.525473714 batch mAP 0.584442139 batch PCKh 0.5625\n",
      "Trained batch 2287 batch loss 0.538289487 batch mAP 0.56942749 batch PCKh 0.625\n",
      "Trained batch 2288 batch loss 0.54781127 batch mAP 0.58078 batch PCKh 0.25\n",
      "Trained batch 2289 batch loss 0.5368433 batch mAP 0.597717285 batch PCKh 0.6875\n",
      "Trained batch 2290 batch loss 0.489113569 batch mAP 0.575317383 batch PCKh 0.125\n",
      "Trained batch 2291 batch loss 0.525641322 batch mAP 0.584716797 batch PCKh 0.3125\n",
      "Trained batch 2292 batch loss 0.56291461 batch mAP 0.577026367 batch PCKh 0.4375\n",
      "Trained batch 2293 batch loss 0.64223516 batch mAP 0.518219 batch PCKh 0.25\n",
      "Trained batch 2294 batch loss 0.531374872 batch mAP 0.566589355 batch PCKh 0.25\n",
      "Trained batch 2295 batch loss 0.59920454 batch mAP 0.543029785 batch PCKh 0.625\n",
      "Trained batch 2296 batch loss 0.610967636 batch mAP 0.573150635 batch PCKh 0.5\n",
      "Trained batch 2297 batch loss 0.536389351 batch mAP 0.562744141 batch PCKh 0.625\n",
      "Trained batch 2298 batch loss 0.599189162 batch mAP 0.556213379 batch PCKh 0.6875\n",
      "Trained batch 2299 batch loss 0.540050685 batch mAP 0.592773438 batch PCKh 0.3125\n",
      "Trained batch 2300 batch loss 0.615558565 batch mAP 0.515899658 batch PCKh 0.375\n",
      "Trained batch 2301 batch loss 0.63797003 batch mAP 0.560302734 batch PCKh 0.5625\n",
      "Trained batch 2302 batch loss 0.548482835 batch mAP 0.578949 batch PCKh 0.5625\n",
      "Trained batch 2303 batch loss 0.533274055 batch mAP 0.487823486 batch PCKh 0.25\n",
      "Trained batch 2304 batch loss 0.512869835 batch mAP 0.464355469 batch PCKh 0.5\n",
      "Trained batch 2305 batch loss 0.499625295 batch mAP 0.496398926 batch PCKh 0.3125\n",
      "Trained batch 2306 batch loss 0.469119608 batch mAP 0.638549805 batch PCKh 0.5625\n",
      "Trained batch 2307 batch loss 0.448759019 batch mAP 0.586730957 batch PCKh 0.625\n",
      "Trained batch 2308 batch loss 0.486151844 batch mAP 0.46987915 batch PCKh 0.6875\n",
      "Trained batch 2309 batch loss 0.483053148 batch mAP 0.601013184 batch PCKh 0.25\n",
      "Trained batch 2310 batch loss 0.517790437 batch mAP 0.644348145 batch PCKh 0.1875\n",
      "Trained batch 2311 batch loss 0.702312112 batch mAP 0.498168945 batch PCKh 0.125\n",
      "Trained batch 2312 batch loss 0.741073728 batch mAP 0.525939941 batch PCKh 0\n",
      "Trained batch 2313 batch loss 0.630976 batch mAP 0.593658447 batch PCKh 0.125\n",
      "Trained batch 2314 batch loss 0.581014514 batch mAP 0.609558105 batch PCKh 0.4375\n",
      "Trained batch 2315 batch loss 0.544308662 batch mAP 0.577972412 batch PCKh 0.8125\n",
      "Trained batch 2316 batch loss 0.577025533 batch mAP 0.510345459 batch PCKh 0.4375\n",
      "Trained batch 2317 batch loss 0.472256273 batch mAP 0.500152588 batch PCKh 0.125\n",
      "Trained batch 2318 batch loss 0.489269882 batch mAP 0.541503906 batch PCKh 0.625\n",
      "Trained batch 2319 batch loss 0.558549345 batch mAP 0.527038574 batch PCKh 0.3125\n",
      "Trained batch 2320 batch loss 0.591424942 batch mAP 0.579406738 batch PCKh 0.3125\n",
      "Trained batch 2321 batch loss 0.531443238 batch mAP 0.606689453 batch PCKh 0.375\n",
      "Trained batch 2322 batch loss 0.515478075 batch mAP 0.604400635 batch PCKh 0.5\n",
      "Trained batch 2323 batch loss 0.523308039 batch mAP 0.624572754 batch PCKh 0.5\n",
      "Trained batch 2324 batch loss 0.478799105 batch mAP 0.634368896 batch PCKh 0.5\n",
      "Trained batch 2325 batch loss 0.517455 batch mAP 0.59854126 batch PCKh 0.6875\n",
      "Trained batch 2326 batch loss 0.51392746 batch mAP 0.608062744 batch PCKh 0.5\n",
      "Trained batch 2327 batch loss 0.659572303 batch mAP 0.593383789 batch PCKh 0.3125\n",
      "Trained batch 2328 batch loss 0.578108132 batch mAP 0.643890381 batch PCKh 0.5\n",
      "Trained batch 2329 batch loss 0.591119289 batch mAP 0.548492432 batch PCKh 0.4375\n",
      "Trained batch 2330 batch loss 0.537760437 batch mAP 0.647918701 batch PCKh 0.6875\n",
      "Trained batch 2331 batch loss 0.598831177 batch mAP 0.665100098 batch PCKh 0.5625\n",
      "Trained batch 2332 batch loss 0.529793143 batch mAP 0.626373291 batch PCKh 0.625\n",
      "Trained batch 2333 batch loss 0.521974444 batch mAP 0.618133545 batch PCKh 0.375\n",
      "Trained batch 2334 batch loss 0.613184333 batch mAP 0.625091553 batch PCKh 0.3125\n",
      "Trained batch 2335 batch loss 0.570810556 batch mAP 0.618896484 batch PCKh 0.5625\n",
      "Trained batch 2336 batch loss 0.515928745 batch mAP 0.64263916 batch PCKh 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2337 batch loss 0.487017572 batch mAP 0.646057129 batch PCKh 0.0625\n",
      "Trained batch 2338 batch loss 0.454685807 batch mAP 0.717407227 batch PCKh 0.5625\n",
      "Trained batch 2339 batch loss 0.596578121 batch mAP 0.610229492 batch PCKh 0.3125\n",
      "Trained batch 2340 batch loss 0.508697629 batch mAP 0.689544678 batch PCKh 0.1875\n",
      "Trained batch 2341 batch loss 0.526921511 batch mAP 0.698425293 batch PCKh 0.375\n",
      "Trained batch 2342 batch loss 0.551271 batch mAP 0.623016357 batch PCKh 0.5625\n",
      "Trained batch 2343 batch loss 0.507264137 batch mAP 0.615936279 batch PCKh 0.8125\n",
      "Trained batch 2344 batch loss 0.640053213 batch mAP 0.523132324 batch PCKh 0.375\n",
      "Trained batch 2345 batch loss 0.520528674 batch mAP 0.579284668 batch PCKh 0.875\n",
      "Trained batch 2346 batch loss 0.57858181 batch mAP 0.512146 batch PCKh 0.6875\n",
      "Trained batch 2347 batch loss 0.558458924 batch mAP 0.566741943 batch PCKh 0.4375\n",
      "Trained batch 2348 batch loss 0.509432793 batch mAP 0.580963135 batch PCKh 0.25\n",
      "Trained batch 2349 batch loss 0.438733071 batch mAP 0.709106445 batch PCKh 0.5\n",
      "Trained batch 2350 batch loss 0.402917266 batch mAP 0.726104736 batch PCKh 0.5\n",
      "Trained batch 2351 batch loss 0.451920748 batch mAP 0.680664062 batch PCKh 0.3125\n",
      "Trained batch 2352 batch loss 0.553895116 batch mAP 0.597320557 batch PCKh 0.25\n",
      "Trained batch 2353 batch loss 0.475892276 batch mAP 0.666351318 batch PCKh 0.4375\n",
      "Trained batch 2354 batch loss 0.497772247 batch mAP 0.68081665 batch PCKh 0.3125\n",
      "Trained batch 2355 batch loss 0.512256 batch mAP 0.592956543 batch PCKh 0.1875\n",
      "Trained batch 2356 batch loss 0.531341076 batch mAP 0.68939209 batch PCKh 0.4375\n",
      "Trained batch 2357 batch loss 0.538573205 batch mAP 0.673492432 batch PCKh 0.6875\n",
      "Trained batch 2358 batch loss 0.508374512 batch mAP 0.635192871 batch PCKh 0.3125\n",
      "Trained batch 2359 batch loss 0.489649475 batch mAP 0.546081543 batch PCKh 0.5\n",
      "Trained batch 2360 batch loss 0.525813103 batch mAP 0.577453613 batch PCKh 0.0625\n",
      "Trained batch 2361 batch loss 0.551068246 batch mAP 0.625732422 batch PCKh 0.4375\n",
      "Trained batch 2362 batch loss 0.648167253 batch mAP 0.667358398 batch PCKh 0.6875\n",
      "Trained batch 2363 batch loss 0.68341887 batch mAP 0.622283936 batch PCKh 0.375\n",
      "Trained batch 2364 batch loss 0.583134651 batch mAP 0.625457764 batch PCKh 0.125\n",
      "Trained batch 2365 batch loss 0.634177566 batch mAP 0.579956055 batch PCKh 0.625\n",
      "Trained batch 2366 batch loss 0.631321907 batch mAP 0.458496094 batch PCKh 0.6875\n",
      "Trained batch 2367 batch loss 0.570441782 batch mAP 0.487915039 batch PCKh 0.3125\n",
      "Trained batch 2368 batch loss 0.651781201 batch mAP 0.451934814 batch PCKh 0.3125\n",
      "Trained batch 2369 batch loss 0.508978844 batch mAP 0.46685791 batch PCKh 0.25\n",
      "Trained batch 2370 batch loss 0.497443527 batch mAP 0.495758057 batch PCKh 0.1875\n",
      "Trained batch 2371 batch loss 0.48734206 batch mAP 0.547973633 batch PCKh 0.1875\n",
      "Trained batch 2372 batch loss 0.518193901 batch mAP 0.626800537 batch PCKh 0.6875\n",
      "Trained batch 2373 batch loss 0.522874594 batch mAP 0.556182861 batch PCKh 0.75\n",
      "Trained batch 2374 batch loss 0.582941353 batch mAP 0.498718262 batch PCKh 0.75\n",
      "Trained batch 2375 batch loss 0.588124633 batch mAP 0.579193115 batch PCKh 0.75\n",
      "Trained batch 2376 batch loss 0.596688867 batch mAP 0.550323486 batch PCKh 0.3125\n",
      "Trained batch 2377 batch loss 0.629195333 batch mAP 0.559814453 batch PCKh 0.375\n",
      "Trained batch 2378 batch loss 0.621674418 batch mAP 0.569091797 batch PCKh 0.4375\n",
      "Trained batch 2379 batch loss 0.630242944 batch mAP 0.555938721 batch PCKh 0.4375\n",
      "Trained batch 2380 batch loss 0.549351096 batch mAP 0.648712158 batch PCKh 0.375\n",
      "Trained batch 2381 batch loss 0.595598459 batch mAP 0.656738281 batch PCKh 0.375\n",
      "Trained batch 2382 batch loss 0.623702526 batch mAP 0.585754395 batch PCKh 0.3125\n",
      "Trained batch 2383 batch loss 0.511584401 batch mAP 0.554382324 batch PCKh 0.4375\n",
      "Trained batch 2384 batch loss 0.541566968 batch mAP 0.559539795 batch PCKh 0.1875\n",
      "Trained batch 2385 batch loss 0.529598355 batch mAP 0.532775879 batch PCKh 0.1875\n",
      "Trained batch 2386 batch loss 0.557889819 batch mAP 0.595153809 batch PCKh 0.3125\n",
      "Trained batch 2387 batch loss 0.604420424 batch mAP 0.60244751 batch PCKh 0.3125\n",
      "Trained batch 2388 batch loss 0.585258603 batch mAP 0.586975098 batch PCKh 0.25\n",
      "Trained batch 2389 batch loss 0.565544486 batch mAP 0.580200195 batch PCKh 0.625\n",
      "Trained batch 2390 batch loss 0.515319228 batch mAP 0.615783691 batch PCKh 0.3125\n",
      "Trained batch 2391 batch loss 0.652890682 batch mAP 0.534942627 batch PCKh 0.5625\n",
      "Trained batch 2392 batch loss 0.629089355 batch mAP 0.527099609 batch PCKh 0\n",
      "Trained batch 2393 batch loss 0.523263514 batch mAP 0.513122559 batch PCKh 0.6875\n",
      "Trained batch 2394 batch loss 0.471591324 batch mAP 0.568389893 batch PCKh 0.75\n",
      "Trained batch 2395 batch loss 0.547367632 batch mAP 0.54876709 batch PCKh 0.75\n",
      "Trained batch 2396 batch loss 0.621748209 batch mAP 0.586883545 batch PCKh 0.625\n",
      "Trained batch 2397 batch loss 0.591662288 batch mAP 0.575561523 batch PCKh 0.5625\n",
      "Trained batch 2398 batch loss 0.539142 batch mAP 0.518554688 batch PCKh 0.3125\n",
      "Trained batch 2399 batch loss 0.560126603 batch mAP 0.573913574 batch PCKh 0.5625\n",
      "Trained batch 2400 batch loss 0.507533312 batch mAP 0.559661865 batch PCKh 0.4375\n",
      "Trained batch 2401 batch loss 0.541561961 batch mAP 0.564178467 batch PCKh 0\n",
      "Trained batch 2402 batch loss 0.538762212 batch mAP 0.542572 batch PCKh 0.6875\n",
      "Trained batch 2403 batch loss 0.561136603 batch mAP 0.519989 batch PCKh 0.3125\n",
      "Trained batch 2404 batch loss 0.644008219 batch mAP 0.486663818 batch PCKh 0.75\n",
      "Trained batch 2405 batch loss 0.601133466 batch mAP 0.540435791 batch PCKh 0.625\n",
      "Trained batch 2406 batch loss 0.59432447 batch mAP 0.476745605 batch PCKh 0.5625\n",
      "Trained batch 2407 batch loss 0.6811831 batch mAP 0.498626709 batch PCKh 0.75\n",
      "Trained batch 2408 batch loss 0.641530633 batch mAP 0.509796143 batch PCKh 0.4375\n",
      "Trained batch 2409 batch loss 0.634125173 batch mAP 0.488128662 batch PCKh 0.75\n",
      "Trained batch 2410 batch loss 0.56625104 batch mAP 0.551452637 batch PCKh 0.75\n",
      "Trained batch 2411 batch loss 0.615760744 batch mAP 0.557281494 batch PCKh 0.4375\n",
      "Trained batch 2412 batch loss 0.541200519 batch mAP 0.623657227 batch PCKh 0.875\n",
      "Trained batch 2413 batch loss 0.592984378 batch mAP 0.562103271 batch PCKh 0.0625\n",
      "Trained batch 2414 batch loss 0.619769812 batch mAP 0.539276123 batch PCKh 0.4375\n",
      "Trained batch 2415 batch loss 0.570140421 batch mAP 0.541442871 batch PCKh 0.375\n",
      "Trained batch 2416 batch loss 0.593763828 batch mAP 0.600891113 batch PCKh 0.6875\n",
      "Trained batch 2417 batch loss 0.581568599 batch mAP 0.599975586 batch PCKh 0.625\n",
      "Trained batch 2418 batch loss 0.53743726 batch mAP 0.578430176 batch PCKh 0.625\n",
      "Trained batch 2419 batch loss 0.581766605 batch mAP 0.503753662 batch PCKh 0.5625\n",
      "Trained batch 2420 batch loss 0.583665371 batch mAP 0.55960083 batch PCKh 0.5\n",
      "Trained batch 2421 batch loss 0.564838886 batch mAP 0.553314209 batch PCKh 0.4375\n",
      "Trained batch 2422 batch loss 0.528380573 batch mAP 0.565582275 batch PCKh 0.0625\n",
      "Trained batch 2423 batch loss 0.591385 batch mAP 0.500915527 batch PCKh 0.6875\n",
      "Trained batch 2424 batch loss 0.496780306 batch mAP 0.602142334 batch PCKh 0.75\n",
      "Trained batch 2425 batch loss 0.490964293 batch mAP 0.629119873 batch PCKh 0.375\n",
      "Trained batch 2426 batch loss 0.483115673 batch mAP 0.571960449 batch PCKh 0.375\n",
      "Trained batch 2427 batch loss 0.451949656 batch mAP 0.527801514 batch PCKh 0.625\n",
      "Trained batch 2428 batch loss 0.515586376 batch mAP 0.609649658 batch PCKh 0.75\n",
      "Trained batch 2429 batch loss 0.588024139 batch mAP 0.573486328 batch PCKh 0.875\n",
      "Trained batch 2430 batch loss 0.588597298 batch mAP 0.513458252 batch PCKh 0\n",
      "Trained batch 2431 batch loss 0.491318 batch mAP 0.530578613 batch PCKh 0.25\n",
      "Trained batch 2432 batch loss 0.57113719 batch mAP 0.565063477 batch PCKh 0.5625\n",
      "Trained batch 2433 batch loss 0.632796705 batch mAP 0.52432251 batch PCKh 0.6875\n",
      "Trained batch 2434 batch loss 0.554020286 batch mAP 0.515350342 batch PCKh 0.25\n",
      "Trained batch 2435 batch loss 0.623163819 batch mAP 0.538085938 batch PCKh 0\n",
      "Trained batch 2436 batch loss 0.633906841 batch mAP 0.582428 batch PCKh 0.5625\n",
      "Trained batch 2437 batch loss 0.53152281 batch mAP 0.602966309 batch PCKh 0.875\n",
      "Trained batch 2438 batch loss 0.565485716 batch mAP 0.55065918 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2439 batch loss 0.511537433 batch mAP 0.522644043 batch PCKh 0.625\n",
      "Trained batch 2440 batch loss 0.583845377 batch mAP 0.491271973 batch PCKh 0.625\n",
      "Trained batch 2441 batch loss 0.517865837 batch mAP 0.546936035 batch PCKh 0.625\n",
      "Trained batch 2442 batch loss 0.541346 batch mAP 0.556152344 batch PCKh 0.8125\n",
      "Trained batch 2443 batch loss 0.583429277 batch mAP 0.51550293 batch PCKh 0.1875\n",
      "Trained batch 2444 batch loss 0.614181876 batch mAP 0.526519775 batch PCKh 0.75\n",
      "Trained batch 2445 batch loss 0.616700053 batch mAP 0.574279785 batch PCKh 0.4375\n",
      "Trained batch 2446 batch loss 0.635917962 batch mAP 0.597412109 batch PCKh 0.1875\n",
      "Trained batch 2447 batch loss 0.50687027 batch mAP 0.580810547 batch PCKh 0.375\n",
      "Trained batch 2448 batch loss 0.504216313 batch mAP 0.595245361 batch PCKh 0.625\n",
      "Trained batch 2449 batch loss 0.480888 batch mAP 0.594482422 batch PCKh 0.625\n",
      "Trained batch 2450 batch loss 0.507748306 batch mAP 0.537597656 batch PCKh 0.3125\n",
      "Trained batch 2451 batch loss 0.42462787 batch mAP 0.58782959 batch PCKh 0.3125\n",
      "Trained batch 2452 batch loss 0.603742242 batch mAP 0.544494629 batch PCKh 0.625\n",
      "Trained batch 2453 batch loss 0.49791947 batch mAP 0.571899414 batch PCKh 0.1875\n",
      "Trained batch 2454 batch loss 0.582675457 batch mAP 0.531402588 batch PCKh 0.0625\n",
      "Trained batch 2455 batch loss 0.571819901 batch mAP 0.625335693 batch PCKh 0.875\n",
      "Trained batch 2456 batch loss 0.515977502 batch mAP 0.646209717 batch PCKh 0.4375\n",
      "Trained batch 2457 batch loss 0.457614511 batch mAP 0.649475098 batch PCKh 0.3125\n",
      "Trained batch 2458 batch loss 0.534045517 batch mAP 0.622161865 batch PCKh 0.4375\n",
      "Trained batch 2459 batch loss 0.608842313 batch mAP 0.567718506 batch PCKh 0.5625\n",
      "Trained batch 2460 batch loss 0.592604041 batch mAP 0.556854248 batch PCKh 0.4375\n",
      "Trained batch 2461 batch loss 0.546103239 batch mAP 0.593109131 batch PCKh 0.5625\n",
      "Trained batch 2462 batch loss 0.537637949 batch mAP 0.57800293 batch PCKh 0.5\n",
      "Trained batch 2463 batch loss 0.596689343 batch mAP 0.578308105 batch PCKh 0.8125\n",
      "Trained batch 2464 batch loss 0.634759 batch mAP 0.552429199 batch PCKh 0.25\n",
      "Trained batch 2465 batch loss 0.614743054 batch mAP 0.504516602 batch PCKh 0.5625\n",
      "Trained batch 2466 batch loss 0.553649604 batch mAP 0.501037598 batch PCKh 0.5625\n",
      "Trained batch 2467 batch loss 0.629952133 batch mAP 0.526977539 batch PCKh 0.75\n",
      "Trained batch 2468 batch loss 0.548446774 batch mAP 0.557067871 batch PCKh 0.75\n",
      "Trained batch 2469 batch loss 0.706509769 batch mAP 0.52166748 batch PCKh 0.375\n",
      "Trained batch 2470 batch loss 0.666666627 batch mAP 0.56439209 batch PCKh 0.375\n",
      "Trained batch 2471 batch loss 0.675174654 batch mAP 0.53302 batch PCKh 0.4375\n",
      "Trained batch 2472 batch loss 0.587202072 batch mAP 0.610046387 batch PCKh 0.375\n",
      "Trained batch 2473 batch loss 0.626190364 batch mAP 0.559295654 batch PCKh 0.5\n",
      "Trained batch 2474 batch loss 0.568356395 batch mAP 0.528533936 batch PCKh 0.1875\n",
      "Trained batch 2475 batch loss 0.565983057 batch mAP 0.600219727 batch PCKh 0.4375\n",
      "Trained batch 2476 batch loss 0.599495173 batch mAP 0.579345703 batch PCKh 0.25\n",
      "Trained batch 2477 batch loss 0.62811923 batch mAP 0.581695557 batch PCKh 0.375\n",
      "Trained batch 2478 batch loss 0.551018357 batch mAP 0.618438721 batch PCKh 0.25\n",
      "Trained batch 2479 batch loss 0.547215104 batch mAP 0.621887207 batch PCKh 0.1875\n",
      "Trained batch 2480 batch loss 0.563016236 batch mAP 0.6484375 batch PCKh 0.3125\n",
      "Trained batch 2481 batch loss 0.496631891 batch mAP 0.680542 batch PCKh 0.25\n",
      "Trained batch 2482 batch loss 0.50170356 batch mAP 0.64352417 batch PCKh 0.3125\n",
      "Trained batch 2483 batch loss 0.602050543 batch mAP 0.626739502 batch PCKh 0.375\n",
      "Trained batch 2484 batch loss 0.559752 batch mAP 0.63797 batch PCKh 0.375\n",
      "Trained batch 2485 batch loss 0.519955 batch mAP 0.671813965 batch PCKh 0.625\n",
      "Trained batch 2486 batch loss 0.480716556 batch mAP 0.682373047 batch PCKh 0.3125\n",
      "Trained batch 2487 batch loss 0.50803721 batch mAP 0.64630127 batch PCKh 0.6875\n",
      "Trained batch 2488 batch loss 0.522711456 batch mAP 0.565032959 batch PCKh 0.5625\n",
      "Trained batch 2489 batch loss 0.537517428 batch mAP 0.524169922 batch PCKh 0.375\n",
      "Trained batch 2490 batch loss 0.557740092 batch mAP 0.559814453 batch PCKh 0.375\n",
      "Trained batch 2491 batch loss 0.54275924 batch mAP 0.531799316 batch PCKh 0.8125\n",
      "Trained batch 2492 batch loss 0.5533005 batch mAP 0.544189453 batch PCKh 0.8125\n",
      "Trained batch 2493 batch loss 0.525728822 batch mAP 0.56350708 batch PCKh 0.6875\n",
      "Trained batch 2494 batch loss 0.484080791 batch mAP 0.539276123 batch PCKh 0.125\n",
      "Trained batch 2495 batch loss 0.569878101 batch mAP 0.565124512 batch PCKh 0.4375\n",
      "Trained batch 2496 batch loss 0.553733 batch mAP 0.559112549 batch PCKh 0.4375\n",
      "Trained batch 2497 batch loss 0.527302623 batch mAP 0.535308838 batch PCKh 0.875\n",
      "Trained batch 2498 batch loss 0.523970306 batch mAP 0.537963867 batch PCKh 0.4375\n",
      "Trained batch 2499 batch loss 0.488685101 batch mAP 0.609375 batch PCKh 0.75\n",
      "Trained batch 2500 batch loss 0.547368586 batch mAP 0.599487305 batch PCKh 0.5\n",
      "Trained batch 2501 batch loss 0.522304535 batch mAP 0.570800781 batch PCKh 0.5\n",
      "Trained batch 2502 batch loss 0.555601358 batch mAP 0.57711792 batch PCKh 0.25\n",
      "Trained batch 2503 batch loss 0.5504722 batch mAP 0.556762695 batch PCKh 0.5\n",
      "Trained batch 2504 batch loss 0.581972063 batch mAP 0.522888184 batch PCKh 0.4375\n",
      "Trained batch 2505 batch loss 0.575854659 batch mAP 0.553955078 batch PCKh 0.5\n",
      "Trained batch 2506 batch loss 0.472014487 batch mAP 0.541564941 batch PCKh 0.5\n",
      "Trained batch 2507 batch loss 0.544685364 batch mAP 0.557556152 batch PCKh 0.5\n",
      "Trained batch 2508 batch loss 0.531368375 batch mAP 0.581420898 batch PCKh 0.375\n",
      "Trained batch 2509 batch loss 0.533955097 batch mAP 0.591827393 batch PCKh 0.25\n",
      "Trained batch 2510 batch loss 0.579346418 batch mAP 0.604156494 batch PCKh 0.3125\n",
      "Trained batch 2511 batch loss 0.52770251 batch mAP 0.612945557 batch PCKh 0.1875\n",
      "Trained batch 2512 batch loss 0.471464664 batch mAP 0.584350586 batch PCKh 0.1875\n",
      "Trained batch 2513 batch loss 0.555037081 batch mAP 0.507782 batch PCKh 0.375\n",
      "Trained batch 2514 batch loss 0.633522034 batch mAP 0.436157227 batch PCKh 0.1875\n",
      "Trained batch 2515 batch loss 0.617063522 batch mAP 0.558868408 batch PCKh 0.625\n",
      "Trained batch 2516 batch loss 0.525101662 batch mAP 0.602294922 batch PCKh 0.5\n",
      "Trained batch 2517 batch loss 0.644511 batch mAP 0.461761475 batch PCKh 0.6875\n",
      "Trained batch 2518 batch loss 0.557934761 batch mAP 0.485595703 batch PCKh 0.3125\n",
      "Trained batch 2519 batch loss 0.50143069 batch mAP 0.567382812 batch PCKh 0.375\n",
      "Trained batch 2520 batch loss 0.511635184 batch mAP 0.55581665 batch PCKh 0.5\n",
      "Trained batch 2521 batch loss 0.545160532 batch mAP 0.562805176 batch PCKh 0.1875\n",
      "Trained batch 2522 batch loss 0.579594851 batch mAP 0.567810059 batch PCKh 0.6875\n",
      "Trained batch 2523 batch loss 0.586463213 batch mAP 0.541931152 batch PCKh 0.5625\n",
      "Trained batch 2524 batch loss 0.580902278 batch mAP 0.559814453 batch PCKh 0.5\n",
      "Trained batch 2525 batch loss 0.493106961 batch mAP 0.424346924 batch PCKh 0\n",
      "Trained batch 2526 batch loss 0.534235299 batch mAP 0.507385254 batch PCKh 0.5625\n",
      "Trained batch 2527 batch loss 0.524384379 batch mAP 0.561431885 batch PCKh 0.5625\n",
      "Trained batch 2528 batch loss 0.491635114 batch mAP 0.651580811 batch PCKh 0.4375\n",
      "Trained batch 2529 batch loss 0.538573921 batch mAP 0.625915527 batch PCKh 0.6875\n",
      "Trained batch 2530 batch loss 0.544565856 batch mAP 0.612823486 batch PCKh 0.5\n",
      "Trained batch 2531 batch loss 0.520143926 batch mAP 0.638275146 batch PCKh 0.4375\n",
      "Trained batch 2532 batch loss 0.472923875 batch mAP 0.641357422 batch PCKh 0.3125\n",
      "Trained batch 2533 batch loss 0.492662609 batch mAP 0.612670898 batch PCKh 0.625\n",
      "Trained batch 2534 batch loss 0.455918431 batch mAP 0.636688232 batch PCKh 0.3125\n",
      "Trained batch 2535 batch loss 0.403927267 batch mAP 0.670684814 batch PCKh 0.375\n",
      "Trained batch 2536 batch loss 0.442360699 batch mAP 0.640136719 batch PCKh 0.6875\n",
      "Trained batch 2537 batch loss 0.462460399 batch mAP 0.595367432 batch PCKh 0.5625\n",
      "Trained batch 2538 batch loss 0.523209095 batch mAP 0.605621338 batch PCKh 0.75\n",
      "Trained batch 2539 batch loss 0.483532339 batch mAP 0.61630249 batch PCKh 0.75\n",
      "Trained batch 2540 batch loss 0.534526 batch mAP 0.598571777 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2541 batch loss 0.521850169 batch mAP 0.570037842 batch PCKh 0.625\n",
      "Trained batch 2542 batch loss 0.489096 batch mAP 0.521087646 batch PCKh 0.1875\n",
      "Trained batch 2543 batch loss 0.461990952 batch mAP 0.609313965 batch PCKh 0.625\n",
      "Trained batch 2544 batch loss 0.511728168 batch mAP 0.597625732 batch PCKh 0.1875\n",
      "Trained batch 2545 batch loss 0.52328068 batch mAP 0.501098633 batch PCKh 0.625\n",
      "Trained batch 2546 batch loss 0.49291563 batch mAP 0.538421631 batch PCKh 0.4375\n",
      "Trained batch 2547 batch loss 0.576580703 batch mAP 0.516845703 batch PCKh 0.5625\n",
      "Trained batch 2548 batch loss 0.530684948 batch mAP 0.550262451 batch PCKh 0.375\n",
      "Trained batch 2549 batch loss 0.414362907 batch mAP 0.662231445 batch PCKh 0.625\n",
      "Trained batch 2550 batch loss 0.593787253 batch mAP 0.589294434 batch PCKh 0.0625\n",
      "Trained batch 2551 batch loss 0.632261872 batch mAP 0.528259277 batch PCKh 0.1875\n",
      "Trained batch 2552 batch loss 0.579052329 batch mAP 0.605712891 batch PCKh 0.6875\n",
      "Trained batch 2553 batch loss 0.560202599 batch mAP 0.633880615 batch PCKh 0.1875\n",
      "Trained batch 2554 batch loss 0.632810593 batch mAP 0.635101318 batch PCKh 0.125\n",
      "Trained batch 2555 batch loss 0.541258097 batch mAP 0.65814209 batch PCKh 0.625\n",
      "Trained batch 2556 batch loss 0.585626 batch mAP 0.634521484 batch PCKh 0.1875\n",
      "Trained batch 2557 batch loss 0.665043712 batch mAP 0.609222412 batch PCKh 0.5\n",
      "Trained batch 2558 batch loss 0.615666687 batch mAP 0.642913818 batch PCKh 0.3125\n",
      "Trained batch 2559 batch loss 0.593824208 batch mAP 0.537353516 batch PCKh 0.5625\n",
      "Trained batch 2560 batch loss 0.541534424 batch mAP 0.572143555 batch PCKh 0.5625\n",
      "Trained batch 2561 batch loss 0.469317615 batch mAP 0.572235107 batch PCKh 0.5625\n",
      "Trained batch 2562 batch loss 0.498988092 batch mAP 0.596710205 batch PCKh 0.75\n",
      "Trained batch 2563 batch loss 0.590795159 batch mAP 0.514312744 batch PCKh 0.8125\n",
      "Trained batch 2564 batch loss 0.692571819 batch mAP 0.483276367 batch PCKh 0.625\n",
      "Trained batch 2565 batch loss 0.555768073 batch mAP 0.534545898 batch PCKh 0.5\n",
      "Trained batch 2566 batch loss 0.576067388 batch mAP 0.611206055 batch PCKh 0.75\n",
      "Trained batch 2567 batch loss 0.564126849 batch mAP 0.601013184 batch PCKh 0.5625\n",
      "Trained batch 2568 batch loss 0.680754423 batch mAP 0.56539917 batch PCKh 0.4375\n",
      "Trained batch 2569 batch loss 0.650033832 batch mAP 0.511169434 batch PCKh 0.125\n",
      "Trained batch 2570 batch loss 0.638174593 batch mAP 0.535400391 batch PCKh 0.3125\n",
      "Trained batch 2571 batch loss 0.644619763 batch mAP 0.566040039 batch PCKh 0.3125\n",
      "Trained batch 2572 batch loss 0.756382406 batch mAP 0.509338379 batch PCKh 0.5\n",
      "Trained batch 2573 batch loss 0.705373049 batch mAP 0.523468 batch PCKh 0.0625\n",
      "Trained batch 2574 batch loss 0.579033911 batch mAP 0.519165039 batch PCKh 0.3125\n",
      "Trained batch 2575 batch loss 0.515298 batch mAP 0.48349 batch PCKh 0.5\n",
      "Trained batch 2576 batch loss 0.443883151 batch mAP 0.498687744 batch PCKh 0\n",
      "Trained batch 2577 batch loss 0.462094724 batch mAP 0.605224609 batch PCKh 0.375\n",
      "Trained batch 2578 batch loss 0.553155601 batch mAP 0.563079834 batch PCKh 0.3125\n",
      "Trained batch 2579 batch loss 0.631397069 batch mAP 0.517181396 batch PCKh 0.1875\n",
      "Trained batch 2580 batch loss 0.514764428 batch mAP 0.565582275 batch PCKh 0.5625\n",
      "Trained batch 2581 batch loss 0.540145636 batch mAP 0.59262085 batch PCKh 0.375\n",
      "Trained batch 2582 batch loss 0.443892181 batch mAP 0.555114746 batch PCKh 0\n",
      "Trained batch 2583 batch loss 0.516597033 batch mAP 0.593353271 batch PCKh 0.625\n",
      "Trained batch 2584 batch loss 0.506410837 batch mAP 0.641357422 batch PCKh 0.375\n",
      "Trained batch 2585 batch loss 0.493207455 batch mAP 0.638916 batch PCKh 0.25\n",
      "Trained batch 2586 batch loss 0.446631104 batch mAP 0.660125732 batch PCKh 0.8125\n",
      "Trained batch 2587 batch loss 0.49206847 batch mAP 0.680297852 batch PCKh 0.375\n",
      "Trained batch 2588 batch loss 0.481265098 batch mAP 0.665435791 batch PCKh 0.5\n",
      "Trained batch 2589 batch loss 0.535545707 batch mAP 0.599823 batch PCKh 0.3125\n",
      "Trained batch 2590 batch loss 0.435650319 batch mAP 0.717010498 batch PCKh 0.25\n",
      "Trained batch 2591 batch loss 0.420162469 batch mAP 0.684539795 batch PCKh 0.375\n",
      "Trained batch 2592 batch loss 0.420742244 batch mAP 0.686248779 batch PCKh 0.3125\n",
      "Trained batch 2593 batch loss 0.456755638 batch mAP 0.650115967 batch PCKh 0.0625\n",
      "Trained batch 2594 batch loss 0.451809645 batch mAP 0.66607666 batch PCKh 0.5625\n",
      "Trained batch 2595 batch loss 0.518420637 batch mAP 0.596191406 batch PCKh 0.25\n",
      "Trained batch 2596 batch loss 0.53096962 batch mAP 0.625091553 batch PCKh 0.3125\n",
      "Trained batch 2597 batch loss 0.657396555 batch mAP 0.545227051 batch PCKh 0.3125\n",
      "Trained batch 2598 batch loss 0.530220866 batch mAP 0.643127441 batch PCKh 0.3125\n",
      "Trained batch 2599 batch loss 0.506150484 batch mAP 0.688598633 batch PCKh 0.3125\n",
      "Trained batch 2600 batch loss 0.414605021 batch mAP 0.656219482 batch PCKh 0.375\n",
      "Trained batch 2601 batch loss 0.419987082 batch mAP 0.629180908 batch PCKh 0.375\n",
      "Trained batch 2602 batch loss 0.462775379 batch mAP 0.621765137 batch PCKh 0.5\n",
      "Trained batch 2603 batch loss 0.52519381 batch mAP 0.635467529 batch PCKh 0.25\n",
      "Trained batch 2604 batch loss 0.457879543 batch mAP 0.556365967 batch PCKh 0.5\n",
      "Trained batch 2605 batch loss 0.585535765 batch mAP 0.46673584 batch PCKh 0.75\n",
      "Trained batch 2606 batch loss 0.557709277 batch mAP 0.597564697 batch PCKh 0.1875\n",
      "Trained batch 2607 batch loss 0.632911682 batch mAP 0.533813477 batch PCKh 0.0625\n",
      "Trained batch 2608 batch loss 0.66119045 batch mAP 0.509185791 batch PCKh 0.8125\n",
      "Trained batch 2609 batch loss 0.655427575 batch mAP 0.534545898 batch PCKh 0.25\n",
      "Trained batch 2610 batch loss 0.601067424 batch mAP 0.564147949 batch PCKh 0.125\n",
      "Trained batch 2611 batch loss 0.685875297 batch mAP 0.531158447 batch PCKh 0.25\n",
      "Trained batch 2612 batch loss 0.648266375 batch mAP 0.524627686 batch PCKh 0.5\n",
      "Trained batch 2613 batch loss 0.651684165 batch mAP 0.565338135 batch PCKh 0.4375\n",
      "Trained batch 2614 batch loss 0.49230206 batch mAP 0.639556885 batch PCKh 0.6875\n",
      "Trained batch 2615 batch loss 0.723232448 batch mAP 0.524963379 batch PCKh 0.125\n",
      "Trained batch 2616 batch loss 0.624715924 batch mAP 0.490966797 batch PCKh 0.5625\n",
      "Trained batch 2617 batch loss 0.523195803 batch mAP 0.536773682 batch PCKh 0.0625\n",
      "Trained batch 2618 batch loss 0.611834347 batch mAP 0.588623047 batch PCKh 0.375\n",
      "Trained batch 2619 batch loss 0.581494927 batch mAP 0.562957764 batch PCKh 0.5625\n",
      "Trained batch 2620 batch loss 0.508288741 batch mAP 0.605163574 batch PCKh 0.5625\n",
      "Trained batch 2621 batch loss 0.519139171 batch mAP 0.470703125 batch PCKh 0.4375\n",
      "Trained batch 2622 batch loss 0.389469564 batch mAP 0.526672363 batch PCKh 0.375\n",
      "Trained batch 2623 batch loss 0.622552931 batch mAP 0.560577393 batch PCKh 0.5\n",
      "Trained batch 2624 batch loss 0.547427654 batch mAP 0.625091553 batch PCKh 0.75\n",
      "Trained batch 2625 batch loss 0.580415368 batch mAP 0.578277588 batch PCKh 0.375\n",
      "Trained batch 2626 batch loss 0.564732969 batch mAP 0.531951904 batch PCKh 0.25\n",
      "Trained batch 2627 batch loss 0.674408138 batch mAP 0.489471436 batch PCKh 0.25\n",
      "Trained batch 2628 batch loss 0.586414635 batch mAP 0.533630371 batch PCKh 0.625\n",
      "Trained batch 2629 batch loss 0.544198453 batch mAP 0.522583 batch PCKh 0.6875\n",
      "Trained batch 2630 batch loss 0.542483807 batch mAP 0.459716797 batch PCKh 0\n",
      "Trained batch 2631 batch loss 0.605064929 batch mAP 0.573059082 batch PCKh 0.875\n",
      "Trained batch 2632 batch loss 0.522455096 batch mAP 0.583465576 batch PCKh 0.375\n",
      "Trained batch 2633 batch loss 0.460694969 batch mAP 0.540588379 batch PCKh 0.375\n",
      "Trained batch 2634 batch loss 0.46940276 batch mAP 0.66619873 batch PCKh 0.6875\n",
      "Trained batch 2635 batch loss 0.517234921 batch mAP 0.635742188 batch PCKh 0.4375\n",
      "Trained batch 2636 batch loss 0.43551451 batch mAP 0.621917725 batch PCKh 0.1875\n",
      "Trained batch 2637 batch loss 0.510590672 batch mAP 0.656280518 batch PCKh 0.6875\n",
      "Trained batch 2638 batch loss 0.49711895 batch mAP 0.545379639 batch PCKh 0.3125\n",
      "Trained batch 2639 batch loss 0.487515807 batch mAP 0.635314941 batch PCKh 0.4375\n",
      "Trained batch 2640 batch loss 0.560045958 batch mAP 0.583953857 batch PCKh 0.75\n",
      "Trained batch 2641 batch loss 0.562376857 batch mAP 0.576507568 batch PCKh 0.5625\n",
      "Trained batch 2642 batch loss 0.504231691 batch mAP 0.611785889 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2643 batch loss 0.471426457 batch mAP 0.631866455 batch PCKh 0.3125\n",
      "Trained batch 2644 batch loss 0.53606 batch mAP 0.66003418 batch PCKh 0.375\n",
      "Trained batch 2645 batch loss 0.529703379 batch mAP 0.614990234 batch PCKh 0.1875\n",
      "Trained batch 2646 batch loss 0.601649463 batch mAP 0.637756348 batch PCKh 0.6875\n",
      "Trained batch 2647 batch loss 0.651127458 batch mAP 0.490509033 batch PCKh 0.5\n",
      "Trained batch 2648 batch loss 0.656947494 batch mAP 0.514678955 batch PCKh 0.1875\n",
      "Trained batch 2649 batch loss 0.633661151 batch mAP 0.495910645 batch PCKh 0.125\n",
      "Trained batch 2650 batch loss 0.57813251 batch mAP 0.619659424 batch PCKh 0.375\n",
      "Trained batch 2651 batch loss 0.552964568 batch mAP 0.583587646 batch PCKh 0.125\n",
      "Trained batch 2652 batch loss 0.664597273 batch mAP 0.50793457 batch PCKh 0.5625\n",
      "Trained batch 2653 batch loss 0.553088069 batch mAP 0.555053711 batch PCKh 0.5\n",
      "Trained batch 2654 batch loss 0.455854803 batch mAP 0.544036865 batch PCKh 0.3125\n",
      "Trained batch 2655 batch loss 0.511904478 batch mAP 0.483154297 batch PCKh 0.25\n",
      "Trained batch 2656 batch loss 0.598499775 batch mAP 0.552825928 batch PCKh 0.25\n",
      "Trained batch 2657 batch loss 0.621995211 batch mAP 0.501525879 batch PCKh 0.75\n",
      "Trained batch 2658 batch loss 0.605419278 batch mAP 0.595336914 batch PCKh 0.5625\n",
      "Trained batch 2659 batch loss 0.609069049 batch mAP 0.582000732 batch PCKh 0.4375\n",
      "Trained batch 2660 batch loss 0.534066677 batch mAP 0.503936768 batch PCKh 0.4375\n",
      "Trained batch 2661 batch loss 0.45646888 batch mAP 0.432800293 batch PCKh 0.375\n",
      "Trained batch 2662 batch loss 0.469651192 batch mAP 0.410461426 batch PCKh 0.6875\n",
      "Trained batch 2663 batch loss 0.427520812 batch mAP 0.468292236 batch PCKh 0.5\n",
      "Trained batch 2664 batch loss 0.488290608 batch mAP 0.47177124 batch PCKh 0.625\n",
      "Trained batch 2665 batch loss 0.473449171 batch mAP 0.429504395 batch PCKh 0.375\n",
      "Trained batch 2666 batch loss 0.505270839 batch mAP 0.531921387 batch PCKh 0.625\n",
      "Trained batch 2667 batch loss 0.533745885 batch mAP 0.492950439 batch PCKh 0.6875\n",
      "Trained batch 2668 batch loss 0.564238727 batch mAP 0.638977051 batch PCKh 0.625\n",
      "Trained batch 2669 batch loss 0.521995664 batch mAP 0.581268311 batch PCKh 0.6875\n",
      "Trained batch 2670 batch loss 0.490950048 batch mAP 0.616699219 batch PCKh 0.5\n",
      "Trained batch 2671 batch loss 0.437466055 batch mAP 0.624115 batch PCKh 0.5\n",
      "Trained batch 2672 batch loss 0.443942934 batch mAP 0.596069336 batch PCKh 0.4375\n",
      "Trained batch 2673 batch loss 0.566448331 batch mAP 0.60824585 batch PCKh 0.5625\n",
      "Trained batch 2674 batch loss 0.555093467 batch mAP 0.575561523 batch PCKh 0.5625\n",
      "Trained batch 2675 batch loss 0.562311411 batch mAP 0.540496826 batch PCKh 0.5625\n",
      "Trained batch 2676 batch loss 0.571998954 batch mAP 0.486724854 batch PCKh 0.75\n",
      "Trained batch 2677 batch loss 0.542638302 batch mAP 0.505310059 batch PCKh 0.5\n",
      "Trained batch 2678 batch loss 0.564171553 batch mAP 0.488525391 batch PCKh 0.6875\n",
      "Trained batch 2679 batch loss 0.499890655 batch mAP 0.546325684 batch PCKh 0.625\n",
      "Trained batch 2680 batch loss 0.550586522 batch mAP 0.585601807 batch PCKh 0.875\n",
      "Trained batch 2681 batch loss 0.464853644 batch mAP 0.56463623 batch PCKh 0.5625\n",
      "Trained batch 2682 batch loss 0.556262791 batch mAP 0.603271484 batch PCKh 0.6875\n",
      "Trained batch 2683 batch loss 0.567377865 batch mAP 0.565582275 batch PCKh 0.5\n",
      "Trained batch 2684 batch loss 0.635626256 batch mAP 0.535705566 batch PCKh 0.6875\n",
      "Trained batch 2685 batch loss 0.605373263 batch mAP 0.56137085 batch PCKh 0.0625\n",
      "Trained batch 2686 batch loss 0.678860724 batch mAP 0.647918701 batch PCKh 0.3125\n",
      "Trained batch 2687 batch loss 0.577869415 batch mAP 0.570648193 batch PCKh 0.1875\n",
      "Trained batch 2688 batch loss 0.523416221 batch mAP 0.546325684 batch PCKh 0.3125\n",
      "Trained batch 2689 batch loss 0.532708764 batch mAP 0.538574219 batch PCKh 0.3125\n",
      "Trained batch 2690 batch loss 0.524076223 batch mAP 0.597503662 batch PCKh 0.625\n",
      "Trained batch 2691 batch loss 0.556558609 batch mAP 0.603729248 batch PCKh 0.5\n",
      "Trained batch 2692 batch loss 0.567520857 batch mAP 0.599090576 batch PCKh 0.375\n",
      "Trained batch 2693 batch loss 0.603200853 batch mAP 0.553588867 batch PCKh 0.4375\n",
      "Trained batch 2694 batch loss 0.67036438 batch mAP 0.488006592 batch PCKh 0.5\n",
      "Trained batch 2695 batch loss 0.637354851 batch mAP 0.479614258 batch PCKh 0.375\n",
      "Trained batch 2696 batch loss 0.761555672 batch mAP 0.472412109 batch PCKh 0\n",
      "Trained batch 2697 batch loss 0.591103911 batch mAP 0.530914307 batch PCKh 0.3125\n",
      "Trained batch 2698 batch loss 0.630720854 batch mAP 0.583343506 batch PCKh 0.0625\n",
      "Trained batch 2699 batch loss 0.618801832 batch mAP 0.511993408 batch PCKh 0.6875\n",
      "Trained batch 2700 batch loss 0.550748587 batch mAP 0.595367432 batch PCKh 0.5625\n",
      "Trained batch 2701 batch loss 0.610720515 batch mAP 0.447540283 batch PCKh 0.375\n",
      "Trained batch 2702 batch loss 0.507345378 batch mAP 0.55380249 batch PCKh 0.5\n",
      "Trained batch 2703 batch loss 0.498979717 batch mAP 0.550933838 batch PCKh 0.6875\n",
      "Trained batch 2704 batch loss 0.587603807 batch mAP 0.485778809 batch PCKh 0.125\n",
      "Trained batch 2705 batch loss 0.472457349 batch mAP 0.547058105 batch PCKh 0.375\n",
      "Trained batch 2706 batch loss 0.46251744 batch mAP 0.634643555 batch PCKh 0.625\n",
      "Trained batch 2707 batch loss 0.576372504 batch mAP 0.590118408 batch PCKh 0.5625\n",
      "Trained batch 2708 batch loss 0.623476267 batch mAP 0.596618652 batch PCKh 0.5\n",
      "Trained batch 2709 batch loss 0.575299084 batch mAP 0.600311279 batch PCKh 0.8125\n",
      "Trained batch 2710 batch loss 0.525589 batch mAP 0.603424072 batch PCKh 0.6875\n",
      "Trained batch 2711 batch loss 0.627914071 batch mAP 0.59942627 batch PCKh 0.25\n",
      "Trained batch 2712 batch loss 0.418042809 batch mAP 0.623809814 batch PCKh 0.4375\n",
      "Trained batch 2713 batch loss 0.471517205 batch mAP 0.63269043 batch PCKh 0.3125\n",
      "Trained batch 2714 batch loss 0.479265213 batch mAP 0.561767578 batch PCKh 0.125\n",
      "Trained batch 2715 batch loss 0.504975915 batch mAP 0.630340576 batch PCKh 0.375\n",
      "Trained batch 2716 batch loss 0.474120378 batch mAP 0.643615723 batch PCKh 0.1875\n",
      "Trained batch 2717 batch loss 0.449370682 batch mAP 0.719024658 batch PCKh 0.4375\n",
      "Trained batch 2718 batch loss 0.469494939 batch mAP 0.716156 batch PCKh 0.375\n",
      "Trained batch 2719 batch loss 0.480231553 batch mAP 0.674469 batch PCKh 0.4375\n",
      "Trained batch 2720 batch loss 0.556504369 batch mAP 0.641235352 batch PCKh 0.4375\n",
      "Trained batch 2721 batch loss 0.638186157 batch mAP 0.629577637 batch PCKh 0.4375\n",
      "Trained batch 2722 batch loss 0.686668873 batch mAP 0.529663086 batch PCKh 0.4375\n",
      "Trained batch 2723 batch loss 0.770170212 batch mAP 0.505706787 batch PCKh 0.375\n",
      "Trained batch 2724 batch loss 0.610528171 batch mAP 0.58026123 batch PCKh 0.5\n",
      "Trained batch 2725 batch loss 0.681675553 batch mAP 0.56729126 batch PCKh 0.5\n",
      "Trained batch 2726 batch loss 0.648752213 batch mAP 0.580322266 batch PCKh 0.75\n",
      "Trained batch 2727 batch loss 0.587830782 batch mAP 0.532653809 batch PCKh 0.625\n",
      "Trained batch 2728 batch loss 0.569009423 batch mAP 0.502868652 batch PCKh 0.75\n",
      "Trained batch 2729 batch loss 0.650601268 batch mAP 0.468139648 batch PCKh 0.625\n",
      "Trained batch 2730 batch loss 0.600255489 batch mAP 0.527191162 batch PCKh 0.375\n",
      "Trained batch 2731 batch loss 0.671852946 batch mAP 0.500732422 batch PCKh 0.375\n",
      "Trained batch 2732 batch loss 0.701173961 batch mAP 0.446105957 batch PCKh 0.125\n",
      "Trained batch 2733 batch loss 0.60870105 batch mAP 0.43737793 batch PCKh 0.1875\n",
      "Trained batch 2734 batch loss 0.544454038 batch mAP 0.489837646 batch PCKh 0.75\n",
      "Trained batch 2735 batch loss 0.647035956 batch mAP 0.481872559 batch PCKh 0.6875\n",
      "Trained batch 2736 batch loss 0.555300534 batch mAP 0.495147705 batch PCKh 0.5625\n",
      "Trained batch 2737 batch loss 0.515727639 batch mAP 0.502441406 batch PCKh 0.625\n",
      "Trained batch 2738 batch loss 0.468733937 batch mAP 0.518737793 batch PCKh 0.5\n",
      "Trained batch 2739 batch loss 0.496317029 batch mAP 0.531677246 batch PCKh 0.1875\n",
      "Trained batch 2740 batch loss 0.464670688 batch mAP 0.584716797 batch PCKh 0.625\n",
      "Trained batch 2741 batch loss 0.47536248 batch mAP 0.593017578 batch PCKh 0.6875\n",
      "Trained batch 2742 batch loss 0.394750535 batch mAP 0.562438965 batch PCKh 0.6875\n",
      "Trained batch 2743 batch loss 0.442155898 batch mAP 0.589019775 batch PCKh 0.75\n",
      "Trained batch 2744 batch loss 0.56145072 batch mAP 0.524475098 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2745 batch loss 0.645954192 batch mAP 0.523101807 batch PCKh 0.5625\n",
      "Trained batch 2746 batch loss 0.595925212 batch mAP 0.506164551 batch PCKh 0.625\n",
      "Trained batch 2747 batch loss 0.539670885 batch mAP 0.533233643 batch PCKh 0.6875\n",
      "Trained batch 2748 batch loss 0.662164 batch mAP 0.529602051 batch PCKh 0.25\n",
      "Trained batch 2749 batch loss 0.619567513 batch mAP 0.578826904 batch PCKh 0.0625\n",
      "Trained batch 2750 batch loss 0.612984061 batch mAP 0.600250244 batch PCKh 0.25\n",
      "Trained batch 2751 batch loss 0.613904238 batch mAP 0.582489 batch PCKh 0.625\n",
      "Trained batch 2752 batch loss 0.430726111 batch mAP 0.640625 batch PCKh 0.5\n",
      "Trained batch 2753 batch loss 0.561956525 batch mAP 0.546813965 batch PCKh 0.0625\n",
      "Trained batch 2754 batch loss 0.451271653 batch mAP 0.571014404 batch PCKh 0.375\n",
      "Trained batch 2755 batch loss 0.482722908 batch mAP 0.588867188 batch PCKh 0.5625\n",
      "Trained batch 2756 batch loss 0.506250203 batch mAP 0.58795166 batch PCKh 0.375\n",
      "Trained batch 2757 batch loss 0.59473455 batch mAP 0.521057129 batch PCKh 0.1875\n",
      "Trained batch 2758 batch loss 0.624226868 batch mAP 0.568389893 batch PCKh 0.875\n",
      "Trained batch 2759 batch loss 0.516158462 batch mAP 0.578949 batch PCKh 0.6875\n",
      "Trained batch 2760 batch loss 0.579265475 batch mAP 0.567199707 batch PCKh 0.625\n",
      "Trained batch 2761 batch loss 0.660030305 batch mAP 0.55279541 batch PCKh 0.5625\n",
      "Trained batch 2762 batch loss 0.545956373 batch mAP 0.604370117 batch PCKh 0.25\n",
      "Trained batch 2763 batch loss 0.533429503 batch mAP 0.591583252 batch PCKh 0.625\n",
      "Trained batch 2764 batch loss 0.547854066 batch mAP 0.606079102 batch PCKh 0.25\n",
      "Trained batch 2765 batch loss 0.600317 batch mAP 0.600250244 batch PCKh 0.8125\n",
      "Trained batch 2766 batch loss 0.663070261 batch mAP 0.544799805 batch PCKh 0.75\n",
      "Trained batch 2767 batch loss 0.662909 batch mAP 0.480133057 batch PCKh 0.75\n",
      "Trained batch 2768 batch loss 0.574143648 batch mAP 0.499847412 batch PCKh 0.875\n",
      "Trained batch 2769 batch loss 0.546567321 batch mAP 0.602264404 batch PCKh 0.6875\n",
      "Trained batch 2770 batch loss 0.560160398 batch mAP 0.5753479 batch PCKh 0.875\n",
      "Trained batch 2771 batch loss 0.608188 batch mAP 0.584503174 batch PCKh 0.375\n",
      "Trained batch 2772 batch loss 0.545376122 batch mAP 0.531982422 batch PCKh 0.875\n",
      "Trained batch 2773 batch loss 0.422927052 batch mAP 0.592803955 batch PCKh 0.5\n",
      "Trained batch 2774 batch loss 0.492500156 batch mAP 0.552276611 batch PCKh 0.5\n",
      "Trained batch 2775 batch loss 0.539280891 batch mAP 0.564239502 batch PCKh 0.5625\n",
      "Trained batch 2776 batch loss 0.505625606 batch mAP 0.583129883 batch PCKh 0.5625\n",
      "Epoch 5 train loss 0.5583776831626892 train mAP 0.5570821166038513 train PCKh\n",
      "Validated batch 1 batch loss 0.539973378 batch mAP 0.612121582 batch PCKh 0.625\n",
      "Validated batch 2 batch loss 0.610909343 batch mAP 0.601928711 batch PCKh 0.1875\n",
      "Validated batch 3 batch loss 0.612105727 batch mAP 0.496765137 batch PCKh 0.6875\n",
      "Validated batch 4 batch loss 0.503074467 batch mAP 0.576629639 batch PCKh 0.375\n",
      "Validated batch 5 batch loss 0.608359456 batch mAP 0.443847656 batch PCKh 0.125\n",
      "Validated batch 6 batch loss 0.472033024 batch mAP 0.640045166 batch PCKh 0.1875\n",
      "Validated batch 7 batch loss 0.555736542 batch mAP 0.527252197 batch PCKh 0\n",
      "Validated batch 8 batch loss 0.589853168 batch mAP 0.59274292 batch PCKh 0.75\n",
      "Validated batch 9 batch loss 0.608177 batch mAP 0.590271 batch PCKh 0.5\n",
      "Validated batch 10 batch loss 0.536060929 batch mAP 0.62286377 batch PCKh 0.3125\n",
      "Validated batch 11 batch loss 0.77545464 batch mAP 0.515136719 batch PCKh 0\n",
      "Validated batch 12 batch loss 0.538915455 batch mAP 0.541107178 batch PCKh 0.5\n",
      "Validated batch 13 batch loss 0.53962 batch mAP 0.611145 batch PCKh 0.5\n",
      "Validated batch 14 batch loss 0.650852561 batch mAP 0.498535156 batch PCKh 0.375\n",
      "Validated batch 15 batch loss 0.450615048 batch mAP 0.562042236 batch PCKh 0.1875\n",
      "Validated batch 16 batch loss 0.527958274 batch mAP 0.62802124 batch PCKh 0.25\n",
      "Validated batch 17 batch loss 0.541987 batch mAP 0.628997803 batch PCKh 0.75\n",
      "Validated batch 18 batch loss 0.545991302 batch mAP 0.632537842 batch PCKh 0.25\n",
      "Validated batch 19 batch loss 0.648514152 batch mAP 0.521179199 batch PCKh 0.125\n",
      "Validated batch 20 batch loss 0.502990127 batch mAP 0.656768799 batch PCKh 0.4375\n",
      "Validated batch 21 batch loss 0.540575 batch mAP 0.612060547 batch PCKh 0.4375\n",
      "Validated batch 22 batch loss 0.546258 batch mAP 0.536499 batch PCKh 0.1875\n",
      "Validated batch 23 batch loss 0.631996214 batch mAP 0.541351318 batch PCKh 0\n",
      "Validated batch 24 batch loss 0.530358136 batch mAP 0.528564453 batch PCKh 0.125\n",
      "Validated batch 25 batch loss 0.498471618 batch mAP 0.603790283 batch PCKh 0.4375\n",
      "Validated batch 26 batch loss 0.64806962 batch mAP 0.507202148 batch PCKh 0.375\n",
      "Validated batch 27 batch loss 0.54307276 batch mAP 0.619720459 batch PCKh 0.3125\n",
      "Validated batch 28 batch loss 0.623479366 batch mAP 0.595184326 batch PCKh 0.875\n",
      "Validated batch 29 batch loss 0.651594281 batch mAP 0.554748535 batch PCKh 0.5625\n",
      "Validated batch 30 batch loss 0.645376742 batch mAP 0.454681396 batch PCKh 0.25\n",
      "Validated batch 31 batch loss 0.656638503 batch mAP 0.536499 batch PCKh 0.1875\n",
      "Validated batch 32 batch loss 0.626189709 batch mAP 0.548278809 batch PCKh 0.625\n",
      "Validated batch 33 batch loss 0.565440953 batch mAP 0.52923584 batch PCKh 0.875\n",
      "Validated batch 34 batch loss 0.57136488 batch mAP 0.471496582 batch PCKh 0.1875\n",
      "Validated batch 35 batch loss 0.727781653 batch mAP 0.439117432 batch PCKh 0\n",
      "Validated batch 36 batch loss 0.575568318 batch mAP 0.599609375 batch PCKh 0.1875\n",
      "Validated batch 37 batch loss 0.452215433 batch mAP 0.667907715 batch PCKh 0.25\n",
      "Validated batch 38 batch loss 0.527866125 batch mAP 0.569030762 batch PCKh 0\n",
      "Validated batch 39 batch loss 0.46423474 batch mAP 0.592987061 batch PCKh 0.0625\n",
      "Validated batch 40 batch loss 0.639274836 batch mAP 0.445800781 batch PCKh 0.4375\n",
      "Validated batch 41 batch loss 0.545386314 batch mAP 0.578369141 batch PCKh 0.8125\n",
      "Validated batch 42 batch loss 0.487449884 batch mAP 0.597106934 batch PCKh 0.5\n",
      "Validated batch 43 batch loss 0.577361524 batch mAP 0.541809082 batch PCKh 0.5\n",
      "Validated batch 44 batch loss 0.632954061 batch mAP 0.531738281 batch PCKh 0.5625\n",
      "Validated batch 45 batch loss 0.733622074 batch mAP 0.518737793 batch PCKh 0.125\n",
      "Validated batch 46 batch loss 0.643850565 batch mAP 0.473846436 batch PCKh 0.5625\n",
      "Validated batch 47 batch loss 0.594557285 batch mAP 0.530395508 batch PCKh 0.375\n",
      "Validated batch 48 batch loss 0.57648313 batch mAP 0.459136963 batch PCKh 0.0625\n",
      "Validated batch 49 batch loss 0.518242836 batch mAP 0.571014404 batch PCKh 0.5625\n",
      "Validated batch 50 batch loss 0.560560763 batch mAP 0.507659912 batch PCKh 0.25\n",
      "Validated batch 51 batch loss 0.568653107 batch mAP 0.557098389 batch PCKh 0.625\n",
      "Validated batch 52 batch loss 0.591376781 batch mAP 0.508850098 batch PCKh 0.5\n",
      "Validated batch 53 batch loss 0.525643885 batch mAP 0.653106689 batch PCKh 0.3125\n",
      "Validated batch 54 batch loss 0.546944618 batch mAP 0.575683594 batch PCKh 0.6875\n",
      "Validated batch 55 batch loss 0.596421 batch mAP 0.532989502 batch PCKh 0.75\n",
      "Validated batch 56 batch loss 0.546488166 batch mAP 0.613525391 batch PCKh 0.8125\n",
      "Validated batch 57 batch loss 0.616249084 batch mAP 0.578033447 batch PCKh 0.125\n",
      "Validated batch 58 batch loss 0.625357687 batch mAP 0.491912842 batch PCKh 0.625\n",
      "Validated batch 59 batch loss 0.552669168 batch mAP 0.596710205 batch PCKh 0.8125\n",
      "Validated batch 60 batch loss 0.633323908 batch mAP 0.439117432 batch PCKh 0.125\n",
      "Validated batch 61 batch loss 0.62961632 batch mAP 0.518859863 batch PCKh 0.5\n",
      "Validated batch 62 batch loss 0.69874686 batch mAP 0.486755371 batch PCKh 0\n",
      "Validated batch 63 batch loss 0.545841038 batch mAP 0.500396729 batch PCKh 0.6875\n",
      "Validated batch 64 batch loss 0.658790708 batch mAP 0.603088379 batch PCKh 0.1875\n",
      "Validated batch 65 batch loss 0.588161469 batch mAP 0.520629883 batch PCKh 0.375\n",
      "Validated batch 66 batch loss 0.554285288 batch mAP 0.528076172 batch PCKh 0.5\n",
      "Validated batch 67 batch loss 0.616458535 batch mAP 0.575042725 batch PCKh 0.4375\n",
      "Validated batch 68 batch loss 0.638055921 batch mAP 0.592468262 batch PCKh 0.75\n",
      "Validated batch 69 batch loss 0.64598763 batch mAP 0.48059082 batch PCKh 0.6875\n",
      "Validated batch 70 batch loss 0.635243773 batch mAP 0.561431885 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 71 batch loss 0.676350057 batch mAP 0.500701904 batch PCKh 0.5\n",
      "Validated batch 72 batch loss 0.655589402 batch mAP 0.508544922 batch PCKh 0.3125\n",
      "Validated batch 73 batch loss 0.6575827 batch mAP 0.601074219 batch PCKh 0.625\n",
      "Validated batch 74 batch loss 0.657625198 batch mAP 0.537750244 batch PCKh 0.375\n",
      "Validated batch 75 batch loss 0.614730239 batch mAP 0.593261719 batch PCKh 0.875\n",
      "Validated batch 76 batch loss 0.641804218 batch mAP 0.544769287 batch PCKh 0.625\n",
      "Validated batch 77 batch loss 0.73232609 batch mAP 0.492004395 batch PCKh 0.6875\n",
      "Validated batch 78 batch loss 0.607678652 batch mAP 0.532867432 batch PCKh 0.75\n",
      "Validated batch 79 batch loss 0.701920092 batch mAP 0.441802979 batch PCKh 0.1875\n",
      "Validated batch 80 batch loss 0.585922539 batch mAP 0.533050537 batch PCKh 0.3125\n",
      "Validated batch 81 batch loss 0.571065247 batch mAP 0.614044189 batch PCKh 0.4375\n",
      "Validated batch 82 batch loss 0.568105817 batch mAP 0.559112549 batch PCKh 0.1875\n",
      "Validated batch 83 batch loss 0.562226653 batch mAP 0.608001709 batch PCKh 0.625\n",
      "Validated batch 84 batch loss 0.635206401 batch mAP 0.529724121 batch PCKh 0.25\n",
      "Validated batch 85 batch loss 0.643832624 batch mAP 0.51663208 batch PCKh 0.4375\n",
      "Validated batch 86 batch loss 0.64530921 batch mAP 0.554168701 batch PCKh 0.25\n",
      "Validated batch 87 batch loss 0.646059871 batch mAP 0.570343 batch PCKh 0.75\n",
      "Validated batch 88 batch loss 0.534975648 batch mAP 0.581665039 batch PCKh 0.5\n",
      "Validated batch 89 batch loss 0.542179406 batch mAP 0.547241211 batch PCKh 0.3125\n",
      "Validated batch 90 batch loss 0.619898081 batch mAP 0.555358887 batch PCKh 0.4375\n",
      "Validated batch 91 batch loss 0.630464613 batch mAP 0.466094971 batch PCKh 0.375\n",
      "Validated batch 92 batch loss 0.533668399 batch mAP 0.563140869 batch PCKh 0.4375\n",
      "Validated batch 93 batch loss 0.600359797 batch mAP 0.481292725 batch PCKh 0.25\n",
      "Validated batch 94 batch loss 0.638371587 batch mAP 0.489593506 batch PCKh 0.5625\n",
      "Validated batch 95 batch loss 0.673888 batch mAP 0.531921387 batch PCKh 0.4375\n",
      "Validated batch 96 batch loss 0.527587354 batch mAP 0.630065918 batch PCKh 0.625\n",
      "Validated batch 97 batch loss 0.470048457 batch mAP 0.673065186 batch PCKh 0.3125\n",
      "Validated batch 98 batch loss 0.67218405 batch mAP 0.442382812 batch PCKh 0.125\n",
      "Validated batch 99 batch loss 0.614596248 batch mAP 0.526092529 batch PCKh 0.4375\n",
      "Validated batch 100 batch loss 0.545998096 batch mAP 0.501312256 batch PCKh 0.5\n",
      "Validated batch 101 batch loss 0.60457778 batch mAP 0.496734619 batch PCKh 0.5625\n",
      "Validated batch 102 batch loss 0.596977174 batch mAP 0.595306396 batch PCKh 0.1875\n",
      "Validated batch 103 batch loss 0.610215306 batch mAP 0.507141113 batch PCKh 0.5625\n",
      "Validated batch 104 batch loss 0.606789 batch mAP 0.641693115 batch PCKh 0.375\n",
      "Validated batch 105 batch loss 0.641618609 batch mAP 0.529418945 batch PCKh 0.4375\n",
      "Validated batch 106 batch loss 0.554659963 batch mAP 0.5753479 batch PCKh 0.1875\n",
      "Validated batch 107 batch loss 0.575477242 batch mAP 0.487060547 batch PCKh 0.4375\n",
      "Validated batch 108 batch loss 0.607796609 batch mAP 0.570831299 batch PCKh 0.6875\n",
      "Validated batch 109 batch loss 0.495643675 batch mAP 0.581268311 batch PCKh 0.25\n",
      "Validated batch 110 batch loss 0.711941898 batch mAP 0.585662842 batch PCKh 0.6875\n",
      "Validated batch 111 batch loss 0.645693541 batch mAP 0.603973389 batch PCKh 0\n",
      "Validated batch 112 batch loss 0.610362411 batch mAP 0.642303467 batch PCKh 0.375\n",
      "Validated batch 113 batch loss 0.595288038 batch mAP 0.610595703 batch PCKh 0.0625\n",
      "Validated batch 114 batch loss 0.512774348 batch mAP 0.555419922 batch PCKh 0.3125\n",
      "Validated batch 115 batch loss 0.550005376 batch mAP 0.595123291 batch PCKh 0.4375\n",
      "Validated batch 116 batch loss 0.489602357 batch mAP 0.580535889 batch PCKh 0.75\n",
      "Validated batch 117 batch loss 0.608186364 batch mAP 0.499511719 batch PCKh 0.25\n",
      "Validated batch 118 batch loss 0.602938533 batch mAP 0.539733887 batch PCKh 0.1875\n",
      "Validated batch 119 batch loss 0.585890234 batch mAP 0.572387695 batch PCKh 0.625\n",
      "Validated batch 120 batch loss 0.640554428 batch mAP 0.484436035 batch PCKh 0.3125\n",
      "Validated batch 121 batch loss 0.671329618 batch mAP 0.406585693 batch PCKh 0\n",
      "Validated batch 122 batch loss 0.677506626 batch mAP 0.405487061 batch PCKh 0.125\n",
      "Validated batch 123 batch loss 0.659588873 batch mAP 0.448577881 batch PCKh 0.5625\n",
      "Validated batch 124 batch loss 0.644070208 batch mAP 0.51953125 batch PCKh 0.5625\n",
      "Validated batch 125 batch loss 0.569946706 batch mAP 0.534912109 batch PCKh 0.5625\n",
      "Validated batch 126 batch loss 0.652330875 batch mAP 0.53237915 batch PCKh 0.0625\n",
      "Validated batch 127 batch loss 0.69759059 batch mAP 0.470977783 batch PCKh 0.625\n",
      "Validated batch 128 batch loss 0.63371557 batch mAP 0.411407471 batch PCKh 0.0625\n",
      "Validated batch 129 batch loss 0.587810278 batch mAP 0.550811768 batch PCKh 0.4375\n",
      "Validated batch 130 batch loss 0.637240291 batch mAP 0.483337402 batch PCKh 0.625\n",
      "Validated batch 131 batch loss 0.719478726 batch mAP 0.489959717 batch PCKh 0.25\n",
      "Validated batch 132 batch loss 0.499915183 batch mAP 0.554595947 batch PCKh 0.1875\n",
      "Validated batch 133 batch loss 0.64064467 batch mAP 0.480834961 batch PCKh 0.5625\n",
      "Validated batch 134 batch loss 0.580603957 batch mAP 0.553588867 batch PCKh 0.75\n",
      "Validated batch 135 batch loss 0.489834607 batch mAP 0.600311279 batch PCKh 0.1875\n",
      "Validated batch 136 batch loss 0.45348984 batch mAP 0.618225098 batch PCKh 0.625\n",
      "Validated batch 137 batch loss 0.569682837 batch mAP 0.600341797 batch PCKh 0.4375\n",
      "Validated batch 138 batch loss 0.652555406 batch mAP 0.543518066 batch PCKh 0.375\n",
      "Validated batch 139 batch loss 0.554232895 batch mAP 0.606506348 batch PCKh 0.3125\n",
      "Validated batch 140 batch loss 0.597255409 batch mAP 0.498931885 batch PCKh 0.625\n",
      "Validated batch 141 batch loss 0.573554754 batch mAP 0.483856201 batch PCKh 0\n",
      "Validated batch 142 batch loss 0.504460275 batch mAP 0.530944824 batch PCKh 0.625\n",
      "Validated batch 143 batch loss 0.569819212 batch mAP 0.517669678 batch PCKh 0.5\n",
      "Validated batch 144 batch loss 0.565669119 batch mAP 0.460235596 batch PCKh 0.25\n",
      "Validated batch 145 batch loss 0.594599247 batch mAP 0.510131836 batch PCKh 0.5625\n",
      "Validated batch 146 batch loss 0.497365355 batch mAP 0.563476562 batch PCKh 0.625\n",
      "Validated batch 147 batch loss 0.589162946 batch mAP 0.527587891 batch PCKh 0.75\n",
      "Validated batch 148 batch loss 0.527800083 batch mAP 0.587310791 batch PCKh 0.625\n",
      "Validated batch 149 batch loss 0.59748 batch mAP 0.471679688 batch PCKh 0.5\n",
      "Validated batch 150 batch loss 0.613515556 batch mAP 0.551269531 batch PCKh 0.5625\n",
      "Validated batch 151 batch loss 0.59610647 batch mAP 0.493469238 batch PCKh 0.5625\n",
      "Validated batch 152 batch loss 0.616465449 batch mAP 0.457794189 batch PCKh 0.25\n",
      "Validated batch 153 batch loss 0.570843756 batch mAP 0.524108887 batch PCKh 0.375\n",
      "Validated batch 154 batch loss 0.606597543 batch mAP 0.566070557 batch PCKh 0.6875\n",
      "Validated batch 155 batch loss 0.641566038 batch mAP 0.476898193 batch PCKh 0.25\n",
      "Validated batch 156 batch loss 0.540111184 batch mAP 0.576538086 batch PCKh 0.5\n",
      "Validated batch 157 batch loss 0.516014338 batch mAP 0.653198242 batch PCKh 0.8125\n",
      "Validated batch 158 batch loss 0.603538692 batch mAP 0.631622314 batch PCKh 0.25\n",
      "Validated batch 159 batch loss 0.649703383 batch mAP 0.515319824 batch PCKh 0.25\n",
      "Validated batch 160 batch loss 0.550362587 batch mAP 0.544311523 batch PCKh 0.3125\n",
      "Validated batch 161 batch loss 0.712385297 batch mAP 0.503295898 batch PCKh 0.125\n",
      "Validated batch 162 batch loss 0.537231565 batch mAP 0.552856445 batch PCKh 0.25\n",
      "Validated batch 163 batch loss 0.492167622 batch mAP 0.635223389 batch PCKh 0.0625\n",
      "Validated batch 164 batch loss 0.612097621 batch mAP 0.562530518 batch PCKh 0.4375\n",
      "Validated batch 165 batch loss 0.572912097 batch mAP 0.516662598 batch PCKh 0.5625\n",
      "Validated batch 166 batch loss 0.542703807 batch mAP 0.587127686 batch PCKh 0.1875\n",
      "Validated batch 167 batch loss 0.653155446 batch mAP 0.541778564 batch PCKh 0.5\n",
      "Validated batch 168 batch loss 0.575716 batch mAP 0.556274414 batch PCKh 0.3125\n",
      "Validated batch 169 batch loss 0.564205 batch mAP 0.554260254 batch PCKh 0.5625\n",
      "Validated batch 170 batch loss 0.578316331 batch mAP 0.582855225 batch PCKh 0.125\n",
      "Validated batch 171 batch loss 0.658287346 batch mAP 0.506591797 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 172 batch loss 0.654002786 batch mAP 0.517913818 batch PCKh 0.4375\n",
      "Validated batch 173 batch loss 0.519887388 batch mAP 0.54876709 batch PCKh 0.1875\n",
      "Validated batch 174 batch loss 0.643579483 batch mAP 0.481719971 batch PCKh 0.4375\n",
      "Validated batch 175 batch loss 0.620832562 batch mAP 0.532470703 batch PCKh 0.25\n",
      "Validated batch 176 batch loss 0.470746577 batch mAP 0.630401611 batch PCKh 0.625\n",
      "Validated batch 177 batch loss 0.599786818 batch mAP 0.545715332 batch PCKh 0.5625\n",
      "Validated batch 178 batch loss 0.632636189 batch mAP 0.40737915 batch PCKh 0.3125\n",
      "Validated batch 179 batch loss 0.624406159 batch mAP 0.497955322 batch PCKh 0.375\n",
      "Validated batch 180 batch loss 0.602540374 batch mAP 0.548095703 batch PCKh 0.1875\n",
      "Validated batch 181 batch loss 0.620662212 batch mAP 0.505889893 batch PCKh 0.0625\n",
      "Validated batch 182 batch loss 0.495777786 batch mAP 0.50881958 batch PCKh 0.625\n",
      "Validated batch 183 batch loss 0.574326158 batch mAP 0.540100098 batch PCKh 0.3125\n",
      "Validated batch 184 batch loss 0.577089965 batch mAP 0.555450439 batch PCKh 0.6875\n",
      "Validated batch 185 batch loss 0.59956038 batch mAP 0.61895752 batch PCKh 0.875\n",
      "Validated batch 186 batch loss 0.607528508 batch mAP 0.545898438 batch PCKh 0.4375\n",
      "Validated batch 187 batch loss 0.599208 batch mAP 0.537231445 batch PCKh 0.625\n",
      "Validated batch 188 batch loss 0.587551117 batch mAP 0.577575684 batch PCKh 0.5625\n",
      "Validated batch 189 batch loss 0.585503 batch mAP 0.531188965 batch PCKh 0.5\n",
      "Validated batch 190 batch loss 0.560595036 batch mAP 0.57824707 batch PCKh 0.625\n",
      "Validated batch 191 batch loss 0.60412246 batch mAP 0.514831543 batch PCKh 0.6875\n",
      "Validated batch 192 batch loss 0.601195633 batch mAP 0.572113037 batch PCKh 0.5\n",
      "Validated batch 193 batch loss 0.637245715 batch mAP 0.490600586 batch PCKh 0.0625\n",
      "Validated batch 194 batch loss 0.596046746 batch mAP 0.51083374 batch PCKh 0.6875\n",
      "Validated batch 195 batch loss 0.719692171 batch mAP 0.407592773 batch PCKh 0.1875\n",
      "Validated batch 196 batch loss 0.639028072 batch mAP 0.54083252 batch PCKh 0.5\n",
      "Validated batch 197 batch loss 0.534132183 batch mAP 0.57244873 batch PCKh 0.125\n",
      "Validated batch 198 batch loss 0.621220291 batch mAP 0.602630615 batch PCKh 0.375\n",
      "Validated batch 199 batch loss 0.533803344 batch mAP 0.587158203 batch PCKh 0.8125\n",
      "Validated batch 200 batch loss 0.662465096 batch mAP 0.506317139 batch PCKh 0.625\n",
      "Validated batch 201 batch loss 0.606513381 batch mAP 0.485931396 batch PCKh 0.25\n",
      "Validated batch 202 batch loss 0.587458789 batch mAP 0.618225098 batch PCKh 0.5\n",
      "Validated batch 203 batch loss 0.635935903 batch mAP 0.532714844 batch PCKh 0.1875\n",
      "Validated batch 204 batch loss 0.571903706 batch mAP 0.565429688 batch PCKh 0.5625\n",
      "Validated batch 205 batch loss 0.594304085 batch mAP 0.652160645 batch PCKh 0.6875\n",
      "Validated batch 206 batch loss 0.582608 batch mAP 0.56741333 batch PCKh 0.875\n",
      "Validated batch 207 batch loss 0.589654565 batch mAP 0.539581299 batch PCKh 0.6875\n",
      "Validated batch 208 batch loss 0.537017822 batch mAP 0.633361816 batch PCKh 0.5\n",
      "Validated batch 209 batch loss 0.559047043 batch mAP 0.577667236 batch PCKh 0.6875\n",
      "Validated batch 210 batch loss 0.698518395 batch mAP 0.587982178 batch PCKh 0.6875\n",
      "Validated batch 211 batch loss 0.699577451 batch mAP 0.559448242 batch PCKh 0.125\n",
      "Validated batch 212 batch loss 0.527083933 batch mAP 0.587158203 batch PCKh 0.5\n",
      "Validated batch 213 batch loss 0.573815405 batch mAP 0.485870361 batch PCKh 0.4375\n",
      "Validated batch 214 batch loss 0.674303651 batch mAP 0.55279541 batch PCKh 0.3125\n",
      "Validated batch 215 batch loss 0.575745583 batch mAP 0.601806641 batch PCKh 0\n",
      "Validated batch 216 batch loss 0.662897825 batch mAP 0.586242676 batch PCKh 0.625\n",
      "Validated batch 217 batch loss 0.569634378 batch mAP 0.533325195 batch PCKh 0.75\n",
      "Validated batch 218 batch loss 0.667470872 batch mAP 0.46194458 batch PCKh 0.375\n",
      "Validated batch 219 batch loss 0.636071444 batch mAP 0.515533447 batch PCKh 0.625\n",
      "Validated batch 220 batch loss 0.441025138 batch mAP 0.60345459 batch PCKh 0.5\n",
      "Validated batch 221 batch loss 0.574501574 batch mAP 0.592437744 batch PCKh 0.75\n",
      "Validated batch 222 batch loss 0.605153203 batch mAP 0.424102783 batch PCKh 0.6875\n",
      "Validated batch 223 batch loss 0.627620399 batch mAP 0.561126709 batch PCKh 0.375\n",
      "Validated batch 224 batch loss 0.556782186 batch mAP 0.538391113 batch PCKh 0.5\n",
      "Validated batch 225 batch loss 0.533174813 batch mAP 0.622131348 batch PCKh 0.625\n",
      "Validated batch 226 batch loss 0.543499291 batch mAP 0.583648682 batch PCKh 0.5625\n",
      "Validated batch 227 batch loss 0.645244658 batch mAP 0.557952881 batch PCKh 0.5625\n",
      "Validated batch 228 batch loss 0.551185846 batch mAP 0.61203 batch PCKh 0.25\n",
      "Validated batch 229 batch loss 0.550182 batch mAP 0.564849854 batch PCKh 0.5625\n",
      "Validated batch 230 batch loss 0.574993193 batch mAP 0.548126221 batch PCKh 0.25\n",
      "Validated batch 231 batch loss 0.686163068 batch mAP 0.54107666 batch PCKh 0.6875\n",
      "Validated batch 232 batch loss 0.476585567 batch mAP 0.550964355 batch PCKh 0.5\n",
      "Validated batch 233 batch loss 0.538019776 batch mAP 0.5831604 batch PCKh 0.75\n",
      "Validated batch 234 batch loss 0.565395772 batch mAP 0.56817627 batch PCKh 0.3125\n",
      "Validated batch 235 batch loss 0.602663934 batch mAP 0.601623535 batch PCKh 0.75\n",
      "Validated batch 236 batch loss 0.606506407 batch mAP 0.495452881 batch PCKh 0.75\n",
      "Validated batch 237 batch loss 0.553632498 batch mAP 0.611450195 batch PCKh 0.1875\n",
      "Validated batch 238 batch loss 0.525236726 batch mAP 0.649963379 batch PCKh 0.4375\n",
      "Validated batch 239 batch loss 0.616751969 batch mAP 0.557647705 batch PCKh 0.5625\n",
      "Validated batch 240 batch loss 0.58495307 batch mAP 0.581604 batch PCKh 0.6875\n",
      "Validated batch 241 batch loss 0.712452233 batch mAP 0.516998291 batch PCKh 0.375\n",
      "Validated batch 242 batch loss 0.652556539 batch mAP 0.507141113 batch PCKh 0.625\n",
      "Validated batch 243 batch loss 0.553003907 batch mAP 0.600189209 batch PCKh 0.625\n",
      "Validated batch 244 batch loss 0.478714228 batch mAP 0.591918945 batch PCKh 0.875\n",
      "Validated batch 245 batch loss 0.561666429 batch mAP 0.633178711 batch PCKh 0.1875\n",
      "Validated batch 246 batch loss 0.563887894 batch mAP 0.597442627 batch PCKh 0.5\n",
      "Validated batch 247 batch loss 0.603362441 batch mAP 0.517700195 batch PCKh 0.375\n",
      "Validated batch 248 batch loss 0.531012654 batch mAP 0.555633545 batch PCKh 0.25\n",
      "Validated batch 249 batch loss 0.608683705 batch mAP 0.507720947 batch PCKh 0.75\n",
      "Validated batch 250 batch loss 0.634093404 batch mAP 0.531066895 batch PCKh 0.4375\n",
      "Validated batch 251 batch loss 0.628993154 batch mAP 0.494110107 batch PCKh 0.1875\n",
      "Validated batch 252 batch loss 0.575358 batch mAP 0.534851074 batch PCKh 0.5625\n",
      "Validated batch 253 batch loss 0.516100764 batch mAP 0.621826172 batch PCKh 0.5625\n",
      "Validated batch 254 batch loss 0.52072531 batch mAP 0.51348877 batch PCKh 0.75\n",
      "Validated batch 255 batch loss 0.383232385 batch mAP 0.600708 batch PCKh 0.4375\n",
      "Validated batch 256 batch loss 0.520995378 batch mAP 0.584320068 batch PCKh 0.5625\n",
      "Validated batch 257 batch loss 0.581442356 batch mAP 0.56073 batch PCKh 0.875\n",
      "Validated batch 258 batch loss 0.564606905 batch mAP 0.588134766 batch PCKh 0.625\n",
      "Validated batch 259 batch loss 0.540290713 batch mAP 0.61126709 batch PCKh 0.5\n",
      "Validated batch 260 batch loss 0.502847433 batch mAP 0.643493652 batch PCKh 0.4375\n",
      "Validated batch 261 batch loss 0.595338285 batch mAP 0.582489 batch PCKh 0.75\n",
      "Validated batch 262 batch loss 0.573638678 batch mAP 0.438934326 batch PCKh 0.5625\n",
      "Validated batch 263 batch loss 0.641562343 batch mAP 0.445617676 batch PCKh 0.125\n",
      "Validated batch 264 batch loss 0.59373796 batch mAP 0.453796387 batch PCKh 0.875\n",
      "Validated batch 265 batch loss 0.542328537 batch mAP 0.655944824 batch PCKh 0.5\n",
      "Validated batch 266 batch loss 0.460826755 batch mAP 0.644348145 batch PCKh 0.625\n",
      "Validated batch 267 batch loss 0.607987761 batch mAP 0.565460205 batch PCKh 0.0625\n",
      "Validated batch 268 batch loss 0.585106611 batch mAP 0.542785645 batch PCKh 0.3125\n",
      "Validated batch 269 batch loss 0.642300367 batch mAP 0.500976562 batch PCKh 0.0625\n",
      "Validated batch 270 batch loss 0.68046242 batch mAP 0.540100098 batch PCKh 0.3125\n",
      "Validated batch 271 batch loss 0.597234607 batch mAP 0.557952881 batch PCKh 0.3125\n",
      "Validated batch 272 batch loss 0.623683751 batch mAP 0.622161865 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 273 batch loss 0.606934786 batch mAP 0.542663574 batch PCKh 0.25\n",
      "Validated batch 274 batch loss 0.5967502 batch mAP 0.552490234 batch PCKh 0.625\n",
      "Validated batch 275 batch loss 0.493641734 batch mAP 0.56854248 batch PCKh 0.75\n",
      "Validated batch 276 batch loss 0.523272038 batch mAP 0.54486084 batch PCKh 0.1875\n",
      "Validated batch 277 batch loss 0.529614925 batch mAP 0.623382568 batch PCKh 0.5625\n",
      "Validated batch 278 batch loss 0.580107 batch mAP 0.590606689 batch PCKh 0.875\n",
      "Validated batch 279 batch loss 0.621467829 batch mAP 0.49508667 batch PCKh 0.4375\n",
      "Validated batch 280 batch loss 0.585180581 batch mAP 0.596130371 batch PCKh 0.75\n",
      "Validated batch 281 batch loss 0.581795573 batch mAP 0.548187256 batch PCKh 0.375\n",
      "Validated batch 282 batch loss 0.65350771 batch mAP 0.551239 batch PCKh 0.8125\n",
      "Validated batch 283 batch loss 0.467327297 batch mAP 0.513397217 batch PCKh 0.25\n",
      "Validated batch 284 batch loss 0.592435837 batch mAP 0.527832031 batch PCKh 0.375\n",
      "Validated batch 285 batch loss 0.612316728 batch mAP 0.600494385 batch PCKh 0.25\n",
      "Validated batch 286 batch loss 0.574354529 batch mAP 0.590759277 batch PCKh 0.4375\n",
      "Validated batch 287 batch loss 0.608321071 batch mAP 0.557556152 batch PCKh 0.6875\n",
      "Validated batch 288 batch loss 0.6287449 batch mAP 0.69039917 batch PCKh 0.875\n",
      "Validated batch 289 batch loss 0.678780675 batch mAP 0.557800293 batch PCKh 0.3125\n",
      "Validated batch 290 batch loss 0.606556892 batch mAP 0.569824219 batch PCKh 0.625\n",
      "Validated batch 291 batch loss 0.618751884 batch mAP 0.534606934 batch PCKh 0.375\n",
      "Validated batch 292 batch loss 0.665654421 batch mAP 0.503417969 batch PCKh 0.125\n",
      "Validated batch 293 batch loss 0.609855652 batch mAP 0.496734619 batch PCKh 0.625\n",
      "Validated batch 294 batch loss 0.628500581 batch mAP 0.505096436 batch PCKh 0\n",
      "Validated batch 295 batch loss 0.579601645 batch mAP 0.586090088 batch PCKh 0.6875\n",
      "Validated batch 296 batch loss 0.572986364 batch mAP 0.603851318 batch PCKh 0.375\n",
      "Validated batch 297 batch loss 0.581195 batch mAP 0.606109619 batch PCKh 0.375\n",
      "Validated batch 298 batch loss 0.590080261 batch mAP 0.637420654 batch PCKh 0.3125\n",
      "Validated batch 299 batch loss 0.566461504 batch mAP 0.56652832 batch PCKh 0.4375\n",
      "Validated batch 300 batch loss 0.651198506 batch mAP 0.492004395 batch PCKh 0.3125\n",
      "Validated batch 301 batch loss 0.548024714 batch mAP 0.449493408 batch PCKh 0.375\n",
      "Validated batch 302 batch loss 0.658895493 batch mAP 0.445465088 batch PCKh 0.0625\n",
      "Validated batch 303 batch loss 0.597037077 batch mAP 0.541381836 batch PCKh 0.4375\n",
      "Validated batch 304 batch loss 0.635611415 batch mAP 0.492523193 batch PCKh 0\n",
      "Validated batch 305 batch loss 0.706598 batch mAP 0.485534668 batch PCKh 0.1875\n",
      "Validated batch 306 batch loss 0.643749356 batch mAP 0.444030762 batch PCKh 0.125\n",
      "Validated batch 307 batch loss 0.620151818 batch mAP 0.539703369 batch PCKh 0.5625\n",
      "Validated batch 308 batch loss 0.590192378 batch mAP 0.59564209 batch PCKh 0.25\n",
      "Validated batch 309 batch loss 0.599986792 batch mAP 0.580169678 batch PCKh 0.5625\n",
      "Validated batch 310 batch loss 0.713873386 batch mAP 0.440704346 batch PCKh 0.25\n",
      "Validated batch 311 batch loss 0.629612148 batch mAP 0.521606445 batch PCKh 0.5\n",
      "Validated batch 312 batch loss 0.537414134 batch mAP 0.53237915 batch PCKh 0.1875\n",
      "Validated batch 313 batch loss 0.622106075 batch mAP 0.556762695 batch PCKh 0.125\n",
      "Validated batch 314 batch loss 0.559351206 batch mAP 0.541351318 batch PCKh 0.1875\n",
      "Validated batch 315 batch loss 0.667611361 batch mAP 0.543792725 batch PCKh 0.625\n",
      "Validated batch 316 batch loss 0.483853579 batch mAP 0.594207764 batch PCKh 0.375\n",
      "Validated batch 317 batch loss 0.524857402 batch mAP 0.611358643 batch PCKh 0.5625\n",
      "Validated batch 318 batch loss 0.550366223 batch mAP 0.558319092 batch PCKh 0.6875\n",
      "Validated batch 319 batch loss 0.708471656 batch mAP 0.529327393 batch PCKh 0.125\n",
      "Validated batch 320 batch loss 0.567665756 batch mAP 0.611450195 batch PCKh 0.75\n",
      "Validated batch 321 batch loss 0.645808697 batch mAP 0.513061523 batch PCKh 0.5\n",
      "Validated batch 322 batch loss 0.517739058 batch mAP 0.602600098 batch PCKh 0.625\n",
      "Validated batch 323 batch loss 0.657426 batch mAP 0.499969482 batch PCKh 0.875\n",
      "Validated batch 324 batch loss 0.65269053 batch mAP 0.455993652 batch PCKh 0.25\n",
      "Validated batch 325 batch loss 0.624956131 batch mAP 0.570922852 batch PCKh 0.1875\n",
      "Validated batch 326 batch loss 0.553795338 batch mAP 0.613189697 batch PCKh 0.625\n",
      "Validated batch 327 batch loss 0.645586 batch mAP 0.559234619 batch PCKh 0.375\n",
      "Validated batch 328 batch loss 0.533597946 batch mAP 0.58215332 batch PCKh 0.5\n",
      "Validated batch 329 batch loss 0.592434883 batch mAP 0.596923828 batch PCKh 0.4375\n",
      "Validated batch 330 batch loss 0.560672641 batch mAP 0.655853271 batch PCKh 0.625\n",
      "Validated batch 331 batch loss 0.572076619 batch mAP 0.591217041 batch PCKh 0.875\n",
      "Validated batch 332 batch loss 0.493953049 batch mAP 0.645782471 batch PCKh 0.5625\n",
      "Validated batch 333 batch loss 0.601163089 batch mAP 0.547821045 batch PCKh 0.3125\n",
      "Validated batch 334 batch loss 0.617344 batch mAP 0.606994629 batch PCKh 0.5625\n",
      "Validated batch 335 batch loss 0.565530241 batch mAP 0.618011475 batch PCKh 0.5\n",
      "Validated batch 336 batch loss 0.64209193 batch mAP 0.561828613 batch PCKh 0.6875\n",
      "Validated batch 337 batch loss 0.58309114 batch mAP 0.647827148 batch PCKh 0\n",
      "Validated batch 338 batch loss 0.535875797 batch mAP 0.575439453 batch PCKh 0.5625\n",
      "Validated batch 339 batch loss 0.52238822 batch mAP 0.60369873 batch PCKh 0.5625\n",
      "Validated batch 340 batch loss 0.555274844 batch mAP 0.629364 batch PCKh 0.25\n",
      "Validated batch 341 batch loss 0.632809758 batch mAP 0.570068359 batch PCKh 0.625\n",
      "Validated batch 342 batch loss 0.585668802 batch mAP 0.61126709 batch PCKh 0.4375\n",
      "Validated batch 343 batch loss 0.578597426 batch mAP 0.615814209 batch PCKh 0.3125\n",
      "Validated batch 344 batch loss 0.605435789 batch mAP 0.553924561 batch PCKh 0.1875\n",
      "Validated batch 345 batch loss 0.664256573 batch mAP 0.481719971 batch PCKh 0.4375\n",
      "Validated batch 346 batch loss 0.622159362 batch mAP 0.482910156 batch PCKh 0.5625\n",
      "Validated batch 347 batch loss 0.54988718 batch mAP 0.567077637 batch PCKh 0.5\n",
      "Validated batch 348 batch loss 0.636009455 batch mAP 0.44821167 batch PCKh 0.375\n",
      "Validated batch 349 batch loss 0.508139 batch mAP 0.498687744 batch PCKh 0.4375\n",
      "Validated batch 350 batch loss 0.470336854 batch mAP 0.580291748 batch PCKh 0.5625\n",
      "Validated batch 351 batch loss 0.648549795 batch mAP 0.531646729 batch PCKh 0.5625\n",
      "Validated batch 352 batch loss 0.535998464 batch mAP 0.669342041 batch PCKh 0.4375\n",
      "Validated batch 353 batch loss 0.570403099 batch mAP 0.542144775 batch PCKh 0.6875\n",
      "Validated batch 354 batch loss 0.605833 batch mAP 0.560241699 batch PCKh 0.375\n",
      "Validated batch 355 batch loss 0.55003041 batch mAP 0.577484131 batch PCKh 0.25\n",
      "Validated batch 356 batch loss 0.588273048 batch mAP 0.591095 batch PCKh 0.3125\n",
      "Validated batch 357 batch loss 0.545793593 batch mAP 0.507568359 batch PCKh 0.1875\n",
      "Validated batch 358 batch loss 0.6634624 batch mAP 0.485595703 batch PCKh 0.1875\n",
      "Validated batch 359 batch loss 0.58232969 batch mAP 0.639068604 batch PCKh 0.25\n",
      "Validated batch 360 batch loss 0.59052074 batch mAP 0.548461914 batch PCKh 0.375\n",
      "Validated batch 361 batch loss 0.659064353 batch mAP 0.584655762 batch PCKh 0.25\n",
      "Validated batch 362 batch loss 0.696248472 batch mAP 0.489044189 batch PCKh 0.125\n",
      "Validated batch 363 batch loss 0.697892785 batch mAP 0.494873047 batch PCKh 0.1875\n",
      "Validated batch 364 batch loss 0.641443372 batch mAP 0.555175781 batch PCKh 0.375\n",
      "Validated batch 365 batch loss 0.569974601 batch mAP 0.650421143 batch PCKh 0.5625\n",
      "Validated batch 366 batch loss 0.623796 batch mAP 0.596221924 batch PCKh 0.375\n",
      "Validated batch 367 batch loss 0.558335 batch mAP 0.476715088 batch PCKh 0.4375\n",
      "Validated batch 368 batch loss 0.525610209 batch mAP 0.576202393 batch PCKh 0.4375\n",
      "Validated batch 369 batch loss 0.512602329 batch mAP 0.602081299 batch PCKh 0.5625\n",
      "Epoch 5 val loss 0.5914719104766846 val mAP 0.5510000586509705 val PCKh\n",
      "Epoch 5 completed in 767.20 seconds\n",
      "Start epoch 6 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.475824356 batch mAP 0.560760498 batch PCKh 0.25\n",
      "Trained batch 2 batch loss 0.580710411 batch mAP 0.547821045 batch PCKh 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 3 batch loss 0.574284494 batch mAP 0.516449 batch PCKh 0.5625\n",
      "Trained batch 4 batch loss 0.593988836 batch mAP 0.547943115 batch PCKh 0.5625\n",
      "Trained batch 5 batch loss 0.599811673 batch mAP 0.516601562 batch PCKh 0.5\n",
      "Trained batch 6 batch loss 0.571921468 batch mAP 0.603027344 batch PCKh 0.5625\n",
      "Trained batch 7 batch loss 0.618312836 batch mAP 0.637756348 batch PCKh 0.8125\n",
      "Trained batch 8 batch loss 0.509736836 batch mAP 0.592468262 batch PCKh 0.625\n",
      "Trained batch 9 batch loss 0.620076835 batch mAP 0.640533447 batch PCKh 0.75\n",
      "Trained batch 10 batch loss 0.613654 batch mAP 0.610412598 batch PCKh 0.625\n",
      "Trained batch 11 batch loss 0.530891895 batch mAP 0.615112305 batch PCKh 0.4375\n",
      "Trained batch 12 batch loss 0.55670476 batch mAP 0.645629883 batch PCKh 0.75\n",
      "Trained batch 13 batch loss 0.603540301 batch mAP 0.61315918 batch PCKh 0.5\n",
      "Trained batch 14 batch loss 0.625872612 batch mAP 0.561950684 batch PCKh 0.5625\n",
      "Trained batch 15 batch loss 0.496562332 batch mAP 0.573272705 batch PCKh 0.5\n",
      "Trained batch 16 batch loss 0.620343626 batch mAP 0.547851562 batch PCKh 0.4375\n",
      "Trained batch 17 batch loss 0.592790484 batch mAP 0.542694092 batch PCKh 0.6875\n",
      "Trained batch 18 batch loss 0.630655289 batch mAP 0.52432251 batch PCKh 0.3125\n",
      "Trained batch 19 batch loss 0.627953529 batch mAP 0.539245605 batch PCKh 0.3125\n",
      "Trained batch 20 batch loss 0.679001689 batch mAP 0.528533936 batch PCKh 0.375\n",
      "Trained batch 21 batch loss 0.493072927 batch mAP 0.598571777 batch PCKh 0.625\n",
      "Trained batch 22 batch loss 0.525831103 batch mAP 0.629608154 batch PCKh 0.625\n",
      "Trained batch 23 batch loss 0.527925968 batch mAP 0.555664062 batch PCKh 0.75\n",
      "Trained batch 24 batch loss 0.593431354 batch mAP 0.506347656 batch PCKh 0.1875\n",
      "Trained batch 25 batch loss 0.631189942 batch mAP 0.502960205 batch PCKh 0.4375\n",
      "Trained batch 26 batch loss 0.546831846 batch mAP 0.588287354 batch PCKh 0.3125\n",
      "Trained batch 27 batch loss 0.504227638 batch mAP 0.636199951 batch PCKh 0.4375\n",
      "Trained batch 28 batch loss 0.568668187 batch mAP 0.6378479 batch PCKh 0.1875\n",
      "Trained batch 29 batch loss 0.561053514 batch mAP 0.639892578 batch PCKh 0.625\n",
      "Trained batch 30 batch loss 0.572662055 batch mAP 0.638793945 batch PCKh 0.0625\n",
      "Trained batch 31 batch loss 0.549521208 batch mAP 0.63974 batch PCKh 0.6875\n",
      "Trained batch 32 batch loss 0.581275105 batch mAP 0.628204346 batch PCKh 0.3125\n",
      "Trained batch 33 batch loss 0.566701233 batch mAP 0.542907715 batch PCKh 0.5\n",
      "Trained batch 34 batch loss 0.555604517 batch mAP 0.570251465 batch PCKh 0.6875\n",
      "Trained batch 35 batch loss 0.621772885 batch mAP 0.502655 batch PCKh 0.875\n",
      "Trained batch 36 batch loss 0.506686568 batch mAP 0.546936035 batch PCKh 0.625\n",
      "Trained batch 37 batch loss 0.478203475 batch mAP 0.586975098 batch PCKh 0.25\n",
      "Trained batch 38 batch loss 0.568058 batch mAP 0.542755127 batch PCKh 0.625\n",
      "Trained batch 39 batch loss 0.69915396 batch mAP 0.448303223 batch PCKh 0.5\n",
      "Trained batch 40 batch loss 0.559343457 batch mAP 0.656524658 batch PCKh 0.375\n",
      "Trained batch 41 batch loss 0.522710204 batch mAP 0.619415283 batch PCKh 0.1875\n",
      "Trained batch 42 batch loss 0.593710899 batch mAP 0.604949951 batch PCKh 0.5625\n",
      "Trained batch 43 batch loss 0.502370536 batch mAP 0.60647583 batch PCKh 0.1875\n",
      "Trained batch 44 batch loss 0.481728554 batch mAP 0.501586914 batch PCKh 0.625\n",
      "Trained batch 45 batch loss 0.502402246 batch mAP 0.506561279 batch PCKh 0.8125\n",
      "Trained batch 46 batch loss 0.482688606 batch mAP 0.521484375 batch PCKh 0.625\n",
      "Trained batch 47 batch loss 0.449144095 batch mAP 0.548584 batch PCKh 0.625\n",
      "Trained batch 48 batch loss 0.506244302 batch mAP 0.61895752 batch PCKh 0.625\n",
      "Trained batch 49 batch loss 0.522585392 batch mAP 0.61138916 batch PCKh 0.625\n",
      "Trained batch 50 batch loss 0.582718253 batch mAP 0.559265137 batch PCKh 0.1875\n",
      "Trained batch 51 batch loss 0.554033875 batch mAP 0.530090332 batch PCKh 0.125\n",
      "Trained batch 52 batch loss 0.55946815 batch mAP 0.605804443 batch PCKh 0.375\n",
      "Trained batch 53 batch loss 0.538749099 batch mAP 0.626312256 batch PCKh 0.375\n",
      "Trained batch 54 batch loss 0.544872403 batch mAP 0.604644775 batch PCKh 0\n",
      "Trained batch 55 batch loss 0.553025305 batch mAP 0.590881348 batch PCKh 0.75\n",
      "Trained batch 56 batch loss 0.536733 batch mAP 0.643829346 batch PCKh 0.1875\n",
      "Trained batch 57 batch loss 0.593446 batch mAP 0.598327637 batch PCKh 0.125\n",
      "Trained batch 58 batch loss 0.646153092 batch mAP 0.540771484 batch PCKh 0.125\n",
      "Trained batch 59 batch loss 0.668703675 batch mAP 0.503448486 batch PCKh 0\n",
      "Trained batch 60 batch loss 0.590445876 batch mAP 0.482208252 batch PCKh 0\n",
      "Trained batch 61 batch loss 0.625379443 batch mAP 0.51550293 batch PCKh 0.3125\n",
      "Trained batch 62 batch loss 0.642800689 batch mAP 0.488708496 batch PCKh 0.625\n",
      "Trained batch 63 batch loss 0.600755394 batch mAP 0.470550537 batch PCKh 0.0625\n",
      "Trained batch 64 batch loss 0.628591 batch mAP 0.437530518 batch PCKh 0.1875\n",
      "Trained batch 65 batch loss 0.609388351 batch mAP 0.537841797 batch PCKh 0.125\n",
      "Trained batch 66 batch loss 0.527516723 batch mAP 0.616943359 batch PCKh 0.4375\n",
      "Trained batch 67 batch loss 0.467269391 batch mAP 0.609924316 batch PCKh 0.3125\n",
      "Trained batch 68 batch loss 0.515727878 batch mAP 0.65045166 batch PCKh 0.375\n",
      "Trained batch 69 batch loss 0.552350521 batch mAP 0.499359131 batch PCKh 0.6875\n",
      "Trained batch 70 batch loss 0.607936442 batch mAP 0.508667 batch PCKh 0.625\n",
      "Trained batch 71 batch loss 0.566372395 batch mAP 0.580627441 batch PCKh 0.4375\n",
      "Trained batch 72 batch loss 0.659693 batch mAP 0.60055542 batch PCKh 0.3125\n",
      "Trained batch 73 batch loss 0.456921 batch mAP 0.696929932 batch PCKh 0.5\n",
      "Trained batch 74 batch loss 0.542924166 batch mAP 0.673095703 batch PCKh 0.5\n",
      "Trained batch 75 batch loss 0.548505962 batch mAP 0.693054199 batch PCKh 0.375\n",
      "Trained batch 76 batch loss 0.495833248 batch mAP 0.661376953 batch PCKh 0.8125\n",
      "Trained batch 77 batch loss 0.480508417 batch mAP 0.675689697 batch PCKh 0.5625\n",
      "Trained batch 78 batch loss 0.510191202 batch mAP 0.619812 batch PCKh 0.4375\n",
      "Trained batch 79 batch loss 0.533456087 batch mAP 0.592437744 batch PCKh 0.5625\n",
      "Trained batch 80 batch loss 0.502256155 batch mAP 0.65020752 batch PCKh 0.375\n",
      "Trained batch 81 batch loss 0.482426345 batch mAP 0.672515869 batch PCKh 0.4375\n",
      "Trained batch 82 batch loss 0.456155747 batch mAP 0.646240234 batch PCKh 0.375\n",
      "Trained batch 83 batch loss 0.487463683 batch mAP 0.685150146 batch PCKh 0.5625\n",
      "Trained batch 84 batch loss 0.560108721 batch mAP 0.671691895 batch PCKh 0.0625\n",
      "Trained batch 85 batch loss 0.465177536 batch mAP 0.686065674 batch PCKh 0.4375\n",
      "Trained batch 86 batch loss 0.575027943 batch mAP 0.629303 batch PCKh 0.375\n",
      "Trained batch 87 batch loss 0.558219194 batch mAP 0.625030518 batch PCKh 0.4375\n",
      "Trained batch 88 batch loss 0.514107823 batch mAP 0.634185791 batch PCKh 0.625\n",
      "Trained batch 89 batch loss 0.464277565 batch mAP 0.603973389 batch PCKh 0.875\n",
      "Trained batch 90 batch loss 0.524977684 batch mAP 0.597503662 batch PCKh 0.75\n",
      "Trained batch 91 batch loss 0.637426853 batch mAP 0.482269287 batch PCKh 0.75\n",
      "Trained batch 92 batch loss 0.545149624 batch mAP 0.573699951 batch PCKh 0.75\n",
      "Trained batch 93 batch loss 0.519483626 batch mAP 0.582305908 batch PCKh 0.75\n",
      "Trained batch 94 batch loss 0.551081777 batch mAP 0.63394165 batch PCKh 0.375\n",
      "Trained batch 95 batch loss 0.575755596 batch mAP 0.61895752 batch PCKh 0.375\n",
      "Trained batch 96 batch loss 0.650600076 batch mAP 0.553466797 batch PCKh 0.1875\n",
      "Trained batch 97 batch loss 0.61820507 batch mAP 0.56463623 batch PCKh 0.5\n",
      "Trained batch 98 batch loss 0.620716929 batch mAP 0.570922852 batch PCKh 0.1875\n",
      "Trained batch 99 batch loss 0.604477227 batch mAP 0.563934326 batch PCKh 0.6875\n",
      "Trained batch 100 batch loss 0.772828341 batch mAP 0.478179932 batch PCKh 0\n",
      "Trained batch 101 batch loss 0.632224321 batch mAP 0.562286377 batch PCKh 0.5\n",
      "Trained batch 102 batch loss 0.566394508 batch mAP 0.534667969 batch PCKh 0.25\n",
      "Trained batch 103 batch loss 0.505145669 batch mAP 0.522735596 batch PCKh 0.3125\n",
      "Trained batch 104 batch loss 0.450318664 batch mAP 0.458770752 batch PCKh 0.25\n",
      "Trained batch 105 batch loss 0.445837885 batch mAP 0.573150635 batch PCKh 0.25\n",
      "Trained batch 106 batch loss 0.527603388 batch mAP 0.541381836 batch PCKh 0.1875\n",
      "Trained batch 107 batch loss 0.554058969 batch mAP 0.546630859 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 108 batch loss 0.541313887 batch mAP 0.50680542 batch PCKh 0.5625\n",
      "Trained batch 109 batch loss 0.438234448 batch mAP 0.631286621 batch PCKh 0.5\n",
      "Trained batch 110 batch loss 0.448541164 batch mAP 0.590789795 batch PCKh 0.375\n",
      "Trained batch 111 batch loss 0.408461601 batch mAP 0.612243652 batch PCKh 0\n",
      "Trained batch 112 batch loss 0.473108 batch mAP 0.634643555 batch PCKh 0.375\n",
      "Trained batch 113 batch loss 0.495521605 batch mAP 0.610229492 batch PCKh 0.6875\n",
      "Trained batch 114 batch loss 0.405049592 batch mAP 0.720459 batch PCKh 0.375\n",
      "Trained batch 115 batch loss 0.4597449 batch mAP 0.698608398 batch PCKh 0.375\n",
      "Trained batch 116 batch loss 0.441846967 batch mAP 0.65222168 batch PCKh 0.3125\n",
      "Trained batch 117 batch loss 0.527207136 batch mAP 0.656860352 batch PCKh 0.3125\n",
      "Trained batch 118 batch loss 0.370530128 batch mAP 0.729064941 batch PCKh 0.3125\n",
      "Trained batch 119 batch loss 0.395984054 batch mAP 0.68371582 batch PCKh 0.625\n",
      "Trained batch 120 batch loss 0.383341432 batch mAP 0.678741455 batch PCKh 0.3125\n",
      "Trained batch 121 batch loss 0.471153855 batch mAP 0.603118896 batch PCKh 0.0625\n",
      "Trained batch 122 batch loss 0.445051968 batch mAP 0.649353 batch PCKh 0.375\n",
      "Trained batch 123 batch loss 0.555258632 batch mAP 0.530517578 batch PCKh 0.3125\n",
      "Trained batch 124 batch loss 0.591909051 batch mAP 0.5831604 batch PCKh 0.3125\n",
      "Trained batch 125 batch loss 0.538905 batch mAP 0.573181152 batch PCKh 0.3125\n",
      "Trained batch 126 batch loss 0.479532182 batch mAP 0.627624512 batch PCKh 0.3125\n",
      "Trained batch 127 batch loss 0.352390945 batch mAP 0.696044922 batch PCKh 0.5\n",
      "Trained batch 128 batch loss 0.455322385 batch mAP 0.577789307 batch PCKh 0.3125\n",
      "Trained batch 129 batch loss 0.46586892 batch mAP 0.604522705 batch PCKh 0.3125\n",
      "Trained batch 130 batch loss 0.479830593 batch mAP 0.601196289 batch PCKh 0.4375\n",
      "Trained batch 131 batch loss 0.488149524 batch mAP 0.572021484 batch PCKh 0.375\n",
      "Trained batch 132 batch loss 0.555798173 batch mAP 0.605133057 batch PCKh 0.5\n",
      "Trained batch 133 batch loss 0.486397415 batch mAP 0.64251709 batch PCKh 0.6875\n",
      "Trained batch 134 batch loss 0.524433315 batch mAP 0.638580322 batch PCKh 0.625\n",
      "Trained batch 135 batch loss 0.575323939 batch mAP 0.580383301 batch PCKh 0.3125\n",
      "Trained batch 136 batch loss 0.541214228 batch mAP 0.583221436 batch PCKh 0.125\n",
      "Trained batch 137 batch loss 0.519187391 batch mAP 0.642120361 batch PCKh 0.375\n",
      "Trained batch 138 batch loss 0.563445926 batch mAP 0.611145 batch PCKh 0.3125\n",
      "Trained batch 139 batch loss 0.602082968 batch mAP 0.594543457 batch PCKh 0.3125\n",
      "Trained batch 140 batch loss 0.520475507 batch mAP 0.601898193 batch PCKh 0.375\n",
      "Trained batch 141 batch loss 0.527386487 batch mAP 0.616668701 batch PCKh 0.375\n",
      "Trained batch 142 batch loss 0.574162 batch mAP 0.499786377 batch PCKh 0.375\n",
      "Trained batch 143 batch loss 0.55129981 batch mAP 0.442321777 batch PCKh 0.625\n",
      "Trained batch 144 batch loss 0.507505059 batch mAP 0.513366699 batch PCKh 0.4375\n",
      "Trained batch 145 batch loss 0.574551 batch mAP 0.570404053 batch PCKh 0.375\n",
      "Trained batch 146 batch loss 0.602105558 batch mAP 0.507019043 batch PCKh 0.3125\n",
      "Trained batch 147 batch loss 0.596301317 batch mAP 0.55166626 batch PCKh 0.8125\n",
      "Trained batch 148 batch loss 0.536825299 batch mAP 0.524505615 batch PCKh 0.5625\n",
      "Trained batch 149 batch loss 0.546813309 batch mAP 0.544525146 batch PCKh 0.8125\n",
      "Trained batch 150 batch loss 0.607834101 batch mAP 0.537139893 batch PCKh 0.625\n",
      "Trained batch 151 batch loss 0.569754303 batch mAP 0.576599121 batch PCKh 0.75\n",
      "Trained batch 152 batch loss 0.534387589 batch mAP 0.521179199 batch PCKh 0.625\n",
      "Trained batch 153 batch loss 0.544632614 batch mAP 0.578949 batch PCKh 0.5\n",
      "Trained batch 154 batch loss 0.521605134 batch mAP 0.562744141 batch PCKh 0.5625\n",
      "Trained batch 155 batch loss 0.531726599 batch mAP 0.522979736 batch PCKh 0.125\n",
      "Trained batch 156 batch loss 0.538815 batch mAP 0.583984375 batch PCKh 0.125\n",
      "Trained batch 157 batch loss 0.4846614 batch mAP 0.587432861 batch PCKh 0.5\n",
      "Trained batch 158 batch loss 0.533167839 batch mAP 0.594604492 batch PCKh 0.3125\n",
      "Trained batch 159 batch loss 0.603456676 batch mAP 0.591095 batch PCKh 0.625\n",
      "Trained batch 160 batch loss 0.574626923 batch mAP 0.589477539 batch PCKh 0.5\n",
      "Trained batch 161 batch loss 0.599552572 batch mAP 0.551879883 batch PCKh 0.75\n",
      "Trained batch 162 batch loss 0.587637186 batch mAP 0.520263672 batch PCKh 0.125\n",
      "Trained batch 163 batch loss 0.573622823 batch mAP 0.551879883 batch PCKh 0.125\n",
      "Trained batch 164 batch loss 0.616211653 batch mAP 0.550628662 batch PCKh 0.5625\n",
      "Trained batch 165 batch loss 0.658997774 batch mAP 0.548095703 batch PCKh 0.5625\n",
      "Trained batch 166 batch loss 0.605317056 batch mAP 0.568969727 batch PCKh 0.625\n",
      "Trained batch 167 batch loss 0.635076523 batch mAP 0.543914795 batch PCKh 0.4375\n",
      "Trained batch 168 batch loss 0.634960413 batch mAP 0.56842041 batch PCKh 0.5\n",
      "Trained batch 169 batch loss 0.538242 batch mAP 0.583740234 batch PCKh 0.625\n",
      "Trained batch 170 batch loss 0.625700593 batch mAP 0.544403076 batch PCKh 0.625\n",
      "Trained batch 171 batch loss 0.563303649 batch mAP 0.624786377 batch PCKh 0.4375\n",
      "Trained batch 172 batch loss 0.5357337 batch mAP 0.637329102 batch PCKh 0.25\n",
      "Trained batch 173 batch loss 0.54014492 batch mAP 0.583221436 batch PCKh 0.5625\n",
      "Trained batch 174 batch loss 0.548320353 batch mAP 0.521942139 batch PCKh 0.6875\n",
      "Trained batch 175 batch loss 0.633657 batch mAP 0.544708252 batch PCKh 0.3125\n",
      "Trained batch 176 batch loss 0.700947165 batch mAP 0.478210449 batch PCKh 0.1875\n",
      "Trained batch 177 batch loss 0.580658197 batch mAP 0.528167725 batch PCKh 0.0625\n",
      "Trained batch 178 batch loss 0.56911087 batch mAP 0.573364258 batch PCKh 0.375\n",
      "Trained batch 179 batch loss 0.639332235 batch mAP 0.548461914 batch PCKh 0.25\n",
      "Trained batch 180 batch loss 0.487830281 batch mAP 0.571472168 batch PCKh 0.375\n",
      "Trained batch 181 batch loss 0.435548753 batch mAP 0.566833496 batch PCKh 0.4375\n",
      "Trained batch 182 batch loss 0.447315216 batch mAP 0.525299072 batch PCKh 0.375\n",
      "Trained batch 183 batch loss 0.541414499 batch mAP 0.599365234 batch PCKh 0.375\n",
      "Trained batch 184 batch loss 0.612493038 batch mAP 0.524810791 batch PCKh 0.375\n",
      "Trained batch 185 batch loss 0.57265687 batch mAP 0.611053467 batch PCKh 0.5\n",
      "Trained batch 186 batch loss 0.559678853 batch mAP 0.60849 batch PCKh 0.5625\n",
      "Trained batch 187 batch loss 0.522427499 batch mAP 0.602325439 batch PCKh 0.375\n",
      "Trained batch 188 batch loss 0.45370692 batch mAP 0.608001709 batch PCKh 0.5\n",
      "Trained batch 189 batch loss 0.499522597 batch mAP 0.555175781 batch PCKh 0.625\n",
      "Trained batch 190 batch loss 0.456255764 batch mAP 0.547363281 batch PCKh 0.6875\n",
      "Trained batch 191 batch loss 0.395661861 batch mAP 0.547637939 batch PCKh 0.4375\n",
      "Trained batch 192 batch loss 0.42667827 batch mAP 0.573699951 batch PCKh 0.25\n",
      "Trained batch 193 batch loss 0.473339498 batch mAP 0.570098877 batch PCKh 0.5\n",
      "Trained batch 194 batch loss 0.508339703 batch mAP 0.599914551 batch PCKh 0.75\n",
      "Trained batch 195 batch loss 0.519288599 batch mAP 0.537536621 batch PCKh 0.625\n",
      "Trained batch 196 batch loss 0.514769435 batch mAP 0.610199 batch PCKh 0.5625\n",
      "Trained batch 197 batch loss 0.492754936 batch mAP 0.596252441 batch PCKh 0.5\n",
      "Trained batch 198 batch loss 0.440737873 batch mAP 0.604187 batch PCKh 0.625\n",
      "Trained batch 199 batch loss 0.429849327 batch mAP 0.625 batch PCKh 0.6875\n",
      "Trained batch 200 batch loss 0.497360229 batch mAP 0.600463867 batch PCKh 0.5625\n",
      "Trained batch 201 batch loss 0.547633767 batch mAP 0.581268311 batch PCKh 0.5625\n",
      "Trained batch 202 batch loss 0.476726949 batch mAP 0.563842773 batch PCKh 0.6875\n",
      "Trained batch 203 batch loss 0.545925677 batch mAP 0.534942627 batch PCKh 0.3125\n",
      "Trained batch 204 batch loss 0.549315274 batch mAP 0.508514404 batch PCKh 0.625\n",
      "Trained batch 205 batch loss 0.525738955 batch mAP 0.550506592 batch PCKh 0.625\n",
      "Trained batch 206 batch loss 0.525181413 batch mAP 0.501861572 batch PCKh 0.5\n",
      "Trained batch 207 batch loss 0.505866051 batch mAP 0.575805664 batch PCKh 0.875\n",
      "Trained batch 208 batch loss 0.472212374 batch mAP 0.574646 batch PCKh 0.5\n",
      "Trained batch 209 batch loss 0.454726666 batch mAP 0.609985352 batch PCKh 0.75\n",
      "Trained batch 210 batch loss 0.568336427 batch mAP 0.591400146 batch PCKh 0.375\n",
      "Trained batch 211 batch loss 0.497142255 batch mAP 0.577453613 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 212 batch loss 0.627146363 batch mAP 0.571899414 batch PCKh 0.5\n",
      "Trained batch 213 batch loss 0.624912679 batch mAP 0.575744629 batch PCKh 0.1875\n",
      "Trained batch 214 batch loss 0.485493839 batch mAP 0.650024414 batch PCKh 0.1875\n",
      "Trained batch 215 batch loss 0.549432874 batch mAP 0.523498535 batch PCKh 0.25\n",
      "Trained batch 216 batch loss 0.53395611 batch mAP 0.527496338 batch PCKh 0.3125\n",
      "Trained batch 217 batch loss 0.436179847 batch mAP 0.589263916 batch PCKh 0.375\n",
      "Trained batch 218 batch loss 0.443119824 batch mAP 0.574981689 batch PCKh 0.5\n",
      "Trained batch 219 batch loss 0.433057308 batch mAP 0.582305908 batch PCKh 0.625\n",
      "Trained batch 220 batch loss 0.402710915 batch mAP 0.628540039 batch PCKh 0\n",
      "Trained batch 221 batch loss 0.492578298 batch mAP 0.608581543 batch PCKh 0.6875\n",
      "Trained batch 222 batch loss 0.559726179 batch mAP 0.572631836 batch PCKh 0.6875\n",
      "Trained batch 223 batch loss 0.630385399 batch mAP 0.564086914 batch PCKh 0.4375\n",
      "Trained batch 224 batch loss 0.632520497 batch mAP 0.523132324 batch PCKh 0.3125\n",
      "Trained batch 225 batch loss 0.576163828 batch mAP 0.573516846 batch PCKh 0.8125\n",
      "Trained batch 226 batch loss 0.564762771 batch mAP 0.602966309 batch PCKh 0.75\n",
      "Trained batch 227 batch loss 0.607913136 batch mAP 0.581604 batch PCKh 0.3125\n",
      "Trained batch 228 batch loss 0.530715704 batch mAP 0.602417 batch PCKh 0.8125\n",
      "Trained batch 229 batch loss 0.548751831 batch mAP 0.586608887 batch PCKh 0.6875\n",
      "Trained batch 230 batch loss 0.598756611 batch mAP 0.613922119 batch PCKh 0.4375\n",
      "Trained batch 231 batch loss 0.627248406 batch mAP 0.614929199 batch PCKh 0.625\n",
      "Trained batch 232 batch loss 0.623758614 batch mAP 0.626098633 batch PCKh 0.3125\n",
      "Trained batch 233 batch loss 0.530467749 batch mAP 0.664123535 batch PCKh 0.5\n",
      "Trained batch 234 batch loss 0.528664827 batch mAP 0.663757324 batch PCKh 0.6875\n",
      "Trained batch 235 batch loss 0.569373488 batch mAP 0.639343262 batch PCKh 0.1875\n",
      "Trained batch 236 batch loss 0.637792706 batch mAP 0.573242188 batch PCKh 0.75\n",
      "Trained batch 237 batch loss 0.5741328 batch mAP 0.608520508 batch PCKh 0.3125\n",
      "Trained batch 238 batch loss 0.626577556 batch mAP 0.567047119 batch PCKh 0.5\n",
      "Trained batch 239 batch loss 0.607491374 batch mAP 0.529388428 batch PCKh 0.3125\n",
      "Trained batch 240 batch loss 0.499738187 batch mAP 0.659210205 batch PCKh 0.625\n",
      "Trained batch 241 batch loss 0.530509114 batch mAP 0.544189453 batch PCKh 0.375\n",
      "Trained batch 242 batch loss 0.510069609 batch mAP 0.593139648 batch PCKh 0.1875\n",
      "Trained batch 243 batch loss 0.488884211 batch mAP 0.603546143 batch PCKh 0.1875\n",
      "Trained batch 244 batch loss 0.463145941 batch mAP 0.62689209 batch PCKh 0.4375\n",
      "Trained batch 245 batch loss 0.47313422 batch mAP 0.623962402 batch PCKh 0.5\n",
      "Trained batch 246 batch loss 0.354607821 batch mAP 0.601318359 batch PCKh 0.1875\n",
      "Trained batch 247 batch loss 0.368186325 batch mAP 0.582305908 batch PCKh 0\n",
      "Trained batch 248 batch loss 0.496746778 batch mAP 0.592590332 batch PCKh 0.375\n",
      "Trained batch 249 batch loss 0.506970763 batch mAP 0.607849121 batch PCKh 0.25\n",
      "Trained batch 250 batch loss 0.520058036 batch mAP 0.575622559 batch PCKh 0.1875\n",
      "Trained batch 251 batch loss 0.514392734 batch mAP 0.601318359 batch PCKh 0.3125\n",
      "Trained batch 252 batch loss 0.577221751 batch mAP 0.592254639 batch PCKh 0.3125\n",
      "Trained batch 253 batch loss 0.566725194 batch mAP 0.637359619 batch PCKh 0.5\n",
      "Trained batch 254 batch loss 0.554578304 batch mAP 0.587554932 batch PCKh 0.375\n",
      "Trained batch 255 batch loss 0.525717497 batch mAP 0.548187256 batch PCKh 0.625\n",
      "Trained batch 256 batch loss 0.644324601 batch mAP 0.514526367 batch PCKh 0.8125\n",
      "Trained batch 257 batch loss 0.567973614 batch mAP 0.533935547 batch PCKh 0.75\n",
      "Trained batch 258 batch loss 0.61371696 batch mAP 0.559661865 batch PCKh 0.375\n",
      "Trained batch 259 batch loss 0.523181558 batch mAP 0.57824707 batch PCKh 0.6875\n",
      "Trained batch 260 batch loss 0.4975366 batch mAP 0.540222168 batch PCKh 0.5\n",
      "Trained batch 261 batch loss 0.462433219 batch mAP 0.580810547 batch PCKh 0.375\n",
      "Trained batch 262 batch loss 0.50997138 batch mAP 0.539978 batch PCKh 0.75\n",
      "Trained batch 263 batch loss 0.451335967 batch mAP 0.579772949 batch PCKh 0.75\n",
      "Trained batch 264 batch loss 0.493508458 batch mAP 0.572052 batch PCKh 0.5625\n",
      "Trained batch 265 batch loss 0.569132686 batch mAP 0.515014648 batch PCKh 0.625\n",
      "Trained batch 266 batch loss 0.598259628 batch mAP 0.526580811 batch PCKh 0.5625\n",
      "Trained batch 267 batch loss 0.58292532 batch mAP 0.490661621 batch PCKh 0.5625\n",
      "Trained batch 268 batch loss 0.543832302 batch mAP 0.532226562 batch PCKh 0.3125\n",
      "Trained batch 269 batch loss 0.5294016 batch mAP 0.566589355 batch PCKh 0.3125\n",
      "Trained batch 270 batch loss 0.552683234 batch mAP 0.524597168 batch PCKh 0.5\n",
      "Trained batch 271 batch loss 0.601195633 batch mAP 0.502227783 batch PCKh 0.4375\n",
      "Trained batch 272 batch loss 0.522649944 batch mAP 0.592651367 batch PCKh 0.625\n",
      "Trained batch 273 batch loss 0.528274417 batch mAP 0.536376953 batch PCKh 0.875\n",
      "Trained batch 274 batch loss 0.542578459 batch mAP 0.484191895 batch PCKh 0.5\n",
      "Trained batch 275 batch loss 0.465144753 batch mAP 0.469329834 batch PCKh 0.5625\n",
      "Trained batch 276 batch loss 0.487447917 batch mAP 0.489257812 batch PCKh 0\n",
      "Trained batch 277 batch loss 0.563273787 batch mAP 0.470001221 batch PCKh 0.75\n",
      "Trained batch 278 batch loss 0.564472437 batch mAP 0.564147949 batch PCKh 0.1875\n",
      "Trained batch 279 batch loss 0.537381947 batch mAP 0.644165039 batch PCKh 0.3125\n",
      "Trained batch 280 batch loss 0.559592128 batch mAP 0.634857178 batch PCKh 0.375\n",
      "Trained batch 281 batch loss 0.506592035 batch mAP 0.663238525 batch PCKh 0.625\n",
      "Trained batch 282 batch loss 0.519148 batch mAP 0.658050537 batch PCKh 0.6875\n",
      "Trained batch 283 batch loss 0.400187522 batch mAP 0.694030762 batch PCKh 0.5\n",
      "Trained batch 284 batch loss 0.474675596 batch mAP 0.665863037 batch PCKh 0.625\n",
      "Trained batch 285 batch loss 0.453429669 batch mAP 0.627502441 batch PCKh 0.1875\n",
      "Trained batch 286 batch loss 0.511240721 batch mAP 0.590393066 batch PCKh 0.5625\n",
      "Trained batch 287 batch loss 0.579170644 batch mAP 0.59664917 batch PCKh 0.5625\n",
      "Trained batch 288 batch loss 0.475253642 batch mAP 0.56451416 batch PCKh 0.375\n",
      "Trained batch 289 batch loss 0.597455561 batch mAP 0.54989624 batch PCKh 0.5\n",
      "Trained batch 290 batch loss 0.505703866 batch mAP 0.562347412 batch PCKh 0.375\n",
      "Trained batch 291 batch loss 0.567052424 batch mAP 0.559387207 batch PCKh 0.3125\n",
      "Trained batch 292 batch loss 0.581467807 batch mAP 0.516967773 batch PCKh 0.5625\n",
      "Trained batch 293 batch loss 0.47301048 batch mAP 0.553985596 batch PCKh 0.75\n",
      "Trained batch 294 batch loss 0.523160577 batch mAP 0.563415527 batch PCKh 0.875\n",
      "Trained batch 295 batch loss 0.463158667 batch mAP 0.583068848 batch PCKh 0.875\n",
      "Trained batch 296 batch loss 0.414680749 batch mAP 0.673553467 batch PCKh 0.4375\n",
      "Trained batch 297 batch loss 0.398792505 batch mAP 0.664520264 batch PCKh 0.6875\n",
      "Trained batch 298 batch loss 0.397657573 batch mAP 0.672393799 batch PCKh 0.8125\n",
      "Trained batch 299 batch loss 0.402735382 batch mAP 0.652130127 batch PCKh 0.8125\n",
      "Trained batch 300 batch loss 0.338438213 batch mAP 0.706573486 batch PCKh 0.5625\n",
      "Trained batch 301 batch loss 0.44721365 batch mAP 0.652709961 batch PCKh 0.5625\n",
      "Trained batch 302 batch loss 0.456926465 batch mAP 0.644683838 batch PCKh 0.75\n",
      "Trained batch 303 batch loss 0.511567295 batch mAP 0.572418213 batch PCKh 0.75\n",
      "Trained batch 304 batch loss 0.51205337 batch mAP 0.609863281 batch PCKh 0.375\n",
      "Trained batch 305 batch loss 0.532335162 batch mAP 0.618591309 batch PCKh 0.5\n",
      "Trained batch 306 batch loss 0.584314704 batch mAP 0.565460205 batch PCKh 0.875\n",
      "Trained batch 307 batch loss 0.584701538 batch mAP 0.657226562 batch PCKh 0.75\n",
      "Trained batch 308 batch loss 0.593555868 batch mAP 0.66519165 batch PCKh 0.3125\n",
      "Trained batch 309 batch loss 0.547023773 batch mAP 0.700012207 batch PCKh 0.3125\n",
      "Trained batch 310 batch loss 0.541880608 batch mAP 0.666412354 batch PCKh 0.25\n",
      "Trained batch 311 batch loss 0.559242368 batch mAP 0.639434814 batch PCKh 0.3125\n",
      "Trained batch 312 batch loss 0.619944692 batch mAP 0.619293213 batch PCKh 0.5625\n",
      "Trained batch 313 batch loss 0.563486278 batch mAP 0.597625732 batch PCKh 0.4375\n",
      "Trained batch 314 batch loss 0.69969362 batch mAP 0.495178223 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 315 batch loss 0.610531092 batch mAP 0.525756836 batch PCKh 0.8125\n",
      "Trained batch 316 batch loss 0.599549949 batch mAP 0.542358398 batch PCKh 0.1875\n",
      "Trained batch 317 batch loss 0.569044232 batch mAP 0.54473877 batch PCKh 0.3125\n",
      "Trained batch 318 batch loss 0.534456372 batch mAP 0.409606934 batch PCKh 0.8125\n",
      "Trained batch 319 batch loss 0.627800107 batch mAP 0.46282959 batch PCKh 0.875\n",
      "Trained batch 320 batch loss 0.51744771 batch mAP 0.490509033 batch PCKh 0.875\n",
      "Trained batch 321 batch loss 0.54872036 batch mAP 0.46585083 batch PCKh 0.75\n",
      "Trained batch 322 batch loss 0.55042088 batch mAP 0.496917725 batch PCKh 0.4375\n",
      "Trained batch 323 batch loss 0.546881735 batch mAP 0.558807373 batch PCKh 0.5625\n",
      "Trained batch 324 batch loss 0.528939366 batch mAP 0.506378174 batch PCKh 0.75\n",
      "Trained batch 325 batch loss 0.606304407 batch mAP 0.473114 batch PCKh 0.625\n",
      "Trained batch 326 batch loss 0.56178081 batch mAP 0.556518555 batch PCKh 0.1875\n",
      "Trained batch 327 batch loss 0.623096406 batch mAP 0.565307617 batch PCKh 0\n",
      "Trained batch 328 batch loss 0.580473483 batch mAP 0.56552124 batch PCKh 0.1875\n",
      "Trained batch 329 batch loss 0.591595709 batch mAP 0.49798584 batch PCKh 0.8125\n",
      "Trained batch 330 batch loss 0.658956528 batch mAP 0.473388672 batch PCKh 0.5\n",
      "Trained batch 331 batch loss 0.688239932 batch mAP 0.462493896 batch PCKh 0.625\n",
      "Trained batch 332 batch loss 0.68340832 batch mAP 0.518310547 batch PCKh 0.0625\n",
      "Trained batch 333 batch loss 0.446637779 batch mAP 0.640258789 batch PCKh 0.625\n",
      "Trained batch 334 batch loss 0.55024147 batch mAP 0.569671631 batch PCKh 0.0625\n",
      "Trained batch 335 batch loss 0.53002286 batch mAP 0.586578369 batch PCKh 0\n",
      "Trained batch 336 batch loss 0.563545287 batch mAP 0.509124756 batch PCKh 0.3125\n",
      "Trained batch 337 batch loss 0.639785945 batch mAP 0.528106689 batch PCKh 0.0625\n",
      "Trained batch 338 batch loss 0.561621547 batch mAP 0.596496582 batch PCKh 0.125\n",
      "Trained batch 339 batch loss 0.516180575 batch mAP 0.553772 batch PCKh 0.25\n",
      "Trained batch 340 batch loss 0.591038465 batch mAP 0.508422852 batch PCKh 0.3125\n",
      "Trained batch 341 batch loss 0.619274 batch mAP 0.502685547 batch PCKh 0.375\n",
      "Trained batch 342 batch loss 0.643861 batch mAP 0.480499268 batch PCKh 0.1875\n",
      "Trained batch 343 batch loss 0.687921643 batch mAP 0.506469727 batch PCKh 0\n",
      "Trained batch 344 batch loss 0.496545374 batch mAP 0.634246826 batch PCKh 0.5\n",
      "Trained batch 345 batch loss 0.490498 batch mAP 0.61831665 batch PCKh 0.375\n",
      "Trained batch 346 batch loss 0.434529603 batch mAP 0.558807373 batch PCKh 0.375\n",
      "Trained batch 347 batch loss 0.548157215 batch mAP 0.527740479 batch PCKh 0.4375\n",
      "Trained batch 348 batch loss 0.559888244 batch mAP 0.543579102 batch PCKh 0.375\n",
      "Trained batch 349 batch loss 0.626272261 batch mAP 0.504333496 batch PCKh 0.1875\n",
      "Trained batch 350 batch loss 0.641856909 batch mAP 0.501556396 batch PCKh 0.1875\n",
      "Trained batch 351 batch loss 0.614146233 batch mAP 0.456756592 batch PCKh 0.1875\n",
      "Trained batch 352 batch loss 0.603946805 batch mAP 0.510955811 batch PCKh 0.375\n",
      "Trained batch 353 batch loss 0.523732066 batch mAP 0.589904785 batch PCKh 0.625\n",
      "Trained batch 354 batch loss 0.599170208 batch mAP 0.547302246 batch PCKh 0.75\n",
      "Trained batch 355 batch loss 0.542834342 batch mAP 0.591705322 batch PCKh 0.3125\n",
      "Trained batch 356 batch loss 0.541985 batch mAP 0.499237061 batch PCKh 0.0625\n",
      "Trained batch 357 batch loss 0.550485671 batch mAP 0.518951416 batch PCKh 0.3125\n",
      "Trained batch 358 batch loss 0.539380312 batch mAP 0.552490234 batch PCKh 0.75\n",
      "Trained batch 359 batch loss 0.541238 batch mAP 0.559997559 batch PCKh 0.625\n",
      "Trained batch 360 batch loss 0.591542602 batch mAP 0.554840088 batch PCKh 0.5\n",
      "Trained batch 361 batch loss 0.543555558 batch mAP 0.657653809 batch PCKh 0.4375\n",
      "Trained batch 362 batch loss 0.578938484 batch mAP 0.593933105 batch PCKh 0.625\n",
      "Trained batch 363 batch loss 0.557210207 batch mAP 0.595794678 batch PCKh 0.6875\n",
      "Trained batch 364 batch loss 0.463403046 batch mAP 0.579620361 batch PCKh 0.5\n",
      "Trained batch 365 batch loss 0.583316565 batch mAP 0.546661377 batch PCKh 0.625\n",
      "Trained batch 366 batch loss 0.59836787 batch mAP 0.444610596 batch PCKh 0.1875\n",
      "Trained batch 367 batch loss 0.680067897 batch mAP 0.457092285 batch PCKh 0.3125\n",
      "Trained batch 368 batch loss 0.536129594 batch mAP 0.460479736 batch PCKh 0.625\n",
      "Trained batch 369 batch loss 0.531444073 batch mAP 0.497070312 batch PCKh 0.5625\n",
      "Trained batch 370 batch loss 0.541031778 batch mAP 0.485076904 batch PCKh 0.625\n",
      "Trained batch 371 batch loss 0.56295 batch mAP 0.538238525 batch PCKh 0.0625\n",
      "Trained batch 372 batch loss 0.579267383 batch mAP 0.597595215 batch PCKh 0.5625\n",
      "Trained batch 373 batch loss 0.54051131 batch mAP 0.58706665 batch PCKh 0.25\n",
      "Trained batch 374 batch loss 0.550961614 batch mAP 0.609771729 batch PCKh 0.5\n",
      "Trained batch 375 batch loss 0.464071423 batch mAP 0.616333 batch PCKh 0.4375\n",
      "Trained batch 376 batch loss 0.367255479 batch mAP 0.674407959 batch PCKh 0.5\n",
      "Trained batch 377 batch loss 0.495107234 batch mAP 0.630523682 batch PCKh 0.5625\n",
      "Trained batch 378 batch loss 0.52571255 batch mAP 0.595153809 batch PCKh 0.5\n",
      "Trained batch 379 batch loss 0.512479365 batch mAP 0.599456787 batch PCKh 0.5625\n",
      "Trained batch 380 batch loss 0.532668471 batch mAP 0.591796875 batch PCKh 0.5\n",
      "Trained batch 381 batch loss 0.485484838 batch mAP 0.679748535 batch PCKh 0.4375\n",
      "Trained batch 382 batch loss 0.499206871 batch mAP 0.673828125 batch PCKh 0.75\n",
      "Trained batch 383 batch loss 0.429471523 batch mAP 0.698425293 batch PCKh 0.375\n",
      "Trained batch 384 batch loss 0.450834244 batch mAP 0.684234619 batch PCKh 0.3125\n",
      "Trained batch 385 batch loss 0.516534865 batch mAP 0.664306641 batch PCKh 0.4375\n",
      "Trained batch 386 batch loss 0.484012872 batch mAP 0.693115234 batch PCKh 0.5625\n",
      "Trained batch 387 batch loss 0.491618097 batch mAP 0.667388916 batch PCKh 0.875\n",
      "Trained batch 388 batch loss 0.48700881 batch mAP 0.675262451 batch PCKh 0.625\n",
      "Trained batch 389 batch loss 0.606682777 batch mAP 0.555755615 batch PCKh 0.625\n",
      "Trained batch 390 batch loss 0.626335263 batch mAP 0.541503906 batch PCKh 0.6875\n",
      "Trained batch 391 batch loss 0.606795549 batch mAP 0.519928 batch PCKh 0.5\n",
      "Trained batch 392 batch loss 0.628499091 batch mAP 0.558410645 batch PCKh 0.1875\n",
      "Trained batch 393 batch loss 0.577637672 batch mAP 0.596008301 batch PCKh 0.4375\n",
      "Trained batch 394 batch loss 0.554356694 batch mAP 0.61932373 batch PCKh 0.75\n",
      "Trained batch 395 batch loss 0.513519406 batch mAP 0.65411377 batch PCKh 0.5625\n",
      "Trained batch 396 batch loss 0.578617632 batch mAP 0.637329102 batch PCKh 0.375\n",
      "Trained batch 397 batch loss 0.568711698 batch mAP 0.611541748 batch PCKh 0.3125\n",
      "Trained batch 398 batch loss 0.634680629 batch mAP 0.537750244 batch PCKh 0.75\n",
      "Trained batch 399 batch loss 0.554188609 batch mAP 0.569091797 batch PCKh 0.3125\n",
      "Trained batch 400 batch loss 0.562940359 batch mAP 0.609436035 batch PCKh 0.25\n",
      "Trained batch 401 batch loss 0.673572421 batch mAP 0.55355835 batch PCKh 0.375\n",
      "Trained batch 402 batch loss 0.57380712 batch mAP 0.613983154 batch PCKh 0.3125\n",
      "Trained batch 403 batch loss 0.589601636 batch mAP 0.563446045 batch PCKh 0.6875\n",
      "Trained batch 404 batch loss 0.5632478 batch mAP 0.570617676 batch PCKh 0.125\n",
      "Trained batch 405 batch loss 0.560115576 batch mAP 0.592681885 batch PCKh 0.375\n",
      "Trained batch 406 batch loss 0.551979959 batch mAP 0.560882568 batch PCKh 0.375\n",
      "Trained batch 407 batch loss 0.48139593 batch mAP 0.571136475 batch PCKh 0.5625\n",
      "Trained batch 408 batch loss 0.547450483 batch mAP 0.597717285 batch PCKh 0.375\n",
      "Trained batch 409 batch loss 0.502303243 batch mAP 0.609802246 batch PCKh 0.8125\n",
      "Trained batch 410 batch loss 0.531708777 batch mAP 0.661315918 batch PCKh 0.375\n",
      "Trained batch 411 batch loss 0.528957963 batch mAP 0.623901367 batch PCKh 0.5\n",
      "Trained batch 412 batch loss 0.49807623 batch mAP 0.628570557 batch PCKh 0.625\n",
      "Trained batch 413 batch loss 0.564965487 batch mAP 0.640594482 batch PCKh 0.625\n",
      "Trained batch 414 batch loss 0.515394092 batch mAP 0.656158447 batch PCKh 0.625\n",
      "Trained batch 415 batch loss 0.508637428 batch mAP 0.647796631 batch PCKh 0.4375\n",
      "Trained batch 416 batch loss 0.547948062 batch mAP 0.662902832 batch PCKh 0.75\n",
      "Trained batch 417 batch loss 0.487208575 batch mAP 0.607574463 batch PCKh 0.375\n",
      "Trained batch 418 batch loss 0.556515098 batch mAP 0.614349365 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 419 batch loss 0.424916774 batch mAP 0.659606934 batch PCKh 0.5\n",
      "Trained batch 420 batch loss 0.440326482 batch mAP 0.691650391 batch PCKh 0.375\n",
      "Trained batch 421 batch loss 0.529169321 batch mAP 0.690643311 batch PCKh 0.4375\n",
      "Trained batch 422 batch loss 0.419945747 batch mAP 0.669921875 batch PCKh 0.25\n",
      "Trained batch 423 batch loss 0.466252089 batch mAP 0.557647705 batch PCKh 0.5\n",
      "Trained batch 424 batch loss 0.54399389 batch mAP 0.620452881 batch PCKh 0.8125\n",
      "Trained batch 425 batch loss 0.548795223 batch mAP 0.580535889 batch PCKh 0.625\n",
      "Trained batch 426 batch loss 0.536506951 batch mAP 0.510314941 batch PCKh 0.375\n",
      "Trained batch 427 batch loss 0.549142718 batch mAP 0.528106689 batch PCKh 0.75\n",
      "Trained batch 428 batch loss 0.628683925 batch mAP 0.514953613 batch PCKh 0.375\n",
      "Trained batch 429 batch loss 0.563098848 batch mAP 0.541473389 batch PCKh 0.4375\n",
      "Trained batch 430 batch loss 0.550083458 batch mAP 0.563812256 batch PCKh 0.625\n",
      "Trained batch 431 batch loss 0.540777087 batch mAP 0.505554199 batch PCKh 0.5625\n",
      "Trained batch 432 batch loss 0.631135 batch mAP 0.502288818 batch PCKh 0.375\n",
      "Trained batch 433 batch loss 0.603436708 batch mAP 0.507080078 batch PCKh 0.5625\n",
      "Trained batch 434 batch loss 0.637172461 batch mAP 0.534240723 batch PCKh 0.8125\n",
      "Trained batch 435 batch loss 0.535251796 batch mAP 0.522857666 batch PCKh 0.25\n",
      "Trained batch 436 batch loss 0.552343249 batch mAP 0.575500488 batch PCKh 0.8125\n",
      "Trained batch 437 batch loss 0.614532053 batch mAP 0.556274414 batch PCKh 0.25\n",
      "Trained batch 438 batch loss 0.58935 batch mAP 0.551208496 batch PCKh 0.875\n",
      "Trained batch 439 batch loss 0.492112726 batch mAP 0.551300049 batch PCKh 0.25\n",
      "Trained batch 440 batch loss 0.564941049 batch mAP 0.514587402 batch PCKh 0.5625\n",
      "Trained batch 441 batch loss 0.519951701 batch mAP 0.545013428 batch PCKh 0.6875\n",
      "Trained batch 442 batch loss 0.646601677 batch mAP 0.479309082 batch PCKh 0.5\n",
      "Trained batch 443 batch loss 0.604571462 batch mAP 0.536621094 batch PCKh 0.5625\n",
      "Trained batch 444 batch loss 0.610414743 batch mAP 0.487518311 batch PCKh 0.6875\n",
      "Trained batch 445 batch loss 0.553578794 batch mAP 0.508544922 batch PCKh 0.625\n",
      "Trained batch 446 batch loss 0.495375752 batch mAP 0.578582764 batch PCKh 0.4375\n",
      "Trained batch 447 batch loss 0.581155837 batch mAP 0.474517822 batch PCKh 0.6875\n",
      "Trained batch 448 batch loss 0.568973839 batch mAP 0.516662598 batch PCKh 0.375\n",
      "Trained batch 449 batch loss 0.520169139 batch mAP 0.531341553 batch PCKh 0.375\n",
      "Trained batch 450 batch loss 0.5446769 batch mAP 0.496826172 batch PCKh 0.625\n",
      "Trained batch 451 batch loss 0.45938316 batch mAP 0.481292725 batch PCKh 0.5\n",
      "Trained batch 452 batch loss 0.542542636 batch mAP 0.529418945 batch PCKh 0.375\n",
      "Trained batch 453 batch loss 0.542057931 batch mAP 0.519683838 batch PCKh 0.625\n",
      "Trained batch 454 batch loss 0.511930048 batch mAP 0.62097168 batch PCKh 0.3125\n",
      "Trained batch 455 batch loss 0.478058457 batch mAP 0.660400391 batch PCKh 0.4375\n",
      "Trained batch 456 batch loss 0.476122767 batch mAP 0.664245605 batch PCKh 0.25\n",
      "Trained batch 457 batch loss 0.568080425 batch mAP 0.653869629 batch PCKh 0.75\n",
      "Trained batch 458 batch loss 0.481792808 batch mAP 0.66305542 batch PCKh 0.625\n",
      "Trained batch 459 batch loss 0.597460032 batch mAP 0.611694336 batch PCKh 0.375\n",
      "Trained batch 460 batch loss 0.591767907 batch mAP 0.579437256 batch PCKh 0.75\n",
      "Trained batch 461 batch loss 0.637025535 batch mAP 0.505065918 batch PCKh 0.75\n",
      "Trained batch 462 batch loss 0.628511488 batch mAP 0.554138184 batch PCKh 0.375\n",
      "Trained batch 463 batch loss 0.6523664 batch mAP 0.525787354 batch PCKh 0.5\n",
      "Trained batch 464 batch loss 0.590217769 batch mAP 0.561706543 batch PCKh 0.4375\n",
      "Trained batch 465 batch loss 0.60656178 batch mAP 0.565704346 batch PCKh 0.3125\n",
      "Trained batch 466 batch loss 0.647458851 batch mAP 0.486083984 batch PCKh 0.4375\n",
      "Trained batch 467 batch loss 0.627260268 batch mAP 0.52645874 batch PCKh 0.25\n",
      "Trained batch 468 batch loss 0.516014636 batch mAP 0.490905762 batch PCKh 0.75\n",
      "Trained batch 469 batch loss 0.498397559 batch mAP 0.485534668 batch PCKh 0\n",
      "Trained batch 470 batch loss 0.475245804 batch mAP 0.53604126 batch PCKh 0.3125\n",
      "Trained batch 471 batch loss 0.460396528 batch mAP 0.528930664 batch PCKh 0.625\n",
      "Trained batch 472 batch loss 0.441427082 batch mAP 0.468139648 batch PCKh 0\n",
      "Trained batch 473 batch loss 0.430924684 batch mAP 0.567199707 batch PCKh 0.6875\n",
      "Trained batch 474 batch loss 0.417116165 batch mAP 0.556182861 batch PCKh 0.75\n",
      "Trained batch 475 batch loss 0.422259271 batch mAP 0.594726562 batch PCKh 0.625\n",
      "Trained batch 476 batch loss 0.420955062 batch mAP 0.591156 batch PCKh 0\n",
      "Trained batch 477 batch loss 0.484579057 batch mAP 0.582794189 batch PCKh 0.625\n",
      "Trained batch 478 batch loss 0.640080214 batch mAP 0.560089111 batch PCKh 0.125\n",
      "Trained batch 479 batch loss 0.60249722 batch mAP 0.614654541 batch PCKh 0.3125\n",
      "Trained batch 480 batch loss 0.615876794 batch mAP 0.618011475 batch PCKh 0.25\n",
      "Trained batch 481 batch loss 0.468214512 batch mAP 0.602844238 batch PCKh 0.5625\n",
      "Trained batch 482 batch loss 0.563616276 batch mAP 0.617004395 batch PCKh 0.25\n",
      "Trained batch 483 batch loss 0.544178963 batch mAP 0.619628906 batch PCKh 0.1875\n",
      "Trained batch 484 batch loss 0.525846303 batch mAP 0.563690186 batch PCKh 0.5625\n",
      "Trained batch 485 batch loss 0.519332767 batch mAP 0.61126709 batch PCKh 0.1875\n",
      "Trained batch 486 batch loss 0.595727205 batch mAP 0.592926 batch PCKh 0.75\n",
      "Trained batch 487 batch loss 0.61381495 batch mAP 0.564147949 batch PCKh 0.1875\n",
      "Trained batch 488 batch loss 0.540946 batch mAP 0.585754395 batch PCKh 0.25\n",
      "Trained batch 489 batch loss 0.530355453 batch mAP 0.550933838 batch PCKh 0.3125\n",
      "Trained batch 490 batch loss 0.531582654 batch mAP 0.547973633 batch PCKh 0.25\n",
      "Trained batch 491 batch loss 0.616206825 batch mAP 0.544067383 batch PCKh 0.375\n",
      "Trained batch 492 batch loss 0.562663138 batch mAP 0.613800049 batch PCKh 0.875\n",
      "Trained batch 493 batch loss 0.534839869 batch mAP 0.654907227 batch PCKh 0.8125\n",
      "Trained batch 494 batch loss 0.560852706 batch mAP 0.591583252 batch PCKh 0.625\n",
      "Trained batch 495 batch loss 0.623838603 batch mAP 0.54586792 batch PCKh 0.375\n",
      "Trained batch 496 batch loss 0.599756718 batch mAP 0.550201416 batch PCKh 0.5\n",
      "Trained batch 497 batch loss 0.610349059 batch mAP 0.616607666 batch PCKh 0.25\n",
      "Trained batch 498 batch loss 0.580409825 batch mAP 0.639434814 batch PCKh 0.75\n",
      "Trained batch 499 batch loss 0.643235266 batch mAP 0.640533447 batch PCKh 0.75\n",
      "Trained batch 500 batch loss 0.554742277 batch mAP 0.559295654 batch PCKh 0.5\n",
      "Trained batch 501 batch loss 0.611024 batch mAP 0.499176025 batch PCKh 0.5\n",
      "Trained batch 502 batch loss 0.57750535 batch mAP 0.496673584 batch PCKh 0.75\n",
      "Trained batch 503 batch loss 0.559956372 batch mAP 0.507782 batch PCKh 0.75\n",
      "Trained batch 504 batch loss 0.523646891 batch mAP 0.586212158 batch PCKh 0.6875\n",
      "Trained batch 505 batch loss 0.52048254 batch mAP 0.603027344 batch PCKh 0.5625\n",
      "Trained batch 506 batch loss 0.540555596 batch mAP 0.570953369 batch PCKh 0.625\n",
      "Trained batch 507 batch loss 0.579078853 batch mAP 0.579986572 batch PCKh 0.75\n",
      "Trained batch 508 batch loss 0.54091239 batch mAP 0.611907959 batch PCKh 0.5625\n",
      "Trained batch 509 batch loss 0.541322112 batch mAP 0.568237305 batch PCKh 0.125\n",
      "Trained batch 510 batch loss 0.589901686 batch mAP 0.542602539 batch PCKh 0.3125\n",
      "Trained batch 511 batch loss 0.574424267 batch mAP 0.617675781 batch PCKh 0.875\n",
      "Trained batch 512 batch loss 0.541022718 batch mAP 0.631652832 batch PCKh 0.375\n",
      "Trained batch 513 batch loss 0.539858937 batch mAP 0.629852295 batch PCKh 0.5\n",
      "Trained batch 514 batch loss 0.599870205 batch mAP 0.600860596 batch PCKh 0.375\n",
      "Trained batch 515 batch loss 0.587928176 batch mAP 0.559265137 batch PCKh 0.375\n",
      "Trained batch 516 batch loss 0.548483968 batch mAP 0.604949951 batch PCKh 0.1875\n",
      "Trained batch 517 batch loss 0.596973896 batch mAP 0.644897461 batch PCKh 0.25\n",
      "Trained batch 518 batch loss 0.543632507 batch mAP 0.663970947 batch PCKh 0.3125\n",
      "Trained batch 519 batch loss 0.512493968 batch mAP 0.668884277 batch PCKh 0.625\n",
      "Trained batch 520 batch loss 0.59438926 batch mAP 0.56829834 batch PCKh 0\n",
      "Trained batch 521 batch loss 0.56120491 batch mAP 0.568328857 batch PCKh 0.75\n",
      "Trained batch 522 batch loss 0.502856851 batch mAP 0.588989258 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 523 batch loss 0.530970752 batch mAP 0.605011 batch PCKh 0.75\n",
      "Trained batch 524 batch loss 0.527437687 batch mAP 0.647918701 batch PCKh 0.5\n",
      "Trained batch 525 batch loss 0.491337299 batch mAP 0.621063232 batch PCKh 0.3125\n",
      "Trained batch 526 batch loss 0.500618577 batch mAP 0.613006592 batch PCKh 0.5\n",
      "Trained batch 527 batch loss 0.555660248 batch mAP 0.554321289 batch PCKh 0.375\n",
      "Trained batch 528 batch loss 0.493713081 batch mAP 0.619506836 batch PCKh 0.25\n",
      "Trained batch 529 batch loss 0.495560884 batch mAP 0.5703125 batch PCKh 0.5\n",
      "Trained batch 530 batch loss 0.514980078 batch mAP 0.647064209 batch PCKh 0.375\n",
      "Trained batch 531 batch loss 0.5285514 batch mAP 0.64465332 batch PCKh 0.75\n",
      "Trained batch 532 batch loss 0.525924444 batch mAP 0.556549072 batch PCKh 0.25\n",
      "Trained batch 533 batch loss 0.582261801 batch mAP 0.575408936 batch PCKh 0.1875\n",
      "Trained batch 534 batch loss 0.511745572 batch mAP 0.580322266 batch PCKh 0.375\n",
      "Trained batch 535 batch loss 0.535590172 batch mAP 0.608734131 batch PCKh 0.5\n",
      "Trained batch 536 batch loss 0.509203255 batch mAP 0.581939697 batch PCKh 0\n",
      "Trained batch 537 batch loss 0.494448245 batch mAP 0.503143311 batch PCKh 0.0625\n",
      "Trained batch 538 batch loss 0.537559509 batch mAP 0.496002197 batch PCKh 0.5\n",
      "Trained batch 539 batch loss 0.413713872 batch mAP 0.57522583 batch PCKh 0\n",
      "Trained batch 540 batch loss 0.293179572 batch mAP 0.609680176 batch PCKh 0.1875\n",
      "Trained batch 541 batch loss 0.414683402 batch mAP 0.600647 batch PCKh 0\n",
      "Trained batch 542 batch loss 0.49843955 batch mAP 0.538665771 batch PCKh 0.125\n",
      "Trained batch 543 batch loss 0.494879782 batch mAP 0.625335693 batch PCKh 0.25\n",
      "Trained batch 544 batch loss 0.565660596 batch mAP 0.562072754 batch PCKh 0.125\n",
      "Trained batch 545 batch loss 0.622066379 batch mAP 0.462097168 batch PCKh 0.625\n",
      "Trained batch 546 batch loss 0.501031816 batch mAP 0.537353516 batch PCKh 0.5\n",
      "Trained batch 547 batch loss 0.495971322 batch mAP 0.547149658 batch PCKh 0.3125\n",
      "Trained batch 548 batch loss 0.488939226 batch mAP 0.589324951 batch PCKh 0.6875\n",
      "Trained batch 549 batch loss 0.535865247 batch mAP 0.462280273 batch PCKh 0\n",
      "Trained batch 550 batch loss 0.576317191 batch mAP 0.500274658 batch PCKh 0.75\n",
      "Trained batch 551 batch loss 0.528461874 batch mAP 0.555633545 batch PCKh 0.6875\n",
      "Trained batch 552 batch loss 0.50799495 batch mAP 0.628570557 batch PCKh 0.8125\n",
      "Trained batch 553 batch loss 0.502896249 batch mAP 0.623474121 batch PCKh 0.5\n",
      "Trained batch 554 batch loss 0.412464678 batch mAP 0.651367188 batch PCKh 0.5625\n",
      "Trained batch 555 batch loss 0.44132185 batch mAP 0.600341797 batch PCKh 0.5\n",
      "Trained batch 556 batch loss 0.518399775 batch mAP 0.467956543 batch PCKh 0.3125\n",
      "Trained batch 557 batch loss 0.490128189 batch mAP 0.548614502 batch PCKh 0.5\n",
      "Trained batch 558 batch loss 0.459880173 batch mAP 0.444061279 batch PCKh 0.125\n",
      "Trained batch 559 batch loss 0.506225884 batch mAP 0.556640625 batch PCKh 0.4375\n",
      "Trained batch 560 batch loss 0.634523809 batch mAP 0.424285889 batch PCKh 0.25\n",
      "Trained batch 561 batch loss 0.585236788 batch mAP 0.485839844 batch PCKh 0.1875\n",
      "Trained batch 562 batch loss 0.438456208 batch mAP 0.496246338 batch PCKh 0.3125\n",
      "Trained batch 563 batch loss 0.589188337 batch mAP 0.464691162 batch PCKh 0.5625\n",
      "Trained batch 564 batch loss 0.522478342 batch mAP 0.555633545 batch PCKh 0\n",
      "Trained batch 565 batch loss 0.553962588 batch mAP 0.523803711 batch PCKh 0.1875\n",
      "Trained batch 566 batch loss 0.519508898 batch mAP 0.555267334 batch PCKh 0.6875\n",
      "Trained batch 567 batch loss 0.570064664 batch mAP 0.425598145 batch PCKh 0.3125\n",
      "Trained batch 568 batch loss 0.489659309 batch mAP 0.536651611 batch PCKh 0.75\n",
      "Trained batch 569 batch loss 0.574223518 batch mAP 0.593261719 batch PCKh 0.5\n",
      "Trained batch 570 batch loss 0.553030491 batch mAP 0.585845947 batch PCKh 0.8125\n",
      "Trained batch 571 batch loss 0.572405934 batch mAP 0.519622803 batch PCKh 0.0625\n",
      "Trained batch 572 batch loss 0.564095616 batch mAP 0.560272217 batch PCKh 0.25\n",
      "Trained batch 573 batch loss 0.585632741 batch mAP 0.547119141 batch PCKh 0.125\n",
      "Trained batch 574 batch loss 0.592849731 batch mAP 0.544464111 batch PCKh 0.3125\n",
      "Trained batch 575 batch loss 0.587550044 batch mAP 0.50869751 batch PCKh 0.8125\n",
      "Trained batch 576 batch loss 0.543929815 batch mAP 0.521728516 batch PCKh 0.8125\n",
      "Trained batch 577 batch loss 0.528814 batch mAP 0.490539551 batch PCKh 0.25\n",
      "Trained batch 578 batch loss 0.485694766 batch mAP 0.568084717 batch PCKh 0.1875\n",
      "Trained batch 579 batch loss 0.561609 batch mAP 0.53729248 batch PCKh 0.75\n",
      "Trained batch 580 batch loss 0.525163174 batch mAP 0.519287109 batch PCKh 0.875\n",
      "Trained batch 581 batch loss 0.452406824 batch mAP 0.552398682 batch PCKh 0.3125\n",
      "Trained batch 582 batch loss 0.461881816 batch mAP 0.564880371 batch PCKh 0.625\n",
      "Trained batch 583 batch loss 0.433953226 batch mAP 0.607208252 batch PCKh 0.8125\n",
      "Trained batch 584 batch loss 0.422235787 batch mAP 0.586029053 batch PCKh 0.75\n",
      "Trained batch 585 batch loss 0.440515399 batch mAP 0.577148438 batch PCKh 0.6875\n",
      "Trained batch 586 batch loss 0.452829152 batch mAP 0.590209961 batch PCKh 0.8125\n",
      "Trained batch 587 batch loss 0.521741867 batch mAP 0.571380615 batch PCKh 0.3125\n",
      "Trained batch 588 batch loss 0.504796147 batch mAP 0.526977539 batch PCKh 0.75\n",
      "Trained batch 589 batch loss 0.485725164 batch mAP 0.613555908 batch PCKh 0.6875\n",
      "Trained batch 590 batch loss 0.498066604 batch mAP 0.590118408 batch PCKh 0.5625\n",
      "Trained batch 591 batch loss 0.552596927 batch mAP 0.633361816 batch PCKh 0.75\n",
      "Trained batch 592 batch loss 0.411638677 batch mAP 0.610107422 batch PCKh 0.625\n",
      "Trained batch 593 batch loss 0.569077909 batch mAP 0.618438721 batch PCKh 0.625\n",
      "Trained batch 594 batch loss 0.500770092 batch mAP 0.600952148 batch PCKh 0.375\n",
      "Trained batch 595 batch loss 0.497071564 batch mAP 0.602142334 batch PCKh 0.6875\n",
      "Trained batch 596 batch loss 0.553167701 batch mAP 0.620025635 batch PCKh 0.25\n",
      "Trained batch 597 batch loss 0.440117329 batch mAP 0.65814209 batch PCKh 0.75\n",
      "Trained batch 598 batch loss 0.473516345 batch mAP 0.615020752 batch PCKh 0.4375\n",
      "Trained batch 599 batch loss 0.437277287 batch mAP 0.639282227 batch PCKh 0.5625\n",
      "Trained batch 600 batch loss 0.617457807 batch mAP 0.587585449 batch PCKh 0.25\n",
      "Trained batch 601 batch loss 0.663204551 batch mAP 0.575439453 batch PCKh 0.875\n",
      "Trained batch 602 batch loss 0.578191698 batch mAP 0.567199707 batch PCKh 0.25\n",
      "Trained batch 603 batch loss 0.605110765 batch mAP 0.556854248 batch PCKh 0.5625\n",
      "Trained batch 604 batch loss 0.592405319 batch mAP 0.607025146 batch PCKh 0.6875\n",
      "Trained batch 605 batch loss 0.555218 batch mAP 0.621124268 batch PCKh 0.6875\n",
      "Trained batch 606 batch loss 0.544596791 batch mAP 0.574310303 batch PCKh 0.5625\n",
      "Trained batch 607 batch loss 0.588802 batch mAP 0.491973877 batch PCKh 0.4375\n",
      "Trained batch 608 batch loss 0.539429665 batch mAP 0.515075684 batch PCKh 0.375\n",
      "Trained batch 609 batch loss 0.588325441 batch mAP 0.555267334 batch PCKh 0.5\n",
      "Trained batch 610 batch loss 0.587467849 batch mAP 0.515960693 batch PCKh 0\n",
      "Trained batch 611 batch loss 0.588276923 batch mAP 0.549194336 batch PCKh 0.6875\n",
      "Trained batch 612 batch loss 0.529106796 batch mAP 0.396179199 batch PCKh 0.5\n",
      "Trained batch 613 batch loss 0.539635181 batch mAP 0.497741699 batch PCKh 0.875\n",
      "Trained batch 614 batch loss 0.497975111 batch mAP 0.388366699 batch PCKh 0.375\n",
      "Trained batch 615 batch loss 0.400013864 batch mAP 0.598724365 batch PCKh 0\n",
      "Trained batch 616 batch loss 0.438201129 batch mAP 0.498779297 batch PCKh 0.5625\n",
      "Trained batch 617 batch loss 0.427962 batch mAP 0.538879395 batch PCKh 0.5625\n",
      "Trained batch 618 batch loss 0.414889276 batch mAP 0.63458252 batch PCKh 0.3125\n",
      "Trained batch 619 batch loss 0.62620306 batch mAP 0.590332031 batch PCKh 0.375\n",
      "Trained batch 620 batch loss 0.652592301 batch mAP 0.566467285 batch PCKh 0.3125\n",
      "Trained batch 621 batch loss 0.738907039 batch mAP 0.508636475 batch PCKh 0.0625\n",
      "Trained batch 622 batch loss 0.568355381 batch mAP 0.569854736 batch PCKh 0.125\n",
      "Trained batch 623 batch loss 0.527967572 batch mAP 0.650268555 batch PCKh 0.4375\n",
      "Trained batch 624 batch loss 0.532057703 batch mAP 0.540496826 batch PCKh 0.375\n",
      "Trained batch 625 batch loss 0.536489129 batch mAP 0.540771484 batch PCKh 0.1875\n",
      "Trained batch 626 batch loss 0.472142756 batch mAP 0.627227783 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 627 batch loss 0.477772474 batch mAP 0.612915039 batch PCKh 0.375\n",
      "Trained batch 628 batch loss 0.50176537 batch mAP 0.603820801 batch PCKh 0.375\n",
      "Trained batch 629 batch loss 0.551175475 batch mAP 0.591064453 batch PCKh 0.25\n",
      "Trained batch 630 batch loss 0.478840053 batch mAP 0.634796143 batch PCKh 0.1875\n",
      "Trained batch 631 batch loss 0.53653872 batch mAP 0.613006592 batch PCKh 0.8125\n",
      "Trained batch 632 batch loss 0.476157248 batch mAP 0.629303 batch PCKh 0.5625\n",
      "Trained batch 633 batch loss 0.451927274 batch mAP 0.628082275 batch PCKh 0.625\n",
      "Trained batch 634 batch loss 0.565384209 batch mAP 0.519104 batch PCKh 0.6875\n",
      "Trained batch 635 batch loss 0.529322803 batch mAP 0.66973877 batch PCKh 0.375\n",
      "Trained batch 636 batch loss 0.559043825 batch mAP 0.621032715 batch PCKh 0.375\n",
      "Trained batch 637 batch loss 0.563503921 batch mAP 0.556427 batch PCKh 0.5625\n",
      "Trained batch 638 batch loss 0.498436064 batch mAP 0.599243164 batch PCKh 0.4375\n",
      "Trained batch 639 batch loss 0.539132357 batch mAP 0.636383057 batch PCKh 0.6875\n",
      "Trained batch 640 batch loss 0.566939533 batch mAP 0.623687744 batch PCKh 0.375\n",
      "Trained batch 641 batch loss 0.585580587 batch mAP 0.593170166 batch PCKh 0.5625\n",
      "Trained batch 642 batch loss 0.567566693 batch mAP 0.627655 batch PCKh 0.1875\n",
      "Trained batch 643 batch loss 0.583194137 batch mAP 0.611572266 batch PCKh 0.375\n",
      "Trained batch 644 batch loss 0.515599489 batch mAP 0.621887207 batch PCKh 0.625\n",
      "Trained batch 645 batch loss 0.460275471 batch mAP 0.678436279 batch PCKh 0.1875\n",
      "Trained batch 646 batch loss 0.480359763 batch mAP 0.692260742 batch PCKh 0.5\n",
      "Trained batch 647 batch loss 0.513064 batch mAP 0.682312 batch PCKh 0.25\n",
      "Trained batch 648 batch loss 0.517851412 batch mAP 0.655639648 batch PCKh 0.125\n",
      "Trained batch 649 batch loss 0.548887074 batch mAP 0.684753418 batch PCKh 0.5\n",
      "Trained batch 650 batch loss 0.569425523 batch mAP 0.646331787 batch PCKh 0.5\n",
      "Trained batch 651 batch loss 0.548070252 batch mAP 0.604705811 batch PCKh 0.25\n",
      "Trained batch 652 batch loss 0.547662437 batch mAP 0.55255127 batch PCKh 0.1875\n",
      "Trained batch 653 batch loss 0.557535887 batch mAP 0.541717529 batch PCKh 0.5\n",
      "Trained batch 654 batch loss 0.497349769 batch mAP 0.573425293 batch PCKh 0.3125\n",
      "Trained batch 655 batch loss 0.528871298 batch mAP 0.598968506 batch PCKh 0.5\n",
      "Trained batch 656 batch loss 0.546676457 batch mAP 0.59765625 batch PCKh 0.5625\n",
      "Trained batch 657 batch loss 0.538833141 batch mAP 0.593261719 batch PCKh 0.3125\n",
      "Trained batch 658 batch loss 0.57916 batch mAP 0.568054199 batch PCKh 0.1875\n",
      "Trained batch 659 batch loss 0.550503 batch mAP 0.584411621 batch PCKh 0.625\n",
      "Trained batch 660 batch loss 0.502008617 batch mAP 0.540527344 batch PCKh 0.1875\n",
      "Trained batch 661 batch loss 0.479922116 batch mAP 0.532012939 batch PCKh 0.5625\n",
      "Trained batch 662 batch loss 0.461797178 batch mAP 0.672699 batch PCKh 0.375\n",
      "Trained batch 663 batch loss 0.515279114 batch mAP 0.639587402 batch PCKh 0.375\n",
      "Trained batch 664 batch loss 0.558873 batch mAP 0.621307373 batch PCKh 0.8125\n",
      "Trained batch 665 batch loss 0.515099 batch mAP 0.6612854 batch PCKh 0.5625\n",
      "Trained batch 666 batch loss 0.47856915 batch mAP 0.654693604 batch PCKh 0.375\n",
      "Trained batch 667 batch loss 0.476875305 batch mAP 0.623596191 batch PCKh 0.375\n",
      "Trained batch 668 batch loss 0.410289735 batch mAP 0.622528076 batch PCKh 0.4375\n",
      "Trained batch 669 batch loss 0.526767433 batch mAP 0.619476318 batch PCKh 0.4375\n",
      "Trained batch 670 batch loss 0.401047915 batch mAP 0.642334 batch PCKh 0.4375\n",
      "Trained batch 671 batch loss 0.443523496 batch mAP 0.63293457 batch PCKh 0.625\n",
      "Trained batch 672 batch loss 0.470663 batch mAP 0.610717773 batch PCKh 0.6875\n",
      "Trained batch 673 batch loss 0.49044916 batch mAP 0.610443115 batch PCKh 0.6875\n",
      "Trained batch 674 batch loss 0.457654059 batch mAP 0.613678 batch PCKh 0.6875\n",
      "Trained batch 675 batch loss 0.514892459 batch mAP 0.601104736 batch PCKh 0.25\n",
      "Trained batch 676 batch loss 0.542185485 batch mAP 0.585113525 batch PCKh 0.6875\n",
      "Trained batch 677 batch loss 0.480459 batch mAP 0.586547852 batch PCKh 0.6875\n",
      "Trained batch 678 batch loss 0.460156828 batch mAP 0.596557617 batch PCKh 0.25\n",
      "Trained batch 679 batch loss 0.449392915 batch mAP 0.58605957 batch PCKh 0.4375\n",
      "Trained batch 680 batch loss 0.490451783 batch mAP 0.546112061 batch PCKh 0.5625\n",
      "Trained batch 681 batch loss 0.534817874 batch mAP 0.592376709 batch PCKh 0.8125\n",
      "Trained batch 682 batch loss 0.480393618 batch mAP 0.599975586 batch PCKh 0.6875\n",
      "Trained batch 683 batch loss 0.491918176 batch mAP 0.597930908 batch PCKh 0.5\n",
      "Trained batch 684 batch loss 0.396626472 batch mAP 0.629669189 batch PCKh 0.625\n",
      "Trained batch 685 batch loss 0.62632525 batch mAP 0.607147217 batch PCKh 0.3125\n",
      "Trained batch 686 batch loss 0.527685463 batch mAP 0.588775635 batch PCKh 0.75\n",
      "Trained batch 687 batch loss 0.558141768 batch mAP 0.586090088 batch PCKh 0.1875\n",
      "Trained batch 688 batch loss 0.536450565 batch mAP 0.645629883 batch PCKh 0.4375\n",
      "Trained batch 689 batch loss 0.601415575 batch mAP 0.611755371 batch PCKh 0.125\n",
      "Trained batch 690 batch loss 0.534767389 batch mAP 0.678436279 batch PCKh 0.5625\n",
      "Trained batch 691 batch loss 0.601908147 batch mAP 0.680969238 batch PCKh 0.3125\n",
      "Trained batch 692 batch loss 0.583172679 batch mAP 0.655853271 batch PCKh 0.375\n",
      "Trained batch 693 batch loss 0.623959482 batch mAP 0.599700928 batch PCKh 0.375\n",
      "Trained batch 694 batch loss 0.501051545 batch mAP 0.585418701 batch PCKh 0.3125\n",
      "Trained batch 695 batch loss 0.581548 batch mAP 0.551971436 batch PCKh 0.5\n",
      "Trained batch 696 batch loss 0.541707039 batch mAP 0.537139893 batch PCKh 0.625\n",
      "Trained batch 697 batch loss 0.545807242 batch mAP 0.571533203 batch PCKh 0.1875\n",
      "Trained batch 698 batch loss 0.566485405 batch mAP 0.497344971 batch PCKh 0.3125\n",
      "Trained batch 699 batch loss 0.510212302 batch mAP 0.523986816 batch PCKh 0.4375\n",
      "Trained batch 700 batch loss 0.631995499 batch mAP 0.504486084 batch PCKh 0.25\n",
      "Trained batch 701 batch loss 0.606457889 batch mAP 0.519989 batch PCKh 0.3125\n",
      "Trained batch 702 batch loss 0.551333666 batch mAP 0.591247559 batch PCKh 0.8125\n",
      "Trained batch 703 batch loss 0.475783616 batch mAP 0.51550293 batch PCKh 0.8125\n",
      "Trained batch 704 batch loss 0.570072174 batch mAP 0.50213623 batch PCKh 0.5625\n",
      "Trained batch 705 batch loss 0.508469701 batch mAP 0.460205078 batch PCKh 0.625\n",
      "Trained batch 706 batch loss 0.616659641 batch mAP 0.475646973 batch PCKh 0.3125\n",
      "Trained batch 707 batch loss 0.547666371 batch mAP 0.570129395 batch PCKh 0.5625\n",
      "Trained batch 708 batch loss 0.519559383 batch mAP 0.485290527 batch PCKh 0.625\n",
      "Trained batch 709 batch loss 0.567516387 batch mAP 0.488891602 batch PCKh 0.4375\n",
      "Trained batch 710 batch loss 0.586583257 batch mAP 0.540405273 batch PCKh 0.6875\n",
      "Trained batch 711 batch loss 0.639612198 batch mAP 0.549346924 batch PCKh 0.25\n",
      "Trained batch 712 batch loss 0.619848371 batch mAP 0.538330078 batch PCKh 0.375\n",
      "Trained batch 713 batch loss 0.405315399 batch mAP 0.58883667 batch PCKh 0.75\n",
      "Trained batch 714 batch loss 0.4352808 batch mAP 0.575897217 batch PCKh 0.25\n",
      "Trained batch 715 batch loss 0.406905055 batch mAP 0.566772461 batch PCKh 0.5625\n",
      "Trained batch 716 batch loss 0.418701202 batch mAP 0.55255127 batch PCKh 0.3125\n",
      "Trained batch 717 batch loss 0.559706628 batch mAP 0.546600342 batch PCKh 0.5625\n",
      "Trained batch 718 batch loss 0.512323201 batch mAP 0.580413818 batch PCKh 0.125\n",
      "Trained batch 719 batch loss 0.556050956 batch mAP 0.587921143 batch PCKh 0.5625\n",
      "Trained batch 720 batch loss 0.558926105 batch mAP 0.606140137 batch PCKh 0.1875\n",
      "Trained batch 721 batch loss 0.518361568 batch mAP 0.645141602 batch PCKh 0.75\n",
      "Trained batch 722 batch loss 0.497130871 batch mAP 0.59262085 batch PCKh 0.4375\n",
      "Trained batch 723 batch loss 0.526202202 batch mAP 0.585571289 batch PCKh 0.1875\n",
      "Trained batch 724 batch loss 0.545746565 batch mAP 0.582366943 batch PCKh 0.1875\n",
      "Trained batch 725 batch loss 0.597240925 batch mAP 0.570709229 batch PCKh 0.8125\n",
      "Trained batch 726 batch loss 0.513045549 batch mAP 0.625518799 batch PCKh 0.6875\n",
      "Trained batch 727 batch loss 0.624635696 batch mAP 0.596893311 batch PCKh 0.6875\n",
      "Trained batch 728 batch loss 0.590296388 batch mAP 0.538879395 batch PCKh 0.3125\n",
      "Trained batch 729 batch loss 0.632957458 batch mAP 0.532653809 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 730 batch loss 0.615664244 batch mAP 0.548278809 batch PCKh 0.1875\n",
      "Trained batch 731 batch loss 0.545231938 batch mAP 0.491455078 batch PCKh 0.125\n",
      "Trained batch 732 batch loss 0.518522203 batch mAP 0.517486572 batch PCKh 0.75\n",
      "Trained batch 733 batch loss 0.534584 batch mAP 0.510101318 batch PCKh 0.75\n",
      "Trained batch 734 batch loss 0.696959376 batch mAP 0.452026367 batch PCKh 0.125\n",
      "Trained batch 735 batch loss 0.648010492 batch mAP 0.568237305 batch PCKh 0.3125\n",
      "Trained batch 736 batch loss 0.654212 batch mAP 0.578521729 batch PCKh 0.375\n",
      "Trained batch 737 batch loss 0.677638769 batch mAP 0.541473389 batch PCKh 0.625\n",
      "Trained batch 738 batch loss 0.592324 batch mAP 0.522369385 batch PCKh 0.6875\n",
      "Trained batch 739 batch loss 0.476947457 batch mAP 0.554168701 batch PCKh 0.5625\n",
      "Trained batch 740 batch loss 0.568472743 batch mAP 0.591705322 batch PCKh 0.6875\n",
      "Trained batch 741 batch loss 0.569122314 batch mAP 0.558288574 batch PCKh 0.5625\n",
      "Trained batch 742 batch loss 0.541908741 batch mAP 0.537963867 batch PCKh 0.4375\n",
      "Trained batch 743 batch loss 0.512561798 batch mAP 0.597045898 batch PCKh 0.75\n",
      "Trained batch 744 batch loss 0.516423225 batch mAP 0.616455078 batch PCKh 0.4375\n",
      "Trained batch 745 batch loss 0.490285635 batch mAP 0.658294678 batch PCKh 0.4375\n",
      "Trained batch 746 batch loss 0.437094063 batch mAP 0.649841309 batch PCKh 0.375\n",
      "Trained batch 747 batch loss 0.419030786 batch mAP 0.666442871 batch PCKh 0.625\n",
      "Trained batch 748 batch loss 0.428006768 batch mAP 0.728515625 batch PCKh 0.5625\n",
      "Trained batch 749 batch loss 0.413144767 batch mAP 0.705047607 batch PCKh 0.25\n",
      "Trained batch 750 batch loss 0.524047136 batch mAP 0.626098633 batch PCKh 0.625\n",
      "Trained batch 751 batch loss 0.556973 batch mAP 0.618743896 batch PCKh 0.1875\n",
      "Trained batch 752 batch loss 0.560855 batch mAP 0.58392334 batch PCKh 0.5625\n",
      "Trained batch 753 batch loss 0.600289583 batch mAP 0.605560303 batch PCKh 0.25\n",
      "Trained batch 754 batch loss 0.579870164 batch mAP 0.608795166 batch PCKh 0.25\n",
      "Trained batch 755 batch loss 0.637792885 batch mAP 0.532409668 batch PCKh 0.5625\n",
      "Trained batch 756 batch loss 0.714810431 batch mAP 0.522033691 batch PCKh 0\n",
      "Trained batch 757 batch loss 0.614095449 batch mAP 0.517700195 batch PCKh 0.25\n",
      "Trained batch 758 batch loss 0.627040148 batch mAP 0.569793701 batch PCKh 0.375\n",
      "Trained batch 759 batch loss 0.609129906 batch mAP 0.546081543 batch PCKh 0.625\n",
      "Trained batch 760 batch loss 0.560488164 batch mAP 0.550445557 batch PCKh 0.375\n",
      "Trained batch 761 batch loss 0.63058871 batch mAP 0.566619873 batch PCKh 0.25\n",
      "Trained batch 762 batch loss 0.585581899 batch mAP 0.559936523 batch PCKh 0.1875\n",
      "Trained batch 763 batch loss 0.588896036 batch mAP 0.583374 batch PCKh 0.5\n",
      "Trained batch 764 batch loss 0.476356864 batch mAP 0.668701172 batch PCKh 0.4375\n",
      "Trained batch 765 batch loss 0.541500807 batch mAP 0.685577393 batch PCKh 0.375\n",
      "Trained batch 766 batch loss 0.497964054 batch mAP 0.618225098 batch PCKh 0.625\n",
      "Trained batch 767 batch loss 0.530508757 batch mAP 0.626037598 batch PCKh 0.3125\n",
      "Trained batch 768 batch loss 0.532268465 batch mAP 0.526641846 batch PCKh 0.375\n",
      "Trained batch 769 batch loss 0.507835388 batch mAP 0.583313 batch PCKh 0.1875\n",
      "Trained batch 770 batch loss 0.576074839 batch mAP 0.641449 batch PCKh 0.4375\n",
      "Trained batch 771 batch loss 0.583262086 batch mAP 0.622070312 batch PCKh 0.375\n",
      "Trained batch 772 batch loss 0.516478181 batch mAP 0.569549561 batch PCKh 0.5625\n",
      "Trained batch 773 batch loss 0.563658774 batch mAP 0.653961182 batch PCKh 0.4375\n",
      "Trained batch 774 batch loss 0.573493063 batch mAP 0.657409668 batch PCKh 0.8125\n",
      "Trained batch 775 batch loss 0.561942041 batch mAP 0.66784668 batch PCKh 0.8125\n",
      "Trained batch 776 batch loss 0.582432032 batch mAP 0.636474609 batch PCKh 0.8125\n",
      "Trained batch 777 batch loss 0.521424472 batch mAP 0.66583252 batch PCKh 0.5625\n",
      "Trained batch 778 batch loss 0.603650272 batch mAP 0.633117676 batch PCKh 0.875\n",
      "Trained batch 779 batch loss 0.545968413 batch mAP 0.682098389 batch PCKh 0.4375\n",
      "Trained batch 780 batch loss 0.583893776 batch mAP 0.613342285 batch PCKh 0.625\n",
      "Trained batch 781 batch loss 0.654796958 batch mAP 0.607971191 batch PCKh 0.6875\n",
      "Trained batch 782 batch loss 0.642851949 batch mAP 0.638305664 batch PCKh 0.1875\n",
      "Trained batch 783 batch loss 0.675833464 batch mAP 0.615600586 batch PCKh 0\n",
      "Trained batch 784 batch loss 0.739558458 batch mAP 0.574951172 batch PCKh 0.25\n",
      "Trained batch 785 batch loss 0.657930255 batch mAP 0.648773193 batch PCKh 0.3125\n",
      "Trained batch 786 batch loss 0.564299285 batch mAP 0.615509033 batch PCKh 0.3125\n",
      "Trained batch 787 batch loss 0.510468721 batch mAP 0.666015625 batch PCKh 0.5625\n",
      "Trained batch 788 batch loss 0.495397627 batch mAP 0.634399414 batch PCKh 0.6875\n",
      "Trained batch 789 batch loss 0.529860079 batch mAP 0.616638184 batch PCKh 0.4375\n",
      "Trained batch 790 batch loss 0.53111589 batch mAP 0.601715088 batch PCKh 0.5625\n",
      "Trained batch 791 batch loss 0.495246172 batch mAP 0.655975342 batch PCKh 0.625\n",
      "Trained batch 792 batch loss 0.530723333 batch mAP 0.593566895 batch PCKh 0.625\n",
      "Trained batch 793 batch loss 0.573059618 batch mAP 0.61126709 batch PCKh 0.625\n",
      "Trained batch 794 batch loss 0.591756821 batch mAP 0.567840576 batch PCKh 0.625\n",
      "Trained batch 795 batch loss 0.578375936 batch mAP 0.504394531 batch PCKh 0.125\n",
      "Trained batch 796 batch loss 0.568784952 batch mAP 0.534759521 batch PCKh 0.25\n",
      "Trained batch 797 batch loss 0.532296598 batch mAP 0.539001465 batch PCKh 0.375\n",
      "Trained batch 798 batch loss 0.564404726 batch mAP 0.521026611 batch PCKh 0.75\n",
      "Trained batch 799 batch loss 0.551166058 batch mAP 0.465332031 batch PCKh 0\n",
      "Trained batch 800 batch loss 0.598508596 batch mAP 0.539306641 batch PCKh 0.875\n",
      "Trained batch 801 batch loss 0.625668764 batch mAP 0.566070557 batch PCKh 0.5625\n",
      "Trained batch 802 batch loss 0.619931221 batch mAP 0.57043457 batch PCKh 0.25\n",
      "Trained batch 803 batch loss 0.660277784 batch mAP 0.54989624 batch PCKh 0.5625\n",
      "Trained batch 804 batch loss 0.597276 batch mAP 0.556671143 batch PCKh 0.625\n",
      "Trained batch 805 batch loss 0.72233516 batch mAP 0.498352051 batch PCKh 0.375\n",
      "Trained batch 806 batch loss 0.576173961 batch mAP 0.634643555 batch PCKh 0.6875\n",
      "Trained batch 807 batch loss 0.553307891 batch mAP 0.703094482 batch PCKh 0.5\n",
      "Trained batch 808 batch loss 0.53566277 batch mAP 0.631744385 batch PCKh 0.375\n",
      "Trained batch 809 batch loss 0.58814013 batch mAP 0.640075684 batch PCKh 0.5\n",
      "Trained batch 810 batch loss 0.519890308 batch mAP 0.559570312 batch PCKh 0.5625\n",
      "Trained batch 811 batch loss 0.518467963 batch mAP 0.617462158 batch PCKh 0.375\n",
      "Trained batch 812 batch loss 0.523101389 batch mAP 0.632720947 batch PCKh 0.1875\n",
      "Trained batch 813 batch loss 0.589237273 batch mAP 0.620422363 batch PCKh 0.625\n",
      "Trained batch 814 batch loss 0.613260031 batch mAP 0.601074219 batch PCKh 0.125\n",
      "Trained batch 815 batch loss 0.525835395 batch mAP 0.618621826 batch PCKh 0.25\n",
      "Trained batch 816 batch loss 0.575202703 batch mAP 0.659240723 batch PCKh 0.75\n",
      "Trained batch 817 batch loss 0.538086057 batch mAP 0.61026 batch PCKh 0.3125\n",
      "Trained batch 818 batch loss 0.58861804 batch mAP 0.638671875 batch PCKh 0.375\n",
      "Trained batch 819 batch loss 0.556849599 batch mAP 0.625640869 batch PCKh 0.6875\n",
      "Trained batch 820 batch loss 0.506092668 batch mAP 0.669372559 batch PCKh 0.6875\n",
      "Trained batch 821 batch loss 0.504206657 batch mAP 0.59362793 batch PCKh 0.25\n",
      "Trained batch 822 batch loss 0.50784564 batch mAP 0.651672363 batch PCKh 0.5\n",
      "Trained batch 823 batch loss 0.550611615 batch mAP 0.626922607 batch PCKh 0.75\n",
      "Trained batch 824 batch loss 0.481713384 batch mAP 0.59765625 batch PCKh 0.375\n",
      "Trained batch 825 batch loss 0.520236433 batch mAP 0.654754639 batch PCKh 0.4375\n",
      "Trained batch 826 batch loss 0.578960776 batch mAP 0.583831787 batch PCKh 0.5625\n",
      "Trained batch 827 batch loss 0.547020912 batch mAP 0.586456299 batch PCKh 0.4375\n",
      "Trained batch 828 batch loss 0.563002408 batch mAP 0.581359863 batch PCKh 0.6875\n",
      "Trained batch 829 batch loss 0.537862897 batch mAP 0.53414917 batch PCKh 0.5\n",
      "Trained batch 830 batch loss 0.628744125 batch mAP 0.501098633 batch PCKh 0.6875\n",
      "Trained batch 831 batch loss 0.546408772 batch mAP 0.558166504 batch PCKh 0.75\n",
      "Trained batch 832 batch loss 0.490908146 batch mAP 0.654724121 batch PCKh 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 833 batch loss 0.603746653 batch mAP 0.59677124 batch PCKh 0.6875\n",
      "Trained batch 834 batch loss 0.704196 batch mAP 0.567810059 batch PCKh 0.3125\n",
      "Trained batch 835 batch loss 0.700197697 batch mAP 0.5519104 batch PCKh 0.375\n",
      "Trained batch 836 batch loss 0.700743079 batch mAP 0.521972656 batch PCKh 0.3125\n",
      "Trained batch 837 batch loss 0.667523324 batch mAP 0.483734131 batch PCKh 0.1875\n",
      "Trained batch 838 batch loss 0.539203167 batch mAP 0.525787354 batch PCKh 0.5\n",
      "Trained batch 839 batch loss 0.512227654 batch mAP 0.561859131 batch PCKh 0.6875\n",
      "Trained batch 840 batch loss 0.523898363 batch mAP 0.576660156 batch PCKh 0.75\n",
      "Trained batch 841 batch loss 0.52070576 batch mAP 0.575836182 batch PCKh 0.75\n",
      "Trained batch 842 batch loss 0.454370439 batch mAP 0.606384277 batch PCKh 0.25\n",
      "Trained batch 843 batch loss 0.378678352 batch mAP 0.542144775 batch PCKh 0.4375\n",
      "Trained batch 844 batch loss 0.415502 batch mAP 0.491424561 batch PCKh 0.5625\n",
      "Trained batch 845 batch loss 0.377384335 batch mAP 0.593841553 batch PCKh 0.5625\n",
      "Trained batch 846 batch loss 0.576723278 batch mAP 0.569366455 batch PCKh 0.5625\n",
      "Trained batch 847 batch loss 0.528002262 batch mAP 0.56817627 batch PCKh 0.4375\n",
      "Trained batch 848 batch loss 0.58648926 batch mAP 0.508361816 batch PCKh 0.4375\n",
      "Trained batch 849 batch loss 0.469089627 batch mAP 0.545562744 batch PCKh 0.3125\n",
      "Trained batch 850 batch loss 0.331845641 batch mAP 0.590789795 batch PCKh 0.25\n",
      "Trained batch 851 batch loss 0.368168414 batch mAP 0.586456299 batch PCKh 0\n",
      "Trained batch 852 batch loss 0.426186502 batch mAP 0.565795898 batch PCKh 0\n",
      "Trained batch 853 batch loss 0.371080786 batch mAP 0.593200684 batch PCKh 0\n",
      "Trained batch 854 batch loss 0.440093458 batch mAP 0.521728516 batch PCKh 0.6875\n",
      "Trained batch 855 batch loss 0.4242405 batch mAP 0.534515381 batch PCKh 0.5625\n",
      "Trained batch 856 batch loss 0.462863952 batch mAP 0.571868896 batch PCKh 0.25\n",
      "Trained batch 857 batch loss 0.526566327 batch mAP 0.541931152 batch PCKh 0.6875\n",
      "Trained batch 858 batch loss 0.491565853 batch mAP 0.550079346 batch PCKh 0.5625\n",
      "Trained batch 859 batch loss 0.484554946 batch mAP 0.571105957 batch PCKh 0.75\n",
      "Trained batch 860 batch loss 0.48940748 batch mAP 0.547912598 batch PCKh 0.5\n",
      "Trained batch 861 batch loss 0.511905372 batch mAP 0.560791 batch PCKh 0.1875\n",
      "Trained batch 862 batch loss 0.625155568 batch mAP 0.577880859 batch PCKh 0\n",
      "Trained batch 863 batch loss 0.534342825 batch mAP 0.600067139 batch PCKh 0.625\n",
      "Trained batch 864 batch loss 0.524163246 batch mAP 0.588195801 batch PCKh 0.375\n",
      "Trained batch 865 batch loss 0.592027307 batch mAP 0.5362854 batch PCKh 0.6875\n",
      "Trained batch 866 batch loss 0.507894576 batch mAP 0.522705078 batch PCKh 0.5\n",
      "Trained batch 867 batch loss 0.570145547 batch mAP 0.536438 batch PCKh 0.4375\n",
      "Trained batch 868 batch loss 0.559342682 batch mAP 0.496490479 batch PCKh 0.375\n",
      "Trained batch 869 batch loss 0.582717061 batch mAP 0.576843262 batch PCKh 0.4375\n",
      "Trained batch 870 batch loss 0.561345696 batch mAP 0.544128418 batch PCKh 0.75\n",
      "Trained batch 871 batch loss 0.556687474 batch mAP 0.593566895 batch PCKh 0.1875\n",
      "Trained batch 872 batch loss 0.578170061 batch mAP 0.54776 batch PCKh 0.875\n",
      "Trained batch 873 batch loss 0.556366324 batch mAP 0.590667725 batch PCKh 0.6875\n",
      "Trained batch 874 batch loss 0.554717541 batch mAP 0.529907227 batch PCKh 0.625\n",
      "Trained batch 875 batch loss 0.425965816 batch mAP 0.596191406 batch PCKh 0.3125\n",
      "Trained batch 876 batch loss 0.406385303 batch mAP 0.591644287 batch PCKh 0.3125\n",
      "Trained batch 877 batch loss 0.464052975 batch mAP 0.613037109 batch PCKh 0.5625\n",
      "Trained batch 878 batch loss 0.568114519 batch mAP 0.557800293 batch PCKh 0.5625\n",
      "Trained batch 879 batch loss 0.57400918 batch mAP 0.595825195 batch PCKh 0.625\n",
      "Trained batch 880 batch loss 0.500739932 batch mAP 0.593811035 batch PCKh 0.6875\n",
      "Trained batch 881 batch loss 0.549879432 batch mAP 0.59198 batch PCKh 0.625\n",
      "Trained batch 882 batch loss 0.584810436 batch mAP 0.537017822 batch PCKh 0.5\n",
      "Trained batch 883 batch loss 0.632712424 batch mAP 0.579193115 batch PCKh 0.75\n",
      "Trained batch 884 batch loss 0.637084365 batch mAP 0.565643311 batch PCKh 0.625\n",
      "Trained batch 885 batch loss 0.533806384 batch mAP 0.578949 batch PCKh 0.875\n",
      "Trained batch 886 batch loss 0.546346843 batch mAP 0.636749268 batch PCKh 0.25\n",
      "Trained batch 887 batch loss 0.555960178 batch mAP 0.613861084 batch PCKh 0.375\n",
      "Trained batch 888 batch loss 0.50401032 batch mAP 0.606994629 batch PCKh 0.5625\n",
      "Trained batch 889 batch loss 0.490086854 batch mAP 0.618530273 batch PCKh 0.875\n",
      "Trained batch 890 batch loss 0.54829222 batch mAP 0.578704834 batch PCKh 0.1875\n",
      "Trained batch 891 batch loss 0.612260818 batch mAP 0.494476318 batch PCKh 0.4375\n",
      "Trained batch 892 batch loss 0.618118823 batch mAP 0.558380127 batch PCKh 0.6875\n",
      "Trained batch 893 batch loss 0.559178233 batch mAP 0.510681152 batch PCKh 0.875\n",
      "Trained batch 894 batch loss 0.550851464 batch mAP 0.5652771 batch PCKh 0.875\n",
      "Trained batch 895 batch loss 0.55331111 batch mAP 0.541778564 batch PCKh 0.625\n",
      "Trained batch 896 batch loss 0.538225174 batch mAP 0.540496826 batch PCKh 0.6875\n",
      "Trained batch 897 batch loss 0.611547112 batch mAP 0.435089111 batch PCKh 0.4375\n",
      "Trained batch 898 batch loss 0.58419466 batch mAP 0.491149902 batch PCKh 0.5\n",
      "Trained batch 899 batch loss 0.625252604 batch mAP 0.520019531 batch PCKh 0.5625\n",
      "Trained batch 900 batch loss 0.509667456 batch mAP 0.496002197 batch PCKh 0.375\n",
      "Trained batch 901 batch loss 0.509483635 batch mAP 0.497253418 batch PCKh 0\n",
      "Trained batch 902 batch loss 0.505196393 batch mAP 0.547241211 batch PCKh 0.3125\n",
      "Trained batch 903 batch loss 0.579529285 batch mAP 0.52923584 batch PCKh 0.1875\n",
      "Trained batch 904 batch loss 0.489771962 batch mAP 0.555114746 batch PCKh 0.625\n",
      "Trained batch 905 batch loss 0.537373126 batch mAP 0.591583252 batch PCKh 0.375\n",
      "Trained batch 906 batch loss 0.590001404 batch mAP 0.551116943 batch PCKh 0.25\n",
      "Trained batch 907 batch loss 0.586688161 batch mAP 0.532012939 batch PCKh 0.875\n",
      "Trained batch 908 batch loss 0.538084447 batch mAP 0.561798096 batch PCKh 0.75\n",
      "Trained batch 909 batch loss 0.620242238 batch mAP 0.535400391 batch PCKh 0.6875\n",
      "Trained batch 910 batch loss 0.529145896 batch mAP 0.547821045 batch PCKh 0.625\n",
      "Trained batch 911 batch loss 0.503759742 batch mAP 0.572753906 batch PCKh 0.5\n",
      "Trained batch 912 batch loss 0.646800041 batch mAP 0.49887085 batch PCKh 0.3125\n",
      "Trained batch 913 batch loss 0.599140286 batch mAP 0.515869141 batch PCKh 0.75\n",
      "Trained batch 914 batch loss 0.546229303 batch mAP 0.54977417 batch PCKh 0.125\n",
      "Trained batch 915 batch loss 0.611545861 batch mAP 0.551574707 batch PCKh 0.5625\n",
      "Trained batch 916 batch loss 0.54530108 batch mAP 0.544128418 batch PCKh 0.4375\n",
      "Trained batch 917 batch loss 0.603985667 batch mAP 0.4737854 batch PCKh 0.75\n",
      "Trained batch 918 batch loss 0.506854415 batch mAP 0.59954834 batch PCKh 0.75\n",
      "Trained batch 919 batch loss 0.605893493 batch mAP 0.579711914 batch PCKh 0.6875\n",
      "Trained batch 920 batch loss 0.581900954 batch mAP 0.57244873 batch PCKh 0.5625\n",
      "Trained batch 921 batch loss 0.580198586 batch mAP 0.561126709 batch PCKh 0.5625\n",
      "Trained batch 922 batch loss 0.662867546 batch mAP 0.598053 batch PCKh 0.5625\n",
      "Trained batch 923 batch loss 0.535574257 batch mAP 0.593170166 batch PCKh 0.6875\n",
      "Trained batch 924 batch loss 0.538930476 batch mAP 0.586853 batch PCKh 0.5\n",
      "Trained batch 925 batch loss 0.423349977 batch mAP 0.613098145 batch PCKh 0.5\n",
      "Trained batch 926 batch loss 0.547798932 batch mAP 0.579589844 batch PCKh 0.3125\n",
      "Trained batch 927 batch loss 0.543663383 batch mAP 0.571044922 batch PCKh 0.3125\n",
      "Trained batch 928 batch loss 0.458137095 batch mAP 0.524200439 batch PCKh 0.5\n",
      "Trained batch 929 batch loss 0.501430154 batch mAP 0.574401855 batch PCKh 0.5\n",
      "Trained batch 930 batch loss 0.436508894 batch mAP 0.520843506 batch PCKh 0.25\n",
      "Trained batch 931 batch loss 0.490075618 batch mAP 0.540039062 batch PCKh 0.4375\n",
      "Trained batch 932 batch loss 0.505550325 batch mAP 0.518219 batch PCKh 0.75\n",
      "Trained batch 933 batch loss 0.367242455 batch mAP 0.614898682 batch PCKh 0.25\n",
      "Trained batch 934 batch loss 0.533706307 batch mAP 0.544494629 batch PCKh 0.6875\n",
      "Trained batch 935 batch loss 0.554444969 batch mAP 0.534332275 batch PCKh 0.875\n",
      "Trained batch 936 batch loss 0.432922065 batch mAP 0.548065186 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 937 batch loss 0.375274211 batch mAP 0.596954346 batch PCKh 0\n",
      "Trained batch 938 batch loss 0.365768015 batch mAP 0.630493164 batch PCKh 0.625\n",
      "Trained batch 939 batch loss 0.347407341 batch mAP 0.649871826 batch PCKh 0.5\n",
      "Trained batch 940 batch loss 0.397762418 batch mAP 0.613342285 batch PCKh 0.0625\n",
      "Trained batch 941 batch loss 0.438065648 batch mAP 0.606933594 batch PCKh 0\n",
      "Trained batch 942 batch loss 0.368498385 batch mAP 0.596618652 batch PCKh 0\n",
      "Trained batch 943 batch loss 0.376114845 batch mAP 0.625976562 batch PCKh 0.25\n",
      "Trained batch 944 batch loss 0.486586958 batch mAP 0.594451904 batch PCKh 0.5\n",
      "Trained batch 945 batch loss 0.486925453 batch mAP 0.618896484 batch PCKh 0.375\n",
      "Trained batch 946 batch loss 0.515797138 batch mAP 0.593170166 batch PCKh 0.6875\n",
      "Trained batch 947 batch loss 0.581796765 batch mAP 0.625 batch PCKh 0.375\n",
      "Trained batch 948 batch loss 0.493507087 batch mAP 0.675048828 batch PCKh 0.4375\n",
      "Trained batch 949 batch loss 0.539438367 batch mAP 0.669586182 batch PCKh 0.3125\n",
      "Trained batch 950 batch loss 0.520358086 batch mAP 0.653991699 batch PCKh 0.5\n",
      "Trained batch 951 batch loss 0.583856404 batch mAP 0.624603271 batch PCKh 0.375\n",
      "Trained batch 952 batch loss 0.485243797 batch mAP 0.615570068 batch PCKh 0.5\n",
      "Trained batch 953 batch loss 0.510929286 batch mAP 0.60672 batch PCKh 0.5\n",
      "Trained batch 954 batch loss 0.51380831 batch mAP 0.592590332 batch PCKh 0.375\n",
      "Trained batch 955 batch loss 0.513552666 batch mAP 0.562316895 batch PCKh 0.1875\n",
      "Trained batch 956 batch loss 0.657547534 batch mAP 0.51965332 batch PCKh 0.4375\n",
      "Trained batch 957 batch loss 0.643721938 batch mAP 0.518554688 batch PCKh 0.375\n",
      "Trained batch 958 batch loss 0.584828734 batch mAP 0.484100342 batch PCKh 0.4375\n",
      "Trained batch 959 batch loss 0.472388089 batch mAP 0.582855225 batch PCKh 0.25\n",
      "Trained batch 960 batch loss 0.527463913 batch mAP 0.555419922 batch PCKh 0.1875\n",
      "Trained batch 961 batch loss 0.551247954 batch mAP 0.581848145 batch PCKh 0.3125\n",
      "Trained batch 962 batch loss 0.595037222 batch mAP 0.551818848 batch PCKh 0.25\n",
      "Trained batch 963 batch loss 0.650509894 batch mAP 0.449768066 batch PCKh 0.1875\n",
      "Trained batch 964 batch loss 0.613231063 batch mAP 0.326171875 batch PCKh 0.25\n",
      "Trained batch 965 batch loss 0.418351918 batch mAP 0.132995605 batch PCKh 0\n",
      "Trained batch 966 batch loss 0.477701932 batch mAP 0.204345703 batch PCKh 0.1875\n",
      "Trained batch 967 batch loss 0.587360859 batch mAP 0.257781982 batch PCKh 0.0625\n",
      "Trained batch 968 batch loss 0.5660882 batch mAP 0.279815674 batch PCKh 0.5\n",
      "Trained batch 969 batch loss 0.634722292 batch mAP 0.396087646 batch PCKh 0.3125\n",
      "Trained batch 970 batch loss 0.688947737 batch mAP 0.454833984 batch PCKh 0.1875\n",
      "Trained batch 971 batch loss 0.607346475 batch mAP 0.522491455 batch PCKh 0.8125\n",
      "Trained batch 972 batch loss 0.587047577 batch mAP 0.630706787 batch PCKh 0.25\n",
      "Trained batch 973 batch loss 0.464243203 batch mAP 0.693847656 batch PCKh 0.375\n",
      "Trained batch 974 batch loss 0.409925878 batch mAP 0.71194458 batch PCKh 0.4375\n",
      "Trained batch 975 batch loss 0.501484752 batch mAP 0.681152344 batch PCKh 0.5625\n",
      "Trained batch 976 batch loss 0.410091519 batch mAP 0.678863525 batch PCKh 0.6875\n",
      "Trained batch 977 batch loss 0.475238383 batch mAP 0.60736084 batch PCKh 0.375\n",
      "Trained batch 978 batch loss 0.510078609 batch mAP 0.575531 batch PCKh 0.5\n",
      "Trained batch 979 batch loss 0.444473386 batch mAP 0.530151367 batch PCKh 0.4375\n",
      "Trained batch 980 batch loss 0.512482166 batch mAP 0.562042236 batch PCKh 0.5625\n",
      "Trained batch 981 batch loss 0.487888455 batch mAP 0.642364502 batch PCKh 0.375\n",
      "Trained batch 982 batch loss 0.432538807 batch mAP 0.689361572 batch PCKh 0.5\n",
      "Trained batch 983 batch loss 0.516689539 batch mAP 0.676513672 batch PCKh 0.5625\n",
      "Trained batch 984 batch loss 0.449677229 batch mAP 0.619995117 batch PCKh 0\n",
      "Trained batch 985 batch loss 0.510051489 batch mAP 0.559417725 batch PCKh 0.4375\n",
      "Trained batch 986 batch loss 0.486192435 batch mAP 0.571228 batch PCKh 0.75\n",
      "Trained batch 987 batch loss 0.502426207 batch mAP 0.604003906 batch PCKh 0.875\n",
      "Trained batch 988 batch loss 0.52217263 batch mAP 0.558868408 batch PCKh 0.875\n",
      "Trained batch 989 batch loss 0.490330219 batch mAP 0.595031738 batch PCKh 0.875\n",
      "Trained batch 990 batch loss 0.573504329 batch mAP 0.569763184 batch PCKh 0.625\n",
      "Trained batch 991 batch loss 0.490166843 batch mAP 0.591095 batch PCKh 0.875\n",
      "Trained batch 992 batch loss 0.602574706 batch mAP 0.52835083 batch PCKh 0.75\n",
      "Trained batch 993 batch loss 0.572309136 batch mAP 0.548156738 batch PCKh 0.875\n",
      "Trained batch 994 batch loss 0.591859818 batch mAP 0.546173096 batch PCKh 0.8125\n",
      "Trained batch 995 batch loss 0.506708622 batch mAP 0.556518555 batch PCKh 0.5\n",
      "Trained batch 996 batch loss 0.581443489 batch mAP 0.511779785 batch PCKh 0.375\n",
      "Trained batch 997 batch loss 0.446822464 batch mAP 0.498657227 batch PCKh 0.1875\n",
      "Trained batch 998 batch loss 0.540444851 batch mAP 0.607330322 batch PCKh 0.25\n",
      "Trained batch 999 batch loss 0.447133243 batch mAP 0.639770508 batch PCKh 0.25\n",
      "Trained batch 1000 batch loss 0.45068872 batch mAP 0.635070801 batch PCKh 0.375\n",
      "Trained batch 1001 batch loss 0.488176703 batch mAP 0.663726807 batch PCKh 0.4375\n",
      "Trained batch 1002 batch loss 0.458845496 batch mAP 0.681945801 batch PCKh 0.6875\n",
      "Trained batch 1003 batch loss 0.558228731 batch mAP 0.600799561 batch PCKh 0.75\n",
      "Trained batch 1004 batch loss 0.492567 batch mAP 0.620697 batch PCKh 0.5\n",
      "Trained batch 1005 batch loss 0.546444178 batch mAP 0.591461182 batch PCKh 0.25\n",
      "Trained batch 1006 batch loss 0.53542 batch mAP 0.545074463 batch PCKh 0.5625\n",
      "Trained batch 1007 batch loss 0.537471712 batch mAP 0.609069824 batch PCKh 0.4375\n",
      "Trained batch 1008 batch loss 0.575173736 batch mAP 0.54586792 batch PCKh 0.5625\n",
      "Trained batch 1009 batch loss 0.608223677 batch mAP 0.574462891 batch PCKh 0.375\n",
      "Trained batch 1010 batch loss 0.508155704 batch mAP 0.607666 batch PCKh 0.5\n",
      "Trained batch 1011 batch loss 0.652960539 batch mAP 0.505188 batch PCKh 0.375\n",
      "Trained batch 1012 batch loss 0.598400772 batch mAP 0.47668457 batch PCKh 0.75\n",
      "Trained batch 1013 batch loss 0.568950653 batch mAP 0.567504883 batch PCKh 0.75\n",
      "Trained batch 1014 batch loss 0.67715466 batch mAP 0.529693604 batch PCKh 0.6875\n",
      "Trained batch 1015 batch loss 0.488270611 batch mAP 0.544952393 batch PCKh 0\n",
      "Trained batch 1016 batch loss 0.539360464 batch mAP 0.577636719 batch PCKh 0.0625\n",
      "Trained batch 1017 batch loss 0.593386769 batch mAP 0.599700928 batch PCKh 0.5\n",
      "Trained batch 1018 batch loss 0.596126914 batch mAP 0.60723877 batch PCKh 0.4375\n",
      "Trained batch 1019 batch loss 0.52623111 batch mAP 0.589996338 batch PCKh 0.3125\n",
      "Trained batch 1020 batch loss 0.532292604 batch mAP 0.573608398 batch PCKh 0.1875\n",
      "Trained batch 1021 batch loss 0.551954925 batch mAP 0.572601318 batch PCKh 0.25\n",
      "Trained batch 1022 batch loss 0.48296386 batch mAP 0.674072266 batch PCKh 0.5625\n",
      "Trained batch 1023 batch loss 0.581374705 batch mAP 0.519836426 batch PCKh 0.375\n",
      "Trained batch 1024 batch loss 0.567901134 batch mAP 0.577026367 batch PCKh 0.375\n",
      "Trained batch 1025 batch loss 0.600269496 batch mAP 0.520263672 batch PCKh 0.6875\n",
      "Trained batch 1026 batch loss 0.59581387 batch mAP 0.516235352 batch PCKh 0.4375\n",
      "Trained batch 1027 batch loss 0.599386215 batch mAP 0.539642334 batch PCKh 0.5\n",
      "Trained batch 1028 batch loss 0.576934934 batch mAP 0.549804688 batch PCKh 0.375\n",
      "Trained batch 1029 batch loss 0.612396061 batch mAP 0.501831055 batch PCKh 0.4375\n",
      "Trained batch 1030 batch loss 0.51466167 batch mAP 0.436645508 batch PCKh 0.1875\n",
      "Trained batch 1031 batch loss 0.534900606 batch mAP 0.446716309 batch PCKh 0.625\n",
      "Trained batch 1032 batch loss 0.550028443 batch mAP 0.500488281 batch PCKh 0.1875\n",
      "Trained batch 1033 batch loss 0.577198267 batch mAP 0.509307861 batch PCKh 0.125\n",
      "Trained batch 1034 batch loss 0.643743753 batch mAP 0.482055664 batch PCKh 0.125\n",
      "Trained batch 1035 batch loss 0.610852122 batch mAP 0.501190186 batch PCKh 0.5\n",
      "Trained batch 1036 batch loss 0.65662396 batch mAP 0.488586426 batch PCKh 0\n",
      "Trained batch 1037 batch loss 0.585186481 batch mAP 0.558380127 batch PCKh 0.6875\n",
      "Trained batch 1038 batch loss 0.517977 batch mAP 0.645996094 batch PCKh 0.625\n",
      "Trained batch 1039 batch loss 0.52741611 batch mAP 0.603668213 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1040 batch loss 0.527687371 batch mAP 0.596862793 batch PCKh 0.75\n",
      "Trained batch 1041 batch loss 0.547953844 batch mAP 0.67791748 batch PCKh 0.375\n",
      "Trained batch 1042 batch loss 0.555288851 batch mAP 0.667663574 batch PCKh 0.5\n",
      "Trained batch 1043 batch loss 0.610878706 batch mAP 0.611694336 batch PCKh 0.5625\n",
      "Trained batch 1044 batch loss 0.550195873 batch mAP 0.639068604 batch PCKh 0.3125\n",
      "Trained batch 1045 batch loss 0.542340815 batch mAP 0.602935791 batch PCKh 0.625\n",
      "Trained batch 1046 batch loss 0.578582644 batch mAP 0.60345459 batch PCKh 0.375\n",
      "Trained batch 1047 batch loss 0.585806072 batch mAP 0.547271729 batch PCKh 0.125\n",
      "Trained batch 1048 batch loss 0.556489766 batch mAP 0.558319092 batch PCKh 0.6875\n",
      "Trained batch 1049 batch loss 0.507053614 batch mAP 0.594635 batch PCKh 0.75\n",
      "Trained batch 1050 batch loss 0.46949631 batch mAP 0.569458 batch PCKh 0.6875\n",
      "Trained batch 1051 batch loss 0.602998257 batch mAP 0.589416504 batch PCKh 0.75\n",
      "Trained batch 1052 batch loss 0.643153071 batch mAP 0.577026367 batch PCKh 0.4375\n",
      "Trained batch 1053 batch loss 0.520284295 batch mAP 0.637146 batch PCKh 0.75\n",
      "Trained batch 1054 batch loss 0.522527874 batch mAP 0.66708374 batch PCKh 0.375\n",
      "Trained batch 1055 batch loss 0.453676701 batch mAP 0.583496094 batch PCKh 0.25\n",
      "Trained batch 1056 batch loss 0.545461655 batch mAP 0.592895508 batch PCKh 0.625\n",
      "Trained batch 1057 batch loss 0.523501039 batch mAP 0.604797363 batch PCKh 0.75\n",
      "Trained batch 1058 batch loss 0.506050587 batch mAP 0.550262451 batch PCKh 0.6875\n",
      "Trained batch 1059 batch loss 0.552063525 batch mAP 0.563293457 batch PCKh 0.5625\n",
      "Trained batch 1060 batch loss 0.667790294 batch mAP 0.527771 batch PCKh 0.75\n",
      "Trained batch 1061 batch loss 0.548878551 batch mAP 0.570465088 batch PCKh 0.5\n",
      "Trained batch 1062 batch loss 0.557949722 batch mAP 0.48916626 batch PCKh 0.5\n",
      "Trained batch 1063 batch loss 0.573560715 batch mAP 0.556671143 batch PCKh 0.75\n",
      "Trained batch 1064 batch loss 0.571974516 batch mAP 0.474090576 batch PCKh 0.8125\n",
      "Trained batch 1065 batch loss 0.611293197 batch mAP 0.523010254 batch PCKh 0.75\n",
      "Trained batch 1066 batch loss 0.605746627 batch mAP 0.554595947 batch PCKh 0.8125\n",
      "Trained batch 1067 batch loss 0.626478314 batch mAP 0.590423584 batch PCKh 0.6875\n",
      "Trained batch 1068 batch loss 0.541654229 batch mAP 0.584259033 batch PCKh 0.5625\n",
      "Trained batch 1069 batch loss 0.606833696 batch mAP 0.580200195 batch PCKh 0.125\n",
      "Trained batch 1070 batch loss 0.635935724 batch mAP 0.517089844 batch PCKh 0.6875\n",
      "Trained batch 1071 batch loss 0.541121066 batch mAP 0.609863281 batch PCKh 0.5625\n",
      "Trained batch 1072 batch loss 0.601866305 batch mAP 0.636932373 batch PCKh 0.75\n",
      "Trained batch 1073 batch loss 0.544000745 batch mAP 0.617950439 batch PCKh 0.5625\n",
      "Trained batch 1074 batch loss 0.567837715 batch mAP 0.520385742 batch PCKh 0.25\n",
      "Trained batch 1075 batch loss 0.573002 batch mAP 0.563049316 batch PCKh 0.875\n",
      "Trained batch 1076 batch loss 0.496073633 batch mAP 0.567108154 batch PCKh 0.4375\n",
      "Trained batch 1077 batch loss 0.493647188 batch mAP 0.58404541 batch PCKh 0.125\n",
      "Trained batch 1078 batch loss 0.57746923 batch mAP 0.557556152 batch PCKh 0.5625\n",
      "Trained batch 1079 batch loss 0.536129355 batch mAP 0.56741333 batch PCKh 0.3125\n",
      "Trained batch 1080 batch loss 0.492719769 batch mAP 0.613098145 batch PCKh 0.875\n",
      "Trained batch 1081 batch loss 0.502194524 batch mAP 0.543487549 batch PCKh 0.3125\n",
      "Trained batch 1082 batch loss 0.481928647 batch mAP 0.572418213 batch PCKh 0.75\n",
      "Trained batch 1083 batch loss 0.441013217 batch mAP 0.640960693 batch PCKh 0.75\n",
      "Trained batch 1084 batch loss 0.494355053 batch mAP 0.602783203 batch PCKh 0.125\n",
      "Trained batch 1085 batch loss 0.560911179 batch mAP 0.489624023 batch PCKh 0.5\n",
      "Trained batch 1086 batch loss 0.549870491 batch mAP 0.567382812 batch PCKh 0\n",
      "Trained batch 1087 batch loss 0.57443893 batch mAP 0.51373291 batch PCKh 0\n",
      "Trained batch 1088 batch loss 0.639084 batch mAP 0.606994629 batch PCKh 0.4375\n",
      "Trained batch 1089 batch loss 0.64738369 batch mAP 0.658355713 batch PCKh 0.1875\n",
      "Trained batch 1090 batch loss 0.689045906 batch mAP 0.658233643 batch PCKh 0.625\n",
      "Trained batch 1091 batch loss 0.642813206 batch mAP 0.699554443 batch PCKh 0.3125\n",
      "Trained batch 1092 batch loss 0.628758192 batch mAP 0.66217041 batch PCKh 0.25\n",
      "Trained batch 1093 batch loss 0.631775558 batch mAP 0.601928711 batch PCKh 0.875\n",
      "Trained batch 1094 batch loss 0.643211365 batch mAP 0.524230957 batch PCKh 0.8125\n",
      "Trained batch 1095 batch loss 0.631699443 batch mAP 0.50289917 batch PCKh 0.125\n",
      "Trained batch 1096 batch loss 0.570456207 batch mAP 0.521972656 batch PCKh 0.5\n",
      "Trained batch 1097 batch loss 0.620127559 batch mAP 0.561340332 batch PCKh 0.25\n",
      "Trained batch 1098 batch loss 0.556903839 batch mAP 0.53527832 batch PCKh 0.5625\n",
      "Trained batch 1099 batch loss 0.601428866 batch mAP 0.513458252 batch PCKh 0.875\n",
      "Trained batch 1100 batch loss 0.548101485 batch mAP 0.447570801 batch PCKh 0.125\n",
      "Trained batch 1101 batch loss 0.559285283 batch mAP 0.466766357 batch PCKh 0.125\n",
      "Trained batch 1102 batch loss 0.616329849 batch mAP 0.423339844 batch PCKh 0.625\n",
      "Trained batch 1103 batch loss 0.628332794 batch mAP 0.468292236 batch PCKh 0.1875\n",
      "Trained batch 1104 batch loss 0.595193326 batch mAP 0.48815918 batch PCKh 0.3125\n",
      "Trained batch 1105 batch loss 0.529073179 batch mAP 0.523925781 batch PCKh 0.3125\n",
      "Trained batch 1106 batch loss 0.606300116 batch mAP 0.510894775 batch PCKh 0.8125\n",
      "Trained batch 1107 batch loss 0.586206317 batch mAP 0.433410645 batch PCKh 0.1875\n",
      "Trained batch 1108 batch loss 0.545749605 batch mAP 0.499908447 batch PCKh 0.25\n",
      "Trained batch 1109 batch loss 0.587259054 batch mAP 0.495513916 batch PCKh 0.4375\n",
      "Trained batch 1110 batch loss 0.593271434 batch mAP 0.458618164 batch PCKh 0.625\n",
      "Trained batch 1111 batch loss 0.525711596 batch mAP 0.48638916 batch PCKh 0.6875\n",
      "Trained batch 1112 batch loss 0.567983925 batch mAP 0.419219971 batch PCKh 0.5\n",
      "Trained batch 1113 batch loss 0.522267461 batch mAP 0.488800049 batch PCKh 0.1875\n",
      "Trained batch 1114 batch loss 0.479855686 batch mAP 0.585968 batch PCKh 0.5\n",
      "Trained batch 1115 batch loss 0.492074192 batch mAP 0.530609131 batch PCKh 0.6875\n",
      "Trained batch 1116 batch loss 0.4531973 batch mAP 0.530578613 batch PCKh 0.5\n",
      "Trained batch 1117 batch loss 0.429060668 batch mAP 0.54574585 batch PCKh 0.75\n",
      "Trained batch 1118 batch loss 0.424156129 batch mAP 0.558868408 batch PCKh 0.75\n",
      "Trained batch 1119 batch loss 0.422692299 batch mAP 0.562438965 batch PCKh 0.75\n",
      "Trained batch 1120 batch loss 0.598293662 batch mAP 0.380828857 batch PCKh 0.6875\n",
      "Trained batch 1121 batch loss 0.608516395 batch mAP 0.427001953 batch PCKh 0.625\n",
      "Trained batch 1122 batch loss 0.620397091 batch mAP 0.36932373 batch PCKh 0.625\n",
      "Trained batch 1123 batch loss 0.53516221 batch mAP 0.354919434 batch PCKh 0.875\n",
      "Trained batch 1124 batch loss 0.573796451 batch mAP 0.42074585 batch PCKh 0.625\n",
      "Trained batch 1125 batch loss 0.668088436 batch mAP 0.438568115 batch PCKh 0.5\n",
      "Trained batch 1126 batch loss 0.646652162 batch mAP 0.520385742 batch PCKh 0.4375\n",
      "Trained batch 1127 batch loss 0.585631132 batch mAP 0.532135 batch PCKh 0.5\n",
      "Trained batch 1128 batch loss 0.559377968 batch mAP 0.584960938 batch PCKh 0.5\n",
      "Trained batch 1129 batch loss 0.505680263 batch mAP 0.559234619 batch PCKh 0.375\n",
      "Trained batch 1130 batch loss 0.562318563 batch mAP 0.504058838 batch PCKh 0.4375\n",
      "Trained batch 1131 batch loss 0.563531518 batch mAP 0.593017578 batch PCKh 0.1875\n",
      "Trained batch 1132 batch loss 0.470873475 batch mAP 0.582366943 batch PCKh 0.5625\n",
      "Trained batch 1133 batch loss 0.500551939 batch mAP 0.542327881 batch PCKh 0.625\n",
      "Trained batch 1134 batch loss 0.532548904 batch mAP 0.592315674 batch PCKh 0.625\n",
      "Trained batch 1135 batch loss 0.513019741 batch mAP 0.56149292 batch PCKh 0.5\n",
      "Trained batch 1136 batch loss 0.589793324 batch mAP 0.533508301 batch PCKh 0.625\n",
      "Trained batch 1137 batch loss 0.58482486 batch mAP 0.539367676 batch PCKh 0.625\n",
      "Trained batch 1138 batch loss 0.517000139 batch mAP 0.55645752 batch PCKh 0.75\n",
      "Trained batch 1139 batch loss 0.521294236 batch mAP 0.567810059 batch PCKh 0.625\n",
      "Trained batch 1140 batch loss 0.483659774 batch mAP 0.546112061 batch PCKh 0.5\n",
      "Trained batch 1141 batch loss 0.53171742 batch mAP 0.577575684 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1142 batch loss 0.461183548 batch mAP 0.621459961 batch PCKh 0.1875\n",
      "Trained batch 1143 batch loss 0.505900919 batch mAP 0.604431152 batch PCKh 0.5\n",
      "Trained batch 1144 batch loss 0.570433378 batch mAP 0.576171875 batch PCKh 0.6875\n",
      "Trained batch 1145 batch loss 0.520893872 batch mAP 0.606536865 batch PCKh 0.75\n",
      "Trained batch 1146 batch loss 0.525193691 batch mAP 0.612670898 batch PCKh 0.625\n",
      "Trained batch 1147 batch loss 0.575597167 batch mAP 0.51965332 batch PCKh 0.625\n",
      "Trained batch 1148 batch loss 0.615732431 batch mAP 0.496124268 batch PCKh 0.6875\n",
      "Trained batch 1149 batch loss 0.54572618 batch mAP 0.518188477 batch PCKh 0.375\n",
      "Trained batch 1150 batch loss 0.528157473 batch mAP 0.550231934 batch PCKh 0.5625\n",
      "Trained batch 1151 batch loss 0.460839719 batch mAP 0.579925537 batch PCKh 0.5\n",
      "Trained batch 1152 batch loss 0.488110214 batch mAP 0.564880371 batch PCKh 0.625\n",
      "Trained batch 1153 batch loss 0.486987442 batch mAP 0.553710938 batch PCKh 0.5\n",
      "Trained batch 1154 batch loss 0.521158278 batch mAP 0.603393555 batch PCKh 0.25\n",
      "Trained batch 1155 batch loss 0.541440248 batch mAP 0.604064941 batch PCKh 0.375\n",
      "Trained batch 1156 batch loss 0.574363828 batch mAP 0.59677124 batch PCKh 0.5\n",
      "Trained batch 1157 batch loss 0.550235629 batch mAP 0.645996094 batch PCKh 0.75\n",
      "Trained batch 1158 batch loss 0.538814247 batch mAP 0.623352051 batch PCKh 0.375\n",
      "Trained batch 1159 batch loss 0.494418621 batch mAP 0.62487793 batch PCKh 0.875\n",
      "Trained batch 1160 batch loss 0.569395065 batch mAP 0.571777344 batch PCKh 0.375\n",
      "Trained batch 1161 batch loss 0.564011872 batch mAP 0.585022 batch PCKh 0.375\n",
      "Trained batch 1162 batch loss 0.525574207 batch mAP 0.636169434 batch PCKh 0.625\n",
      "Trained batch 1163 batch loss 0.559241533 batch mAP 0.606140137 batch PCKh 0.25\n",
      "Trained batch 1164 batch loss 0.492867112 batch mAP 0.528961182 batch PCKh 0.5\n",
      "Trained batch 1165 batch loss 0.483217508 batch mAP 0.589294434 batch PCKh 0.75\n",
      "Trained batch 1166 batch loss 0.462662905 batch mAP 0.552703857 batch PCKh 0.375\n",
      "Trained batch 1167 batch loss 0.503075957 batch mAP 0.47467041 batch PCKh 0.625\n",
      "Trained batch 1168 batch loss 0.522621036 batch mAP 0.577819824 batch PCKh 0.625\n",
      "Trained batch 1169 batch loss 0.5287323 batch mAP 0.643188477 batch PCKh 0.6875\n",
      "Trained batch 1170 batch loss 0.538209856 batch mAP 0.584716797 batch PCKh 0.1875\n",
      "Trained batch 1171 batch loss 0.478789687 batch mAP 0.576507568 batch PCKh 0\n",
      "Trained batch 1172 batch loss 0.64959836 batch mAP 0.539672852 batch PCKh 0.125\n",
      "Trained batch 1173 batch loss 0.614066958 batch mAP 0.574859619 batch PCKh 0\n",
      "Trained batch 1174 batch loss 0.638741076 batch mAP 0.584106445 batch PCKh 0.375\n",
      "Trained batch 1175 batch loss 0.556295633 batch mAP 0.534759521 batch PCKh 0.125\n",
      "Trained batch 1176 batch loss 0.510053396 batch mAP 0.559387207 batch PCKh 0.5\n",
      "Trained batch 1177 batch loss 0.460314453 batch mAP 0.570800781 batch PCKh 0.5\n",
      "Trained batch 1178 batch loss 0.474200606 batch mAP 0.582275391 batch PCKh 0.1875\n",
      "Trained batch 1179 batch loss 0.422770619 batch mAP 0.583984375 batch PCKh 0.375\n",
      "Trained batch 1180 batch loss 0.636906385 batch mAP 0.554504395 batch PCKh 0.1875\n",
      "Trained batch 1181 batch loss 0.547742 batch mAP 0.550109863 batch PCKh 0.1875\n",
      "Trained batch 1182 batch loss 0.598534644 batch mAP 0.536132812 batch PCKh 0.125\n",
      "Trained batch 1183 batch loss 0.542019963 batch mAP 0.604126 batch PCKh 0.3125\n",
      "Trained batch 1184 batch loss 0.600529432 batch mAP 0.605560303 batch PCKh 0.75\n",
      "Trained batch 1185 batch loss 0.558352 batch mAP 0.622680664 batch PCKh 0.375\n",
      "Trained batch 1186 batch loss 0.628668308 batch mAP 0.587646484 batch PCKh 0.375\n",
      "Trained batch 1187 batch loss 0.635503292 batch mAP 0.570007324 batch PCKh 0.3125\n",
      "Trained batch 1188 batch loss 0.539493561 batch mAP 0.575592041 batch PCKh 0.8125\n",
      "Trained batch 1189 batch loss 0.531308055 batch mAP 0.553985596 batch PCKh 0.1875\n",
      "Trained batch 1190 batch loss 0.635752082 batch mAP 0.574523926 batch PCKh 0.3125\n",
      "Trained batch 1191 batch loss 0.593869805 batch mAP 0.635284424 batch PCKh 0.5\n",
      "Trained batch 1192 batch loss 0.590807796 batch mAP 0.623901367 batch PCKh 0.4375\n",
      "Trained batch 1193 batch loss 0.559792042 batch mAP 0.61151123 batch PCKh 0.1875\n",
      "Trained batch 1194 batch loss 0.503483474 batch mAP 0.541809082 batch PCKh 0.3125\n",
      "Trained batch 1195 batch loss 0.590094328 batch mAP 0.548034668 batch PCKh 0.5\n",
      "Trained batch 1196 batch loss 0.617152691 batch mAP 0.468292236 batch PCKh 0.375\n",
      "Trained batch 1197 batch loss 0.490733534 batch mAP 0.461120605 batch PCKh 0.0625\n",
      "Trained batch 1198 batch loss 0.542401373 batch mAP 0.507263184 batch PCKh 0.1875\n",
      "Trained batch 1199 batch loss 0.534158349 batch mAP 0.546936035 batch PCKh 0.625\n",
      "Trained batch 1200 batch loss 0.586155534 batch mAP 0.53717041 batch PCKh 0.1875\n",
      "Trained batch 1201 batch loss 0.571000695 batch mAP 0.537200928 batch PCKh 0.5625\n",
      "Trained batch 1202 batch loss 0.525901854 batch mAP 0.551208496 batch PCKh 0.875\n",
      "Trained batch 1203 batch loss 0.574705541 batch mAP 0.553924561 batch PCKh 0.875\n",
      "Trained batch 1204 batch loss 0.587815 batch mAP 0.57220459 batch PCKh 0.8125\n",
      "Trained batch 1205 batch loss 0.556265593 batch mAP 0.590881348 batch PCKh 0.375\n",
      "Trained batch 1206 batch loss 0.57842344 batch mAP 0.590515137 batch PCKh 0.25\n",
      "Trained batch 1207 batch loss 0.590214 batch mAP 0.566619873 batch PCKh 0.6875\n",
      "Trained batch 1208 batch loss 0.56390661 batch mAP 0.613800049 batch PCKh 0.5\n",
      "Trained batch 1209 batch loss 0.539633393 batch mAP 0.585083 batch PCKh 0.375\n",
      "Trained batch 1210 batch loss 0.606160045 batch mAP 0.576507568 batch PCKh 0.375\n",
      "Trained batch 1211 batch loss 0.688091099 batch mAP 0.534301758 batch PCKh 0.125\n",
      "Trained batch 1212 batch loss 0.563606858 batch mAP 0.620117188 batch PCKh 0.3125\n",
      "Trained batch 1213 batch loss 0.612790585 batch mAP 0.624145508 batch PCKh 0.6875\n",
      "Trained batch 1214 batch loss 0.633249879 batch mAP 0.577819824 batch PCKh 0.1875\n",
      "Trained batch 1215 batch loss 0.559641302 batch mAP 0.570648193 batch PCKh 0.8125\n",
      "Trained batch 1216 batch loss 0.592785656 batch mAP 0.552734375 batch PCKh 0.6875\n",
      "Trained batch 1217 batch loss 0.588981807 batch mAP 0.609405518 batch PCKh 0.25\n",
      "Trained batch 1218 batch loss 0.51166594 batch mAP 0.511932373 batch PCKh 0.75\n",
      "Trained batch 1219 batch loss 0.548107147 batch mAP 0.58203125 batch PCKh 0.5\n",
      "Trained batch 1220 batch loss 0.558427334 batch mAP 0.589233398 batch PCKh 0.25\n",
      "Trained batch 1221 batch loss 0.589227676 batch mAP 0.542449951 batch PCKh 0.1875\n",
      "Trained batch 1222 batch loss 0.627701759 batch mAP 0.522644043 batch PCKh 0.4375\n",
      "Trained batch 1223 batch loss 0.56994915 batch mAP 0.563812256 batch PCKh 0.25\n",
      "Trained batch 1224 batch loss 0.54485333 batch mAP 0.556304932 batch PCKh 0.1875\n",
      "Trained batch 1225 batch loss 0.577301502 batch mAP 0.563720703 batch PCKh 0.875\n",
      "Trained batch 1226 batch loss 0.56043613 batch mAP 0.528595 batch PCKh 0.5\n",
      "Trained batch 1227 batch loss 0.450829118 batch mAP 0.664581299 batch PCKh 0.8125\n",
      "Trained batch 1228 batch loss 0.597810805 batch mAP 0.662536621 batch PCKh 0.125\n",
      "Trained batch 1229 batch loss 0.551004827 batch mAP 0.630554199 batch PCKh 0.8125\n",
      "Trained batch 1230 batch loss 0.5798859 batch mAP 0.568756104 batch PCKh 0.4375\n",
      "Trained batch 1231 batch loss 0.487628341 batch mAP 0.693603516 batch PCKh 0.75\n",
      "Trained batch 1232 batch loss 0.490301847 batch mAP 0.6875 batch PCKh 0.4375\n",
      "Trained batch 1233 batch loss 0.494134486 batch mAP 0.692443848 batch PCKh 0.875\n",
      "Trained batch 1234 batch loss 0.487659216 batch mAP 0.690307617 batch PCKh 0.625\n",
      "Trained batch 1235 batch loss 0.528264403 batch mAP 0.673248291 batch PCKh 0.625\n",
      "Trained batch 1236 batch loss 0.568244219 batch mAP 0.626373291 batch PCKh 0.4375\n",
      "Trained batch 1237 batch loss 0.622767806 batch mAP 0.607910156 batch PCKh 0.25\n",
      "Trained batch 1238 batch loss 0.569182694 batch mAP 0.6144104 batch PCKh 0.6875\n",
      "Trained batch 1239 batch loss 0.59625715 batch mAP 0.585693359 batch PCKh 0.1875\n",
      "Trained batch 1240 batch loss 0.463345379 batch mAP 0.657775879 batch PCKh 0.5625\n",
      "Trained batch 1241 batch loss 0.563535929 batch mAP 0.649047852 batch PCKh 0.5\n",
      "Trained batch 1242 batch loss 0.60975337 batch mAP 0.551055908 batch PCKh 0.4375\n",
      "Trained batch 1243 batch loss 0.564849138 batch mAP 0.62286377 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1244 batch loss 0.522939146 batch mAP 0.639160156 batch PCKh 0.875\n",
      "Trained batch 1245 batch loss 0.570219874 batch mAP 0.567138672 batch PCKh 0.5\n",
      "Trained batch 1246 batch loss 0.649759352 batch mAP 0.543884277 batch PCKh 0.6875\n",
      "Trained batch 1247 batch loss 0.479119927 batch mAP 0.627746582 batch PCKh 0.75\n",
      "Trained batch 1248 batch loss 0.528610051 batch mAP 0.59487915 batch PCKh 0.375\n",
      "Trained batch 1249 batch loss 0.561365783 batch mAP 0.611602783 batch PCKh 0.25\n",
      "Trained batch 1250 batch loss 0.544739306 batch mAP 0.608184814 batch PCKh 0.625\n",
      "Trained batch 1251 batch loss 0.475910276 batch mAP 0.655303955 batch PCKh 0.5\n",
      "Trained batch 1252 batch loss 0.482454479 batch mAP 0.651092529 batch PCKh 0.375\n",
      "Trained batch 1253 batch loss 0.51727879 batch mAP 0.604400635 batch PCKh 0.375\n",
      "Trained batch 1254 batch loss 0.568764 batch mAP 0.594085693 batch PCKh 0.625\n",
      "Trained batch 1255 batch loss 0.481049359 batch mAP 0.670684814 batch PCKh 0.6875\n",
      "Trained batch 1256 batch loss 0.463883758 batch mAP 0.65435791 batch PCKh 0.75\n",
      "Trained batch 1257 batch loss 0.488713115 batch mAP 0.590240479 batch PCKh 0.8125\n",
      "Trained batch 1258 batch loss 0.532536507 batch mAP 0.563873291 batch PCKh 0.6875\n",
      "Trained batch 1259 batch loss 0.457660139 batch mAP 0.62399292 batch PCKh 0.8125\n",
      "Trained batch 1260 batch loss 0.478766084 batch mAP 0.618896484 batch PCKh 0.75\n",
      "Trained batch 1261 batch loss 0.502990723 batch mAP 0.643035889 batch PCKh 0.4375\n",
      "Trained batch 1262 batch loss 0.412818521 batch mAP 0.650085449 batch PCKh 0.5\n",
      "Trained batch 1263 batch loss 0.469583035 batch mAP 0.610748291 batch PCKh 0.375\n",
      "Trained batch 1264 batch loss 0.490177929 batch mAP 0.640106201 batch PCKh 0.8125\n",
      "Trained batch 1265 batch loss 0.457120955 batch mAP 0.646942139 batch PCKh 0.75\n",
      "Trained batch 1266 batch loss 0.557677507 batch mAP 0.598724365 batch PCKh 0.5625\n",
      "Trained batch 1267 batch loss 0.577648461 batch mAP 0.56930542 batch PCKh 0.75\n",
      "Trained batch 1268 batch loss 0.547819674 batch mAP 0.579986572 batch PCKh 0.1875\n",
      "Trained batch 1269 batch loss 0.570369065 batch mAP 0.60672 batch PCKh 0.125\n",
      "Trained batch 1270 batch loss 0.523088455 batch mAP 0.612213135 batch PCKh 0.5\n",
      "Trained batch 1271 batch loss 0.574686408 batch mAP 0.58380127 batch PCKh 0.6875\n",
      "Trained batch 1272 batch loss 0.560637116 batch mAP 0.57333374 batch PCKh 0.6875\n",
      "Trained batch 1273 batch loss 0.585458815 batch mAP 0.572906494 batch PCKh 0.6875\n",
      "Trained batch 1274 batch loss 0.470780253 batch mAP 0.58380127 batch PCKh 0.4375\n",
      "Trained batch 1275 batch loss 0.442361861 batch mAP 0.52532959 batch PCKh 0.1875\n",
      "Trained batch 1276 batch loss 0.395210862 batch mAP 0.573547363 batch PCKh 0.375\n",
      "Trained batch 1277 batch loss 0.556275785 batch mAP 0.562469482 batch PCKh 0.4375\n",
      "Trained batch 1278 batch loss 0.515644848 batch mAP 0.544189453 batch PCKh 0.1875\n",
      "Trained batch 1279 batch loss 0.495234519 batch mAP 0.591064453 batch PCKh 0.1875\n",
      "Trained batch 1280 batch loss 0.554507554 batch mAP 0.591522217 batch PCKh 0.6875\n",
      "Trained batch 1281 batch loss 0.51411736 batch mAP 0.632598877 batch PCKh 0.75\n",
      "Trained batch 1282 batch loss 0.511182308 batch mAP 0.5703125 batch PCKh 0.25\n",
      "Trained batch 1283 batch loss 0.526376367 batch mAP 0.573059082 batch PCKh 0.0625\n",
      "Trained batch 1284 batch loss 0.623432159 batch mAP 0.564788818 batch PCKh 0.375\n",
      "Trained batch 1285 batch loss 0.601640463 batch mAP 0.609191895 batch PCKh 0.3125\n",
      "Trained batch 1286 batch loss 0.551523626 batch mAP 0.620178223 batch PCKh 0.4375\n",
      "Trained batch 1287 batch loss 0.515823722 batch mAP 0.621337891 batch PCKh 0.6875\n",
      "Trained batch 1288 batch loss 0.547323644 batch mAP 0.607605 batch PCKh 0.5625\n",
      "Trained batch 1289 batch loss 0.549963236 batch mAP 0.589294434 batch PCKh 0.3125\n",
      "Trained batch 1290 batch loss 0.538194478 batch mAP 0.58001709 batch PCKh 0.625\n",
      "Trained batch 1291 batch loss 0.529906929 batch mAP 0.595733643 batch PCKh 0.5625\n",
      "Trained batch 1292 batch loss 0.578983605 batch mAP 0.559936523 batch PCKh 0.375\n",
      "Trained batch 1293 batch loss 0.528091311 batch mAP 0.6065979 batch PCKh 0.4375\n",
      "Trained batch 1294 batch loss 0.448030829 batch mAP 0.602020264 batch PCKh 0\n",
      "Trained batch 1295 batch loss 0.560458839 batch mAP 0.590209961 batch PCKh 0.625\n",
      "Trained batch 1296 batch loss 0.592075765 batch mAP 0.567657471 batch PCKh 0.625\n",
      "Trained batch 1297 batch loss 0.487112343 batch mAP 0.536224365 batch PCKh 0\n",
      "Trained batch 1298 batch loss 0.551525593 batch mAP 0.623809814 batch PCKh 0.5\n",
      "Trained batch 1299 batch loss 0.505614638 batch mAP 0.629577637 batch PCKh 0.75\n",
      "Trained batch 1300 batch loss 0.581250489 batch mAP 0.599761963 batch PCKh 0.4375\n",
      "Trained batch 1301 batch loss 0.527666807 batch mAP 0.607971191 batch PCKh 0.4375\n",
      "Trained batch 1302 batch loss 0.47513783 batch mAP 0.671966553 batch PCKh 0.25\n",
      "Trained batch 1303 batch loss 0.42233175 batch mAP 0.610412598 batch PCKh 0\n",
      "Trained batch 1304 batch loss 0.390128881 batch mAP 0.552825928 batch PCKh 0\n",
      "Trained batch 1305 batch loss 0.448844641 batch mAP 0.576477051 batch PCKh 0\n",
      "Trained batch 1306 batch loss 0.417542815 batch mAP 0.595916748 batch PCKh 0.5\n",
      "Trained batch 1307 batch loss 0.536312 batch mAP 0.629821777 batch PCKh 0.25\n",
      "Trained batch 1308 batch loss 0.543964267 batch mAP 0.626098633 batch PCKh 0.25\n",
      "Trained batch 1309 batch loss 0.607508123 batch mAP 0.636383057 batch PCKh 0.875\n",
      "Trained batch 1310 batch loss 0.656751633 batch mAP 0.566467285 batch PCKh 0.3125\n",
      "Trained batch 1311 batch loss 0.701532722 batch mAP 0.549835205 batch PCKh 0.0625\n",
      "Trained batch 1312 batch loss 0.744873583 batch mAP 0.532592773 batch PCKh 0.25\n",
      "Trained batch 1313 batch loss 0.636617064 batch mAP 0.572998047 batch PCKh 0.1875\n",
      "Trained batch 1314 batch loss 0.643254638 batch mAP 0.59185791 batch PCKh 0.5625\n",
      "Trained batch 1315 batch loss 0.524188638 batch mAP 0.53604126 batch PCKh 0.25\n",
      "Trained batch 1316 batch loss 0.480596304 batch mAP 0.553009033 batch PCKh 0.6875\n",
      "Trained batch 1317 batch loss 0.52857995 batch mAP 0.516052246 batch PCKh 0.75\n",
      "Trained batch 1318 batch loss 0.500429809 batch mAP 0.479217529 batch PCKh 0.375\n",
      "Trained batch 1319 batch loss 0.552700937 batch mAP 0.604278564 batch PCKh 0.25\n",
      "Trained batch 1320 batch loss 0.545264 batch mAP 0.560852051 batch PCKh 0.25\n",
      "Trained batch 1321 batch loss 0.565706 batch mAP 0.552368164 batch PCKh 0.625\n",
      "Trained batch 1322 batch loss 0.58305335 batch mAP 0.606292725 batch PCKh 0.5\n",
      "Trained batch 1323 batch loss 0.586841583 batch mAP 0.553070068 batch PCKh 0.375\n",
      "Trained batch 1324 batch loss 0.56580162 batch mAP 0.581970215 batch PCKh 0.625\n",
      "Trained batch 1325 batch loss 0.57926017 batch mAP 0.581390381 batch PCKh 0.375\n",
      "Trained batch 1326 batch loss 0.539180517 batch mAP 0.630828857 batch PCKh 0.375\n",
      "Trained batch 1327 batch loss 0.51639545 batch mAP 0.609771729 batch PCKh 0.6875\n",
      "Trained batch 1328 batch loss 0.625760853 batch mAP 0.561706543 batch PCKh 0.4375\n",
      "Trained batch 1329 batch loss 0.579044461 batch mAP 0.577941895 batch PCKh 0.625\n",
      "Trained batch 1330 batch loss 0.543931484 batch mAP 0.564117432 batch PCKh 0.75\n",
      "Trained batch 1331 batch loss 0.552783608 batch mAP 0.614074707 batch PCKh 0.8125\n",
      "Trained batch 1332 batch loss 0.493601203 batch mAP 0.622802734 batch PCKh 0.1875\n",
      "Trained batch 1333 batch loss 0.50032115 batch mAP 0.61895752 batch PCKh 0.375\n",
      "Trained batch 1334 batch loss 0.502065957 batch mAP 0.670440674 batch PCKh 0.375\n",
      "Trained batch 1335 batch loss 0.503356159 batch mAP 0.610778809 batch PCKh 0.375\n",
      "Trained batch 1336 batch loss 0.504080534 batch mAP 0.599914551 batch PCKh 0.5625\n",
      "Trained batch 1337 batch loss 0.515088916 batch mAP 0.658966064 batch PCKh 0.4375\n",
      "Trained batch 1338 batch loss 0.546013951 batch mAP 0.639129639 batch PCKh 0.0625\n",
      "Trained batch 1339 batch loss 0.5885095 batch mAP 0.617248535 batch PCKh 0.625\n",
      "Trained batch 1340 batch loss 0.610002637 batch mAP 0.626037598 batch PCKh 0.5625\n",
      "Trained batch 1341 batch loss 0.534606218 batch mAP 0.660827637 batch PCKh 0.625\n",
      "Trained batch 1342 batch loss 0.49856326 batch mAP 0.593536377 batch PCKh 0.6875\n",
      "Trained batch 1343 batch loss 0.488635957 batch mAP 0.601318359 batch PCKh 0.1875\n",
      "Trained batch 1344 batch loss 0.379538119 batch mAP 0.584564209 batch PCKh 0.3125\n",
      "Trained batch 1345 batch loss 0.496691227 batch mAP 0.536956787 batch PCKh 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1346 batch loss 0.560357153 batch mAP 0.577819824 batch PCKh 0.3125\n",
      "Trained batch 1347 batch loss 0.547226489 batch mAP 0.62979126 batch PCKh 0.3125\n",
      "Trained batch 1348 batch loss 0.552691102 batch mAP 0.553009033 batch PCKh 0.6875\n",
      "Trained batch 1349 batch loss 0.594105422 batch mAP 0.565887451 batch PCKh 0.375\n",
      "Trained batch 1350 batch loss 0.460784286 batch mAP 0.610290527 batch PCKh 0.125\n",
      "Trained batch 1351 batch loss 0.407181859 batch mAP 0.64654541 batch PCKh 0.4375\n",
      "Trained batch 1352 batch loss 0.426410764 batch mAP 0.650756836 batch PCKh 0.5\n",
      "Trained batch 1353 batch loss 0.53377676 batch mAP 0.622039795 batch PCKh 0.25\n",
      "Trained batch 1354 batch loss 0.528382778 batch mAP 0.621673584 batch PCKh 0.375\n",
      "Trained batch 1355 batch loss 0.469782501 batch mAP 0.686920166 batch PCKh 0.375\n",
      "Trained batch 1356 batch loss 0.517007828 batch mAP 0.67855835 batch PCKh 0.1875\n",
      "Trained batch 1357 batch loss 0.479173779 batch mAP 0.689666748 batch PCKh 0.4375\n",
      "Trained batch 1358 batch loss 0.45117411 batch mAP 0.695648193 batch PCKh 0.625\n",
      "Trained batch 1359 batch loss 0.470825493 batch mAP 0.690002441 batch PCKh 0.625\n",
      "Trained batch 1360 batch loss 0.431106776 batch mAP 0.698638916 batch PCKh 0.6875\n",
      "Trained batch 1361 batch loss 0.519913793 batch mAP 0.686553955 batch PCKh 0.625\n",
      "Trained batch 1362 batch loss 0.466367722 batch mAP 0.674957275 batch PCKh 0.4375\n",
      "Trained batch 1363 batch loss 0.47660923 batch mAP 0.660064697 batch PCKh 0.1875\n",
      "Trained batch 1364 batch loss 0.607211411 batch mAP 0.591369629 batch PCKh 0.25\n",
      "Trained batch 1365 batch loss 0.530543685 batch mAP 0.623748779 batch PCKh 0.5625\n",
      "Trained batch 1366 batch loss 0.475239754 batch mAP 0.666259766 batch PCKh 0.75\n",
      "Trained batch 1367 batch loss 0.521099329 batch mAP 0.639526367 batch PCKh 0.25\n",
      "Trained batch 1368 batch loss 0.462342381 batch mAP 0.664550781 batch PCKh 0.1875\n",
      "Trained batch 1369 batch loss 0.502918482 batch mAP 0.567932129 batch PCKh 0\n",
      "Trained batch 1370 batch loss 0.511129498 batch mAP 0.608062744 batch PCKh 0.375\n",
      "Trained batch 1371 batch loss 0.548150897 batch mAP 0.608947754 batch PCKh 0.3125\n",
      "Trained batch 1372 batch loss 0.588959098 batch mAP 0.635192871 batch PCKh 0.5625\n",
      "Trained batch 1373 batch loss 0.545106411 batch mAP 0.609558105 batch PCKh 0.625\n",
      "Trained batch 1374 batch loss 0.585259914 batch mAP 0.608581543 batch PCKh 0.375\n",
      "Trained batch 1375 batch loss 0.551234603 batch mAP 0.635559082 batch PCKh 0.25\n",
      "Trained batch 1376 batch loss 0.65682894 batch mAP 0.56463623 batch PCKh 0.125\n",
      "Trained batch 1377 batch loss 0.558048964 batch mAP 0.635742188 batch PCKh 0.3125\n",
      "Trained batch 1378 batch loss 0.58816427 batch mAP 0.55557251 batch PCKh 0.5\n",
      "Trained batch 1379 batch loss 0.548654318 batch mAP 0.588928223 batch PCKh 0.3125\n",
      "Trained batch 1380 batch loss 0.545385599 batch mAP 0.603118896 batch PCKh 0.3125\n",
      "Trained batch 1381 batch loss 0.549006402 batch mAP 0.520477295 batch PCKh 0.3125\n",
      "Trained batch 1382 batch loss 0.505399883 batch mAP 0.554260254 batch PCKh 0.625\n",
      "Trained batch 1383 batch loss 0.469688416 batch mAP 0.607543945 batch PCKh 0.625\n",
      "Trained batch 1384 batch loss 0.546810269 batch mAP 0.586303711 batch PCKh 0.625\n",
      "Trained batch 1385 batch loss 0.550978065 batch mAP 0.595336914 batch PCKh 0.5\n",
      "Trained batch 1386 batch loss 0.55447036 batch mAP 0.638824463 batch PCKh 0.4375\n",
      "Trained batch 1387 batch loss 0.547362566 batch mAP 0.600524902 batch PCKh 0.3125\n",
      "Trained batch 1388 batch loss 0.533545732 batch mAP 0.620880127 batch PCKh 0.25\n",
      "Trained batch 1389 batch loss 0.523858607 batch mAP 0.601318359 batch PCKh 0.5\n",
      "Trained batch 1390 batch loss 0.535998583 batch mAP 0.651916504 batch PCKh 0.375\n",
      "Trained batch 1391 batch loss 0.610320568 batch mAP 0.612854 batch PCKh 0.75\n",
      "Trained batch 1392 batch loss 0.535206139 batch mAP 0.655822754 batch PCKh 0.5\n",
      "Trained batch 1393 batch loss 0.498428941 batch mAP 0.691223145 batch PCKh 0.1875\n",
      "Trained batch 1394 batch loss 0.622748 batch mAP 0.644195557 batch PCKh 0.3125\n",
      "Trained batch 1395 batch loss 0.498064756 batch mAP 0.698364258 batch PCKh 0.25\n",
      "Trained batch 1396 batch loss 0.519712508 batch mAP 0.656402588 batch PCKh 0.3125\n",
      "Trained batch 1397 batch loss 0.603874266 batch mAP 0.584259033 batch PCKh 0.4375\n",
      "Trained batch 1398 batch loss 0.680469334 batch mAP 0.548522949 batch PCKh 0\n",
      "Trained batch 1399 batch loss 0.680925846 batch mAP 0.511444092 batch PCKh 0\n",
      "Trained batch 1400 batch loss 0.628050148 batch mAP 0.55456543 batch PCKh 0.5\n",
      "Trained batch 1401 batch loss 0.500418663 batch mAP 0.5987854 batch PCKh 0.625\n",
      "Trained batch 1402 batch loss 0.595317125 batch mAP 0.458770752 batch PCKh 0.5625\n",
      "Trained batch 1403 batch loss 0.471366823 batch mAP 0.463775635 batch PCKh 0.125\n",
      "Trained batch 1404 batch loss 0.650248408 batch mAP 0.397766113 batch PCKh 0.3125\n",
      "Trained batch 1405 batch loss 0.635158479 batch mAP 0.38470459 batch PCKh 0.3125\n",
      "Trained batch 1406 batch loss 0.698298693 batch mAP 0.40145874 batch PCKh 0.6875\n",
      "Trained batch 1407 batch loss 0.572909951 batch mAP 0.46887207 batch PCKh 0.8125\n",
      "Trained batch 1408 batch loss 0.56584686 batch mAP 0.473602295 batch PCKh 0.5625\n",
      "Trained batch 1409 batch loss 0.539731741 batch mAP 0.555145264 batch PCKh 0.3125\n",
      "Trained batch 1410 batch loss 0.549510121 batch mAP 0.520263672 batch PCKh 0.4375\n",
      "Trained batch 1411 batch loss 0.600773335 batch mAP 0.53616333 batch PCKh 0.1875\n",
      "Trained batch 1412 batch loss 0.53944844 batch mAP 0.536956787 batch PCKh 0.4375\n",
      "Trained batch 1413 batch loss 0.516557693 batch mAP 0.624694824 batch PCKh 0.625\n",
      "Trained batch 1414 batch loss 0.515962 batch mAP 0.549041748 batch PCKh 0.4375\n",
      "Trained batch 1415 batch loss 0.487451077 batch mAP 0.630828857 batch PCKh 0.125\n",
      "Trained batch 1416 batch loss 0.562434673 batch mAP 0.556030273 batch PCKh 0.625\n",
      "Trained batch 1417 batch loss 0.596745133 batch mAP 0.589386 batch PCKh 0.875\n",
      "Trained batch 1418 batch loss 0.555191278 batch mAP 0.607391357 batch PCKh 0.5625\n",
      "Trained batch 1419 batch loss 0.591574609 batch mAP 0.602355957 batch PCKh 0.6875\n",
      "Trained batch 1420 batch loss 0.523976445 batch mAP 0.611480713 batch PCKh 0.625\n",
      "Trained batch 1421 batch loss 0.627268314 batch mAP 0.578399658 batch PCKh 0.4375\n",
      "Trained batch 1422 batch loss 0.580030859 batch mAP 0.577972412 batch PCKh 0.3125\n",
      "Trained batch 1423 batch loss 0.468852699 batch mAP 0.604705811 batch PCKh 0.3125\n",
      "Trained batch 1424 batch loss 0.50806123 batch mAP 0.622345 batch PCKh 0.5\n",
      "Trained batch 1425 batch loss 0.570853055 batch mAP 0.651062 batch PCKh 0.3125\n",
      "Trained batch 1426 batch loss 0.537002742 batch mAP 0.644012451 batch PCKh 0.3125\n",
      "Trained batch 1427 batch loss 0.581397057 batch mAP 0.700897217 batch PCKh 0.3125\n",
      "Trained batch 1428 batch loss 0.570757747 batch mAP 0.625854492 batch PCKh 0.25\n",
      "Trained batch 1429 batch loss 0.49816373 batch mAP 0.586608887 batch PCKh 0.6875\n",
      "Trained batch 1430 batch loss 0.572373509 batch mAP 0.514343262 batch PCKh 0.5625\n",
      "Trained batch 1431 batch loss 0.498230368 batch mAP 0.589355469 batch PCKh 0.3125\n",
      "Trained batch 1432 batch loss 0.515671492 batch mAP 0.615264893 batch PCKh 0.25\n",
      "Trained batch 1433 batch loss 0.523456335 batch mAP 0.59198 batch PCKh 0.375\n",
      "Trained batch 1434 batch loss 0.518573 batch mAP 0.561584473 batch PCKh 0.375\n",
      "Trained batch 1435 batch loss 0.50694412 batch mAP 0.609893799 batch PCKh 0.5625\n",
      "Trained batch 1436 batch loss 0.489755571 batch mAP 0.630859375 batch PCKh 0.3125\n",
      "Trained batch 1437 batch loss 0.472346455 batch mAP 0.670410156 batch PCKh 0.625\n",
      "Trained batch 1438 batch loss 0.495329171 batch mAP 0.582366943 batch PCKh 0.25\n",
      "Trained batch 1439 batch loss 0.501695096 batch mAP 0.678070068 batch PCKh 0.4375\n",
      "Trained batch 1440 batch loss 0.468263 batch mAP 0.672332764 batch PCKh 0.4375\n",
      "Trained batch 1441 batch loss 0.439704508 batch mAP 0.708313 batch PCKh 0.5\n",
      "Trained batch 1442 batch loss 0.577473462 batch mAP 0.638214111 batch PCKh 0.1875\n",
      "Trained batch 1443 batch loss 0.446131051 batch mAP 0.71395874 batch PCKh 0.375\n",
      "Trained batch 1444 batch loss 0.380561292 batch mAP 0.716949463 batch PCKh 0.4375\n",
      "Trained batch 1445 batch loss 0.497615695 batch mAP 0.639160156 batch PCKh 0.375\n",
      "Trained batch 1446 batch loss 0.507463872 batch mAP 0.623901367 batch PCKh 0.3125\n",
      "Trained batch 1447 batch loss 0.476217419 batch mAP 0.658630371 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1448 batch loss 0.547293365 batch mAP 0.62689209 batch PCKh 0.875\n",
      "Trained batch 1449 batch loss 0.492544889 batch mAP 0.639862061 batch PCKh 0.75\n",
      "Trained batch 1450 batch loss 0.454342782 batch mAP 0.673492432 batch PCKh 0.3125\n",
      "Trained batch 1451 batch loss 0.424763918 batch mAP 0.688293457 batch PCKh 0.375\n",
      "Trained batch 1452 batch loss 0.432778955 batch mAP 0.651672363 batch PCKh 0.25\n",
      "Trained batch 1453 batch loss 0.426703721 batch mAP 0.67288208 batch PCKh 0.3125\n",
      "Trained batch 1454 batch loss 0.556989789 batch mAP 0.64944458 batch PCKh 0.375\n",
      "Trained batch 1455 batch loss 0.508566797 batch mAP 0.687896729 batch PCKh 0.625\n",
      "Trained batch 1456 batch loss 0.508923471 batch mAP 0.597686768 batch PCKh 0.125\n",
      "Trained batch 1457 batch loss 0.493916035 batch mAP 0.601104736 batch PCKh 0.125\n",
      "Trained batch 1458 batch loss 0.636509418 batch mAP 0.542877197 batch PCKh 0.875\n",
      "Trained batch 1459 batch loss 0.625658095 batch mAP 0.547454834 batch PCKh 0.5625\n",
      "Trained batch 1460 batch loss 0.677791834 batch mAP 0.500946045 batch PCKh 0\n",
      "Trained batch 1461 batch loss 0.677109659 batch mAP 0.557556152 batch PCKh 0.5\n",
      "Trained batch 1462 batch loss 0.613325834 batch mAP 0.621521 batch PCKh 0.5\n",
      "Trained batch 1463 batch loss 0.456848294 batch mAP 0.662353516 batch PCKh 0.3125\n",
      "Trained batch 1464 batch loss 0.606724739 batch mAP 0.604309082 batch PCKh 0.1875\n",
      "Trained batch 1465 batch loss 0.643008292 batch mAP 0.535339355 batch PCKh 0.0625\n",
      "Trained batch 1466 batch loss 0.583182216 batch mAP 0.515045166 batch PCKh 0.25\n",
      "Trained batch 1467 batch loss 0.511605561 batch mAP 0.477020264 batch PCKh 0.4375\n",
      "Trained batch 1468 batch loss 0.585403323 batch mAP 0.468139648 batch PCKh 0.1875\n",
      "Trained batch 1469 batch loss 0.515206218 batch mAP 0.498931885 batch PCKh 0.5\n",
      "Trained batch 1470 batch loss 0.558813 batch mAP 0.49520874 batch PCKh 0.125\n",
      "Trained batch 1471 batch loss 0.635852635 batch mAP 0.509979248 batch PCKh 0.0625\n",
      "Trained batch 1472 batch loss 0.634075761 batch mAP 0.56842041 batch PCKh 0.5\n",
      "Trained batch 1473 batch loss 0.479112089 batch mAP 0.603118896 batch PCKh 0.75\n",
      "Trained batch 1474 batch loss 0.449845076 batch mAP 0.566589355 batch PCKh 0.8125\n",
      "Trained batch 1475 batch loss 0.531249106 batch mAP 0.57434082 batch PCKh 0.75\n",
      "Trained batch 1476 batch loss 0.473980725 batch mAP 0.635864258 batch PCKh 0.75\n",
      "Trained batch 1477 batch loss 0.639611483 batch mAP 0.573883057 batch PCKh 0.625\n",
      "Trained batch 1478 batch loss 0.533234239 batch mAP 0.620605469 batch PCKh 0.5625\n",
      "Trained batch 1479 batch loss 0.438711494 batch mAP 0.642425537 batch PCKh 0.375\n",
      "Trained batch 1480 batch loss 0.520473599 batch mAP 0.58807373 batch PCKh 0.3125\n",
      "Trained batch 1481 batch loss 0.49396503 batch mAP 0.618743896 batch PCKh 0.6875\n",
      "Trained batch 1482 batch loss 0.535339236 batch mAP 0.618682861 batch PCKh 0.5625\n",
      "Trained batch 1483 batch loss 0.527804613 batch mAP 0.557739258 batch PCKh 0.5625\n",
      "Trained batch 1484 batch loss 0.508606076 batch mAP 0.539917 batch PCKh 0.1875\n",
      "Trained batch 1485 batch loss 0.648209691 batch mAP 0.581085205 batch PCKh 0.5\n",
      "Trained batch 1486 batch loss 0.568369627 batch mAP 0.610351562 batch PCKh 0\n",
      "Trained batch 1487 batch loss 0.532128811 batch mAP 0.642486572 batch PCKh 0.25\n",
      "Trained batch 1488 batch loss 0.589915872 batch mAP 0.606689453 batch PCKh 0\n",
      "Trained batch 1489 batch loss 0.637154222 batch mAP 0.503143311 batch PCKh 0.1875\n",
      "Trained batch 1490 batch loss 0.581853271 batch mAP 0.549987793 batch PCKh 0.625\n",
      "Trained batch 1491 batch loss 0.698665142 batch mAP 0.489501953 batch PCKh 0.25\n",
      "Trained batch 1492 batch loss 0.557441235 batch mAP 0.580383301 batch PCKh 0.625\n",
      "Trained batch 1493 batch loss 0.536079168 batch mAP 0.508514404 batch PCKh 0.75\n",
      "Trained batch 1494 batch loss 0.540975511 batch mAP 0.448059082 batch PCKh 0.75\n",
      "Trained batch 1495 batch loss 0.579915404 batch mAP 0.454040527 batch PCKh 0.75\n",
      "Trained batch 1496 batch loss 0.591908634 batch mAP 0.465301514 batch PCKh 0.6875\n",
      "Trained batch 1497 batch loss 0.634418547 batch mAP 0.412261963 batch PCKh 0.5\n",
      "Trained batch 1498 batch loss 0.568524 batch mAP 0.449768066 batch PCKh 0.875\n",
      "Trained batch 1499 batch loss 0.576370239 batch mAP 0.494812 batch PCKh 0.625\n",
      "Trained batch 1500 batch loss 0.531341076 batch mAP 0.539123535 batch PCKh 0.875\n",
      "Trained batch 1501 batch loss 0.510407805 batch mAP 0.531188965 batch PCKh 0.6875\n",
      "Trained batch 1502 batch loss 0.583758652 batch mAP 0.578979492 batch PCKh 0.6875\n",
      "Trained batch 1503 batch loss 0.543975294 batch mAP 0.590179443 batch PCKh 0.625\n",
      "Trained batch 1504 batch loss 0.504382491 batch mAP 0.617095947 batch PCKh 0.5625\n",
      "Trained batch 1505 batch loss 0.500974059 batch mAP 0.662689209 batch PCKh 0.3125\n",
      "Trained batch 1506 batch loss 0.469922602 batch mAP 0.676239 batch PCKh 0.5\n",
      "Trained batch 1507 batch loss 0.489664584 batch mAP 0.669342041 batch PCKh 0.25\n",
      "Trained batch 1508 batch loss 0.637718558 batch mAP 0.565612793 batch PCKh 0.5625\n",
      "Trained batch 1509 batch loss 0.534408569 batch mAP 0.624572754 batch PCKh 0.4375\n",
      "Trained batch 1510 batch loss 0.540692747 batch mAP 0.692474365 batch PCKh 0.5\n",
      "Trained batch 1511 batch loss 0.569951713 batch mAP 0.621032715 batch PCKh 0.3125\n",
      "Trained batch 1512 batch loss 0.530441701 batch mAP 0.589050293 batch PCKh 0.6875\n",
      "Trained batch 1513 batch loss 0.468850762 batch mAP 0.520263672 batch PCKh 0.625\n",
      "Trained batch 1514 batch loss 0.498559415 batch mAP 0.518585205 batch PCKh 0.5625\n",
      "Trained batch 1515 batch loss 0.491556227 batch mAP 0.579864502 batch PCKh 0.6875\n",
      "Trained batch 1516 batch loss 0.598395705 batch mAP 0.564758301 batch PCKh 0.375\n",
      "Trained batch 1517 batch loss 0.465502143 batch mAP 0.608612061 batch PCKh 0.625\n",
      "Trained batch 1518 batch loss 0.446714878 batch mAP 0.614868164 batch PCKh 0.75\n",
      "Trained batch 1519 batch loss 0.551679552 batch mAP 0.635528564 batch PCKh 0.5\n",
      "Trained batch 1520 batch loss 0.500472546 batch mAP 0.634063721 batch PCKh 0.25\n",
      "Trained batch 1521 batch loss 0.549956381 batch mAP 0.596130371 batch PCKh 0.5\n",
      "Trained batch 1522 batch loss 0.581025 batch mAP 0.630767822 batch PCKh 0.75\n",
      "Trained batch 1523 batch loss 0.591600537 batch mAP 0.579681396 batch PCKh 0.625\n",
      "Trained batch 1524 batch loss 0.633675933 batch mAP 0.559417725 batch PCKh 0.875\n",
      "Trained batch 1525 batch loss 0.650298893 batch mAP 0.5546875 batch PCKh 0.1875\n",
      "Trained batch 1526 batch loss 0.575672805 batch mAP 0.530975342 batch PCKh 0.8125\n",
      "Trained batch 1527 batch loss 0.565481305 batch mAP 0.545196533 batch PCKh 0.625\n",
      "Trained batch 1528 batch loss 0.579192877 batch mAP 0.520568848 batch PCKh 0.625\n",
      "Trained batch 1529 batch loss 0.539965153 batch mAP 0.515655518 batch PCKh 0.5\n",
      "Trained batch 1530 batch loss 0.621315777 batch mAP 0.50958252 batch PCKh 0.875\n",
      "Trained batch 1531 batch loss 0.66522038 batch mAP 0.453826904 batch PCKh 0.875\n",
      "Trained batch 1532 batch loss 0.513121843 batch mAP 0.564880371 batch PCKh 0.4375\n",
      "Trained batch 1533 batch loss 0.615027487 batch mAP 0.429046631 batch PCKh 0.75\n",
      "Trained batch 1534 batch loss 0.575909734 batch mAP 0.466217041 batch PCKh 0.8125\n",
      "Trained batch 1535 batch loss 0.576265931 batch mAP 0.488922119 batch PCKh 0.4375\n",
      "Trained batch 1536 batch loss 0.549965262 batch mAP 0.564788818 batch PCKh 0.4375\n",
      "Trained batch 1537 batch loss 0.566856265 batch mAP 0.515563965 batch PCKh 0.5\n",
      "Trained batch 1538 batch loss 0.601320863 batch mAP 0.515441895 batch PCKh 0.5625\n",
      "Trained batch 1539 batch loss 0.526747584 batch mAP 0.51159668 batch PCKh 0.1875\n",
      "Trained batch 1540 batch loss 0.544842422 batch mAP 0.546813965 batch PCKh 0.4375\n",
      "Trained batch 1541 batch loss 0.612846732 batch mAP 0.571807861 batch PCKh 0.8125\n",
      "Trained batch 1542 batch loss 0.577441216 batch mAP 0.557312 batch PCKh 0.625\n",
      "Trained batch 1543 batch loss 0.625371754 batch mAP 0.555786133 batch PCKh 0.1875\n",
      "Trained batch 1544 batch loss 0.635929227 batch mAP 0.573516846 batch PCKh 0.25\n",
      "Trained batch 1545 batch loss 0.634259343 batch mAP 0.542511 batch PCKh 0.375\n",
      "Trained batch 1546 batch loss 0.558711886 batch mAP 0.603088379 batch PCKh 0.75\n",
      "Trained batch 1547 batch loss 0.602178335 batch mAP 0.550811768 batch PCKh 0.875\n",
      "Trained batch 1548 batch loss 0.546766639 batch mAP 0.608551 batch PCKh 0.3125\n",
      "Trained batch 1549 batch loss 0.528238595 batch mAP 0.643188477 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1550 batch loss 0.409447461 batch mAP 0.657318115 batch PCKh 0.375\n",
      "Trained batch 1551 batch loss 0.433878034 batch mAP 0.681732178 batch PCKh 0.375\n",
      "Trained batch 1552 batch loss 0.437063336 batch mAP 0.697265625 batch PCKh 0.25\n",
      "Trained batch 1553 batch loss 0.505952358 batch mAP 0.645050049 batch PCKh 0.3125\n",
      "Trained batch 1554 batch loss 0.474682957 batch mAP 0.667541504 batch PCKh 0.125\n",
      "Trained batch 1555 batch loss 0.502495706 batch mAP 0.658538818 batch PCKh 0.5\n",
      "Trained batch 1556 batch loss 0.504987717 batch mAP 0.663909912 batch PCKh 0.1875\n",
      "Trained batch 1557 batch loss 0.552476645 batch mAP 0.583679199 batch PCKh 0.5\n",
      "Trained batch 1558 batch loss 0.595852435 batch mAP 0.494232178 batch PCKh 0.25\n",
      "Trained batch 1559 batch loss 0.601427078 batch mAP 0.469635 batch PCKh 0.5625\n",
      "Trained batch 1560 batch loss 0.621972322 batch mAP 0.530853271 batch PCKh 0.25\n",
      "Trained batch 1561 batch loss 0.571985 batch mAP 0.556488037 batch PCKh 0.4375\n",
      "Trained batch 1562 batch loss 0.573354483 batch mAP 0.531585693 batch PCKh 0.8125\n",
      "Trained batch 1563 batch loss 0.622913837 batch mAP 0.561004639 batch PCKh 0.5625\n",
      "Trained batch 1564 batch loss 0.585639179 batch mAP 0.517852783 batch PCKh 0.75\n",
      "Trained batch 1565 batch loss 0.539302528 batch mAP 0.60534668 batch PCKh 0.875\n",
      "Trained batch 1566 batch loss 0.587464392 batch mAP 0.609985352 batch PCKh 0.6875\n",
      "Trained batch 1567 batch loss 0.575866282 batch mAP 0.590118408 batch PCKh 0.875\n",
      "Trained batch 1568 batch loss 0.590230703 batch mAP 0.598876953 batch PCKh 0.625\n",
      "Trained batch 1569 batch loss 0.588346183 batch mAP 0.627044678 batch PCKh 0.375\n",
      "Trained batch 1570 batch loss 0.569356143 batch mAP 0.582641602 batch PCKh 0.0625\n",
      "Trained batch 1571 batch loss 0.536963 batch mAP 0.603607178 batch PCKh 0.5625\n",
      "Trained batch 1572 batch loss 0.626356661 batch mAP 0.579467773 batch PCKh 0.0625\n",
      "Trained batch 1573 batch loss 0.60458833 batch mAP 0.628997803 batch PCKh 0.375\n",
      "Trained batch 1574 batch loss 0.597451091 batch mAP 0.602233887 batch PCKh 0.3125\n",
      "Trained batch 1575 batch loss 0.50211823 batch mAP 0.623779297 batch PCKh 0.875\n",
      "Trained batch 1576 batch loss 0.593383253 batch mAP 0.578826904 batch PCKh 0.5625\n",
      "Trained batch 1577 batch loss 0.495670319 batch mAP 0.59185791 batch PCKh 0.6875\n",
      "Trained batch 1578 batch loss 0.45986408 batch mAP 0.612609863 batch PCKh 0.6875\n",
      "Trained batch 1579 batch loss 0.578698099 batch mAP 0.589019775 batch PCKh 0.5625\n",
      "Trained batch 1580 batch loss 0.563759267 batch mAP 0.641052246 batch PCKh 0.375\n",
      "Trained batch 1581 batch loss 0.646176577 batch mAP 0.563751221 batch PCKh 0.375\n",
      "Trained batch 1582 batch loss 0.673021793 batch mAP 0.554931641 batch PCKh 0.1875\n",
      "Trained batch 1583 batch loss 0.637849391 batch mAP 0.528839111 batch PCKh 0.1875\n",
      "Trained batch 1584 batch loss 0.62115258 batch mAP 0.573303223 batch PCKh 0.3125\n",
      "Trained batch 1585 batch loss 0.556188822 batch mAP 0.64654541 batch PCKh 0.375\n",
      "Trained batch 1586 batch loss 0.520046294 batch mAP 0.669464111 batch PCKh 0.3125\n",
      "Trained batch 1587 batch loss 0.564452112 batch mAP 0.636077881 batch PCKh 0.25\n",
      "Trained batch 1588 batch loss 0.554547906 batch mAP 0.645111084 batch PCKh 0.3125\n",
      "Trained batch 1589 batch loss 0.514430463 batch mAP 0.660064697 batch PCKh 0.5625\n",
      "Trained batch 1590 batch loss 0.587565184 batch mAP 0.614624 batch PCKh 0.5\n",
      "Trained batch 1591 batch loss 0.619484901 batch mAP 0.635314941 batch PCKh 0.25\n",
      "Trained batch 1592 batch loss 0.472647071 batch mAP 0.635681152 batch PCKh 0.0625\n",
      "Trained batch 1593 batch loss 0.555028737 batch mAP 0.585601807 batch PCKh 0.8125\n",
      "Trained batch 1594 batch loss 0.618273258 batch mAP 0.544342041 batch PCKh 0.875\n",
      "Trained batch 1595 batch loss 0.681877434 batch mAP 0.503356934 batch PCKh 0.5\n",
      "Trained batch 1596 batch loss 0.611344099 batch mAP 0.472229 batch PCKh 0.8125\n",
      "Trained batch 1597 batch loss 0.587958932 batch mAP 0.448913574 batch PCKh 0.75\n",
      "Trained batch 1598 batch loss 0.603607237 batch mAP 0.473266602 batch PCKh 0.125\n",
      "Trained batch 1599 batch loss 0.625513792 batch mAP 0.506561279 batch PCKh 0.375\n",
      "Trained batch 1600 batch loss 0.594274402 batch mAP 0.51348877 batch PCKh 0.75\n",
      "Trained batch 1601 batch loss 0.476645052 batch mAP 0.533874512 batch PCKh 0.5\n",
      "Trained batch 1602 batch loss 0.456510246 batch mAP 0.485870361 batch PCKh 0.3125\n",
      "Trained batch 1603 batch loss 0.451856822 batch mAP 0.421173096 batch PCKh 0\n",
      "Trained batch 1604 batch loss 0.446168303 batch mAP 0.436462402 batch PCKh 0.3125\n",
      "Trained batch 1605 batch loss 0.507398605 batch mAP 0.496185303 batch PCKh 0.5\n",
      "Trained batch 1606 batch loss 0.541950941 batch mAP 0.597625732 batch PCKh 0.5\n",
      "Trained batch 1607 batch loss 0.627623498 batch mAP 0.582763672 batch PCKh 0.6875\n",
      "Trained batch 1608 batch loss 0.572618961 batch mAP 0.619751 batch PCKh 0.75\n",
      "Trained batch 1609 batch loss 0.521112144 batch mAP 0.62878418 batch PCKh 0.5625\n",
      "Trained batch 1610 batch loss 0.563465238 batch mAP 0.578521729 batch PCKh 0.75\n",
      "Trained batch 1611 batch loss 0.598294616 batch mAP 0.600952148 batch PCKh 0.6875\n",
      "Trained batch 1612 batch loss 0.507183373 batch mAP 0.627105713 batch PCKh 0.3125\n",
      "Trained batch 1613 batch loss 0.627586603 batch mAP 0.561828613 batch PCKh 0.8125\n",
      "Trained batch 1614 batch loss 0.572456956 batch mAP 0.611358643 batch PCKh 0.875\n",
      "Trained batch 1615 batch loss 0.562529802 batch mAP 0.581329346 batch PCKh 0.8125\n",
      "Trained batch 1616 batch loss 0.572622478 batch mAP 0.61618042 batch PCKh 0.4375\n",
      "Trained batch 1617 batch loss 0.566864908 batch mAP 0.529510498 batch PCKh 0.875\n",
      "Trained batch 1618 batch loss 0.564772248 batch mAP 0.515106201 batch PCKh 0.875\n",
      "Trained batch 1619 batch loss 0.568547487 batch mAP 0.600463867 batch PCKh 0.6875\n",
      "Trained batch 1620 batch loss 0.551423907 batch mAP 0.512176514 batch PCKh 0.75\n",
      "Trained batch 1621 batch loss 0.48581323 batch mAP 0.582275391 batch PCKh 0.5\n",
      "Trained batch 1622 batch loss 0.535129547 batch mAP 0.568267822 batch PCKh 0.5625\n",
      "Trained batch 1623 batch loss 0.473238021 batch mAP 0.54486084 batch PCKh 0.75\n",
      "Trained batch 1624 batch loss 0.492383301 batch mAP 0.561828613 batch PCKh 0.375\n",
      "Trained batch 1625 batch loss 0.560878873 batch mAP 0.533447266 batch PCKh 0.75\n",
      "Trained batch 1626 batch loss 0.578088582 batch mAP 0.518035889 batch PCKh 0.625\n",
      "Trained batch 1627 batch loss 0.52728343 batch mAP 0.535125732 batch PCKh 0.75\n",
      "Trained batch 1628 batch loss 0.578662634 batch mAP 0.587127686 batch PCKh 0.8125\n",
      "Trained batch 1629 batch loss 0.680551171 batch mAP 0.549621582 batch PCKh 0.3125\n",
      "Trained batch 1630 batch loss 0.652293146 batch mAP 0.517883301 batch PCKh 0.625\n",
      "Trained batch 1631 batch loss 0.547965467 batch mAP 0.568939209 batch PCKh 0.1875\n",
      "Trained batch 1632 batch loss 0.513406157 batch mAP 0.593933105 batch PCKh 0.125\n",
      "Trained batch 1633 batch loss 0.559929371 batch mAP 0.614471436 batch PCKh 0.5625\n",
      "Trained batch 1634 batch loss 0.544911444 batch mAP 0.564941406 batch PCKh 0.75\n",
      "Trained batch 1635 batch loss 0.569464922 batch mAP 0.589019775 batch PCKh 0.5\n",
      "Trained batch 1636 batch loss 0.502603889 batch mAP 0.617645264 batch PCKh 0.5\n",
      "Trained batch 1637 batch loss 0.593519926 batch mAP 0.562194824 batch PCKh 0.75\n",
      "Trained batch 1638 batch loss 0.49924776 batch mAP 0.589019775 batch PCKh 0.6875\n",
      "Trained batch 1639 batch loss 0.482214868 batch mAP 0.587615967 batch PCKh 0.125\n",
      "Trained batch 1640 batch loss 0.431020737 batch mAP 0.558807373 batch PCKh 0.5\n",
      "Trained batch 1641 batch loss 0.487849474 batch mAP 0.567260742 batch PCKh 0.625\n",
      "Trained batch 1642 batch loss 0.624405742 batch mAP 0.547027588 batch PCKh 0.4375\n",
      "Trained batch 1643 batch loss 0.590428054 batch mAP 0.503601074 batch PCKh 0.875\n",
      "Trained batch 1644 batch loss 0.618473351 batch mAP 0.483551025 batch PCKh 0.375\n",
      "Trained batch 1645 batch loss 0.585114479 batch mAP 0.513793945 batch PCKh 0\n",
      "Trained batch 1646 batch loss 0.588392 batch mAP 0.52331543 batch PCKh 0.3125\n",
      "Trained batch 1647 batch loss 0.614536583 batch mAP 0.531066895 batch PCKh 0.25\n",
      "Trained batch 1648 batch loss 0.61068964 batch mAP 0.517730713 batch PCKh 0.3125\n",
      "Trained batch 1649 batch loss 0.554351807 batch mAP 0.516662598 batch PCKh 0.3125\n",
      "Trained batch 1650 batch loss 0.62629348 batch mAP 0.525604248 batch PCKh 0.5\n",
      "Trained batch 1651 batch loss 0.632160366 batch mAP 0.525238037 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1652 batch loss 0.509629846 batch mAP 0.604370117 batch PCKh 0.125\n",
      "Trained batch 1653 batch loss 0.504171 batch mAP 0.561553955 batch PCKh 0.5\n",
      "Trained batch 1654 batch loss 0.647213519 batch mAP 0.503997803 batch PCKh 0.875\n",
      "Trained batch 1655 batch loss 0.49426043 batch mAP 0.611938477 batch PCKh 0.75\n",
      "Trained batch 1656 batch loss 0.524478495 batch mAP 0.586456299 batch PCKh 0.375\n",
      "Trained batch 1657 batch loss 0.518084288 batch mAP 0.578094482 batch PCKh 0.375\n",
      "Trained batch 1658 batch loss 0.504944265 batch mAP 0.508850098 batch PCKh 0.25\n",
      "Trained batch 1659 batch loss 0.542573631 batch mAP 0.554321289 batch PCKh 0.75\n",
      "Trained batch 1660 batch loss 0.525566697 batch mAP 0.522003174 batch PCKh 0.3125\n",
      "Trained batch 1661 batch loss 0.488691568 batch mAP 0.507110596 batch PCKh 0.875\n",
      "Trained batch 1662 batch loss 0.594579339 batch mAP 0.498443604 batch PCKh 0.1875\n",
      "Trained batch 1663 batch loss 0.484026879 batch mAP 0.541900635 batch PCKh 0.4375\n",
      "Trained batch 1664 batch loss 0.639213 batch mAP 0.552429199 batch PCKh 0.6875\n",
      "Trained batch 1665 batch loss 0.608106 batch mAP 0.53225708 batch PCKh 0.5625\n",
      "Trained batch 1666 batch loss 0.535089254 batch mAP 0.66506958 batch PCKh 0.3125\n",
      "Trained batch 1667 batch loss 0.545873761 batch mAP 0.58303833 batch PCKh 0.25\n",
      "Trained batch 1668 batch loss 0.534965217 batch mAP 0.624023438 batch PCKh 0.3125\n",
      "Trained batch 1669 batch loss 0.529704273 batch mAP 0.612457275 batch PCKh 0.5625\n",
      "Trained batch 1670 batch loss 0.587196708 batch mAP 0.637054443 batch PCKh 0.4375\n",
      "Trained batch 1671 batch loss 0.520602226 batch mAP 0.592956543 batch PCKh 0.4375\n",
      "Trained batch 1672 batch loss 0.477397084 batch mAP 0.627502441 batch PCKh 0.625\n",
      "Trained batch 1673 batch loss 0.50065726 batch mAP 0.625549316 batch PCKh 0.625\n",
      "Trained batch 1674 batch loss 0.547900379 batch mAP 0.628509521 batch PCKh 0.6875\n",
      "Trained batch 1675 batch loss 0.510832191 batch mAP 0.621337891 batch PCKh 0.5\n",
      "Trained batch 1676 batch loss 0.577921271 batch mAP 0.569000244 batch PCKh 0.625\n",
      "Trained batch 1677 batch loss 0.565625608 batch mAP 0.516571045 batch PCKh 0.75\n",
      "Trained batch 1678 batch loss 0.511754811 batch mAP 0.550598145 batch PCKh 0.6875\n",
      "Trained batch 1679 batch loss 0.424500942 batch mAP 0.540313721 batch PCKh 0.625\n",
      "Trained batch 1680 batch loss 0.501943707 batch mAP 0.511688232 batch PCKh 0.625\n",
      "Trained batch 1681 batch loss 0.579957485 batch mAP 0.553527832 batch PCKh 0.875\n",
      "Trained batch 1682 batch loss 0.604510427 batch mAP 0.584198 batch PCKh 0.75\n",
      "Trained batch 1683 batch loss 0.531996191 batch mAP 0.613861084 batch PCKh 0.375\n",
      "Trained batch 1684 batch loss 0.530661643 batch mAP 0.680297852 batch PCKh 0.25\n",
      "Trained batch 1685 batch loss 0.610236466 batch mAP 0.625732422 batch PCKh 0\n",
      "Trained batch 1686 batch loss 0.545912504 batch mAP 0.636077881 batch PCKh 0.5\n",
      "Trained batch 1687 batch loss 0.52328366 batch mAP 0.674926758 batch PCKh 0.6875\n",
      "Trained batch 1688 batch loss 0.501288056 batch mAP 0.660491943 batch PCKh 0.3125\n",
      "Trained batch 1689 batch loss 0.493255764 batch mAP 0.691467285 batch PCKh 0.3125\n",
      "Trained batch 1690 batch loss 0.464981019 batch mAP 0.700683594 batch PCKh 0.25\n",
      "Trained batch 1691 batch loss 0.446393 batch mAP 0.672119141 batch PCKh 0.3125\n",
      "Trained batch 1692 batch loss 0.412364662 batch mAP 0.676208496 batch PCKh 0.375\n",
      "Trained batch 1693 batch loss 0.497435451 batch mAP 0.602600098 batch PCKh 0.625\n",
      "Trained batch 1694 batch loss 0.494211733 batch mAP 0.600219727 batch PCKh 0.6875\n",
      "Trained batch 1695 batch loss 0.50304234 batch mAP 0.602203369 batch PCKh 0.5\n",
      "Trained batch 1696 batch loss 0.522679 batch mAP 0.618286133 batch PCKh 0.5625\n",
      "Trained batch 1697 batch loss 0.495516568 batch mAP 0.637664795 batch PCKh 0.6875\n",
      "Trained batch 1698 batch loss 0.535777092 batch mAP 0.565307617 batch PCKh 0.25\n",
      "Trained batch 1699 batch loss 0.543400526 batch mAP 0.639160156 batch PCKh 0.3125\n",
      "Trained batch 1700 batch loss 0.530480742 batch mAP 0.653076172 batch PCKh 0.625\n",
      "Trained batch 1701 batch loss 0.546167254 batch mAP 0.654876709 batch PCKh 0.3125\n",
      "Trained batch 1702 batch loss 0.527048111 batch mAP 0.649017334 batch PCKh 0.6875\n",
      "Trained batch 1703 batch loss 0.599334896 batch mAP 0.583374 batch PCKh 0.8125\n",
      "Trained batch 1704 batch loss 0.56677866 batch mAP 0.549407959 batch PCKh 0.5\n",
      "Trained batch 1705 batch loss 0.582407117 batch mAP 0.571044922 batch PCKh 0\n",
      "Trained batch 1706 batch loss 0.611112297 batch mAP 0.548584 batch PCKh 0\n",
      "Trained batch 1707 batch loss 0.612826884 batch mAP 0.572998047 batch PCKh 0.625\n",
      "Trained batch 1708 batch loss 0.586101949 batch mAP 0.611328125 batch PCKh 0.8125\n",
      "Trained batch 1709 batch loss 0.585011959 batch mAP 0.591278076 batch PCKh 0.625\n",
      "Trained batch 1710 batch loss 0.626143456 batch mAP 0.572906494 batch PCKh 0.375\n",
      "Trained batch 1711 batch loss 0.57539624 batch mAP 0.571746826 batch PCKh 0.4375\n",
      "Trained batch 1712 batch loss 0.528259635 batch mAP 0.596466064 batch PCKh 0.3125\n",
      "Trained batch 1713 batch loss 0.540462613 batch mAP 0.657104492 batch PCKh 0.875\n",
      "Trained batch 1714 batch loss 0.56874907 batch mAP 0.574981689 batch PCKh 0.3125\n",
      "Trained batch 1715 batch loss 0.521630466 batch mAP 0.577758789 batch PCKh 0.75\n",
      "Trained batch 1716 batch loss 0.494224936 batch mAP 0.545654297 batch PCKh 0.5625\n",
      "Trained batch 1717 batch loss 0.48442325 batch mAP 0.601837158 batch PCKh 0.75\n",
      "Trained batch 1718 batch loss 0.473304063 batch mAP 0.547637939 batch PCKh 0.75\n",
      "Trained batch 1719 batch loss 0.566191375 batch mAP 0.522033691 batch PCKh 0.6875\n",
      "Trained batch 1720 batch loss 0.518774152 batch mAP 0.522705078 batch PCKh 0.3125\n",
      "Trained batch 1721 batch loss 0.556023955 batch mAP 0.558197 batch PCKh 0.6875\n",
      "Trained batch 1722 batch loss 0.575215876 batch mAP 0.464172363 batch PCKh 0.75\n",
      "Trained batch 1723 batch loss 0.62812525 batch mAP 0.533996582 batch PCKh 0.75\n",
      "Trained batch 1724 batch loss 0.593100846 batch mAP 0.552948 batch PCKh 0.6875\n",
      "Trained batch 1725 batch loss 0.561004817 batch mAP 0.570037842 batch PCKh 0.5\n",
      "Trained batch 1726 batch loss 0.589005053 batch mAP 0.569061279 batch PCKh 0.75\n",
      "Trained batch 1727 batch loss 0.552598476 batch mAP 0.571472168 batch PCKh 0.4375\n",
      "Trained batch 1728 batch loss 0.564434409 batch mAP 0.588134766 batch PCKh 0.625\n",
      "Trained batch 1729 batch loss 0.54150784 batch mAP 0.556091309 batch PCKh 0.1875\n",
      "Trained batch 1730 batch loss 0.504954 batch mAP 0.566986084 batch PCKh 0.125\n",
      "Trained batch 1731 batch loss 0.607789397 batch mAP 0.610443115 batch PCKh 0.4375\n",
      "Trained batch 1732 batch loss 0.529478252 batch mAP 0.614074707 batch PCKh 0.5625\n",
      "Trained batch 1733 batch loss 0.462467134 batch mAP 0.624389648 batch PCKh 0.4375\n",
      "Trained batch 1734 batch loss 0.431854546 batch mAP 0.575744629 batch PCKh 0.375\n",
      "Trained batch 1735 batch loss 0.481318831 batch mAP 0.707733154 batch PCKh 0.75\n",
      "Trained batch 1736 batch loss 0.480357 batch mAP 0.723999 batch PCKh 0.5\n",
      "Trained batch 1737 batch loss 0.544379711 batch mAP 0.696136475 batch PCKh 0.4375\n",
      "Trained batch 1738 batch loss 0.540694 batch mAP 0.68649292 batch PCKh 0.5\n",
      "Trained batch 1739 batch loss 0.451767176 batch mAP 0.715332031 batch PCKh 0.5625\n",
      "Trained batch 1740 batch loss 0.448605597 batch mAP 0.653564453 batch PCKh 0.25\n",
      "Trained batch 1741 batch loss 0.456709266 batch mAP 0.610778809 batch PCKh 0.1875\n",
      "Trained batch 1742 batch loss 0.47986275 batch mAP 0.616424561 batch PCKh 0.6875\n",
      "Trained batch 1743 batch loss 0.448507696 batch mAP 0.610199 batch PCKh 0\n",
      "Trained batch 1744 batch loss 0.496405959 batch mAP 0.629638672 batch PCKh 0.25\n",
      "Trained batch 1745 batch loss 0.596799135 batch mAP 0.645782471 batch PCKh 0.3125\n",
      "Trained batch 1746 batch loss 0.550327778 batch mAP 0.602996826 batch PCKh 0.4375\n",
      "Trained batch 1747 batch loss 0.618078113 batch mAP 0.60672 batch PCKh 0.375\n",
      "Trained batch 1748 batch loss 0.57077682 batch mAP 0.594848633 batch PCKh 0.25\n",
      "Trained batch 1749 batch loss 0.560344398 batch mAP 0.632873535 batch PCKh 0.1875\n",
      "Trained batch 1750 batch loss 0.559371352 batch mAP 0.62701416 batch PCKh 0.3125\n",
      "Trained batch 1751 batch loss 0.601163745 batch mAP 0.547912598 batch PCKh 0.125\n",
      "Trained batch 1752 batch loss 0.614772141 batch mAP 0.487243652 batch PCKh 0.375\n",
      "Trained batch 1753 batch loss 0.662907124 batch mAP 0.376434326 batch PCKh 0.0625\n",
      "Trained batch 1754 batch loss 0.678907037 batch mAP 0.424438477 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1755 batch loss 0.516170144 batch mAP 0.39855957 batch PCKh 0.375\n",
      "Trained batch 1756 batch loss 0.519193828 batch mAP 0.362701416 batch PCKh 0.3125\n",
      "Trained batch 1757 batch loss 0.563949823 batch mAP 0.464050293 batch PCKh 0.4375\n",
      "Trained batch 1758 batch loss 0.487217486 batch mAP 0.526245117 batch PCKh 0.1875\n",
      "Trained batch 1759 batch loss 0.551680565 batch mAP 0.56829834 batch PCKh 0.5\n",
      "Trained batch 1760 batch loss 0.508922935 batch mAP 0.588012695 batch PCKh 0\n",
      "Trained batch 1761 batch loss 0.585256457 batch mAP 0.577484131 batch PCKh 0.1875\n",
      "Trained batch 1762 batch loss 0.555142 batch mAP 0.590820312 batch PCKh 0.75\n",
      "Trained batch 1763 batch loss 0.557414591 batch mAP 0.623199463 batch PCKh 0.3125\n",
      "Trained batch 1764 batch loss 0.607016444 batch mAP 0.590423584 batch PCKh 0.125\n",
      "Trained batch 1765 batch loss 0.465193748 batch mAP 0.601470947 batch PCKh 0.1875\n",
      "Trained batch 1766 batch loss 0.42482245 batch mAP 0.585723877 batch PCKh 0\n",
      "Trained batch 1767 batch loss 0.372040629 batch mAP 0.511871338 batch PCKh 0.0625\n",
      "Trained batch 1768 batch loss 0.400776088 batch mAP 0.520782471 batch PCKh 0\n",
      "Trained batch 1769 batch loss 0.418570399 batch mAP 0.592956543 batch PCKh 0.5\n",
      "Trained batch 1770 batch loss 0.525668621 batch mAP 0.639129639 batch PCKh 0.75\n",
      "Trained batch 1771 batch loss 0.58116591 batch mAP 0.654998779 batch PCKh 0.5625\n",
      "Trained batch 1772 batch loss 0.588471234 batch mAP 0.648223877 batch PCKh 0.8125\n",
      "Trained batch 1773 batch loss 0.656171918 batch mAP 0.620788574 batch PCKh 0.6875\n",
      "Trained batch 1774 batch loss 0.605377436 batch mAP 0.644378662 batch PCKh 0.625\n",
      "Trained batch 1775 batch loss 0.589668751 batch mAP 0.604064941 batch PCKh 0.3125\n",
      "Trained batch 1776 batch loss 0.631113887 batch mAP 0.52520752 batch PCKh 0.375\n",
      "Trained batch 1777 batch loss 0.62546438 batch mAP 0.566772461 batch PCKh 0.3125\n",
      "Trained batch 1778 batch loss 0.6108042 batch mAP 0.602294922 batch PCKh 0.3125\n",
      "Trained batch 1779 batch loss 0.555693 batch mAP 0.516601562 batch PCKh 0.75\n",
      "Trained batch 1780 batch loss 0.577190697 batch mAP 0.570495605 batch PCKh 0.75\n",
      "Trained batch 1781 batch loss 0.593893111 batch mAP 0.542755127 batch PCKh 0.375\n",
      "Trained batch 1782 batch loss 0.561157703 batch mAP 0.585723877 batch PCKh 0.75\n",
      "Trained batch 1783 batch loss 0.504680932 batch mAP 0.550354 batch PCKh 0.625\n",
      "Trained batch 1784 batch loss 0.417432308 batch mAP 0.559967041 batch PCKh 0\n",
      "Trained batch 1785 batch loss 0.391465455 batch mAP 0.546508789 batch PCKh 0.5625\n",
      "Trained batch 1786 batch loss 0.478115529 batch mAP 0.540008545 batch PCKh 0.75\n",
      "Trained batch 1787 batch loss 0.391411662 batch mAP 0.511749268 batch PCKh 0.625\n",
      "Trained batch 1788 batch loss 0.441535681 batch mAP 0.518951416 batch PCKh 0.6875\n",
      "Trained batch 1789 batch loss 0.45969829 batch mAP 0.541931152 batch PCKh 0.6875\n",
      "Trained batch 1790 batch loss 0.588191867 batch mAP 0.448547363 batch PCKh 0.625\n",
      "Trained batch 1791 batch loss 0.536697745 batch mAP 0.512420654 batch PCKh 0.6875\n",
      "Trained batch 1792 batch loss 0.575727284 batch mAP 0.483947754 batch PCKh 0.5625\n",
      "Trained batch 1793 batch loss 0.582869172 batch mAP 0.501586914 batch PCKh 0.1875\n",
      "Trained batch 1794 batch loss 0.606753647 batch mAP 0.477325439 batch PCKh 0.1875\n",
      "Trained batch 1795 batch loss 0.591174901 batch mAP 0.561126709 batch PCKh 0.125\n",
      "Trained batch 1796 batch loss 0.630619347 batch mAP 0.516265869 batch PCKh 0.6875\n",
      "Trained batch 1797 batch loss 0.540516675 batch mAP 0.635345459 batch PCKh 0.75\n",
      "Trained batch 1798 batch loss 0.505640209 batch mAP 0.665740967 batch PCKh 0.375\n",
      "Trained batch 1799 batch loss 0.483240396 batch mAP 0.666626 batch PCKh 0.4375\n",
      "Trained batch 1800 batch loss 0.467185855 batch mAP 0.667633057 batch PCKh 0.375\n",
      "Trained batch 1801 batch loss 0.399131805 batch mAP 0.693084717 batch PCKh 0.625\n",
      "Trained batch 1802 batch loss 0.64311 batch mAP 0.561035156 batch PCKh 0.125\n",
      "Trained batch 1803 batch loss 0.522824824 batch mAP 0.646026611 batch PCKh 0.625\n",
      "Trained batch 1804 batch loss 0.602401495 batch mAP 0.661346436 batch PCKh 0.75\n",
      "Trained batch 1805 batch loss 0.51171726 batch mAP 0.663299561 batch PCKh 0.8125\n",
      "Trained batch 1806 batch loss 0.584128499 batch mAP 0.595489502 batch PCKh 0.375\n",
      "Trained batch 1807 batch loss 0.552681446 batch mAP 0.612121582 batch PCKh 0.625\n",
      "Trained batch 1808 batch loss 0.508402824 batch mAP 0.614776611 batch PCKh 0.3125\n",
      "Trained batch 1809 batch loss 0.51048696 batch mAP 0.624755859 batch PCKh 0.6875\n",
      "Trained batch 1810 batch loss 0.541813731 batch mAP 0.606414795 batch PCKh 0.8125\n",
      "Trained batch 1811 batch loss 0.616459668 batch mAP 0.566619873 batch PCKh 0.5\n",
      "Trained batch 1812 batch loss 0.686566412 batch mAP 0.504669189 batch PCKh 0.5625\n",
      "Trained batch 1813 batch loss 0.665159225 batch mAP 0.479309082 batch PCKh 0.875\n",
      "Trained batch 1814 batch loss 0.496359825 batch mAP 0.540130615 batch PCKh 0.75\n",
      "Trained batch 1815 batch loss 0.531224549 batch mAP 0.623260498 batch PCKh 0.875\n",
      "Trained batch 1816 batch loss 0.513862491 batch mAP 0.56854248 batch PCKh 0.75\n",
      "Trained batch 1817 batch loss 0.567316949 batch mAP 0.612670898 batch PCKh 0.625\n",
      "Trained batch 1818 batch loss 0.53780067 batch mAP 0.548126221 batch PCKh 0.4375\n",
      "Trained batch 1819 batch loss 0.473637342 batch mAP 0.581512451 batch PCKh 0.6875\n",
      "Trained batch 1820 batch loss 0.464736283 batch mAP 0.549407959 batch PCKh 0.6875\n",
      "Trained batch 1821 batch loss 0.469046474 batch mAP 0.630645752 batch PCKh 0.625\n",
      "Trained batch 1822 batch loss 0.580610871 batch mAP 0.566436768 batch PCKh 0.4375\n",
      "Trained batch 1823 batch loss 0.4979707 batch mAP 0.592926 batch PCKh 0.3125\n",
      "Trained batch 1824 batch loss 0.610136867 batch mAP 0.585205078 batch PCKh 0\n",
      "Trained batch 1825 batch loss 0.52952987 batch mAP 0.647918701 batch PCKh 0.375\n",
      "Trained batch 1826 batch loss 0.503702641 batch mAP 0.68258667 batch PCKh 0.3125\n",
      "Trained batch 1827 batch loss 0.604010463 batch mAP 0.663970947 batch PCKh 0.25\n",
      "Trained batch 1828 batch loss 0.521327138 batch mAP 0.536224365 batch PCKh 0.5625\n",
      "Trained batch 1829 batch loss 0.609097302 batch mAP 0.578338623 batch PCKh 0.5\n",
      "Trained batch 1830 batch loss 0.509074569 batch mAP 0.615631104 batch PCKh 0.6875\n",
      "Trained batch 1831 batch loss 0.568954349 batch mAP 0.534423828 batch PCKh 0.25\n",
      "Trained batch 1832 batch loss 0.611791432 batch mAP 0.613861084 batch PCKh 0.625\n",
      "Trained batch 1833 batch loss 0.559985518 batch mAP 0.579345703 batch PCKh 0.875\n",
      "Trained batch 1834 batch loss 0.634895205 batch mAP 0.519165039 batch PCKh 0.4375\n",
      "Trained batch 1835 batch loss 0.662426 batch mAP 0.517334 batch PCKh 0.5625\n",
      "Trained batch 1836 batch loss 0.59989953 batch mAP 0.518615723 batch PCKh 0.1875\n",
      "Trained batch 1837 batch loss 0.707748473 batch mAP 0.499328613 batch PCKh 0.125\n",
      "Trained batch 1838 batch loss 0.483351767 batch mAP 0.651580811 batch PCKh 0.5625\n",
      "Trained batch 1839 batch loss 0.471699774 batch mAP 0.624694824 batch PCKh 0\n",
      "Trained batch 1840 batch loss 0.514683783 batch mAP 0.648407 batch PCKh 0.4375\n",
      "Trained batch 1841 batch loss 0.600252211 batch mAP 0.565826416 batch PCKh 0.6875\n",
      "Trained batch 1842 batch loss 0.536997557 batch mAP 0.523468 batch PCKh 0.1875\n",
      "Trained batch 1843 batch loss 0.605655611 batch mAP 0.561828613 batch PCKh 0.0625\n",
      "Trained batch 1844 batch loss 0.564194798 batch mAP 0.534484863 batch PCKh 0.3125\n",
      "Trained batch 1845 batch loss 0.58347404 batch mAP 0.517211914 batch PCKh 0.75\n",
      "Trained batch 1846 batch loss 0.546978 batch mAP 0.486358643 batch PCKh 0.4375\n",
      "Trained batch 1847 batch loss 0.515163779 batch mAP 0.490264893 batch PCKh 0.5625\n",
      "Trained batch 1848 batch loss 0.644487798 batch mAP 0.51751709 batch PCKh 0.0625\n",
      "Trained batch 1849 batch loss 0.506485581 batch mAP 0.498535156 batch PCKh 0.4375\n",
      "Trained batch 1850 batch loss 0.560746431 batch mAP 0.528533936 batch PCKh 0.625\n",
      "Trained batch 1851 batch loss 0.547640622 batch mAP 0.518829346 batch PCKh 0.5625\n",
      "Trained batch 1852 batch loss 0.580109477 batch mAP 0.568206787 batch PCKh 0.75\n",
      "Trained batch 1853 batch loss 0.645110905 batch mAP 0.528289795 batch PCKh 0.1875\n",
      "Trained batch 1854 batch loss 0.579331636 batch mAP 0.523651123 batch PCKh 0.375\n",
      "Trained batch 1855 batch loss 0.618154228 batch mAP 0.468994141 batch PCKh 0.125\n",
      "Trained batch 1856 batch loss 0.684372902 batch mAP 0.519592285 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1857 batch loss 0.589603364 batch mAP 0.523925781 batch PCKh 0.875\n",
      "Trained batch 1858 batch loss 0.562598 batch mAP 0.566070557 batch PCKh 0.375\n",
      "Trained batch 1859 batch loss 0.615771651 batch mAP 0.555725098 batch PCKh 0.8125\n",
      "Trained batch 1860 batch loss 0.572259486 batch mAP 0.513580322 batch PCKh 0.1875\n",
      "Trained batch 1861 batch loss 0.576207697 batch mAP 0.532348633 batch PCKh 0.625\n",
      "Trained batch 1862 batch loss 0.561898172 batch mAP 0.522216797 batch PCKh 0.4375\n",
      "Trained batch 1863 batch loss 0.692513585 batch mAP 0.471984863 batch PCKh 0.375\n",
      "Trained batch 1864 batch loss 0.513985932 batch mAP 0.473754883 batch PCKh 0.4375\n",
      "Trained batch 1865 batch loss 0.504494965 batch mAP 0.506225586 batch PCKh 0.5625\n",
      "Trained batch 1866 batch loss 0.569718957 batch mAP 0.5027771 batch PCKh 0.625\n",
      "Trained batch 1867 batch loss 0.517186046 batch mAP 0.532196045 batch PCKh 0.875\n",
      "Trained batch 1868 batch loss 0.54208833 batch mAP 0.539398193 batch PCKh 0.3125\n",
      "Trained batch 1869 batch loss 0.449140608 batch mAP 0.627807617 batch PCKh 0.625\n",
      "Trained batch 1870 batch loss 0.572846 batch mAP 0.561553955 batch PCKh 0.25\n",
      "Trained batch 1871 batch loss 0.619166195 batch mAP 0.530303955 batch PCKh 0.5\n",
      "Trained batch 1872 batch loss 0.582125127 batch mAP 0.519683838 batch PCKh 0.3125\n",
      "Trained batch 1873 batch loss 0.545065284 batch mAP 0.554290771 batch PCKh 0.3125\n",
      "Trained batch 1874 batch loss 0.482330203 batch mAP 0.642120361 batch PCKh 0.4375\n",
      "Trained batch 1875 batch loss 0.538083792 batch mAP 0.597564697 batch PCKh 0.25\n",
      "Trained batch 1876 batch loss 0.502905965 batch mAP 0.645050049 batch PCKh 0.3125\n",
      "Trained batch 1877 batch loss 0.496247768 batch mAP 0.527954102 batch PCKh 0.0625\n",
      "Trained batch 1878 batch loss 0.570139766 batch mAP 0.555969238 batch PCKh 0.75\n",
      "Trained batch 1879 batch loss 0.516125143 batch mAP 0.53994751 batch PCKh 0.8125\n",
      "Trained batch 1880 batch loss 0.490374804 batch mAP 0.584289551 batch PCKh 0.6875\n",
      "Trained batch 1881 batch loss 0.511744618 batch mAP 0.608581543 batch PCKh 0.625\n",
      "Trained batch 1882 batch loss 0.487847507 batch mAP 0.597412109 batch PCKh 0.75\n",
      "Trained batch 1883 batch loss 0.469793737 batch mAP 0.637512207 batch PCKh 0.5625\n",
      "Trained batch 1884 batch loss 0.471265376 batch mAP 0.625213623 batch PCKh 0.6875\n",
      "Trained batch 1885 batch loss 0.633547485 batch mAP 0.592529297 batch PCKh 0.875\n",
      "Trained batch 1886 batch loss 0.515933335 batch mAP 0.65335083 batch PCKh 0.3125\n",
      "Trained batch 1887 batch loss 0.557780743 batch mAP 0.615203857 batch PCKh 0.125\n",
      "Trained batch 1888 batch loss 0.555577278 batch mAP 0.629119873 batch PCKh 0.125\n",
      "Trained batch 1889 batch loss 0.42585808 batch mAP 0.705932617 batch PCKh 0.1875\n",
      "Trained batch 1890 batch loss 0.578249216 batch mAP 0.568725586 batch PCKh 0.375\n",
      "Trained batch 1891 batch loss 0.551796854 batch mAP 0.552734375 batch PCKh 0.4375\n",
      "Trained batch 1892 batch loss 0.528794527 batch mAP 0.636474609 batch PCKh 0.5\n",
      "Trained batch 1893 batch loss 0.579297841 batch mAP 0.652374268 batch PCKh 0.6875\n",
      "Trained batch 1894 batch loss 0.463689744 batch mAP 0.640197754 batch PCKh 0.5\n",
      "Trained batch 1895 batch loss 0.55327493 batch mAP 0.648590088 batch PCKh 0.5625\n",
      "Trained batch 1896 batch loss 0.506958127 batch mAP 0.637939453 batch PCKh 0.75\n",
      "Trained batch 1897 batch loss 0.581565261 batch mAP 0.6065979 batch PCKh 0.4375\n",
      "Trained batch 1898 batch loss 0.659669638 batch mAP 0.568389893 batch PCKh 0.75\n",
      "Trained batch 1899 batch loss 0.620512843 batch mAP 0.513946533 batch PCKh 0.625\n",
      "Trained batch 1900 batch loss 0.521757305 batch mAP 0.539794922 batch PCKh 0.125\n",
      "Trained batch 1901 batch loss 0.466201097 batch mAP 0.540863037 batch PCKh 0.1875\n",
      "Trained batch 1902 batch loss 0.580516338 batch mAP 0.504974365 batch PCKh 0.375\n",
      "Trained batch 1903 batch loss 0.56272316 batch mAP 0.522247314 batch PCKh 0.25\n",
      "Trained batch 1904 batch loss 0.512408197 batch mAP 0.397033691 batch PCKh 0.1875\n",
      "Trained batch 1905 batch loss 0.527065158 batch mAP 0.564880371 batch PCKh 0.625\n",
      "Trained batch 1906 batch loss 0.535053492 batch mAP 0.443450928 batch PCKh 0.5\n",
      "Trained batch 1907 batch loss 0.485529244 batch mAP 0.592132568 batch PCKh 0.75\n",
      "Trained batch 1908 batch loss 0.555111468 batch mAP 0.593139648 batch PCKh 0.8125\n",
      "Trained batch 1909 batch loss 0.590760589 batch mAP 0.619751 batch PCKh 0.25\n",
      "Trained batch 1910 batch loss 0.496976435 batch mAP 0.650024414 batch PCKh 0.3125\n",
      "Trained batch 1911 batch loss 0.517282 batch mAP 0.642578125 batch PCKh 0.5\n",
      "Trained batch 1912 batch loss 0.568035603 batch mAP 0.613006592 batch PCKh 0.6875\n",
      "Trained batch 1913 batch loss 0.56871295 batch mAP 0.607452393 batch PCKh 0.1875\n",
      "Trained batch 1914 batch loss 0.534526289 batch mAP 0.603881836 batch PCKh 0.5\n",
      "Trained batch 1915 batch loss 0.494582832 batch mAP 0.652771 batch PCKh 0.375\n",
      "Trained batch 1916 batch loss 0.461551726 batch mAP 0.649749756 batch PCKh 0.1875\n",
      "Trained batch 1917 batch loss 0.525080621 batch mAP 0.651367188 batch PCKh 0.1875\n",
      "Trained batch 1918 batch loss 0.526868939 batch mAP 0.570159912 batch PCKh 0.5625\n",
      "Trained batch 1919 batch loss 0.450964928 batch mAP 0.607513428 batch PCKh 0.125\n",
      "Trained batch 1920 batch loss 0.619201 batch mAP 0.480011 batch PCKh 0.5\n",
      "Trained batch 1921 batch loss 0.617415547 batch mAP 0.499938965 batch PCKh 0.6875\n",
      "Trained batch 1922 batch loss 0.507893264 batch mAP 0.650726318 batch PCKh 0.625\n",
      "Trained batch 1923 batch loss 0.531256 batch mAP 0.621398926 batch PCKh 0.375\n",
      "Trained batch 1924 batch loss 0.450385094 batch mAP 0.634521484 batch PCKh 0.5625\n",
      "Trained batch 1925 batch loss 0.526905179 batch mAP 0.570373535 batch PCKh 0.5\n",
      "Trained batch 1926 batch loss 0.557273567 batch mAP 0.540802 batch PCKh 0.1875\n",
      "Trained batch 1927 batch loss 0.490053117 batch mAP 0.481781 batch PCKh 0.1875\n",
      "Trained batch 1928 batch loss 0.519082904 batch mAP 0.512664795 batch PCKh 0.3125\n",
      "Trained batch 1929 batch loss 0.546023428 batch mAP 0.549041748 batch PCKh 0.125\n",
      "Trained batch 1930 batch loss 0.449255 batch mAP 0.600769043 batch PCKh 0.1875\n",
      "Trained batch 1931 batch loss 0.494877785 batch mAP 0.59161377 batch PCKh 0.4375\n",
      "Trained batch 1932 batch loss 0.537467718 batch mAP 0.643463135 batch PCKh 0.125\n",
      "Trained batch 1933 batch loss 0.421262145 batch mAP 0.555053711 batch PCKh 0.125\n",
      "Trained batch 1934 batch loss 0.503794312 batch mAP 0.518768311 batch PCKh 0.0625\n",
      "Trained batch 1935 batch loss 0.555932283 batch mAP 0.584350586 batch PCKh 0.5\n",
      "Trained batch 1936 batch loss 0.502714694 batch mAP 0.590911865 batch PCKh 0.1875\n",
      "Trained batch 1937 batch loss 0.578092217 batch mAP 0.59677124 batch PCKh 0.625\n",
      "Trained batch 1938 batch loss 0.632895708 batch mAP 0.596893311 batch PCKh 0.3125\n",
      "Trained batch 1939 batch loss 0.539920211 batch mAP 0.583526611 batch PCKh 0.5625\n",
      "Trained batch 1940 batch loss 0.531477749 batch mAP 0.605011 batch PCKh 0.25\n",
      "Trained batch 1941 batch loss 0.521841943 batch mAP 0.652801514 batch PCKh 0.625\n",
      "Trained batch 1942 batch loss 0.572565198 batch mAP 0.654663086 batch PCKh 0.3125\n",
      "Trained batch 1943 batch loss 0.583604 batch mAP 0.633148193 batch PCKh 0.25\n",
      "Trained batch 1944 batch loss 0.618442237 batch mAP 0.612213135 batch PCKh 0.3125\n",
      "Trained batch 1945 batch loss 0.587813735 batch mAP 0.649169922 batch PCKh 0.3125\n",
      "Trained batch 1946 batch loss 0.600010931 batch mAP 0.595733643 batch PCKh 0.3125\n",
      "Trained batch 1947 batch loss 0.554701328 batch mAP 0.562591553 batch PCKh 0.4375\n",
      "Trained batch 1948 batch loss 0.604511559 batch mAP 0.617431641 batch PCKh 0.3125\n",
      "Trained batch 1949 batch loss 0.558236778 batch mAP 0.63659668 batch PCKh 0.3125\n",
      "Trained batch 1950 batch loss 0.625177622 batch mAP 0.61895752 batch PCKh 0.4375\n",
      "Trained batch 1951 batch loss 0.575021625 batch mAP 0.638061523 batch PCKh 0.625\n",
      "Trained batch 1952 batch loss 0.52983 batch mAP 0.625396729 batch PCKh 0.4375\n",
      "Trained batch 1953 batch loss 0.446667194 batch mAP 0.622375488 batch PCKh 0.1875\n",
      "Trained batch 1954 batch loss 0.553266168 batch mAP 0.503936768 batch PCKh 0.625\n",
      "Trained batch 1955 batch loss 0.5846048 batch mAP 0.558532715 batch PCKh 0.6875\n",
      "Trained batch 1956 batch loss 0.536097109 batch mAP 0.560821533 batch PCKh 0\n",
      "Trained batch 1957 batch loss 0.668335438 batch mAP 0.538238525 batch PCKh 0.8125\n",
      "Trained batch 1958 batch loss 0.620821476 batch mAP 0.532928467 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1959 batch loss 0.610035479 batch mAP 0.524963379 batch PCKh 0.375\n",
      "Trained batch 1960 batch loss 0.657612681 batch mAP 0.552398682 batch PCKh 0.3125\n",
      "Trained batch 1961 batch loss 0.638931274 batch mAP 0.540649414 batch PCKh 0.375\n",
      "Trained batch 1962 batch loss 0.663721919 batch mAP 0.614471436 batch PCKh 0.125\n",
      "Trained batch 1963 batch loss 0.560506165 batch mAP 0.644928 batch PCKh 0.5\n",
      "Trained batch 1964 batch loss 0.608211398 batch mAP 0.589874268 batch PCKh 0.1875\n",
      "Trained batch 1965 batch loss 0.604034543 batch mAP 0.547515869 batch PCKh 0.125\n",
      "Trained batch 1966 batch loss 0.582181931 batch mAP 0.521575928 batch PCKh 0.375\n",
      "Trained batch 1967 batch loss 0.599098563 batch mAP 0.55947876 batch PCKh 0.625\n",
      "Trained batch 1968 batch loss 0.56522429 batch mAP 0.595001221 batch PCKh 0.3125\n",
      "Trained batch 1969 batch loss 0.507359922 batch mAP 0.646453857 batch PCKh 0.5625\n",
      "Trained batch 1970 batch loss 0.525126219 batch mAP 0.545379639 batch PCKh 0.75\n",
      "Trained batch 1971 batch loss 0.426400423 batch mAP 0.553497314 batch PCKh 0.625\n",
      "Trained batch 1972 batch loss 0.475428492 batch mAP 0.581756592 batch PCKh 0.3125\n",
      "Trained batch 1973 batch loss 0.583055496 batch mAP 0.609436035 batch PCKh 0.625\n",
      "Trained batch 1974 batch loss 0.517229319 batch mAP 0.626251221 batch PCKh 0.75\n",
      "Trained batch 1975 batch loss 0.565018535 batch mAP 0.552368164 batch PCKh 0\n",
      "Trained batch 1976 batch loss 0.548966765 batch mAP 0.578826904 batch PCKh 0.25\n",
      "Trained batch 1977 batch loss 0.621962905 batch mAP 0.547851562 batch PCKh 0.1875\n",
      "Trained batch 1978 batch loss 0.573238611 batch mAP 0.556945801 batch PCKh 0.625\n",
      "Trained batch 1979 batch loss 0.522383 batch mAP 0.524108887 batch PCKh 0.625\n",
      "Trained batch 1980 batch loss 0.559139252 batch mAP 0.575378418 batch PCKh 0.5625\n",
      "Trained batch 1981 batch loss 0.531682551 batch mAP 0.53112793 batch PCKh 0.5\n",
      "Trained batch 1982 batch loss 0.476440728 batch mAP 0.561645508 batch PCKh 0.1875\n",
      "Trained batch 1983 batch loss 0.446526378 batch mAP 0.617553711 batch PCKh 0.6875\n",
      "Trained batch 1984 batch loss 0.537938356 batch mAP 0.641601562 batch PCKh 0.5\n",
      "Trained batch 1985 batch loss 0.428330779 batch mAP 0.678436279 batch PCKh 0.3125\n",
      "Trained batch 1986 batch loss 0.395999193 batch mAP 0.677032471 batch PCKh 0.1875\n",
      "Trained batch 1987 batch loss 0.473586351 batch mAP 0.696258545 batch PCKh 0.6875\n",
      "Trained batch 1988 batch loss 0.506461143 batch mAP 0.63470459 batch PCKh 0.75\n",
      "Trained batch 1989 batch loss 0.506979823 batch mAP 0.6277771 batch PCKh 0.5\n",
      "Trained batch 1990 batch loss 0.485756 batch mAP 0.643554688 batch PCKh 0.4375\n",
      "Trained batch 1991 batch loss 0.456234932 batch mAP 0.64831543 batch PCKh 0.25\n",
      "Trained batch 1992 batch loss 0.449114084 batch mAP 0.629058838 batch PCKh 0.625\n",
      "Trained batch 1993 batch loss 0.479154766 batch mAP 0.632171631 batch PCKh 0.5\n",
      "Trained batch 1994 batch loss 0.542878628 batch mAP 0.640533447 batch PCKh 0.375\n",
      "Trained batch 1995 batch loss 0.500181079 batch mAP 0.612640381 batch PCKh 0.4375\n",
      "Trained batch 1996 batch loss 0.57106179 batch mAP 0.67453 batch PCKh 0.25\n",
      "Trained batch 1997 batch loss 0.488273978 batch mAP 0.641967773 batch PCKh 0.8125\n",
      "Trained batch 1998 batch loss 0.482673854 batch mAP 0.706359863 batch PCKh 0.625\n",
      "Trained batch 1999 batch loss 0.446412116 batch mAP 0.633789062 batch PCKh 0.5\n",
      "Trained batch 2000 batch loss 0.544681549 batch mAP 0.638366699 batch PCKh 0.5625\n",
      "Trained batch 2001 batch loss 0.517063737 batch mAP 0.653137207 batch PCKh 0.5625\n",
      "Trained batch 2002 batch loss 0.457489252 batch mAP 0.680725098 batch PCKh 0.3125\n",
      "Trained batch 2003 batch loss 0.503910482 batch mAP 0.605712891 batch PCKh 0.75\n",
      "Trained batch 2004 batch loss 0.57918644 batch mAP 0.639007568 batch PCKh 0.1875\n",
      "Trained batch 2005 batch loss 0.591558 batch mAP 0.612640381 batch PCKh 0.5625\n",
      "Trained batch 2006 batch loss 0.540865064 batch mAP 0.609100342 batch PCKh 0.5\n",
      "Trained batch 2007 batch loss 0.604286969 batch mAP 0.570343 batch PCKh 0.5\n",
      "Trained batch 2008 batch loss 0.569047749 batch mAP 0.465789795 batch PCKh 0.625\n",
      "Trained batch 2009 batch loss 0.53605938 batch mAP 0.401733398 batch PCKh 0.3125\n",
      "Trained batch 2010 batch loss 0.565800786 batch mAP 0.479125977 batch PCKh 0.5\n",
      "Trained batch 2011 batch loss 0.518171906 batch mAP 0.562591553 batch PCKh 0.6875\n",
      "Trained batch 2012 batch loss 0.574501753 batch mAP 0.523742676 batch PCKh 0.3125\n",
      "Trained batch 2013 batch loss 0.63718158 batch mAP 0.515869141 batch PCKh 0.125\n",
      "Trained batch 2014 batch loss 0.582151651 batch mAP 0.507598877 batch PCKh 0.3125\n",
      "Trained batch 2015 batch loss 0.572857797 batch mAP 0.505218506 batch PCKh 0.5625\n",
      "Trained batch 2016 batch loss 0.575319648 batch mAP 0.497833252 batch PCKh 0.625\n",
      "Trained batch 2017 batch loss 0.603121042 batch mAP 0.505493164 batch PCKh 0.3125\n",
      "Trained batch 2018 batch loss 0.591649771 batch mAP 0.496887207 batch PCKh 0.625\n",
      "Trained batch 2019 batch loss 0.525981903 batch mAP 0.522766113 batch PCKh 0.6875\n",
      "Trained batch 2020 batch loss 0.595380127 batch mAP 0.496032715 batch PCKh 0.75\n",
      "Trained batch 2021 batch loss 0.47515291 batch mAP 0.571380615 batch PCKh 0.5\n",
      "Trained batch 2022 batch loss 0.471769333 batch mAP 0.606567383 batch PCKh 0.625\n",
      "Trained batch 2023 batch loss 0.487673283 batch mAP 0.573761 batch PCKh 0.1875\n",
      "Trained batch 2024 batch loss 0.534828 batch mAP 0.572967529 batch PCKh 0.4375\n",
      "Trained batch 2025 batch loss 0.542681 batch mAP 0.573059082 batch PCKh 0.0625\n",
      "Trained batch 2026 batch loss 0.596659303 batch mAP 0.617218 batch PCKh 0.4375\n",
      "Trained batch 2027 batch loss 0.576476336 batch mAP 0.540802 batch PCKh 0.25\n",
      "Trained batch 2028 batch loss 0.571718454 batch mAP 0.583618164 batch PCKh 0.3125\n",
      "Trained batch 2029 batch loss 0.629216194 batch mAP 0.605011 batch PCKh 0.3125\n",
      "Trained batch 2030 batch loss 0.651022732 batch mAP 0.634155273 batch PCKh 0.125\n",
      "Trained batch 2031 batch loss 0.584480762 batch mAP 0.675445557 batch PCKh 0.75\n",
      "Trained batch 2032 batch loss 0.579731107 batch mAP 0.572845459 batch PCKh 0.5625\n",
      "Trained batch 2033 batch loss 0.645171285 batch mAP 0.521911621 batch PCKh 0.5\n",
      "Trained batch 2034 batch loss 0.527359068 batch mAP 0.5496521 batch PCKh 0.125\n",
      "Trained batch 2035 batch loss 0.393778652 batch mAP 0.576538086 batch PCKh 0.1875\n",
      "Trained batch 2036 batch loss 0.44494617 batch mAP 0.578552246 batch PCKh 0.5\n",
      "Trained batch 2037 batch loss 0.57241255 batch mAP 0.624145508 batch PCKh 0.875\n",
      "Trained batch 2038 batch loss 0.535395086 batch mAP 0.566009521 batch PCKh 0.5625\n",
      "Trained batch 2039 batch loss 0.568949223 batch mAP 0.565551758 batch PCKh 0.6875\n",
      "Trained batch 2040 batch loss 0.478413403 batch mAP 0.615783691 batch PCKh 0.5625\n",
      "Trained batch 2041 batch loss 0.571318507 batch mAP 0.570953369 batch PCKh 0.625\n",
      "Trained batch 2042 batch loss 0.535814047 batch mAP 0.626617432 batch PCKh 0.625\n",
      "Trained batch 2043 batch loss 0.473520219 batch mAP 0.654418945 batch PCKh 0.4375\n",
      "Trained batch 2044 batch loss 0.454993457 batch mAP 0.684326172 batch PCKh 0.4375\n",
      "Trained batch 2045 batch loss 0.495813698 batch mAP 0.64831543 batch PCKh 0.5\n",
      "Trained batch 2046 batch loss 0.582466602 batch mAP 0.611572266 batch PCKh 0.375\n",
      "Trained batch 2047 batch loss 0.560993552 batch mAP 0.603607178 batch PCKh 0.5625\n",
      "Trained batch 2048 batch loss 0.592431188 batch mAP 0.606018066 batch PCKh 0.4375\n",
      "Trained batch 2049 batch loss 0.549550772 batch mAP 0.585632324 batch PCKh 0.4375\n",
      "Trained batch 2050 batch loss 0.623456717 batch mAP 0.573699951 batch PCKh 0.375\n",
      "Trained batch 2051 batch loss 0.615283966 batch mAP 0.564178467 batch PCKh 0.3125\n",
      "Trained batch 2052 batch loss 0.543696582 batch mAP 0.62902832 batch PCKh 0.6875\n",
      "Trained batch 2053 batch loss 0.579224288 batch mAP 0.545318604 batch PCKh 0.5625\n",
      "Trained batch 2054 batch loss 0.509674668 batch mAP 0.612609863 batch PCKh 0.75\n",
      "Trained batch 2055 batch loss 0.53021282 batch mAP 0.574249268 batch PCKh 0.5\n",
      "Trained batch 2056 batch loss 0.576158762 batch mAP 0.547454834 batch PCKh 0.375\n",
      "Trained batch 2057 batch loss 0.574244142 batch mAP 0.558288574 batch PCKh 0.4375\n",
      "Trained batch 2058 batch loss 0.607040942 batch mAP 0.540863037 batch PCKh 0.4375\n",
      "Trained batch 2059 batch loss 0.564929962 batch mAP 0.547241211 batch PCKh 0.4375\n",
      "Trained batch 2060 batch loss 0.644480348 batch mAP 0.536254883 batch PCKh 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2061 batch loss 0.519157767 batch mAP 0.639526367 batch PCKh 0.625\n",
      "Trained batch 2062 batch loss 0.517255902 batch mAP 0.55267334 batch PCKh 0.5\n",
      "Trained batch 2063 batch loss 0.47372368 batch mAP 0.552429199 batch PCKh 0.4375\n",
      "Trained batch 2064 batch loss 0.528458536 batch mAP 0.556762695 batch PCKh 0.375\n",
      "Trained batch 2065 batch loss 0.52136904 batch mAP 0.574646 batch PCKh 0.4375\n",
      "Trained batch 2066 batch loss 0.554243207 batch mAP 0.595336914 batch PCKh 0.3125\n",
      "Trained batch 2067 batch loss 0.536054492 batch mAP 0.541748047 batch PCKh 0.625\n",
      "Trained batch 2068 batch loss 0.567252278 batch mAP 0.473388672 batch PCKh 0.625\n",
      "Trained batch 2069 batch loss 0.436403513 batch mAP 0.550842285 batch PCKh 0.25\n",
      "Trained batch 2070 batch loss 0.419915259 batch mAP 0.55670166 batch PCKh 0.5625\n",
      "Trained batch 2071 batch loss 0.512842536 batch mAP 0.531616211 batch PCKh 0\n",
      "Trained batch 2072 batch loss 0.446856439 batch mAP 0.585327148 batch PCKh 0.4375\n",
      "Trained batch 2073 batch loss 0.544479 batch mAP 0.508453369 batch PCKh 0.8125\n",
      "Trained batch 2074 batch loss 0.52482456 batch mAP 0.54498291 batch PCKh 0.375\n",
      "Trained batch 2075 batch loss 0.444938481 batch mAP 0.617492676 batch PCKh 0.5625\n",
      "Trained batch 2076 batch loss 0.456176758 batch mAP 0.629211426 batch PCKh 0.5\n",
      "Trained batch 2077 batch loss 0.519285321 batch mAP 0.618042 batch PCKh 0.4375\n",
      "Trained batch 2078 batch loss 0.48216176 batch mAP 0.608001709 batch PCKh 0.5625\n",
      "Trained batch 2079 batch loss 0.44580543 batch mAP 0.625640869 batch PCKh 0.625\n",
      "Trained batch 2080 batch loss 0.516391218 batch mAP 0.602966309 batch PCKh 0.75\n",
      "Trained batch 2081 batch loss 0.505506575 batch mAP 0.552154541 batch PCKh 0.75\n",
      "Trained batch 2082 batch loss 0.592299759 batch mAP 0.582122803 batch PCKh 0.5\n",
      "Trained batch 2083 batch loss 0.720422566 batch mAP 0.505371094 batch PCKh 0.0625\n",
      "Trained batch 2084 batch loss 0.623481154 batch mAP 0.511657715 batch PCKh 0\n",
      "Trained batch 2085 batch loss 0.571077168 batch mAP 0.600463867 batch PCKh 0.5\n",
      "Trained batch 2086 batch loss 0.555296719 batch mAP 0.602874756 batch PCKh 0.8125\n",
      "Trained batch 2087 batch loss 0.610183477 batch mAP 0.567321777 batch PCKh 0.75\n",
      "Trained batch 2088 batch loss 0.508111358 batch mAP 0.549438477 batch PCKh 0.5625\n",
      "Trained batch 2089 batch loss 0.65714705 batch mAP 0.543151855 batch PCKh 0.5\n",
      "Trained batch 2090 batch loss 0.684296489 batch mAP 0.387969971 batch PCKh 0.5\n",
      "Trained batch 2091 batch loss 0.599673867 batch mAP 0.394775391 batch PCKh 0.25\n",
      "Trained batch 2092 batch loss 0.599764 batch mAP 0.427215576 batch PCKh 0.4375\n",
      "Trained batch 2093 batch loss 0.628246605 batch mAP 0.400848389 batch PCKh 0.1875\n",
      "Trained batch 2094 batch loss 0.548715532 batch mAP 0.39541626 batch PCKh 0.6875\n",
      "Trained batch 2095 batch loss 0.580153048 batch mAP 0.399749756 batch PCKh 0.6875\n",
      "Trained batch 2096 batch loss 0.570908904 batch mAP 0.436981201 batch PCKh 0.1875\n",
      "Trained batch 2097 batch loss 0.522185385 batch mAP 0.488250732 batch PCKh 0.3125\n",
      "Trained batch 2098 batch loss 0.568849087 batch mAP 0.501037598 batch PCKh 0.125\n",
      "Trained batch 2099 batch loss 0.489079237 batch mAP 0.527252197 batch PCKh 0.0625\n",
      "Trained batch 2100 batch loss 0.543042362 batch mAP 0.54422 batch PCKh 0.875\n",
      "Trained batch 2101 batch loss 0.558233738 batch mAP 0.565246582 batch PCKh 0.25\n",
      "Trained batch 2102 batch loss 0.556253254 batch mAP 0.54675293 batch PCKh 0.3125\n",
      "Trained batch 2103 batch loss 0.560428619 batch mAP 0.519866943 batch PCKh 0.375\n",
      "Trained batch 2104 batch loss 0.627096593 batch mAP 0.5390625 batch PCKh 0.75\n",
      "Trained batch 2105 batch loss 0.530303478 batch mAP 0.593963623 batch PCKh 0.75\n",
      "Trained batch 2106 batch loss 0.59553504 batch mAP 0.563781738 batch PCKh 0.4375\n",
      "Trained batch 2107 batch loss 0.601809263 batch mAP 0.570709229 batch PCKh 0.6875\n",
      "Trained batch 2108 batch loss 0.64063251 batch mAP 0.51675415 batch PCKh 0.4375\n",
      "Trained batch 2109 batch loss 0.595997095 batch mAP 0.541351318 batch PCKh 0.5625\n",
      "Trained batch 2110 batch loss 0.546541214 batch mAP 0.565307617 batch PCKh 0.4375\n",
      "Trained batch 2111 batch loss 0.527523458 batch mAP 0.605987549 batch PCKh 0.6875\n",
      "Trained batch 2112 batch loss 0.614224672 batch mAP 0.547393799 batch PCKh 0.0625\n",
      "Trained batch 2113 batch loss 0.627971888 batch mAP 0.596557617 batch PCKh 0.375\n",
      "Trained batch 2114 batch loss 0.567374468 batch mAP 0.632629395 batch PCKh 0.4375\n",
      "Trained batch 2115 batch loss 0.443513691 batch mAP 0.651367188 batch PCKh 0.5\n",
      "Trained batch 2116 batch loss 0.485895693 batch mAP 0.66583252 batch PCKh 0.5\n",
      "Trained batch 2117 batch loss 0.516156077 batch mAP 0.605804443 batch PCKh 0.625\n",
      "Trained batch 2118 batch loss 0.529410303 batch mAP 0.541046143 batch PCKh 0.3125\n",
      "Trained batch 2119 batch loss 0.477353513 batch mAP 0.583465576 batch PCKh 0.375\n",
      "Trained batch 2120 batch loss 0.528897524 batch mAP 0.557556152 batch PCKh 0.75\n",
      "Trained batch 2121 batch loss 0.619416535 batch mAP 0.510131836 batch PCKh 0.4375\n",
      "Trained batch 2122 batch loss 0.612371743 batch mAP 0.52243042 batch PCKh 0.125\n",
      "Trained batch 2123 batch loss 0.570149183 batch mAP 0.522705078 batch PCKh 0.1875\n",
      "Trained batch 2124 batch loss 0.547401071 batch mAP 0.722320557 batch PCKh 0.3125\n",
      "Trained batch 2125 batch loss 0.527011037 batch mAP 0.70916748 batch PCKh 0.3125\n",
      "Trained batch 2126 batch loss 0.524614453 batch mAP 0.658782959 batch PCKh 0.375\n",
      "Trained batch 2127 batch loss 0.511500359 batch mAP 0.680114746 batch PCKh 0.625\n",
      "Trained batch 2128 batch loss 0.509553492 batch mAP 0.675811768 batch PCKh 0.5625\n",
      "Trained batch 2129 batch loss 0.555770636 batch mAP 0.606567383 batch PCKh 0.1875\n",
      "Trained batch 2130 batch loss 0.58498019 batch mAP 0.617706299 batch PCKh 0.375\n",
      "Trained batch 2131 batch loss 0.546052217 batch mAP 0.595031738 batch PCKh 0.75\n",
      "Trained batch 2132 batch loss 0.594270647 batch mAP 0.596954346 batch PCKh 0.125\n",
      "Trained batch 2133 batch loss 0.585696459 batch mAP 0.611145 batch PCKh 0.3125\n",
      "Trained batch 2134 batch loss 0.594048262 batch mAP 0.521728516 batch PCKh 0.75\n",
      "Trained batch 2135 batch loss 0.560252666 batch mAP 0.521636963 batch PCKh 0.125\n",
      "Trained batch 2136 batch loss 0.600338936 batch mAP 0.543640137 batch PCKh 0.3125\n",
      "Trained batch 2137 batch loss 0.572127223 batch mAP 0.596679688 batch PCKh 0.8125\n",
      "Trained batch 2138 batch loss 0.51704061 batch mAP 0.611877441 batch PCKh 0.5625\n",
      "Trained batch 2139 batch loss 0.575257361 batch mAP 0.628204346 batch PCKh 0.5\n",
      "Trained batch 2140 batch loss 0.558355689 batch mAP 0.57409668 batch PCKh 0.8125\n",
      "Trained batch 2141 batch loss 0.629343569 batch mAP 0.571655273 batch PCKh 0.5625\n",
      "Trained batch 2142 batch loss 0.610610485 batch mAP 0.568054199 batch PCKh 0.4375\n",
      "Trained batch 2143 batch loss 0.577665687 batch mAP 0.603515625 batch PCKh 0.75\n",
      "Trained batch 2144 batch loss 0.564670503 batch mAP 0.627685547 batch PCKh 0.5625\n",
      "Trained batch 2145 batch loss 0.573797166 batch mAP 0.591461182 batch PCKh 0.375\n",
      "Trained batch 2146 batch loss 0.593761444 batch mAP 0.572937 batch PCKh 0.5625\n",
      "Trained batch 2147 batch loss 0.584832847 batch mAP 0.616912842 batch PCKh 0.375\n",
      "Trained batch 2148 batch loss 0.61901176 batch mAP 0.53012085 batch PCKh 0.0625\n",
      "Trained batch 2149 batch loss 0.598959923 batch mAP 0.542541504 batch PCKh 0.5625\n",
      "Trained batch 2150 batch loss 0.551528811 batch mAP 0.63092041 batch PCKh 0.6875\n",
      "Trained batch 2151 batch loss 0.568082094 batch mAP 0.638946533 batch PCKh 0.6875\n",
      "Trained batch 2152 batch loss 0.616871774 batch mAP 0.515411377 batch PCKh 0.0625\n",
      "Trained batch 2153 batch loss 0.575575 batch mAP 0.571380615 batch PCKh 0.8125\n",
      "Trained batch 2154 batch loss 0.477951318 batch mAP 0.61428833 batch PCKh 0.75\n",
      "Trained batch 2155 batch loss 0.5877738 batch mAP 0.581726074 batch PCKh 0.75\n",
      "Trained batch 2156 batch loss 0.540819466 batch mAP 0.587615967 batch PCKh 0.875\n",
      "Trained batch 2157 batch loss 0.53997618 batch mAP 0.543060303 batch PCKh 0.5625\n",
      "Trained batch 2158 batch loss 0.545308232 batch mAP 0.493682861 batch PCKh 0.25\n",
      "Trained batch 2159 batch loss 0.605480433 batch mAP 0.499725342 batch PCKh 0.3125\n",
      "Trained batch 2160 batch loss 0.599656105 batch mAP 0.47177124 batch PCKh 0.0625\n",
      "Trained batch 2161 batch loss 0.507223785 batch mAP 0.490142822 batch PCKh 0.75\n",
      "Trained batch 2162 batch loss 0.552671671 batch mAP 0.500549316 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2163 batch loss 0.547748387 batch mAP 0.528869629 batch PCKh 0.375\n",
      "Trained batch 2164 batch loss 0.522921801 batch mAP 0.568206787 batch PCKh 0.625\n",
      "Trained batch 2165 batch loss 0.646079481 batch mAP 0.501678467 batch PCKh 0.625\n",
      "Trained batch 2166 batch loss 0.496206552 batch mAP 0.602661133 batch PCKh 0.75\n",
      "Trained batch 2167 batch loss 0.499803305 batch mAP 0.578674316 batch PCKh 0.25\n",
      "Trained batch 2168 batch loss 0.472068101 batch mAP 0.624511719 batch PCKh 0\n",
      "Trained batch 2169 batch loss 0.515843928 batch mAP 0.605621338 batch PCKh 0.5625\n",
      "Trained batch 2170 batch loss 0.581110239 batch mAP 0.555328369 batch PCKh 0.3125\n",
      "Trained batch 2171 batch loss 0.509563506 batch mAP 0.55291748 batch PCKh 0.3125\n",
      "Trained batch 2172 batch loss 0.523531795 batch mAP 0.497650146 batch PCKh 0.25\n",
      "Trained batch 2173 batch loss 0.523878634 batch mAP 0.578216553 batch PCKh 0.0625\n",
      "Trained batch 2174 batch loss 0.593823254 batch mAP 0.514801 batch PCKh 0.125\n",
      "Trained batch 2175 batch loss 0.545910597 batch mAP 0.516479492 batch PCKh 0.0625\n",
      "Trained batch 2176 batch loss 0.549422145 batch mAP 0.56942749 batch PCKh 0.5625\n",
      "Trained batch 2177 batch loss 0.616412342 batch mAP 0.576568604 batch PCKh 0.5625\n",
      "Trained batch 2178 batch loss 0.529860318 batch mAP 0.551574707 batch PCKh 0\n",
      "Trained batch 2179 batch loss 0.568455219 batch mAP 0.508972168 batch PCKh 0.3125\n",
      "Trained batch 2180 batch loss 0.58845067 batch mAP 0.526977539 batch PCKh 0.1875\n",
      "Trained batch 2181 batch loss 0.522247076 batch mAP 0.556793213 batch PCKh 0.8125\n",
      "Trained batch 2182 batch loss 0.512212932 batch mAP 0.573974609 batch PCKh 0.875\n",
      "Trained batch 2183 batch loss 0.536736369 batch mAP 0.564575195 batch PCKh 0.75\n",
      "Trained batch 2184 batch loss 0.485276043 batch mAP 0.510528564 batch PCKh 0.5\n",
      "Trained batch 2185 batch loss 0.479359627 batch mAP 0.52532959 batch PCKh 0.5\n",
      "Trained batch 2186 batch loss 0.508060217 batch mAP 0.531890869 batch PCKh 0.75\n",
      "Trained batch 2187 batch loss 0.537147343 batch mAP 0.540039062 batch PCKh 0.8125\n",
      "Trained batch 2188 batch loss 0.49028337 batch mAP 0.56918335 batch PCKh 0.5625\n",
      "Trained batch 2189 batch loss 0.49743849 batch mAP 0.645782471 batch PCKh 0.625\n",
      "Trained batch 2190 batch loss 0.556047797 batch mAP 0.594665527 batch PCKh 0.4375\n",
      "Trained batch 2191 batch loss 0.560667 batch mAP 0.602355957 batch PCKh 0.3125\n",
      "Trained batch 2192 batch loss 0.501279354 batch mAP 0.650085449 batch PCKh 0.5\n",
      "Trained batch 2193 batch loss 0.51718241 batch mAP 0.625854492 batch PCKh 0.3125\n",
      "Trained batch 2194 batch loss 0.441522926 batch mAP 0.564544678 batch PCKh 0.6875\n",
      "Trained batch 2195 batch loss 0.464546323 batch mAP 0.547821045 batch PCKh 0.375\n",
      "Trained batch 2196 batch loss 0.497677565 batch mAP 0.576416 batch PCKh 0.75\n",
      "Trained batch 2197 batch loss 0.518392861 batch mAP 0.601654053 batch PCKh 0.5\n",
      "Trained batch 2198 batch loss 0.544278383 batch mAP 0.513916 batch PCKh 0.625\n",
      "Trained batch 2199 batch loss 0.712552488 batch mAP 0.487060547 batch PCKh 0.25\n",
      "Trained batch 2200 batch loss 0.601800263 batch mAP 0.48550415 batch PCKh 0.75\n",
      "Trained batch 2201 batch loss 0.558698952 batch mAP 0.579925537 batch PCKh 0.75\n",
      "Trained batch 2202 batch loss 0.552473187 batch mAP 0.534667969 batch PCKh 0.75\n",
      "Trained batch 2203 batch loss 0.540507197 batch mAP 0.548400879 batch PCKh 0.25\n",
      "Trained batch 2204 batch loss 0.505279303 batch mAP 0.536132812 batch PCKh 0.8125\n",
      "Trained batch 2205 batch loss 0.51250577 batch mAP 0.482788086 batch PCKh 0.375\n",
      "Trained batch 2206 batch loss 0.420466751 batch mAP 0.63369751 batch PCKh 0.25\n",
      "Trained batch 2207 batch loss 0.331427932 batch mAP 0.710449219 batch PCKh 0.375\n",
      "Trained batch 2208 batch loss 0.362826765 batch mAP 0.746917725 batch PCKh 0.6875\n",
      "Trained batch 2209 batch loss 0.349551082 batch mAP 0.714935303 batch PCKh 0.8125\n",
      "Trained batch 2210 batch loss 0.384540617 batch mAP 0.733642578 batch PCKh 0.5625\n",
      "Trained batch 2211 batch loss 0.436423063 batch mAP 0.713409424 batch PCKh 0.5625\n",
      "Trained batch 2212 batch loss 0.391123563 batch mAP 0.647003174 batch PCKh 0.625\n",
      "Trained batch 2213 batch loss 0.503261805 batch mAP 0.581237793 batch PCKh 0.875\n",
      "Trained batch 2214 batch loss 0.532322764 batch mAP 0.613098145 batch PCKh 0.75\n",
      "Trained batch 2215 batch loss 0.582493544 batch mAP 0.527587891 batch PCKh 0.75\n",
      "Trained batch 2216 batch loss 0.557119191 batch mAP 0.512939453 batch PCKh 0.75\n",
      "Trained batch 2217 batch loss 0.546757102 batch mAP 0.554382324 batch PCKh 0.625\n",
      "Trained batch 2218 batch loss 0.388662636 batch mAP 0.687957764 batch PCKh 0.375\n",
      "Trained batch 2219 batch loss 0.424792826 batch mAP 0.662078857 batch PCKh 0.5625\n",
      "Trained batch 2220 batch loss 0.380526394 batch mAP 0.703430176 batch PCKh 0.4375\n",
      "Trained batch 2221 batch loss 0.407976061 batch mAP 0.688476562 batch PCKh 0.4375\n",
      "Trained batch 2222 batch loss 0.419616103 batch mAP 0.694702148 batch PCKh 0.375\n",
      "Trained batch 2223 batch loss 0.497941256 batch mAP 0.674621582 batch PCKh 0.3125\n",
      "Trained batch 2224 batch loss 0.501743257 batch mAP 0.656494141 batch PCKh 0.25\n",
      "Trained batch 2225 batch loss 0.474564672 batch mAP 0.680786133 batch PCKh 0.5\n",
      "Trained batch 2226 batch loss 0.54181397 batch mAP 0.687713623 batch PCKh 0.25\n",
      "Trained batch 2227 batch loss 0.507873952 batch mAP 0.683990479 batch PCKh 0.375\n",
      "Trained batch 2228 batch loss 0.539090872 batch mAP 0.614135742 batch PCKh 0.3125\n",
      "Trained batch 2229 batch loss 0.556691706 batch mAP 0.528717041 batch PCKh 0.75\n",
      "Trained batch 2230 batch loss 0.466635466 batch mAP 0.631103516 batch PCKh 0.375\n",
      "Trained batch 2231 batch loss 0.598590791 batch mAP 0.693237305 batch PCKh 0.375\n",
      "Trained batch 2232 batch loss 0.62464869 batch mAP 0.68347168 batch PCKh 0.4375\n",
      "Trained batch 2233 batch loss 0.567797363 batch mAP 0.630981445 batch PCKh 0.25\n",
      "Trained batch 2234 batch loss 0.611597121 batch mAP 0.593688965 batch PCKh 0.4375\n",
      "Trained batch 2235 batch loss 0.535047412 batch mAP 0.484313965 batch PCKh 0.0625\n",
      "Trained batch 2236 batch loss 0.643249929 batch mAP 0.525360107 batch PCKh 0.6875\n",
      "Trained batch 2237 batch loss 0.627460241 batch mAP 0.460266113 batch PCKh 0.75\n",
      "Trained batch 2238 batch loss 0.644687355 batch mAP 0.50302124 batch PCKh 0.25\n",
      "Trained batch 2239 batch loss 0.461851239 batch mAP 0.483703613 batch PCKh 0.5\n",
      "Trained batch 2240 batch loss 0.460133612 batch mAP 0.459899902 batch PCKh 0.0625\n",
      "Trained batch 2241 batch loss 0.43537873 batch mAP 0.647857666 batch PCKh 0.3125\n",
      "Trained batch 2242 batch loss 0.523697674 batch mAP 0.591003418 batch PCKh 0.375\n",
      "Trained batch 2243 batch loss 0.492839694 batch mAP 0.524261475 batch PCKh 0.75\n",
      "Trained batch 2244 batch loss 0.555179358 batch mAP 0.537567139 batch PCKh 0.8125\n",
      "Trained batch 2245 batch loss 0.572875 batch mAP 0.560882568 batch PCKh 0.4375\n",
      "Trained batch 2246 batch loss 0.560444653 batch mAP 0.588287354 batch PCKh 0.75\n",
      "Trained batch 2247 batch loss 0.594471693 batch mAP 0.536346436 batch PCKh 0.3125\n",
      "Trained batch 2248 batch loss 0.639543414 batch mAP 0.540771484 batch PCKh 0.3125\n",
      "Trained batch 2249 batch loss 0.632043779 batch mAP 0.588806152 batch PCKh 0.4375\n",
      "Trained batch 2250 batch loss 0.566187441 batch mAP 0.701812744 batch PCKh 0.4375\n",
      "Trained batch 2251 batch loss 0.537425756 batch mAP 0.650543213 batch PCKh 0.375\n",
      "Trained batch 2252 batch loss 0.589534044 batch mAP 0.57724 batch PCKh 0.0625\n",
      "Trained batch 2253 batch loss 0.562979877 batch mAP 0.578521729 batch PCKh 0.1875\n",
      "Trained batch 2254 batch loss 0.517817259 batch mAP 0.568817139 batch PCKh 0.75\n",
      "Trained batch 2255 batch loss 0.491159678 batch mAP 0.518035889 batch PCKh 0.0625\n",
      "Trained batch 2256 batch loss 0.404872596 batch mAP 0.559783936 batch PCKh 0\n",
      "Trained batch 2257 batch loss 0.414968491 batch mAP 0.577148438 batch PCKh 0.0625\n",
      "Trained batch 2258 batch loss 0.461451352 batch mAP 0.514678955 batch PCKh 0.4375\n",
      "Trained batch 2259 batch loss 0.379239798 batch mAP 0.613616943 batch PCKh 0.4375\n",
      "Trained batch 2260 batch loss 0.544406 batch mAP 0.62109375 batch PCKh 0.25\n",
      "Trained batch 2261 batch loss 0.462193638 batch mAP 0.655212402 batch PCKh 0.8125\n",
      "Trained batch 2262 batch loss 0.584378064 batch mAP 0.566925049 batch PCKh 0.375\n",
      "Trained batch 2263 batch loss 0.503027618 batch mAP 0.629058838 batch PCKh 0.75\n",
      "Trained batch 2264 batch loss 0.509327888 batch mAP 0.624328613 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2265 batch loss 0.511755586 batch mAP 0.638580322 batch PCKh 0.8125\n",
      "Trained batch 2266 batch loss 0.517484069 batch mAP 0.622467041 batch PCKh 0.1875\n",
      "Trained batch 2267 batch loss 0.510608196 batch mAP 0.65927124 batch PCKh 0.5\n",
      "Trained batch 2268 batch loss 0.51399076 batch mAP 0.690734863 batch PCKh 0.625\n",
      "Trained batch 2269 batch loss 0.450303972 batch mAP 0.687835693 batch PCKh 0.75\n",
      "Trained batch 2270 batch loss 0.510372519 batch mAP 0.484069824 batch PCKh 0.3125\n",
      "Trained batch 2271 batch loss 0.481790125 batch mAP 0.434082031 batch PCKh 0.625\n",
      "Trained batch 2272 batch loss 0.624602437 batch mAP 0.329986572 batch PCKh 0.375\n",
      "Trained batch 2273 batch loss 0.573364496 batch mAP 0.35043335 batch PCKh 0.5625\n",
      "Trained batch 2274 batch loss 0.525967777 batch mAP 0.502227783 batch PCKh 0.5625\n",
      "Trained batch 2275 batch loss 0.516654491 batch mAP 0.616699219 batch PCKh 0.375\n",
      "Trained batch 2276 batch loss 0.645600379 batch mAP 0.522857666 batch PCKh 0.25\n",
      "Trained batch 2277 batch loss 0.549827 batch mAP 0.583587646 batch PCKh 0.375\n",
      "Trained batch 2278 batch loss 0.456494063 batch mAP 0.597137451 batch PCKh 0.25\n",
      "Trained batch 2279 batch loss 0.434665024 batch mAP 0.61239624 batch PCKh 0.375\n",
      "Trained batch 2280 batch loss 0.518255711 batch mAP 0.580841064 batch PCKh 0.625\n",
      "Trained batch 2281 batch loss 0.434655875 batch mAP 0.620727539 batch PCKh 0.625\n",
      "Trained batch 2282 batch loss 0.546698809 batch mAP 0.595367432 batch PCKh 0.625\n",
      "Trained batch 2283 batch loss 0.525357902 batch mAP 0.602111816 batch PCKh 0.8125\n",
      "Trained batch 2284 batch loss 0.492040604 batch mAP 0.552429199 batch PCKh 0.375\n",
      "Trained batch 2285 batch loss 0.485160291 batch mAP 0.561523438 batch PCKh 0.3125\n",
      "Trained batch 2286 batch loss 0.593122244 batch mAP 0.575042725 batch PCKh 0.75\n",
      "Trained batch 2287 batch loss 0.532017231 batch mAP 0.56842041 batch PCKh 0.1875\n",
      "Trained batch 2288 batch loss 0.587955058 batch mAP 0.515930176 batch PCKh 0.6875\n",
      "Trained batch 2289 batch loss 0.678817689 batch mAP 0.483551025 batch PCKh 0.5\n",
      "Trained batch 2290 batch loss 0.655057073 batch mAP 0.550689697 batch PCKh 0.625\n",
      "Trained batch 2291 batch loss 0.682006538 batch mAP 0.513793945 batch PCKh 0.25\n",
      "Trained batch 2292 batch loss 0.668003917 batch mAP 0.458374023 batch PCKh 0.125\n",
      "Trained batch 2293 batch loss 0.537819505 batch mAP 0.392150879 batch PCKh 0.375\n",
      "Trained batch 2294 batch loss 0.566275597 batch mAP 0.457611084 batch PCKh 0.75\n",
      "Trained batch 2295 batch loss 0.451409817 batch mAP 0.425537109 batch PCKh 0.4375\n",
      "Trained batch 2296 batch loss 0.528395891 batch mAP 0.427856445 batch PCKh 0.5\n",
      "Trained batch 2297 batch loss 0.452705204 batch mAP 0.372161865 batch PCKh 0.5625\n",
      "Trained batch 2298 batch loss 0.592927814 batch mAP 0.365020752 batch PCKh 0.6875\n",
      "Trained batch 2299 batch loss 0.60787034 batch mAP 0.460388184 batch PCKh 0.4375\n",
      "Trained batch 2300 batch loss 0.523342907 batch mAP 0.529907227 batch PCKh 0.625\n",
      "Trained batch 2301 batch loss 0.568891048 batch mAP 0.546813965 batch PCKh 0.375\n",
      "Trained batch 2302 batch loss 0.609963417 batch mAP 0.542877197 batch PCKh 0.5625\n",
      "Trained batch 2303 batch loss 0.495004237 batch mAP 0.607635498 batch PCKh 0.8125\n",
      "Trained batch 2304 batch loss 0.592663586 batch mAP 0.520385742 batch PCKh 0.875\n",
      "Trained batch 2305 batch loss 0.587579 batch mAP 0.580291748 batch PCKh 0.1875\n",
      "Trained batch 2306 batch loss 0.561597109 batch mAP 0.545532227 batch PCKh 0.75\n",
      "Trained batch 2307 batch loss 0.545307577 batch mAP 0.655975342 batch PCKh 0.375\n",
      "Trained batch 2308 batch loss 0.507607758 batch mAP 0.687133789 batch PCKh 0.4375\n",
      "Trained batch 2309 batch loss 0.470619977 batch mAP 0.699981689 batch PCKh 0.6875\n",
      "Trained batch 2310 batch loss 0.473576069 batch mAP 0.633544922 batch PCKh 0.6875\n",
      "Trained batch 2311 batch loss 0.484903872 batch mAP 0.678436279 batch PCKh 0.75\n",
      "Trained batch 2312 batch loss 0.445330679 batch mAP 0.625061035 batch PCKh 0.75\n",
      "Trained batch 2313 batch loss 0.584650278 batch mAP 0.59979248 batch PCKh 0.4375\n",
      "Trained batch 2314 batch loss 0.485803366 batch mAP 0.699646 batch PCKh 0.75\n",
      "Trained batch 2315 batch loss 0.461416543 batch mAP 0.656707764 batch PCKh 0.375\n",
      "Trained batch 2316 batch loss 0.481827796 batch mAP 0.65612793 batch PCKh 0.5625\n",
      "Trained batch 2317 batch loss 0.495432913 batch mAP 0.676147461 batch PCKh 0.0625\n",
      "Trained batch 2318 batch loss 0.495633155 batch mAP 0.66229248 batch PCKh 0.5\n",
      "Trained batch 2319 batch loss 0.47563386 batch mAP 0.707244873 batch PCKh 0.4375\n",
      "Trained batch 2320 batch loss 0.408036351 batch mAP 0.660766602 batch PCKh 0.5\n",
      "Trained batch 2321 batch loss 0.505033731 batch mAP 0.658935547 batch PCKh 0.375\n",
      "Trained batch 2322 batch loss 0.508153796 batch mAP 0.664306641 batch PCKh 0.4375\n",
      "Trained batch 2323 batch loss 0.4658131 batch mAP 0.659332275 batch PCKh 0.6875\n",
      "Trained batch 2324 batch loss 0.520742238 batch mAP 0.63873291 batch PCKh 0.375\n",
      "Trained batch 2325 batch loss 0.592002153 batch mAP 0.610290527 batch PCKh 0.3125\n",
      "Trained batch 2326 batch loss 0.462090611 batch mAP 0.602935791 batch PCKh 0.25\n",
      "Trained batch 2327 batch loss 0.604301453 batch mAP 0.539825439 batch PCKh 0\n",
      "Trained batch 2328 batch loss 0.55620712 batch mAP 0.608673096 batch PCKh 0.625\n",
      "Trained batch 2329 batch loss 0.540395498 batch mAP 0.57522583 batch PCKh 0\n",
      "Trained batch 2330 batch loss 0.5034073 batch mAP 0.591827393 batch PCKh 0.625\n",
      "Trained batch 2331 batch loss 0.552366853 batch mAP 0.531097412 batch PCKh 0\n",
      "Trained batch 2332 batch loss 0.549910426 batch mAP 0.534881592 batch PCKh 0\n",
      "Trained batch 2333 batch loss 0.616469 batch mAP 0.531707764 batch PCKh 0.125\n",
      "Trained batch 2334 batch loss 0.56302911 batch mAP 0.495758057 batch PCKh 0\n",
      "Trained batch 2335 batch loss 0.494897664 batch mAP 0.61138916 batch PCKh 0.6875\n",
      "Trained batch 2336 batch loss 0.492559969 batch mAP 0.606262207 batch PCKh 0.3125\n",
      "Trained batch 2337 batch loss 0.465836 batch mAP 0.648773193 batch PCKh 0.5625\n",
      "Trained batch 2338 batch loss 0.490376174 batch mAP 0.588775635 batch PCKh 0.375\n",
      "Trained batch 2339 batch loss 0.575725198 batch mAP 0.591522217 batch PCKh 0.1875\n",
      "Trained batch 2340 batch loss 0.559481382 batch mAP 0.54876709 batch PCKh 0.5\n",
      "Trained batch 2341 batch loss 0.590653241 batch mAP 0.564544678 batch PCKh 0\n",
      "Trained batch 2342 batch loss 0.579855919 batch mAP 0.547973633 batch PCKh 0.875\n",
      "Trained batch 2343 batch loss 0.593772888 batch mAP 0.557067871 batch PCKh 0.875\n",
      "Trained batch 2344 batch loss 0.584738493 batch mAP 0.62387085 batch PCKh 0.25\n",
      "Trained batch 2345 batch loss 0.576732934 batch mAP 0.625274658 batch PCKh 0.6875\n",
      "Trained batch 2346 batch loss 0.542173 batch mAP 0.663269043 batch PCKh 0.4375\n",
      "Trained batch 2347 batch loss 0.553870499 batch mAP 0.635528564 batch PCKh 0.4375\n",
      "Trained batch 2348 batch loss 0.523501575 batch mAP 0.694793701 batch PCKh 0.5625\n",
      "Trained batch 2349 batch loss 0.534089923 batch mAP 0.664611816 batch PCKh 0.3125\n",
      "Trained batch 2350 batch loss 0.576764464 batch mAP 0.634735107 batch PCKh 0.75\n",
      "Trained batch 2351 batch loss 0.589411736 batch mAP 0.559295654 batch PCKh 0.625\n",
      "Trained batch 2352 batch loss 0.574933171 batch mAP 0.549072266 batch PCKh 0.5625\n",
      "Trained batch 2353 batch loss 0.563934565 batch mAP 0.578094482 batch PCKh 0.5625\n",
      "Trained batch 2354 batch loss 0.545161068 batch mAP 0.601989746 batch PCKh 0.5\n",
      "Trained batch 2355 batch loss 0.588009 batch mAP 0.596282959 batch PCKh 0.75\n",
      "Trained batch 2356 batch loss 0.518732429 batch mAP 0.537109375 batch PCKh 0.6875\n",
      "Trained batch 2357 batch loss 0.477399826 batch mAP 0.588439941 batch PCKh 0.625\n",
      "Trained batch 2358 batch loss 0.622966051 batch mAP 0.582489 batch PCKh 0.5625\n",
      "Trained batch 2359 batch loss 0.587932408 batch mAP 0.575317383 batch PCKh 0.5625\n",
      "Trained batch 2360 batch loss 0.576381803 batch mAP 0.565094 batch PCKh 0.625\n",
      "Trained batch 2361 batch loss 0.539697409 batch mAP 0.573638916 batch PCKh 0.625\n",
      "Trained batch 2362 batch loss 0.509052277 batch mAP 0.571594238 batch PCKh 0.5\n",
      "Trained batch 2363 batch loss 0.605354548 batch mAP 0.558807373 batch PCKh 0.625\n",
      "Trained batch 2364 batch loss 0.506936491 batch mAP 0.629608154 batch PCKh 0.8125\n",
      "Trained batch 2365 batch loss 0.597628355 batch mAP 0.563293457 batch PCKh 0.75\n",
      "Trained batch 2366 batch loss 0.605962873 batch mAP 0.547912598 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2367 batch loss 0.529653311 batch mAP 0.569488525 batch PCKh 0.6875\n",
      "Trained batch 2368 batch loss 0.674636483 batch mAP 0.514343262 batch PCKh 0.6875\n",
      "Trained batch 2369 batch loss 0.571222901 batch mAP 0.522827148 batch PCKh 0.625\n",
      "Trained batch 2370 batch loss 0.458134115 batch mAP 0.573883057 batch PCKh 0.5\n",
      "Trained batch 2371 batch loss 0.539279521 batch mAP 0.51260376 batch PCKh 0.75\n",
      "Trained batch 2372 batch loss 0.477301 batch mAP 0.5730896 batch PCKh 0.6875\n",
      "Trained batch 2373 batch loss 0.455001742 batch mAP 0.655517578 batch PCKh 0.5625\n",
      "Trained batch 2374 batch loss 0.451262534 batch mAP 0.620269775 batch PCKh 0.5\n",
      "Trained batch 2375 batch loss 0.443514347 batch mAP 0.630859375 batch PCKh 0.625\n",
      "Trained batch 2376 batch loss 0.450759977 batch mAP 0.547180176 batch PCKh 0.75\n",
      "Trained batch 2377 batch loss 0.529411793 batch mAP 0.6199646 batch PCKh 0.875\n",
      "Trained batch 2378 batch loss 0.490009576 batch mAP 0.689880371 batch PCKh 0.5625\n",
      "Trained batch 2379 batch loss 0.495498836 batch mAP 0.629272461 batch PCKh 0.5\n",
      "Trained batch 2380 batch loss 0.566787601 batch mAP 0.59677124 batch PCKh 0.625\n",
      "Trained batch 2381 batch loss 0.49789542 batch mAP 0.575531 batch PCKh 0.75\n",
      "Trained batch 2382 batch loss 0.493843675 batch mAP 0.584350586 batch PCKh 0.5625\n",
      "Trained batch 2383 batch loss 0.422907501 batch mAP 0.673584 batch PCKh 0.25\n",
      "Trained batch 2384 batch loss 0.464705467 batch mAP 0.644958496 batch PCKh 0.5625\n",
      "Trained batch 2385 batch loss 0.46922341 batch mAP 0.602203369 batch PCKh 0.75\n",
      "Trained batch 2386 batch loss 0.514739513 batch mAP 0.565490723 batch PCKh 0.6875\n",
      "Trained batch 2387 batch loss 0.553717494 batch mAP 0.54309082 batch PCKh 0.375\n",
      "Trained batch 2388 batch loss 0.577622056 batch mAP 0.576202393 batch PCKh 0.6875\n",
      "Trained batch 2389 batch loss 0.552281559 batch mAP 0.574768066 batch PCKh 0.5\n",
      "Trained batch 2390 batch loss 0.593062758 batch mAP 0.566101074 batch PCKh 0.8125\n",
      "Trained batch 2391 batch loss 0.542142749 batch mAP 0.492828369 batch PCKh 0.4375\n",
      "Trained batch 2392 batch loss 0.525254905 batch mAP 0.539154053 batch PCKh 0.6875\n",
      "Trained batch 2393 batch loss 0.595398903 batch mAP 0.448150635 batch PCKh 0\n",
      "Trained batch 2394 batch loss 0.605350316 batch mAP 0.520507812 batch PCKh 0.6875\n",
      "Trained batch 2395 batch loss 0.595064282 batch mAP 0.478149414 batch PCKh 0.5\n",
      "Trained batch 2396 batch loss 0.447920859 batch mAP 0.5418396 batch PCKh 0\n",
      "Trained batch 2397 batch loss 0.475876451 batch mAP 0.549255371 batch PCKh 0.5\n",
      "Trained batch 2398 batch loss 0.474709868 batch mAP 0.57711792 batch PCKh 0.5625\n",
      "Trained batch 2399 batch loss 0.380670667 batch mAP 0.688751221 batch PCKh 0.6875\n",
      "Trained batch 2400 batch loss 0.530953109 batch mAP 0.60546875 batch PCKh 0.6875\n",
      "Trained batch 2401 batch loss 0.502095699 batch mAP 0.661834717 batch PCKh 0.4375\n",
      "Trained batch 2402 batch loss 0.631705 batch mAP 0.61151123 batch PCKh 0.3125\n",
      "Trained batch 2403 batch loss 0.654636085 batch mAP 0.587036133 batch PCKh 0.5\n",
      "Trained batch 2404 batch loss 0.573280573 batch mAP 0.566558838 batch PCKh 0.3125\n",
      "Trained batch 2405 batch loss 0.651354551 batch mAP 0.41217041 batch PCKh 0.3125\n",
      "Trained batch 2406 batch loss 0.577225208 batch mAP 0.536193848 batch PCKh 0.625\n",
      "Trained batch 2407 batch loss 0.59579742 batch mAP 0.529785156 batch PCKh 0.3125\n",
      "Trained batch 2408 batch loss 0.603555381 batch mAP 0.575622559 batch PCKh 0.5625\n",
      "Trained batch 2409 batch loss 0.68792814 batch mAP 0.542938232 batch PCKh 0.1875\n",
      "Trained batch 2410 batch loss 0.565956116 batch mAP 0.558807373 batch PCKh 0.4375\n",
      "Trained batch 2411 batch loss 0.620245934 batch mAP 0.574066162 batch PCKh 0.1875\n",
      "Trained batch 2412 batch loss 0.559429288 batch mAP 0.587463379 batch PCKh 0.4375\n",
      "Trained batch 2413 batch loss 0.597208261 batch mAP 0.573120117 batch PCKh 0.1875\n",
      "Trained batch 2414 batch loss 0.574764371 batch mAP 0.584442139 batch PCKh 0.6875\n",
      "Trained batch 2415 batch loss 0.597398758 batch mAP 0.485717773 batch PCKh 0.6875\n",
      "Trained batch 2416 batch loss 0.567571759 batch mAP 0.558563232 batch PCKh 0.75\n",
      "Trained batch 2417 batch loss 0.541656852 batch mAP 0.568725586 batch PCKh 0.3125\n",
      "Trained batch 2418 batch loss 0.556469381 batch mAP 0.551269531 batch PCKh 0.3125\n",
      "Trained batch 2419 batch loss 0.631020546 batch mAP 0.577545166 batch PCKh 0.5625\n",
      "Trained batch 2420 batch loss 0.581764638 batch mAP 0.526397705 batch PCKh 0.6875\n",
      "Trained batch 2421 batch loss 0.526006043 batch mAP 0.561187744 batch PCKh 0.5\n",
      "Trained batch 2422 batch loss 0.644066 batch mAP 0.532897949 batch PCKh 0.4375\n",
      "Trained batch 2423 batch loss 0.596710503 batch mAP 0.489563 batch PCKh 0.6875\n",
      "Trained batch 2424 batch loss 0.634958625 batch mAP 0.572998047 batch PCKh 0.5625\n",
      "Trained batch 2425 batch loss 0.626369238 batch mAP 0.534729 batch PCKh 0.3125\n",
      "Trained batch 2426 batch loss 0.544979393 batch mAP 0.550689697 batch PCKh 0.375\n",
      "Trained batch 2427 batch loss 0.527979612 batch mAP 0.555084229 batch PCKh 0.3125\n",
      "Trained batch 2428 batch loss 0.57633388 batch mAP 0.489318848 batch PCKh 0.625\n",
      "Trained batch 2429 batch loss 0.53166616 batch mAP 0.567657471 batch PCKh 0.5625\n",
      "Trained batch 2430 batch loss 0.422410131 batch mAP 0.588745117 batch PCKh 0\n",
      "Trained batch 2431 batch loss 0.459583759 batch mAP 0.689758301 batch PCKh 0.4375\n",
      "Trained batch 2432 batch loss 0.416131675 batch mAP 0.690246582 batch PCKh 0.75\n",
      "Trained batch 2433 batch loss 0.561019182 batch mAP 0.596954346 batch PCKh 0.5\n",
      "Trained batch 2434 batch loss 0.612591565 batch mAP 0.625427246 batch PCKh 0.3125\n",
      "Trained batch 2435 batch loss 0.591334701 batch mAP 0.553314209 batch PCKh 0.5625\n",
      "Trained batch 2436 batch loss 0.582326889 batch mAP 0.545135498 batch PCKh 0.5625\n",
      "Trained batch 2437 batch loss 0.55890429 batch mAP 0.59475708 batch PCKh 0.5625\n",
      "Trained batch 2438 batch loss 0.59055239 batch mAP 0.599090576 batch PCKh 0.5\n",
      "Trained batch 2439 batch loss 0.581770241 batch mAP 0.570098877 batch PCKh 0.5625\n",
      "Trained batch 2440 batch loss 0.608806 batch mAP 0.574066162 batch PCKh 0.375\n",
      "Trained batch 2441 batch loss 0.582212687 batch mAP 0.593994141 batch PCKh 0.625\n",
      "Trained batch 2442 batch loss 0.584015846 batch mAP 0.585876465 batch PCKh 0.375\n",
      "Trained batch 2443 batch loss 0.532030523 batch mAP 0.574249268 batch PCKh 0.5\n",
      "Trained batch 2444 batch loss 0.525405467 batch mAP 0.638305664 batch PCKh 0.875\n",
      "Trained batch 2445 batch loss 0.453183174 batch mAP 0.609130859 batch PCKh 0.75\n",
      "Trained batch 2446 batch loss 0.470212609 batch mAP 0.565429688 batch PCKh 0.8125\n",
      "Trained batch 2447 batch loss 0.48134625 batch mAP 0.602081299 batch PCKh 0.8125\n",
      "Trained batch 2448 batch loss 0.471603692 batch mAP 0.610626221 batch PCKh 0.8125\n",
      "Trained batch 2449 batch loss 0.505306661 batch mAP 0.584747314 batch PCKh 0.6875\n",
      "Trained batch 2450 batch loss 0.547700107 batch mAP 0.609985352 batch PCKh 0.5625\n",
      "Trained batch 2451 batch loss 0.56580174 batch mAP 0.611938477 batch PCKh 0.6875\n",
      "Trained batch 2452 batch loss 0.529498 batch mAP 0.611877441 batch PCKh 0.375\n",
      "Trained batch 2453 batch loss 0.527205706 batch mAP 0.608978271 batch PCKh 0.6875\n",
      "Trained batch 2454 batch loss 0.544555247 batch mAP 0.596008301 batch PCKh 0.5\n",
      "Trained batch 2455 batch loss 0.61942637 batch mAP 0.492218018 batch PCKh 0.625\n",
      "Trained batch 2456 batch loss 0.603739738 batch mAP 0.52166748 batch PCKh 0.5625\n",
      "Trained batch 2457 batch loss 0.627441227 batch mAP 0.527618408 batch PCKh 0.125\n",
      "Trained batch 2458 batch loss 0.548719168 batch mAP 0.530639648 batch PCKh 0.625\n",
      "Trained batch 2459 batch loss 0.576383591 batch mAP 0.530578613 batch PCKh 0.4375\n",
      "Trained batch 2460 batch loss 0.615725398 batch mAP 0.534576416 batch PCKh 0.75\n",
      "Trained batch 2461 batch loss 0.654999495 batch mAP 0.441772461 batch PCKh 0.75\n",
      "Trained batch 2462 batch loss 0.605549 batch mAP 0.455780029 batch PCKh 0.75\n",
      "Trained batch 2463 batch loss 0.544170141 batch mAP 0.44430542 batch PCKh 0.25\n",
      "Trained batch 2464 batch loss 0.640010118 batch mAP 0.422515869 batch PCKh 0.6875\n",
      "Trained batch 2465 batch loss 0.640429 batch mAP 0.401062 batch PCKh 0.75\n",
      "Trained batch 2466 batch loss 0.594005346 batch mAP 0.43157959 batch PCKh 0.875\n",
      "Trained batch 2467 batch loss 0.575906634 batch mAP 0.377075195 batch PCKh 0.75\n",
      "Trained batch 2468 batch loss 0.599627435 batch mAP 0.479888916 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2469 batch loss 0.671813 batch mAP 0.471557617 batch PCKh 0.625\n",
      "Trained batch 2470 batch loss 0.667154789 batch mAP 0.499328613 batch PCKh 0.4375\n",
      "Trained batch 2471 batch loss 0.56981647 batch mAP 0.502105713 batch PCKh 0.4375\n",
      "Trained batch 2472 batch loss 0.540834069 batch mAP 0.468322754 batch PCKh 0.5\n",
      "Trained batch 2473 batch loss 0.557059646 batch mAP 0.544006348 batch PCKh 0.25\n",
      "Trained batch 2474 batch loss 0.632796645 batch mAP 0.490997314 batch PCKh 0.1875\n",
      "Trained batch 2475 batch loss 0.550193787 batch mAP 0.556793213 batch PCKh 0.375\n",
      "Trained batch 2476 batch loss 0.602226853 batch mAP 0.558197 batch PCKh 0.5625\n",
      "Trained batch 2477 batch loss 0.624114811 batch mAP 0.518798828 batch PCKh 0.3125\n",
      "Trained batch 2478 batch loss 0.519775212 batch mAP 0.564788818 batch PCKh 0.25\n",
      "Trained batch 2479 batch loss 0.417646706 batch mAP 0.612731934 batch PCKh 0.4375\n",
      "Trained batch 2480 batch loss 0.381949723 batch mAP 0.633850098 batch PCKh 0.0625\n",
      "Trained batch 2481 batch loss 0.511361778 batch mAP 0.635650635 batch PCKh 0.875\n",
      "Trained batch 2482 batch loss 0.534109712 batch mAP 0.641937256 batch PCKh 0.3125\n",
      "Trained batch 2483 batch loss 0.474085122 batch mAP 0.692596436 batch PCKh 0.625\n",
      "Trained batch 2484 batch loss 0.494251072 batch mAP 0.654693604 batch PCKh 0.5\n",
      "Trained batch 2485 batch loss 0.471438348 batch mAP 0.663604736 batch PCKh 0.3125\n",
      "Trained batch 2486 batch loss 0.49157548 batch mAP 0.665588379 batch PCKh 0.875\n",
      "Trained batch 2487 batch loss 0.476806372 batch mAP 0.60357666 batch PCKh 0.125\n",
      "Trained batch 2488 batch loss 0.522223115 batch mAP 0.6144104 batch PCKh 0.6875\n",
      "Trained batch 2489 batch loss 0.628498912 batch mAP 0.528503418 batch PCKh 0.1875\n",
      "Trained batch 2490 batch loss 0.568772376 batch mAP 0.523834229 batch PCKh 0.1875\n",
      "Trained batch 2491 batch loss 0.530515969 batch mAP 0.53616333 batch PCKh 0.5\n",
      "Trained batch 2492 batch loss 0.557377279 batch mAP 0.556854248 batch PCKh 0.0625\n",
      "Trained batch 2493 batch loss 0.468877375 batch mAP 0.658325195 batch PCKh 0.25\n",
      "Trained batch 2494 batch loss 0.595968783 batch mAP 0.619415283 batch PCKh 0.5625\n",
      "Trained batch 2495 batch loss 0.521965623 batch mAP 0.634216309 batch PCKh 0.8125\n",
      "Trained batch 2496 batch loss 0.603151321 batch mAP 0.565246582 batch PCKh 0.3125\n",
      "Trained batch 2497 batch loss 0.485236883 batch mAP 0.603881836 batch PCKh 0.5\n",
      "Trained batch 2498 batch loss 0.635119855 batch mAP 0.50390625 batch PCKh 0.5\n",
      "Trained batch 2499 batch loss 0.570334315 batch mAP 0.617767334 batch PCKh 0.5625\n",
      "Trained batch 2500 batch loss 0.742524803 batch mAP 0.521331787 batch PCKh 0\n",
      "Trained batch 2501 batch loss 0.689114094 batch mAP 0.547973633 batch PCKh 0.0625\n",
      "Trained batch 2502 batch loss 0.644816399 batch mAP 0.528533936 batch PCKh 0.0625\n",
      "Trained batch 2503 batch loss 0.728427291 batch mAP 0.527252197 batch PCKh 0\n",
      "Trained batch 2504 batch loss 0.646688104 batch mAP 0.518066406 batch PCKh 0.6875\n",
      "Trained batch 2505 batch loss 0.542489707 batch mAP 0.537841797 batch PCKh 0.375\n",
      "Trained batch 2506 batch loss 0.488477021 batch mAP 0.486053467 batch PCKh 0.3125\n",
      "Trained batch 2507 batch loss 0.487441719 batch mAP 0.539093 batch PCKh 0\n",
      "Trained batch 2508 batch loss 0.411835134 batch mAP 0.481201172 batch PCKh 0\n",
      "Trained batch 2509 batch loss 0.542862892 batch mAP 0.479156494 batch PCKh 0.25\n",
      "Trained batch 2510 batch loss 0.583357215 batch mAP 0.513366699 batch PCKh 0.625\n",
      "Trained batch 2511 batch loss 0.635277 batch mAP 0.508667 batch PCKh 0.3125\n",
      "Trained batch 2512 batch loss 0.68750149 batch mAP 0.456085205 batch PCKh 0.25\n",
      "Trained batch 2513 batch loss 0.629579425 batch mAP 0.420532227 batch PCKh 0.1875\n",
      "Trained batch 2514 batch loss 0.591091871 batch mAP 0.44631958 batch PCKh 0.4375\n",
      "Trained batch 2515 batch loss 0.621020138 batch mAP 0.471160889 batch PCKh 0.8125\n",
      "Trained batch 2516 batch loss 0.557170928 batch mAP 0.553436279 batch PCKh 0.75\n",
      "Trained batch 2517 batch loss 0.539335132 batch mAP 0.527679443 batch PCKh 0.625\n",
      "Trained batch 2518 batch loss 0.586015224 batch mAP 0.630126953 batch PCKh 0.3125\n",
      "Trained batch 2519 batch loss 0.562138557 batch mAP 0.60446167 batch PCKh 0.25\n",
      "Trained batch 2520 batch loss 0.608236 batch mAP 0.561035156 batch PCKh 0.5\n",
      "Trained batch 2521 batch loss 0.60559094 batch mAP 0.632507324 batch PCKh 0.25\n",
      "Trained batch 2522 batch loss 0.592639506 batch mAP 0.644683838 batch PCKh 0.3125\n",
      "Trained batch 2523 batch loss 0.545989394 batch mAP 0.655975342 batch PCKh 0.3125\n",
      "Trained batch 2524 batch loss 0.519224107 batch mAP 0.654998779 batch PCKh 0.3125\n",
      "Trained batch 2525 batch loss 0.545183659 batch mAP 0.644073486 batch PCKh 0.3125\n",
      "Trained batch 2526 batch loss 0.512069225 batch mAP 0.602264404 batch PCKh 0.625\n",
      "Trained batch 2527 batch loss 0.528171241 batch mAP 0.661071777 batch PCKh 0.375\n",
      "Trained batch 2528 batch loss 0.556469738 batch mAP 0.62512207 batch PCKh 0.375\n",
      "Trained batch 2529 batch loss 0.524004638 batch mAP 0.675018311 batch PCKh 0.5625\n",
      "Trained batch 2530 batch loss 0.496370554 batch mAP 0.642578125 batch PCKh 0.4375\n",
      "Trained batch 2531 batch loss 0.535169303 batch mAP 0.601989746 batch PCKh 0.125\n",
      "Trained batch 2532 batch loss 0.532937765 batch mAP 0.563171387 batch PCKh 0.5625\n",
      "Trained batch 2533 batch loss 0.523436725 batch mAP 0.549865723 batch PCKh 0.5625\n",
      "Trained batch 2534 batch loss 0.506164491 batch mAP 0.55456543 batch PCKh 0.75\n",
      "Trained batch 2535 batch loss 0.574403763 batch mAP 0.531005859 batch PCKh 0.8125\n",
      "Trained batch 2536 batch loss 0.53155303 batch mAP 0.55682373 batch PCKh 0.625\n",
      "Trained batch 2537 batch loss 0.459968716 batch mAP 0.571746826 batch PCKh 0.5\n",
      "Trained batch 2538 batch loss 0.489864498 batch mAP 0.582000732 batch PCKh 0.25\n",
      "Trained batch 2539 batch loss 0.545099199 batch mAP 0.571624756 batch PCKh 0.5625\n",
      "Trained batch 2540 batch loss 0.553390443 batch mAP 0.539520264 batch PCKh 0.75\n",
      "Trained batch 2541 batch loss 0.549290955 batch mAP 0.542327881 batch PCKh 0.5\n",
      "Trained batch 2542 batch loss 0.520608664 batch mAP 0.593841553 batch PCKh 0.5625\n",
      "Trained batch 2543 batch loss 0.466115 batch mAP 0.617584229 batch PCKh 0.875\n",
      "Trained batch 2544 batch loss 0.539203823 batch mAP 0.551116943 batch PCKh 0.375\n",
      "Trained batch 2545 batch loss 0.540309668 batch mAP 0.614807129 batch PCKh 0.5625\n",
      "Trained batch 2546 batch loss 0.531218767 batch mAP 0.597564697 batch PCKh 0.5625\n",
      "Trained batch 2547 batch loss 0.485910118 batch mAP 0.561248779 batch PCKh 0.1875\n",
      "Trained batch 2548 batch loss 0.517718315 batch mAP 0.5262146 batch PCKh 0.4375\n",
      "Trained batch 2549 batch loss 0.536721826 batch mAP 0.546722412 batch PCKh 0.5625\n",
      "Trained batch 2550 batch loss 0.487213492 batch mAP 0.593261719 batch PCKh 0.5625\n",
      "Trained batch 2551 batch loss 0.497230679 batch mAP 0.600067139 batch PCKh 0.1875\n",
      "Trained batch 2552 batch loss 0.494993508 batch mAP 0.636901855 batch PCKh 0.4375\n",
      "Trained batch 2553 batch loss 0.591697216 batch mAP 0.605224609 batch PCKh 0.25\n",
      "Trained batch 2554 batch loss 0.581539094 batch mAP 0.553253174 batch PCKh 0.1875\n",
      "Trained batch 2555 batch loss 0.521596849 batch mAP 0.620269775 batch PCKh 0.25\n",
      "Trained batch 2556 batch loss 0.578517139 batch mAP 0.498840332 batch PCKh 0.25\n",
      "Trained batch 2557 batch loss 0.53526938 batch mAP 0.546295166 batch PCKh 0.125\n",
      "Trained batch 2558 batch loss 0.622398317 batch mAP 0.531707764 batch PCKh 0.25\n",
      "Trained batch 2559 batch loss 0.555526793 batch mAP 0.539581299 batch PCKh 0.3125\n",
      "Trained batch 2560 batch loss 0.47727 batch mAP 0.543457031 batch PCKh 0.1875\n",
      "Trained batch 2561 batch loss 0.477659971 batch mAP 0.539123535 batch PCKh 0.0625\n",
      "Trained batch 2562 batch loss 0.578732252 batch mAP 0.571472168 batch PCKh 0.125\n",
      "Trained batch 2563 batch loss 0.615752816 batch mAP 0.47177124 batch PCKh 0.1875\n",
      "Trained batch 2564 batch loss 0.638861656 batch mAP 0.449462891 batch PCKh 0.5\n",
      "Trained batch 2565 batch loss 0.688844621 batch mAP 0.483886719 batch PCKh 0.3125\n",
      "Trained batch 2566 batch loss 0.589391887 batch mAP 0.576751709 batch PCKh 0.125\n",
      "Trained batch 2567 batch loss 0.486086637 batch mAP 0.578857422 batch PCKh 0.0625\n",
      "Trained batch 2568 batch loss 0.565999746 batch mAP 0.54006958 batch PCKh 0.3125\n",
      "Trained batch 2569 batch loss 0.457155228 batch mAP 0.591766357 batch PCKh 0.3125\n",
      "Trained batch 2570 batch loss 0.430935979 batch mAP 0.592773438 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2571 batch loss 0.407241791 batch mAP 0.602294922 batch PCKh 0.625\n",
      "Trained batch 2572 batch loss 0.512260318 batch mAP 0.565673828 batch PCKh 0.0625\n",
      "Trained batch 2573 batch loss 0.459811628 batch mAP 0.577575684 batch PCKh 0.25\n",
      "Trained batch 2574 batch loss 0.571155787 batch mAP 0.645721436 batch PCKh 0.1875\n",
      "Trained batch 2575 batch loss 0.514731765 batch mAP 0.640197754 batch PCKh 0.625\n",
      "Trained batch 2576 batch loss 0.641068697 batch mAP 0.562591553 batch PCKh 0.75\n",
      "Trained batch 2577 batch loss 0.672937155 batch mAP 0.529602051 batch PCKh 0.0625\n",
      "Trained batch 2578 batch loss 0.615078449 batch mAP 0.568145752 batch PCKh 0.0625\n",
      "Trained batch 2579 batch loss 0.579576969 batch mAP 0.546691895 batch PCKh 0.4375\n",
      "Trained batch 2580 batch loss 0.488619447 batch mAP 0.627655 batch PCKh 0.1875\n",
      "Trained batch 2581 batch loss 0.571449935 batch mAP 0.592468262 batch PCKh 0.3125\n",
      "Trained batch 2582 batch loss 0.561003149 batch mAP 0.63381958 batch PCKh 0.5625\n",
      "Trained batch 2583 batch loss 0.514163315 batch mAP 0.612304688 batch PCKh 0.375\n",
      "Trained batch 2584 batch loss 0.470980465 batch mAP 0.6378479 batch PCKh 0.1875\n",
      "Trained batch 2585 batch loss 0.540728927 batch mAP 0.588134766 batch PCKh 0.375\n",
      "Trained batch 2586 batch loss 0.602437735 batch mAP 0.440338135 batch PCKh 0\n",
      "Trained batch 2587 batch loss 0.505137086 batch mAP 0.535430908 batch PCKh 0.4375\n",
      "Trained batch 2588 batch loss 0.54284507 batch mAP 0.52166748 batch PCKh 0.5\n",
      "Trained batch 2589 batch loss 0.539659142 batch mAP 0.548034668 batch PCKh 0.125\n",
      "Trained batch 2590 batch loss 0.579721 batch mAP 0.456542969 batch PCKh 0.25\n",
      "Trained batch 2591 batch loss 0.605225205 batch mAP 0.574005127 batch PCKh 0.8125\n",
      "Trained batch 2592 batch loss 0.605087757 batch mAP 0.571380615 batch PCKh 0\n",
      "Trained batch 2593 batch loss 0.600925207 batch mAP 0.577423096 batch PCKh 0.625\n",
      "Trained batch 2594 batch loss 0.557088614 batch mAP 0.557037354 batch PCKh 0.8125\n",
      "Trained batch 2595 batch loss 0.531446457 batch mAP 0.554992676 batch PCKh 0.875\n",
      "Trained batch 2596 batch loss 0.421527594 batch mAP 0.503753662 batch PCKh 0.5625\n",
      "Trained batch 2597 batch loss 0.470845044 batch mAP 0.528137207 batch PCKh 0.5625\n",
      "Trained batch 2598 batch loss 0.533344269 batch mAP 0.537780762 batch PCKh 0.375\n",
      "Trained batch 2599 batch loss 0.440554798 batch mAP 0.591766357 batch PCKh 0.25\n",
      "Trained batch 2600 batch loss 0.505794048 batch mAP 0.492095947 batch PCKh 0.5\n",
      "Trained batch 2601 batch loss 0.476676047 batch mAP 0.531951904 batch PCKh 0.375\n",
      "Trained batch 2602 batch loss 0.612021 batch mAP 0.554840088 batch PCKh 0.5625\n",
      "Trained batch 2603 batch loss 0.663137376 batch mAP 0.501251221 batch PCKh 0.4375\n",
      "Trained batch 2604 batch loss 0.511581242 batch mAP 0.563964844 batch PCKh 0.1875\n",
      "Trained batch 2605 batch loss 0.525090754 batch mAP 0.598999 batch PCKh 0.125\n",
      "Trained batch 2606 batch loss 0.53020072 batch mAP 0.584014893 batch PCKh 0.4375\n",
      "Trained batch 2607 batch loss 0.593037903 batch mAP 0.480651855 batch PCKh 0.25\n",
      "Trained batch 2608 batch loss 0.560877204 batch mAP 0.478179932 batch PCKh 0.125\n",
      "Trained batch 2609 batch loss 0.60792923 batch mAP 0.470123291 batch PCKh 0\n",
      "Trained batch 2610 batch loss 0.579186499 batch mAP 0.538787842 batch PCKh 0.5\n",
      "Trained batch 2611 batch loss 0.607417107 batch mAP 0.558044434 batch PCKh 0.875\n",
      "Trained batch 2612 batch loss 0.634265363 batch mAP 0.484771729 batch PCKh 0.6875\n",
      "Trained batch 2613 batch loss 0.662831724 batch mAP 0.55581665 batch PCKh 0.8125\n",
      "Trained batch 2614 batch loss 0.636243 batch mAP 0.485351562 batch PCKh 0.0625\n",
      "Trained batch 2615 batch loss 0.62810266 batch mAP 0.554138184 batch PCKh 0.3125\n",
      "Trained batch 2616 batch loss 0.662293077 batch mAP 0.540130615 batch PCKh 0.5\n",
      "Trained batch 2617 batch loss 0.55865109 batch mAP 0.563842773 batch PCKh 0.8125\n",
      "Trained batch 2618 batch loss 0.508469403 batch mAP 0.554199219 batch PCKh 0.75\n",
      "Trained batch 2619 batch loss 0.600081 batch mAP 0.489715576 batch PCKh 0.4375\n",
      "Trained batch 2620 batch loss 0.535478175 batch mAP 0.561218262 batch PCKh 0.6875\n",
      "Trained batch 2621 batch loss 0.545492649 batch mAP 0.585510254 batch PCKh 0.75\n",
      "Trained batch 2622 batch loss 0.500366211 batch mAP 0.508636475 batch PCKh 0.6875\n",
      "Trained batch 2623 batch loss 0.555102229 batch mAP 0.554138184 batch PCKh 0.5\n",
      "Trained batch 2624 batch loss 0.578996181 batch mAP 0.525482178 batch PCKh 0.0625\n",
      "Trained batch 2625 batch loss 0.595145822 batch mAP 0.621887207 batch PCKh 0.375\n",
      "Trained batch 2626 batch loss 0.57689786 batch mAP 0.614227295 batch PCKh 0.4375\n",
      "Trained batch 2627 batch loss 0.600210309 batch mAP 0.534454346 batch PCKh 0.375\n",
      "Trained batch 2628 batch loss 0.651735067 batch mAP 0.54095459 batch PCKh 0.25\n",
      "Trained batch 2629 batch loss 0.629107535 batch mAP 0.576904297 batch PCKh 0.375\n",
      "Trained batch 2630 batch loss 0.59418565 batch mAP 0.547393799 batch PCKh 0.4375\n",
      "Trained batch 2631 batch loss 0.618884802 batch mAP 0.597015381 batch PCKh 0.25\n",
      "Trained batch 2632 batch loss 0.60823977 batch mAP 0.641052246 batch PCKh 0.1875\n",
      "Trained batch 2633 batch loss 0.649475873 batch mAP 0.636566162 batch PCKh 0.375\n",
      "Trained batch 2634 batch loss 0.619645417 batch mAP 0.585693359 batch PCKh 0.25\n",
      "Trained batch 2635 batch loss 0.643793285 batch mAP 0.549285889 batch PCKh 0.4375\n",
      "Trained batch 2636 batch loss 0.625197768 batch mAP 0.568084717 batch PCKh 0.6875\n",
      "Trained batch 2637 batch loss 0.683796346 batch mAP 0.570159912 batch PCKh 0.5625\n",
      "Trained batch 2638 batch loss 0.661608636 batch mAP 0.552978516 batch PCKh 0.5625\n",
      "Trained batch 2639 batch loss 0.566293716 batch mAP 0.549041748 batch PCKh 0.6875\n",
      "Trained batch 2640 batch loss 0.631222844 batch mAP 0.550689697 batch PCKh 0.375\n",
      "Trained batch 2641 batch loss 0.612164378 batch mAP 0.571411133 batch PCKh 0.625\n",
      "Trained batch 2642 batch loss 0.583557427 batch mAP 0.546661377 batch PCKh 0.75\n",
      "Trained batch 2643 batch loss 0.629489183 batch mAP 0.59198 batch PCKh 0.4375\n",
      "Trained batch 2644 batch loss 0.591242 batch mAP 0.538208 batch PCKh 0.8125\n",
      "Trained batch 2645 batch loss 0.552551866 batch mAP 0.501617432 batch PCKh 0.1875\n",
      "Trained batch 2646 batch loss 0.626722455 batch mAP 0.530517578 batch PCKh 0.3125\n",
      "Trained batch 2647 batch loss 0.634677768 batch mAP 0.516021729 batch PCKh 0.125\n",
      "Trained batch 2648 batch loss 0.565810859 batch mAP 0.532989502 batch PCKh 0.375\n",
      "Trained batch 2649 batch loss 0.581188202 batch mAP 0.499053955 batch PCKh 0.625\n",
      "Trained batch 2650 batch loss 0.510162234 batch mAP 0.527099609 batch PCKh 0.6875\n",
      "Trained batch 2651 batch loss 0.591747463 batch mAP 0.555847168 batch PCKh 0.5625\n",
      "Trained batch 2652 batch loss 0.5700019 batch mAP 0.563598633 batch PCKh 0.4375\n",
      "Trained batch 2653 batch loss 0.668684 batch mAP 0.512115479 batch PCKh 0.25\n",
      "Trained batch 2654 batch loss 0.560501218 batch mAP 0.565887451 batch PCKh 0.6875\n",
      "Trained batch 2655 batch loss 0.485301018 batch mAP 0.527526855 batch PCKh 0.5\n",
      "Trained batch 2656 batch loss 0.426204413 batch mAP 0.630889893 batch PCKh 0.75\n",
      "Trained batch 2657 batch loss 0.463482916 batch mAP 0.598999 batch PCKh 0.4375\n",
      "Trained batch 2658 batch loss 0.517450452 batch mAP 0.513305664 batch PCKh 0.0625\n",
      "Trained batch 2659 batch loss 0.563946486 batch mAP 0.533294678 batch PCKh 0.125\n",
      "Trained batch 2660 batch loss 0.542670548 batch mAP 0.557952881 batch PCKh 0.625\n",
      "Trained batch 2661 batch loss 0.565355659 batch mAP 0.524841309 batch PCKh 0.125\n",
      "Trained batch 2662 batch loss 0.597335041 batch mAP 0.483703613 batch PCKh 0.5625\n",
      "Trained batch 2663 batch loss 0.619094372 batch mAP 0.536468506 batch PCKh 0.5625\n",
      "Trained batch 2664 batch loss 0.544213653 batch mAP 0.568603516 batch PCKh 0.125\n",
      "Trained batch 2665 batch loss 0.534248292 batch mAP 0.519775391 batch PCKh 0.3125\n",
      "Trained batch 2666 batch loss 0.53676033 batch mAP 0.615966797 batch PCKh 0.875\n",
      "Trained batch 2667 batch loss 0.516398072 batch mAP 0.620117188 batch PCKh 0.6875\n",
      "Trained batch 2668 batch loss 0.609394193 batch mAP 0.59777832 batch PCKh 0.4375\n",
      "Trained batch 2669 batch loss 0.626683831 batch mAP 0.655761719 batch PCKh 0.6875\n",
      "Trained batch 2670 batch loss 0.561267495 batch mAP 0.620788574 batch PCKh 0.5\n",
      "Trained batch 2671 batch loss 0.497151732 batch mAP 0.571533203 batch PCKh 0.625\n",
      "Trained batch 2672 batch loss 0.51184994 batch mAP 0.5390625 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2673 batch loss 0.475042284 batch mAP 0.582977295 batch PCKh 0.6875\n",
      "Trained batch 2674 batch loss 0.507566571 batch mAP 0.574707031 batch PCKh 0.5625\n",
      "Trained batch 2675 batch loss 0.549214959 batch mAP 0.551513672 batch PCKh 0.5\n",
      "Trained batch 2676 batch loss 0.580509186 batch mAP 0.513427734 batch PCKh 0.4375\n",
      "Trained batch 2677 batch loss 0.56907773 batch mAP 0.497344971 batch PCKh 0.4375\n",
      "Trained batch 2678 batch loss 0.51154542 batch mAP 0.528595 batch PCKh 0.6875\n",
      "Trained batch 2679 batch loss 0.513683081 batch mAP 0.587982178 batch PCKh 0.4375\n",
      "Trained batch 2680 batch loss 0.618685842 batch mAP 0.540008545 batch PCKh 0.25\n",
      "Trained batch 2681 batch loss 0.567936897 batch mAP 0.534606934 batch PCKh 0.6875\n",
      "Trained batch 2682 batch loss 0.551892579 batch mAP 0.568084717 batch PCKh 0.3125\n",
      "Trained batch 2683 batch loss 0.559001267 batch mAP 0.537109375 batch PCKh 0.6875\n",
      "Trained batch 2684 batch loss 0.506978 batch mAP 0.62310791 batch PCKh 0.625\n",
      "Trained batch 2685 batch loss 0.457142979 batch mAP 0.629821777 batch PCKh 0.625\n",
      "Trained batch 2686 batch loss 0.466032773 batch mAP 0.516662598 batch PCKh 0.375\n",
      "Trained batch 2687 batch loss 0.528254747 batch mAP 0.536834717 batch PCKh 0.125\n",
      "Trained batch 2688 batch loss 0.634433806 batch mAP 0.477386475 batch PCKh 0.25\n",
      "Trained batch 2689 batch loss 0.596235156 batch mAP 0.559783936 batch PCKh 0.625\n",
      "Trained batch 2690 batch loss 0.528294325 batch mAP 0.55255127 batch PCKh 0.875\n",
      "Trained batch 2691 batch loss 0.651566565 batch mAP 0.422668457 batch PCKh 0.875\n",
      "Trained batch 2692 batch loss 0.530638695 batch mAP 0.511566162 batch PCKh 0.625\n",
      "Trained batch 2693 batch loss 0.592446089 batch mAP 0.516296387 batch PCKh 0\n",
      "Trained batch 2694 batch loss 0.536609173 batch mAP 0.584960938 batch PCKh 0.5625\n",
      "Trained batch 2695 batch loss 0.545019031 batch mAP 0.581390381 batch PCKh 0.5\n",
      "Trained batch 2696 batch loss 0.526547194 batch mAP 0.661499 batch PCKh 0.4375\n",
      "Trained batch 2697 batch loss 0.463729739 batch mAP 0.658569336 batch PCKh 0.375\n",
      "Trained batch 2698 batch loss 0.543237865 batch mAP 0.638946533 batch PCKh 0.25\n",
      "Trained batch 2699 batch loss 0.503535688 batch mAP 0.666473389 batch PCKh 0.625\n",
      "Trained batch 2700 batch loss 0.531138301 batch mAP 0.563537598 batch PCKh 0.5625\n",
      "Trained batch 2701 batch loss 0.583173275 batch mAP 0.569854736 batch PCKh 0.25\n",
      "Trained batch 2702 batch loss 0.586284399 batch mAP 0.597351074 batch PCKh 0.1875\n",
      "Trained batch 2703 batch loss 0.56112045 batch mAP 0.554962158 batch PCKh 0.75\n",
      "Trained batch 2704 batch loss 0.536795 batch mAP 0.554046631 batch PCKh 0.5625\n",
      "Trained batch 2705 batch loss 0.58418107 batch mAP 0.503601074 batch PCKh 0.5625\n",
      "Trained batch 2706 batch loss 0.551149 batch mAP 0.51763916 batch PCKh 0.4375\n",
      "Trained batch 2707 batch loss 0.550664783 batch mAP 0.538818359 batch PCKh 0.5\n",
      "Trained batch 2708 batch loss 0.595467806 batch mAP 0.572265625 batch PCKh 0.625\n",
      "Trained batch 2709 batch loss 0.597923398 batch mAP 0.522399902 batch PCKh 0.6875\n",
      "Trained batch 2710 batch loss 0.573706865 batch mAP 0.559783936 batch PCKh 0.875\n",
      "Trained batch 2711 batch loss 0.619339705 batch mAP 0.577880859 batch PCKh 0.25\n",
      "Trained batch 2712 batch loss 0.558026075 batch mAP 0.613708496 batch PCKh 0.25\n",
      "Trained batch 2713 batch loss 0.69976747 batch mAP 0.531890869 batch PCKh 0.1875\n",
      "Trained batch 2714 batch loss 0.650978804 batch mAP 0.524963379 batch PCKh 0.4375\n",
      "Trained batch 2715 batch loss 0.545073032 batch mAP 0.55657959 batch PCKh 0.5625\n",
      "Trained batch 2716 batch loss 0.634936452 batch mAP 0.511779785 batch PCKh 0\n",
      "Trained batch 2717 batch loss 0.577055335 batch mAP 0.537200928 batch PCKh 0.375\n",
      "Trained batch 2718 batch loss 0.661766291 batch mAP 0.544281 batch PCKh 0.5625\n",
      "Trained batch 2719 batch loss 0.554831684 batch mAP 0.545440674 batch PCKh 0.25\n",
      "Trained batch 2720 batch loss 0.483484149 batch mAP 0.413635254 batch PCKh 0.25\n",
      "Trained batch 2721 batch loss 0.564283967 batch mAP 0.567901611 batch PCKh 0.4375\n",
      "Trained batch 2722 batch loss 0.43996489 batch mAP 0.585907 batch PCKh 0.1875\n",
      "Trained batch 2723 batch loss 0.55260551 batch mAP 0.574188232 batch PCKh 0.375\n",
      "Trained batch 2724 batch loss 0.541884065 batch mAP 0.597961426 batch PCKh 0.375\n",
      "Trained batch 2725 batch loss 0.549708903 batch mAP 0.603210449 batch PCKh 0.375\n",
      "Trained batch 2726 batch loss 0.47805953 batch mAP 0.56942749 batch PCKh 0.625\n",
      "Trained batch 2727 batch loss 0.446838379 batch mAP 0.644592285 batch PCKh 0.1875\n",
      "Trained batch 2728 batch loss 0.481738836 batch mAP 0.596069336 batch PCKh 0.1875\n",
      "Trained batch 2729 batch loss 0.468139 batch mAP 0.624023438 batch PCKh 0.375\n",
      "Trained batch 2730 batch loss 0.57815671 batch mAP 0.594909668 batch PCKh 0.6875\n",
      "Trained batch 2731 batch loss 0.578179061 batch mAP 0.602233887 batch PCKh 0.3125\n",
      "Trained batch 2732 batch loss 0.496714234 batch mAP 0.66204834 batch PCKh 0.5\n",
      "Trained batch 2733 batch loss 0.551033139 batch mAP 0.606292725 batch PCKh 0.3125\n",
      "Trained batch 2734 batch loss 0.486879 batch mAP 0.604187 batch PCKh 0.5625\n",
      "Trained batch 2735 batch loss 0.526942909 batch mAP 0.606811523 batch PCKh 0.4375\n",
      "Trained batch 2736 batch loss 0.56299603 batch mAP 0.619995117 batch PCKh 0.375\n",
      "Trained batch 2737 batch loss 0.65394032 batch mAP 0.517089844 batch PCKh 0.6875\n",
      "Trained batch 2738 batch loss 0.648042798 batch mAP 0.569641113 batch PCKh 0.0625\n",
      "Trained batch 2739 batch loss 0.659077406 batch mAP 0.513275146 batch PCKh 0\n",
      "Trained batch 2740 batch loss 0.577174187 batch mAP 0.58215332 batch PCKh 0.3125\n",
      "Trained batch 2741 batch loss 0.542759418 batch mAP 0.627716064 batch PCKh 0.4375\n",
      "Trained batch 2742 batch loss 0.605047107 batch mAP 0.562347412 batch PCKh 0.375\n",
      "Trained batch 2743 batch loss 0.592250586 batch mAP 0.567474365 batch PCKh 0.3125\n",
      "Trained batch 2744 batch loss 0.550299585 batch mAP 0.521484375 batch PCKh 0.5625\n",
      "Trained batch 2745 batch loss 0.535264432 batch mAP 0.577575684 batch PCKh 0.6875\n",
      "Trained batch 2746 batch loss 0.510199487 batch mAP 0.510864258 batch PCKh 0.5\n",
      "Trained batch 2747 batch loss 0.530855179 batch mAP 0.487487793 batch PCKh 0.6875\n",
      "Trained batch 2748 batch loss 0.514145195 batch mAP 0.56060791 batch PCKh 0.5625\n",
      "Trained batch 2749 batch loss 0.418223172 batch mAP 0.628448486 batch PCKh 0.125\n",
      "Trained batch 2750 batch loss 0.575474322 batch mAP 0.581085205 batch PCKh 0.4375\n",
      "Trained batch 2751 batch loss 0.553483486 batch mAP 0.600463867 batch PCKh 0.4375\n",
      "Trained batch 2752 batch loss 0.595610619 batch mAP 0.599975586 batch PCKh 0.4375\n",
      "Trained batch 2753 batch loss 0.51005125 batch mAP 0.641204834 batch PCKh 0.75\n",
      "Trained batch 2754 batch loss 0.579181671 batch mAP 0.585754395 batch PCKh 0.6875\n",
      "Trained batch 2755 batch loss 0.466776818 batch mAP 0.599853516 batch PCKh 0.375\n",
      "Trained batch 2756 batch loss 0.473079324 batch mAP 0.640106201 batch PCKh 0.4375\n",
      "Trained batch 2757 batch loss 0.490498841 batch mAP 0.586242676 batch PCKh 0.0625\n",
      "Trained batch 2758 batch loss 0.454767585 batch mAP 0.581665039 batch PCKh 0.25\n",
      "Trained batch 2759 batch loss 0.362887233 batch mAP 0.71307373 batch PCKh 0.4375\n",
      "Trained batch 2760 batch loss 0.475562334 batch mAP 0.698028564 batch PCKh 0.375\n",
      "Trained batch 2761 batch loss 0.407849 batch mAP 0.719970703 batch PCKh 0.375\n",
      "Trained batch 2762 batch loss 0.361099809 batch mAP 0.721435547 batch PCKh 0.3125\n",
      "Trained batch 2763 batch loss 0.482353717 batch mAP 0.663543701 batch PCKh 0.5625\n",
      "Trained batch 2764 batch loss 0.644538701 batch mAP 0.604705811 batch PCKh 0.25\n",
      "Trained batch 2765 batch loss 0.628076 batch mAP 0.617004395 batch PCKh 0.4375\n",
      "Trained batch 2766 batch loss 0.665207148 batch mAP 0.553161621 batch PCKh 0.75\n",
      "Trained batch 2767 batch loss 0.652140081 batch mAP 0.56149292 batch PCKh 0.5625\n",
      "Trained batch 2768 batch loss 0.657104611 batch mAP 0.563781738 batch PCKh 0.5625\n",
      "Trained batch 2769 batch loss 0.732766867 batch mAP 0.518157959 batch PCKh 0.5625\n",
      "Trained batch 2770 batch loss 0.54610455 batch mAP 0.593414307 batch PCKh 0.8125\n",
      "Trained batch 2771 batch loss 0.496485889 batch mAP 0.51159668 batch PCKh 0.75\n",
      "Trained batch 2772 batch loss 0.601309776 batch mAP 0.456970215 batch PCKh 0.75\n",
      "Trained batch 2773 batch loss 0.564023316 batch mAP 0.495239258 batch PCKh 0.3125\n",
      "Trained batch 2774 batch loss 0.649069071 batch mAP 0.431091309 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2775 batch loss 0.655404806 batch mAP 0.437225342 batch PCKh 0.3125\n",
      "Trained batch 2776 batch loss 0.63834548 batch mAP 0.468994141 batch PCKh 0.1875\n",
      "Epoch 6 train loss 0.5443453192710876 train mAP 0.5751921534538269 train PCKh\n",
      "Validated batch 1 batch loss 0.62649405 batch mAP 0.337158203 batch PCKh 0.4375\n",
      "Validated batch 2 batch loss 0.637848 batch mAP 0.361877441 batch PCKh 0.8125\n",
      "Validated batch 3 batch loss 0.610270679 batch mAP 0.158874512 batch PCKh 0.625\n",
      "Validated batch 4 batch loss 0.625924408 batch mAP 0.362213135 batch PCKh 0.5625\n",
      "Validated batch 5 batch loss 0.641515493 batch mAP 0.360900879 batch PCKh 0.5625\n",
      "Validated batch 6 batch loss 0.512243 batch mAP 0.449310303 batch PCKh 0.3125\n",
      "Validated batch 7 batch loss 0.597960174 batch mAP 0.374084473 batch PCKh 0.3125\n",
      "Validated batch 8 batch loss 0.618738055 batch mAP 0.391387939 batch PCKh 0.0625\n",
      "Validated batch 9 batch loss 0.627069116 batch mAP 0.32220459 batch PCKh 0.25\n",
      "Validated batch 10 batch loss 0.602493405 batch mAP 0.422790527 batch PCKh 0.1875\n",
      "Validated batch 11 batch loss 0.63566792 batch mAP 0.425384521 batch PCKh 0.875\n",
      "Validated batch 12 batch loss 0.669113636 batch mAP 0.417877197 batch PCKh 0.3125\n",
      "Validated batch 13 batch loss 0.657935441 batch mAP 0.190826416 batch PCKh 0.3125\n",
      "Validated batch 14 batch loss 0.691782832 batch mAP 0.389190674 batch PCKh 0.125\n",
      "Validated batch 15 batch loss 0.663798153 batch mAP 0.327514648 batch PCKh 0.125\n",
      "Validated batch 16 batch loss 0.615155935 batch mAP 0.409240723 batch PCKh 0.5625\n",
      "Validated batch 17 batch loss 0.668719 batch mAP 0.286193848 batch PCKh 0\n",
      "Validated batch 18 batch loss 0.626903355 batch mAP 0.255340576 batch PCKh 0.5625\n",
      "Validated batch 19 batch loss 0.57209605 batch mAP 0.39553833 batch PCKh 0.25\n",
      "Validated batch 20 batch loss 0.579863727 batch mAP 0.52923584 batch PCKh 0.4375\n",
      "Validated batch 21 batch loss 0.614616394 batch mAP 0.398895264 batch PCKh 0.375\n",
      "Validated batch 22 batch loss 0.655338049 batch mAP 0.413024902 batch PCKh 0.4375\n",
      "Validated batch 23 batch loss 0.553348899 batch mAP 0.465637207 batch PCKh 0.0625\n",
      "Validated batch 24 batch loss 0.679130495 batch mAP 0.180969238 batch PCKh 0.25\n",
      "Validated batch 25 batch loss 0.606490791 batch mAP 0.324798584 batch PCKh 0\n",
      "Validated batch 26 batch loss 0.677951694 batch mAP 0.348724365 batch PCKh 0.5\n",
      "Validated batch 27 batch loss 0.673511326 batch mAP 0.292633057 batch PCKh 0\n",
      "Validated batch 28 batch loss 0.733485281 batch mAP 0.281036377 batch PCKh 0\n",
      "Validated batch 29 batch loss 0.689226687 batch mAP 0.277801514 batch PCKh 0\n",
      "Validated batch 30 batch loss 0.609047413 batch mAP 0.352386475 batch PCKh 0.5625\n",
      "Validated batch 31 batch loss 0.629437447 batch mAP 0.359527588 batch PCKh 0.3125\n",
      "Validated batch 32 batch loss 0.614298224 batch mAP 0.392211914 batch PCKh 0.4375\n",
      "Validated batch 33 batch loss 0.699118793 batch mAP 0.267578125 batch PCKh 0.0625\n",
      "Validated batch 34 batch loss 0.742491543 batch mAP 0.221130371 batch PCKh 0\n",
      "Validated batch 35 batch loss 0.538235247 batch mAP 0.285369873 batch PCKh 0.4375\n",
      "Validated batch 36 batch loss 0.576049626 batch mAP 0.439086914 batch PCKh 0.125\n",
      "Validated batch 37 batch loss 0.660993934 batch mAP 0.215820312 batch PCKh 0.125\n",
      "Validated batch 38 batch loss 0.739720345 batch mAP 0.35168457 batch PCKh 0.1875\n",
      "Validated batch 39 batch loss 0.533685207 batch mAP 0.423919678 batch PCKh 0.375\n",
      "Validated batch 40 batch loss 0.577939868 batch mAP 0.300811768 batch PCKh 0.1875\n",
      "Validated batch 41 batch loss 0.61304307 batch mAP 0.27645874 batch PCKh 0.5\n",
      "Validated batch 42 batch loss 0.70892942 batch mAP 0.281768799 batch PCKh 0.25\n",
      "Validated batch 43 batch loss 0.603095949 batch mAP 0.508636475 batch PCKh 0.625\n",
      "Validated batch 44 batch loss 0.671169281 batch mAP 0.383880615 batch PCKh 0.625\n",
      "Validated batch 45 batch loss 0.54649508 batch mAP 0.379211426 batch PCKh 0.4375\n",
      "Validated batch 46 batch loss 0.606241345 batch mAP 0.409362793 batch PCKh 0.5\n",
      "Validated batch 47 batch loss 0.573692203 batch mAP 0.357849121 batch PCKh 0.625\n",
      "Validated batch 48 batch loss 0.595000923 batch mAP 0.432189941 batch PCKh 0.3125\n",
      "Validated batch 49 batch loss 0.648225486 batch mAP 0.417480469 batch PCKh 0.4375\n",
      "Validated batch 50 batch loss 0.502778947 batch mAP 0.37322998 batch PCKh 0.375\n",
      "Validated batch 51 batch loss 0.712167919 batch mAP 0.179931641 batch PCKh 0.125\n",
      "Validated batch 52 batch loss 0.491408557 batch mAP 0.494293213 batch PCKh 0.1875\n",
      "Validated batch 53 batch loss 0.547555625 batch mAP 0.405059814 batch PCKh 0.0625\n",
      "Validated batch 54 batch loss 0.629536 batch mAP 0.438049316 batch PCKh 0.75\n",
      "Validated batch 55 batch loss 0.670445204 batch mAP 0.325714111 batch PCKh 0.1875\n",
      "Validated batch 56 batch loss 0.517700553 batch mAP 0.464691162 batch PCKh 0.25\n",
      "Validated batch 57 batch loss 0.766055346 batch mAP 0.352386475 batch PCKh 0\n",
      "Validated batch 58 batch loss 0.545194745 batch mAP 0.457855225 batch PCKh 0.5625\n",
      "Validated batch 59 batch loss 0.600602 batch mAP 0.372467041 batch PCKh 0.1875\n",
      "Validated batch 60 batch loss 0.61574173 batch mAP 0.365661621 batch PCKh 0.75\n",
      "Validated batch 61 batch loss 0.564547837 batch mAP 0.347991943 batch PCKh 0\n",
      "Validated batch 62 batch loss 0.54034704 batch mAP 0.397491455 batch PCKh 0.3125\n",
      "Validated batch 63 batch loss 0.556380332 batch mAP 0.410583496 batch PCKh 0.6875\n",
      "Validated batch 64 batch loss 0.62221849 batch mAP 0.295684814 batch PCKh 0.1875\n",
      "Validated batch 65 batch loss 0.643376708 batch mAP 0.362518311 batch PCKh 0\n",
      "Validated batch 66 batch loss 0.538721859 batch mAP 0.436920166 batch PCKh 0.3125\n",
      "Validated batch 67 batch loss 0.572632551 batch mAP 0.436737061 batch PCKh 0.375\n",
      "Validated batch 68 batch loss 0.584354103 batch mAP 0.401123047 batch PCKh 0.4375\n",
      "Validated batch 69 batch loss 0.619004726 batch mAP 0.395813 batch PCKh 0\n",
      "Validated batch 70 batch loss 0.552843332 batch mAP 0.286743164 batch PCKh 0\n",
      "Validated batch 71 batch loss 0.539307773 batch mAP 0.317718506 batch PCKh 0.3125\n",
      "Validated batch 72 batch loss 0.651361883 batch mAP 0.341033936 batch PCKh 0.25\n",
      "Validated batch 73 batch loss 0.613534093 batch mAP 0.314849854 batch PCKh 0.1875\n",
      "Validated batch 74 batch loss 0.687046349 batch mAP 0.327056885 batch PCKh 0.875\n",
      "Validated batch 75 batch loss 0.685940444 batch mAP 0.398071289 batch PCKh 0.625\n",
      "Validated batch 76 batch loss 0.66729939 batch mAP 0.314819336 batch PCKh 0\n",
      "Validated batch 77 batch loss 0.67805773 batch mAP 0.276611328 batch PCKh 0\n",
      "Validated batch 78 batch loss 0.667043328 batch mAP 0.358581543 batch PCKh 0.4375\n",
      "Validated batch 79 batch loss 0.564430594 batch mAP 0.501159668 batch PCKh 0.6875\n",
      "Validated batch 80 batch loss 0.610295117 batch mAP 0.236022949 batch PCKh 0.25\n",
      "Validated batch 81 batch loss 0.693958759 batch mAP 0.326843262 batch PCKh 0.0625\n",
      "Validated batch 82 batch loss 0.682619631 batch mAP 0.294342041 batch PCKh 0.0625\n",
      "Validated batch 83 batch loss 0.521874309 batch mAP 0.424804688 batch PCKh 0.375\n",
      "Validated batch 84 batch loss 0.526100397 batch mAP 0.435180664 batch PCKh 0\n",
      "Validated batch 85 batch loss 0.5714 batch mAP 0.271972656 batch PCKh 0\n",
      "Validated batch 86 batch loss 0.654338717 batch mAP 0.331115723 batch PCKh 0.1875\n",
      "Validated batch 87 batch loss 0.612642765 batch mAP 0.413757324 batch PCKh 0.5\n",
      "Validated batch 88 batch loss 0.508115709 batch mAP 0.490081787 batch PCKh 0.6875\n",
      "Validated batch 89 batch loss 0.599649787 batch mAP 0.415588379 batch PCKh 0.25\n",
      "Validated batch 90 batch loss 0.631344795 batch mAP 0.375274658 batch PCKh 0.25\n",
      "Validated batch 91 batch loss 0.720553279 batch mAP 0.428649902 batch PCKh 0.1875\n",
      "Validated batch 92 batch loss 0.674630523 batch mAP 0.359405518 batch PCKh 0.5625\n",
      "Validated batch 93 batch loss 0.640978694 batch mAP 0.26852417 batch PCKh 0.375\n",
      "Validated batch 94 batch loss 0.505293071 batch mAP 0.32019043 batch PCKh 0.6875\n",
      "Validated batch 95 batch loss 0.571351707 batch mAP 0.36630249 batch PCKh 0.5\n",
      "Validated batch 96 batch loss 0.575525761 batch mAP 0.429656982 batch PCKh 0.625\n",
      "Validated batch 97 batch loss 0.679283857 batch mAP 0.442718506 batch PCKh 0.625\n",
      "Validated batch 98 batch loss 0.594496369 batch mAP 0.391021729 batch PCKh 0.4375\n",
      "Validated batch 99 batch loss 0.630687714 batch mAP 0.375457764 batch PCKh 0\n",
      "Validated batch 100 batch loss 0.551831961 batch mAP 0.544403076 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 101 batch loss 0.617041349 batch mAP 0.394348145 batch PCKh 0.5\n",
      "Validated batch 102 batch loss 0.658928156 batch mAP 0.360321045 batch PCKh 0.1875\n",
      "Validated batch 103 batch loss 0.734779 batch mAP 0.341888428 batch PCKh 0.1875\n",
      "Validated batch 104 batch loss 0.668370843 batch mAP 0.456390381 batch PCKh 0.6875\n",
      "Validated batch 105 batch loss 0.51205349 batch mAP 0.517852783 batch PCKh 0.625\n",
      "Validated batch 106 batch loss 0.511715651 batch mAP 0.486236572 batch PCKh 0.875\n",
      "Validated batch 107 batch loss 0.620409608 batch mAP 0.475189209 batch PCKh 0.125\n",
      "Validated batch 108 batch loss 0.605238378 batch mAP 0.342102051 batch PCKh 0.3125\n",
      "Validated batch 109 batch loss 0.697798669 batch mAP 0.266204834 batch PCKh 0\n",
      "Validated batch 110 batch loss 0.580456376 batch mAP 0.31463623 batch PCKh 0.1875\n",
      "Validated batch 111 batch loss 0.676906347 batch mAP 0.300567627 batch PCKh 0.5625\n",
      "Validated batch 112 batch loss 0.62723732 batch mAP 0.335662842 batch PCKh 0.5\n",
      "Validated batch 113 batch loss 0.601393878 batch mAP 0.398345947 batch PCKh 0.1875\n",
      "Validated batch 114 batch loss 0.666117072 batch mAP 0.336669922 batch PCKh 0.5\n",
      "Validated batch 115 batch loss 0.526390374 batch mAP 0.426757812 batch PCKh 0.625\n",
      "Validated batch 116 batch loss 0.511870265 batch mAP 0.337036133 batch PCKh 0.75\n",
      "Validated batch 117 batch loss 0.399091661 batch mAP 0.408660889 batch PCKh 0.3125\n",
      "Validated batch 118 batch loss 0.63587153 batch mAP 0.392242432 batch PCKh 0.5\n",
      "Validated batch 119 batch loss 0.613603771 batch mAP 0.409973145 batch PCKh 0.8125\n",
      "Validated batch 120 batch loss 0.544373274 batch mAP 0.413330078 batch PCKh 0.3125\n",
      "Validated batch 121 batch loss 0.601784229 batch mAP 0.401733398 batch PCKh 0.25\n",
      "Validated batch 122 batch loss 0.631057382 batch mAP 0.293365479 batch PCKh 0.3125\n",
      "Validated batch 123 batch loss 0.591791391 batch mAP 0.242553711 batch PCKh 0.375\n",
      "Validated batch 124 batch loss 0.604518175 batch mAP 0.234619141 batch PCKh 0.375\n",
      "Validated batch 125 batch loss 0.65154165 batch mAP 0.448913574 batch PCKh 0.5625\n",
      "Validated batch 126 batch loss 0.588291764 batch mAP 0.391448975 batch PCKh 0.625\n",
      "Validated batch 127 batch loss 0.524683058 batch mAP 0.467529297 batch PCKh 0.4375\n",
      "Validated batch 128 batch loss 0.536130488 batch mAP 0.370056152 batch PCKh 0.75\n",
      "Validated batch 129 batch loss 0.692024648 batch mAP 0.413879395 batch PCKh 0\n",
      "Validated batch 130 batch loss 0.599543512 batch mAP 0.368927 batch PCKh 0.0625\n",
      "Validated batch 131 batch loss 0.736479044 batch mAP 0.236022949 batch PCKh 0\n",
      "Validated batch 132 batch loss 0.66234827 batch mAP 0.441986084 batch PCKh 0\n",
      "Validated batch 133 batch loss 0.674362302 batch mAP 0.347106934 batch PCKh 0.5\n",
      "Validated batch 134 batch loss 0.629287422 batch mAP 0.482788086 batch PCKh 0.5625\n",
      "Validated batch 135 batch loss 0.693310797 batch mAP 0.387329102 batch PCKh 0.0625\n",
      "Validated batch 136 batch loss 0.567852437 batch mAP 0.276092529 batch PCKh 0.75\n",
      "Validated batch 137 batch loss 0.53972882 batch mAP 0.318847656 batch PCKh 0.4375\n",
      "Validated batch 138 batch loss 0.59401989 batch mAP 0.400787354 batch PCKh 0.625\n",
      "Validated batch 139 batch loss 0.598518729 batch mAP 0.383667 batch PCKh 0.5\n",
      "Validated batch 140 batch loss 0.62848562 batch mAP 0.364105225 batch PCKh 0.6875\n",
      "Validated batch 141 batch loss 0.615476906 batch mAP 0.308502197 batch PCKh 0\n",
      "Validated batch 142 batch loss 0.571908236 batch mAP 0.50479126 batch PCKh 0.4375\n",
      "Validated batch 143 batch loss 0.683923542 batch mAP 0.204864502 batch PCKh 0\n",
      "Validated batch 144 batch loss 0.583720624 batch mAP 0.24319458 batch PCKh 0.0625\n",
      "Validated batch 145 batch loss 0.652591705 batch mAP 0.253723145 batch PCKh 0.375\n",
      "Validated batch 146 batch loss 0.544620633 batch mAP 0.295684814 batch PCKh 0.5625\n",
      "Validated batch 147 batch loss 0.585565 batch mAP 0.418121338 batch PCKh 0.8125\n",
      "Validated batch 148 batch loss 0.631648839 batch mAP 0.358154297 batch PCKh 0.5\n",
      "Validated batch 149 batch loss 0.671835303 batch mAP 0.316345215 batch PCKh 0.1875\n",
      "Validated batch 150 batch loss 0.651460528 batch mAP 0.290985107 batch PCKh 0.25\n",
      "Validated batch 151 batch loss 0.603235722 batch mAP 0.363525391 batch PCKh 0.625\n",
      "Validated batch 152 batch loss 0.652609 batch mAP 0.305236816 batch PCKh 0\n",
      "Validated batch 153 batch loss 0.565782249 batch mAP 0.379150391 batch PCKh 0.25\n",
      "Validated batch 154 batch loss 0.60073626 batch mAP 0.472595215 batch PCKh 0.625\n",
      "Validated batch 155 batch loss 0.69218111 batch mAP 0.326965332 batch PCKh 0.0625\n",
      "Validated batch 156 batch loss 0.578025103 batch mAP 0.391693115 batch PCKh 0.3125\n",
      "Validated batch 157 batch loss 0.53070581 batch mAP 0.506073 batch PCKh 0.5\n",
      "Validated batch 158 batch loss 0.594591439 batch mAP 0.441558838 batch PCKh 0.4375\n",
      "Validated batch 159 batch loss 0.696434498 batch mAP 0.378631592 batch PCKh 0.3125\n",
      "Validated batch 160 batch loss 0.532497525 batch mAP 0.430480957 batch PCKh 0.125\n",
      "Validated batch 161 batch loss 0.705469251 batch mAP 0.249603271 batch PCKh 0\n",
      "Validated batch 162 batch loss 0.599889874 batch mAP 0.273223877 batch PCKh 0.1875\n",
      "Validated batch 163 batch loss 0.522825599 batch mAP 0.369689941 batch PCKh 0.125\n",
      "Validated batch 164 batch loss 0.655570447 batch mAP 0.359191895 batch PCKh 0.375\n",
      "Validated batch 165 batch loss 0.605218768 batch mAP 0.352508545 batch PCKh 0.25\n",
      "Validated batch 166 batch loss 0.562724411 batch mAP 0.459991455 batch PCKh 0.0625\n",
      "Validated batch 167 batch loss 0.672354 batch mAP 0.33984375 batch PCKh 0.1875\n",
      "Validated batch 168 batch loss 0.618434548 batch mAP 0.346557617 batch PCKh 0.1875\n",
      "Validated batch 169 batch loss 0.580874562 batch mAP 0.393829346 batch PCKh 0.375\n",
      "Validated batch 170 batch loss 0.60833323 batch mAP 0.302459717 batch PCKh 0.125\n",
      "Validated batch 171 batch loss 0.711901784 batch mAP 0.290893555 batch PCKh 0.125\n",
      "Validated batch 172 batch loss 0.708355069 batch mAP 0.249786377 batch PCKh 0.5625\n",
      "Validated batch 173 batch loss 0.567798495 batch mAP 0.34677124 batch PCKh 0.1875\n",
      "Validated batch 174 batch loss 0.683101773 batch mAP 0.350189209 batch PCKh 0.1875\n",
      "Validated batch 175 batch loss 0.647156715 batch mAP 0.411071777 batch PCKh 0.1875\n",
      "Validated batch 176 batch loss 0.495388746 batch mAP 0.429412842 batch PCKh 0.5625\n",
      "Validated batch 177 batch loss 0.632297754 batch mAP 0.321380615 batch PCKh 0.625\n",
      "Validated batch 178 batch loss 0.690233767 batch mAP 0.252227783 batch PCKh 0.0625\n",
      "Validated batch 179 batch loss 0.655359328 batch mAP 0.334075928 batch PCKh 0.4375\n",
      "Validated batch 180 batch loss 0.6399827 batch mAP 0.381958 batch PCKh 0.1875\n",
      "Validated batch 181 batch loss 0.645093501 batch mAP 0.347137451 batch PCKh 0\n",
      "Validated batch 182 batch loss 0.509182155 batch mAP 0.382904053 batch PCKh 0.5625\n",
      "Validated batch 183 batch loss 0.609785318 batch mAP 0.392883301 batch PCKh 0.0625\n",
      "Validated batch 184 batch loss 0.602848232 batch mAP 0.343933105 batch PCKh 0.5625\n",
      "Validated batch 185 batch loss 0.608732104 batch mAP 0.416564941 batch PCKh 0.5\n",
      "Validated batch 186 batch loss 0.691318393 batch mAP 0.338470459 batch PCKh 0.375\n",
      "Validated batch 187 batch loss 0.706928134 batch mAP 0.311645508 batch PCKh 0.1875\n",
      "Validated batch 188 batch loss 0.58694315 batch mAP 0.443511963 batch PCKh 0.5625\n",
      "Validated batch 189 batch loss 0.517711163 batch mAP 0.479644775 batch PCKh 0.125\n",
      "Validated batch 190 batch loss 0.638989747 batch mAP 0.34552002 batch PCKh 0.0625\n",
      "Validated batch 191 batch loss 0.637270212 batch mAP 0.319396973 batch PCKh 0.3125\n",
      "Validated batch 192 batch loss 0.576972723 batch mAP 0.322113037 batch PCKh 0.4375\n",
      "Validated batch 193 batch loss 0.586355567 batch mAP 0.343688965 batch PCKh 0.4375\n",
      "Validated batch 194 batch loss 0.662969172 batch mAP 0.421173096 batch PCKh 0.375\n",
      "Validated batch 195 batch loss 0.557850361 batch mAP 0.315948486 batch PCKh 0.3125\n",
      "Validated batch 196 batch loss 0.642992735 batch mAP 0.428497314 batch PCKh 0.6875\n",
      "Validated batch 197 batch loss 0.614383221 batch mAP 0.422515869 batch PCKh 0.4375\n",
      "Validated batch 198 batch loss 0.605698705 batch mAP 0.380523682 batch PCKh 0.0625\n",
      "Validated batch 199 batch loss 0.517498612 batch mAP 0.435241699 batch PCKh 0.6875\n",
      "Validated batch 200 batch loss 0.665972471 batch mAP 0.324249268 batch PCKh 0.125\n",
      "Validated batch 201 batch loss 0.437801719 batch mAP 0.34677124 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 202 batch loss 0.729861379 batch mAP 0.35546875 batch PCKh 0.625\n",
      "Validated batch 203 batch loss 0.732012331 batch mAP 0.355529785 batch PCKh 0.0625\n",
      "Validated batch 204 batch loss 0.650928557 batch mAP 0.251586914 batch PCKh 0.1875\n",
      "Validated batch 205 batch loss 0.619393 batch mAP 0.361114502 batch PCKh 0.1875\n",
      "Validated batch 206 batch loss 0.581500769 batch mAP 0.446380615 batch PCKh 0.1875\n",
      "Validated batch 207 batch loss 0.546561062 batch mAP 0.298980713 batch PCKh 0.5625\n",
      "Validated batch 208 batch loss 0.571975708 batch mAP 0.441650391 batch PCKh 0.8125\n",
      "Validated batch 209 batch loss 0.660824358 batch mAP 0.275146484 batch PCKh 0.25\n",
      "Validated batch 210 batch loss 0.568089306 batch mAP 0.43460083 batch PCKh 0.3125\n",
      "Validated batch 211 batch loss 0.700017333 batch mAP 0.327362061 batch PCKh 0.6875\n",
      "Validated batch 212 batch loss 0.678263128 batch mAP 0.275421143 batch PCKh 0.3125\n",
      "Validated batch 213 batch loss 0.656010807 batch mAP 0.228485107 batch PCKh 0\n",
      "Validated batch 214 batch loss 0.768687487 batch mAP 0.16027832 batch PCKh 0.125\n",
      "Validated batch 215 batch loss 0.784342766 batch mAP 0.139068604 batch PCKh 0.125\n",
      "Validated batch 216 batch loss 0.739244878 batch mAP 0.232543945 batch PCKh 0.4375\n",
      "Validated batch 217 batch loss 0.592126131 batch mAP 0.363739 batch PCKh 0.6875\n",
      "Validated batch 218 batch loss 0.626162052 batch mAP 0.35446167 batch PCKh 0.4375\n",
      "Validated batch 219 batch loss 0.728234231 batch mAP 0.451934814 batch PCKh 0.25\n",
      "Validated batch 220 batch loss 0.680869341 batch mAP 0.300537109 batch PCKh 0.3125\n",
      "Validated batch 221 batch loss 0.664283574 batch mAP 0.246307373 batch PCKh 0.25\n",
      "Validated batch 222 batch loss 0.59499234 batch mAP 0.487640381 batch PCKh 0.75\n",
      "Validated batch 223 batch loss 0.761158347 batch mAP 0.387298584 batch PCKh 0.125\n",
      "Validated batch 224 batch loss 0.59704423 batch mAP 0.196075439 batch PCKh 0.25\n",
      "Validated batch 225 batch loss 0.621979952 batch mAP 0.394226074 batch PCKh 0.375\n",
      "Validated batch 226 batch loss 0.674727201 batch mAP 0.365844727 batch PCKh 0.8125\n",
      "Validated batch 227 batch loss 0.500425339 batch mAP 0.335998535 batch PCKh 0.1875\n",
      "Validated batch 228 batch loss 0.501306772 batch mAP 0.379333496 batch PCKh 0.25\n",
      "Validated batch 229 batch loss 0.570107341 batch mAP 0.221588135 batch PCKh 0.3125\n",
      "Validated batch 230 batch loss 0.641943336 batch mAP 0.312438965 batch PCKh 0.125\n",
      "Validated batch 231 batch loss 0.662352383 batch mAP 0.372558594 batch PCKh 0.125\n",
      "Validated batch 232 batch loss 0.678909302 batch mAP 0.336578369 batch PCKh 0\n",
      "Validated batch 233 batch loss 0.581069529 batch mAP 0.360046387 batch PCKh 0.1875\n",
      "Validated batch 234 batch loss 0.615928888 batch mAP 0.439056396 batch PCKh 0.5\n",
      "Validated batch 235 batch loss 0.639580786 batch mAP 0.387908936 batch PCKh 0.5625\n",
      "Validated batch 236 batch loss 0.536518276 batch mAP 0.47479248 batch PCKh 0.1875\n",
      "Validated batch 237 batch loss 0.599510908 batch mAP 0.464080811 batch PCKh 0.375\n",
      "Validated batch 238 batch loss 0.616854906 batch mAP 0.464050293 batch PCKh 0.625\n",
      "Validated batch 239 batch loss 0.561048388 batch mAP 0.386566162 batch PCKh 0.6875\n",
      "Validated batch 240 batch loss 0.556347549 batch mAP 0.479156494 batch PCKh 0.375\n",
      "Validated batch 241 batch loss 0.607228756 batch mAP 0.352783203 batch PCKh 0.25\n",
      "Validated batch 242 batch loss 0.596111774 batch mAP 0.475067139 batch PCKh 0.6875\n",
      "Validated batch 243 batch loss 0.645516872 batch mAP 0.427368164 batch PCKh 0.375\n",
      "Validated batch 244 batch loss 0.616872847 batch mAP 0.336975098 batch PCKh 0.5\n",
      "Validated batch 245 batch loss 0.613651872 batch mAP 0.425292969 batch PCKh 0.125\n",
      "Validated batch 246 batch loss 0.552732885 batch mAP 0.322906494 batch PCKh 0.4375\n",
      "Validated batch 247 batch loss 0.563379884 batch mAP 0.448669434 batch PCKh 0.5\n",
      "Validated batch 248 batch loss 0.606301546 batch mAP 0.483398438 batch PCKh 0.25\n",
      "Validated batch 249 batch loss 0.660536051 batch mAP 0.392059326 batch PCKh 0.4375\n",
      "Validated batch 250 batch loss 0.612081587 batch mAP 0.421966553 batch PCKh 0.4375\n",
      "Validated batch 251 batch loss 0.64051652 batch mAP 0.515014648 batch PCKh 0.25\n",
      "Validated batch 252 batch loss 0.678162932 batch mAP 0.239868164 batch PCKh 0\n",
      "Validated batch 253 batch loss 0.700024843 batch mAP 0.303985596 batch PCKh 0.375\n",
      "Validated batch 254 batch loss 0.643006444 batch mAP 0.311126709 batch PCKh 0.4375\n",
      "Validated batch 255 batch loss 0.624177575 batch mAP 0.300872803 batch PCKh 0.25\n",
      "Validated batch 256 batch loss 0.643200576 batch mAP 0.32989502 batch PCKh 0.375\n",
      "Validated batch 257 batch loss 0.498395801 batch mAP 0.317657471 batch PCKh 0.1875\n",
      "Validated batch 258 batch loss 0.654227316 batch mAP 0.209564209 batch PCKh 0.625\n",
      "Validated batch 259 batch loss 0.699406862 batch mAP 0.279785156 batch PCKh 0.125\n",
      "Validated batch 260 batch loss 0.558212459 batch mAP 0.292297363 batch PCKh 0.25\n",
      "Validated batch 261 batch loss 0.641885042 batch mAP 0.303222656 batch PCKh 0.5625\n",
      "Validated batch 262 batch loss 0.699157417 batch mAP 0.0802001953 batch PCKh 0.3125\n",
      "Validated batch 263 batch loss 0.643541336 batch mAP 0.300689697 batch PCKh 0.125\n",
      "Validated batch 264 batch loss 0.732919 batch mAP 0.245452881 batch PCKh 0.3125\n",
      "Validated batch 265 batch loss 0.594690561 batch mAP 0.325653076 batch PCKh 0.125\n",
      "Validated batch 266 batch loss 0.642115772 batch mAP 0.364959717 batch PCKh 0.125\n",
      "Validated batch 267 batch loss 0.595042884 batch mAP 0.393035889 batch PCKh 0.375\n",
      "Validated batch 268 batch loss 0.623409629 batch mAP 0.407165527 batch PCKh 0.25\n",
      "Validated batch 269 batch loss 0.68836987 batch mAP 0.29196167 batch PCKh 0.1875\n",
      "Validated batch 270 batch loss 0.695234179 batch mAP 0.388885498 batch PCKh 0.1875\n",
      "Validated batch 271 batch loss 0.714854479 batch mAP 0.325500488 batch PCKh 0.1875\n",
      "Validated batch 272 batch loss 0.617105603 batch mAP 0.385375977 batch PCKh 0.4375\n",
      "Validated batch 273 batch loss 0.58983314 batch mAP 0.30380249 batch PCKh 0.4375\n",
      "Validated batch 274 batch loss 0.676632762 batch mAP 0.335571289 batch PCKh 0.375\n",
      "Validated batch 275 batch loss 0.574868083 batch mAP 0.307403564 batch PCKh 0.375\n",
      "Validated batch 276 batch loss 0.54158783 batch mAP 0.46270752 batch PCKh 0.125\n",
      "Validated batch 277 batch loss 0.535349548 batch mAP 0.440460205 batch PCKh 0.5625\n",
      "Validated batch 278 batch loss 0.606376112 batch mAP 0.377380371 batch PCKh 0.75\n",
      "Validated batch 279 batch loss 0.670183122 batch mAP 0.29586792 batch PCKh 0.4375\n",
      "Validated batch 280 batch loss 0.600578427 batch mAP 0.412963867 batch PCKh 0.6875\n",
      "Validated batch 281 batch loss 0.647184789 batch mAP 0.320281982 batch PCKh 0.5625\n",
      "Validated batch 282 batch loss 0.581558406 batch mAP 0.461730957 batch PCKh 0.5625\n",
      "Validated batch 283 batch loss 0.658935547 batch mAP 0.374176025 batch PCKh 0.75\n",
      "Validated batch 284 batch loss 0.579203188 batch mAP 0.348754883 batch PCKh 0.3125\n",
      "Validated batch 285 batch loss 0.626373172 batch mAP 0.332702637 batch PCKh 0.125\n",
      "Validated batch 286 batch loss 0.690066338 batch mAP 0.330596924 batch PCKh 0.25\n",
      "Validated batch 287 batch loss 0.682332 batch mAP 0.262359619 batch PCKh 0.5\n",
      "Validated batch 288 batch loss 0.723065734 batch mAP 0.34185791 batch PCKh 0.25\n",
      "Validated batch 289 batch loss 0.621173441 batch mAP 0.3253479 batch PCKh 0\n",
      "Validated batch 290 batch loss 0.59845233 batch mAP 0.461486816 batch PCKh 0.25\n",
      "Validated batch 291 batch loss 0.59618485 batch mAP 0.455780029 batch PCKh 0.75\n",
      "Validated batch 292 batch loss 0.713117659 batch mAP 0.22479248 batch PCKh 0.375\n",
      "Validated batch 293 batch loss 0.64728266 batch mAP 0.214996338 batch PCKh 0.1875\n",
      "Validated batch 294 batch loss 0.534006774 batch mAP 0.522522 batch PCKh 0.5\n",
      "Validated batch 295 batch loss 0.690441489 batch mAP 0.318389893 batch PCKh 0.0625\n",
      "Validated batch 296 batch loss 0.618692219 batch mAP 0.387817383 batch PCKh 0.375\n",
      "Validated batch 297 batch loss 0.634941816 batch mAP 0.417114258 batch PCKh 0.75\n",
      "Validated batch 298 batch loss 0.596174359 batch mAP 0.426300049 batch PCKh 0.625\n",
      "Validated batch 299 batch loss 0.57055974 batch mAP 0.425048828 batch PCKh 0.6875\n",
      "Validated batch 300 batch loss 0.5679878 batch mAP 0.406982422 batch PCKh 0.5625\n",
      "Validated batch 301 batch loss 0.636539102 batch mAP 0.37399292 batch PCKh 0.625\n",
      "Validated batch 302 batch loss 0.649322033 batch mAP 0.367034912 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 303 batch loss 0.707856357 batch mAP 0.387878418 batch PCKh 0.25\n",
      "Validated batch 304 batch loss 0.525653243 batch mAP 0.432617188 batch PCKh 0.4375\n",
      "Validated batch 305 batch loss 0.68419987 batch mAP 0.147247314 batch PCKh 0.25\n",
      "Validated batch 306 batch loss 0.707393944 batch mAP 0.329437256 batch PCKh 0.25\n",
      "Validated batch 307 batch loss 0.595176697 batch mAP 0.395721436 batch PCKh 0\n",
      "Validated batch 308 batch loss 0.636129618 batch mAP 0.514099121 batch PCKh 0.375\n",
      "Validated batch 309 batch loss 0.616602957 batch mAP 0.476501465 batch PCKh 0.6875\n",
      "Validated batch 310 batch loss 0.646527946 batch mAP 0.316558838 batch PCKh 0.3125\n",
      "Validated batch 311 batch loss 0.660461307 batch mAP 0.328796387 batch PCKh 0.75\n",
      "Validated batch 312 batch loss 0.490069717 batch mAP 0.362884521 batch PCKh 0.5\n",
      "Validated batch 313 batch loss 0.60154891 batch mAP 0.325195312 batch PCKh 0.5625\n",
      "Validated batch 314 batch loss 0.635065198 batch mAP 0.314361572 batch PCKh 0.625\n",
      "Validated batch 315 batch loss 0.63431859 batch mAP 0.300079346 batch PCKh 0.5625\n",
      "Validated batch 316 batch loss 0.604931712 batch mAP 0.374389648 batch PCKh 0.625\n",
      "Validated batch 317 batch loss 0.55987668 batch mAP 0.451904297 batch PCKh 0.125\n",
      "Validated batch 318 batch loss 0.618653178 batch mAP 0.385620117 batch PCKh 0.4375\n",
      "Validated batch 319 batch loss 0.625360727 batch mAP 0.326538086 batch PCKh 0.4375\n",
      "Validated batch 320 batch loss 0.586393118 batch mAP 0.38873291 batch PCKh 0\n",
      "Validated batch 321 batch loss 0.632505298 batch mAP 0.353302 batch PCKh 0.6875\n",
      "Validated batch 322 batch loss 0.606784463 batch mAP 0.283630371 batch PCKh 0\n",
      "Validated batch 323 batch loss 0.71196878 batch mAP 0.249359131 batch PCKh 0.0625\n",
      "Validated batch 324 batch loss 0.62037313 batch mAP 0.312530518 batch PCKh 0.4375\n",
      "Validated batch 325 batch loss 0.652258933 batch mAP 0.249664307 batch PCKh 0\n",
      "Validated batch 326 batch loss 0.499010921 batch mAP 0.428314209 batch PCKh 0.5\n",
      "Validated batch 327 batch loss 0.605241656 batch mAP 0.352355957 batch PCKh 0.4375\n",
      "Validated batch 328 batch loss 0.642560542 batch mAP 0.368286133 batch PCKh 0.1875\n",
      "Validated batch 329 batch loss 0.647671 batch mAP 0.376708984 batch PCKh 0.5\n",
      "Validated batch 330 batch loss 0.529927 batch mAP 0.604400635 batch PCKh 0.25\n",
      "Validated batch 331 batch loss 0.595215201 batch mAP 0.439697266 batch PCKh 0.75\n",
      "Validated batch 332 batch loss 0.615339875 batch mAP 0.294250488 batch PCKh 0.6875\n",
      "Validated batch 333 batch loss 0.589810312 batch mAP 0.34161377 batch PCKh 0.375\n",
      "Validated batch 334 batch loss 0.663650393 batch mAP 0.313140869 batch PCKh 0.3125\n",
      "Validated batch 335 batch loss 0.56531769 batch mAP 0.38684082 batch PCKh 0.3125\n",
      "Validated batch 336 batch loss 0.627221 batch mAP 0.368835449 batch PCKh 0.5\n",
      "Validated batch 337 batch loss 0.622564554 batch mAP 0.369781494 batch PCKh 0.3125\n",
      "Validated batch 338 batch loss 0.620913208 batch mAP 0.384307861 batch PCKh 0.4375\n",
      "Validated batch 339 batch loss 0.725000143 batch mAP 0.32244873 batch PCKh 0.375\n",
      "Validated batch 340 batch loss 0.630724072 batch mAP 0.282531738 batch PCKh 0.5\n",
      "Validated batch 341 batch loss 0.697881162 batch mAP 0.398406982 batch PCKh 0.125\n",
      "Validated batch 342 batch loss 0.598295033 batch mAP 0.325469971 batch PCKh 0.375\n",
      "Validated batch 343 batch loss 0.639679551 batch mAP 0.2890625 batch PCKh 0.375\n",
      "Validated batch 344 batch loss 0.635566592 batch mAP 0.322357178 batch PCKh 0.4375\n",
      "Validated batch 345 batch loss 0.667853534 batch mAP 0.286895752 batch PCKh 0.4375\n",
      "Validated batch 346 batch loss 0.770385921 batch mAP 0.307739258 batch PCKh 0.375\n",
      "Validated batch 347 batch loss 0.624090493 batch mAP 0.392364502 batch PCKh 0.5625\n",
      "Validated batch 348 batch loss 0.73609215 batch mAP 0.164794922 batch PCKh 0.625\n",
      "Validated batch 349 batch loss 0.726428926 batch mAP 0.229431152 batch PCKh 0.0625\n",
      "Validated batch 350 batch loss 0.653166652 batch mAP 0.409851074 batch PCKh 0.125\n",
      "Validated batch 351 batch loss 0.66185683 batch mAP 0.447357178 batch PCKh 0.5\n",
      "Validated batch 352 batch loss 0.636963665 batch mAP 0.320556641 batch PCKh 0.75\n",
      "Validated batch 353 batch loss 0.682354271 batch mAP 0.342346191 batch PCKh 0.5625\n",
      "Validated batch 354 batch loss 0.719100714 batch mAP 0.391540527 batch PCKh 0.625\n",
      "Validated batch 355 batch loss 0.642894566 batch mAP 0.318389893 batch PCKh 0.4375\n",
      "Validated batch 356 batch loss 0.715507746 batch mAP 0.457641602 batch PCKh 0.1875\n",
      "Validated batch 357 batch loss 0.593072355 batch mAP 0.520477295 batch PCKh 0.5625\n",
      "Validated batch 358 batch loss 0.581242442 batch mAP 0.315673828 batch PCKh 0.25\n",
      "Validated batch 359 batch loss 0.601500034 batch mAP 0.380523682 batch PCKh 0.25\n",
      "Validated batch 360 batch loss 0.624459624 batch mAP 0.422424316 batch PCKh 0.125\n",
      "Validated batch 361 batch loss 0.659855545 batch mAP 0.355407715 batch PCKh 0.3125\n",
      "Validated batch 362 batch loss 0.651615381 batch mAP 0.39352417 batch PCKh 0.25\n",
      "Validated batch 363 batch loss 0.72663331 batch mAP 0.365142822 batch PCKh 0.5\n",
      "Validated batch 364 batch loss 0.695802391 batch mAP 0.339935303 batch PCKh 0.4375\n",
      "Validated batch 365 batch loss 0.555064678 batch mAP 0.399108887 batch PCKh 0.25\n",
      "Validated batch 366 batch loss 0.602198184 batch mAP 0.410675049 batch PCKh 0.125\n",
      "Validated batch 367 batch loss 0.656841755 batch mAP 0.370819092 batch PCKh 0.5\n",
      "Validated batch 368 batch loss 0.626347244 batch mAP 0.354217529 batch PCKh 0.0625\n",
      "Validated batch 369 batch loss 0.599481821 batch mAP 0.3175354 batch PCKh 0.375\n",
      "Epoch 6 val loss 0.622798502445221 val mAP 0.359684556722641 val PCKh\n",
      "Epoch 6 completed in 769.67 seconds\n",
      "Start epoch 7 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.664484203 batch mAP 0.47744751 batch PCKh 0.1875\n",
      "Trained batch 2 batch loss 0.611798286 batch mAP 0.518341064 batch PCKh 0.3125\n",
      "Trained batch 3 batch loss 0.616209388 batch mAP 0.553894043 batch PCKh 0.1875\n",
      "Trained batch 4 batch loss 0.526718795 batch mAP 0.530975342 batch PCKh 0.0625\n",
      "Trained batch 5 batch loss 0.50874567 batch mAP 0.618499756 batch PCKh 0.25\n",
      "Trained batch 6 batch loss 0.581270516 batch mAP 0.571136475 batch PCKh 0.25\n",
      "Trained batch 7 batch loss 0.48785007 batch mAP 0.587402344 batch PCKh 0.5\n",
      "Trained batch 8 batch loss 0.55881691 batch mAP 0.609283447 batch PCKh 0.75\n",
      "Trained batch 9 batch loss 0.610585928 batch mAP 0.547943115 batch PCKh 0.25\n",
      "Trained batch 10 batch loss 0.576347828 batch mAP 0.535553 batch PCKh 0.3125\n",
      "Trained batch 11 batch loss 0.498232275 batch mAP 0.565307617 batch PCKh 0.125\n",
      "Trained batch 12 batch loss 0.627004743 batch mAP 0.517791748 batch PCKh 0.3125\n",
      "Trained batch 13 batch loss 0.557619691 batch mAP 0.5234375 batch PCKh 0.4375\n",
      "Trained batch 14 batch loss 0.630046487 batch mAP 0.467376709 batch PCKh 0.25\n",
      "Trained batch 15 batch loss 0.480165303 batch mAP 0.599395752 batch PCKh 0.8125\n",
      "Trained batch 16 batch loss 0.566367865 batch mAP 0.575439453 batch PCKh 0.75\n",
      "Trained batch 17 batch loss 0.564305186 batch mAP 0.509246826 batch PCKh 0.625\n",
      "Trained batch 18 batch loss 0.626163363 batch mAP 0.545318604 batch PCKh 0.6875\n",
      "Trained batch 19 batch loss 0.602060437 batch mAP 0.536468506 batch PCKh 0.3125\n",
      "Trained batch 20 batch loss 0.535096526 batch mAP 0.604797363 batch PCKh 0.5\n",
      "Trained batch 21 batch loss 0.642518878 batch mAP 0.565094 batch PCKh 0.625\n",
      "Trained batch 22 batch loss 0.613991857 batch mAP 0.563385 batch PCKh 0.3125\n",
      "Trained batch 23 batch loss 0.604618669 batch mAP 0.587799072 batch PCKh 0.8125\n",
      "Trained batch 24 batch loss 0.545624077 batch mAP 0.458526611 batch PCKh 0.25\n",
      "Trained batch 25 batch loss 0.548966944 batch mAP 0.52633667 batch PCKh 0.6875\n",
      "Trained batch 26 batch loss 0.571672261 batch mAP 0.511352539 batch PCKh 0.625\n",
      "Trained batch 27 batch loss 0.518374622 batch mAP 0.591674805 batch PCKh 0.75\n",
      "Trained batch 28 batch loss 0.515357375 batch mAP 0.583465576 batch PCKh 0.75\n",
      "Trained batch 29 batch loss 0.546831787 batch mAP 0.591339111 batch PCKh 0.3125\n",
      "Trained batch 30 batch loss 0.583925247 batch mAP 0.569213867 batch PCKh 0.625\n",
      "Trained batch 31 batch loss 0.473100394 batch mAP 0.625183105 batch PCKh 0.625\n",
      "Trained batch 32 batch loss 0.539658606 batch mAP 0.563110352 batch PCKh 0.5625\n",
      "Trained batch 33 batch loss 0.536032438 batch mAP 0.554870605 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 34 batch loss 0.572634637 batch mAP 0.612487793 batch PCKh 0.875\n",
      "Trained batch 35 batch loss 0.54280293 batch mAP 0.605682373 batch PCKh 0.5625\n",
      "Trained batch 36 batch loss 0.527028799 batch mAP 0.592315674 batch PCKh 0.1875\n",
      "Trained batch 37 batch loss 0.592762709 batch mAP 0.582244873 batch PCKh 0.375\n",
      "Trained batch 38 batch loss 0.53038 batch mAP 0.599700928 batch PCKh 0.375\n",
      "Trained batch 39 batch loss 0.529837072 batch mAP 0.605712891 batch PCKh 0.0625\n",
      "Trained batch 40 batch loss 0.585382938 batch mAP 0.636932373 batch PCKh 0.625\n",
      "Trained batch 41 batch loss 0.517617106 batch mAP 0.671478271 batch PCKh 0.625\n",
      "Trained batch 42 batch loss 0.490279287 batch mAP 0.646972656 batch PCKh 0.1875\n",
      "Trained batch 43 batch loss 0.607649803 batch mAP 0.615356445 batch PCKh 0.25\n",
      "Trained batch 44 batch loss 0.504023194 batch mAP 0.607879639 batch PCKh 0.375\n",
      "Trained batch 45 batch loss 0.573316514 batch mAP 0.569458 batch PCKh 0.1875\n",
      "Trained batch 46 batch loss 0.647939742 batch mAP 0.519042969 batch PCKh 0.1875\n",
      "Trained batch 47 batch loss 0.561289668 batch mAP 0.522766113 batch PCKh 0.3125\n",
      "Trained batch 48 batch loss 0.514001131 batch mAP 0.591644287 batch PCKh 0.4375\n",
      "Trained batch 49 batch loss 0.494770437 batch mAP 0.613372803 batch PCKh 0.1875\n",
      "Trained batch 50 batch loss 0.507191658 batch mAP 0.615234375 batch PCKh 0.25\n",
      "Trained batch 51 batch loss 0.500603855 batch mAP 0.591095 batch PCKh 0.1875\n",
      "Trained batch 52 batch loss 0.619103611 batch mAP 0.560150146 batch PCKh 0.375\n",
      "Trained batch 53 batch loss 0.565757751 batch mAP 0.487457275 batch PCKh 0.25\n",
      "Trained batch 54 batch loss 0.406050503 batch mAP 0.405151367 batch PCKh 0.125\n",
      "Trained batch 55 batch loss 0.507162035 batch mAP 0.468170166 batch PCKh 0.25\n",
      "Trained batch 56 batch loss 0.617548883 batch mAP 0.391723633 batch PCKh 0.1875\n",
      "Trained batch 57 batch loss 0.600687802 batch mAP 0.439453125 batch PCKh 0.25\n",
      "Trained batch 58 batch loss 0.543256402 batch mAP 0.485870361 batch PCKh 0.4375\n",
      "Trained batch 59 batch loss 0.629199088 batch mAP 0.503845215 batch PCKh 0.625\n",
      "Trained batch 60 batch loss 0.571131349 batch mAP 0.530761719 batch PCKh 0.6875\n",
      "Trained batch 61 batch loss 0.594580293 batch mAP 0.573272705 batch PCKh 0.0625\n",
      "Trained batch 62 batch loss 0.527624249 batch mAP 0.638580322 batch PCKh 0.3125\n",
      "Trained batch 63 batch loss 0.398381829 batch mAP 0.732971191 batch PCKh 0.5\n",
      "Trained batch 64 batch loss 0.473197579 batch mAP 0.684509277 batch PCKh 0.5\n",
      "Trained batch 65 batch loss 0.416028738 batch mAP 0.675750732 batch PCKh 0.625\n",
      "Trained batch 66 batch loss 0.424629748 batch mAP 0.638824463 batch PCKh 0.3125\n",
      "Trained batch 67 batch loss 0.429868877 batch mAP 0.662323 batch PCKh 0.4375\n",
      "Trained batch 68 batch loss 0.477060854 batch mAP 0.623443604 batch PCKh 0.5625\n",
      "Trained batch 69 batch loss 0.499370337 batch mAP 0.607421875 batch PCKh 0.4375\n",
      "Trained batch 70 batch loss 0.451692045 batch mAP 0.63192749 batch PCKh 0.5\n",
      "Trained batch 71 batch loss 0.490483 batch mAP 0.675018311 batch PCKh 0.5625\n",
      "Trained batch 72 batch loss 0.441011429 batch mAP 0.723083496 batch PCKh 0.6875\n",
      "Trained batch 73 batch loss 0.488719612 batch mAP 0.651153564 batch PCKh 0.5625\n",
      "Trained batch 74 batch loss 0.490996391 batch mAP 0.536956787 batch PCKh 0.1875\n",
      "Trained batch 75 batch loss 0.481710345 batch mAP 0.576507568 batch PCKh 0.1875\n",
      "Trained batch 76 batch loss 0.459667265 batch mAP 0.623718262 batch PCKh 0.875\n",
      "Trained batch 77 batch loss 0.510259867 batch mAP 0.579620361 batch PCKh 0.875\n",
      "Trained batch 78 batch loss 0.474835932 batch mAP 0.58001709 batch PCKh 0.875\n",
      "Trained batch 79 batch loss 0.496221811 batch mAP 0.641082764 batch PCKh 0.75\n",
      "Trained batch 80 batch loss 0.512039602 batch mAP 0.569702148 batch PCKh 0.875\n",
      "Trained batch 81 batch loss 0.537348032 batch mAP 0.613586426 batch PCKh 0.6875\n",
      "Trained batch 82 batch loss 0.583270073 batch mAP 0.561035156 batch PCKh 0.875\n",
      "Trained batch 83 batch loss 0.516862571 batch mAP 0.52355957 batch PCKh 0.75\n",
      "Trained batch 84 batch loss 0.599717319 batch mAP 0.541717529 batch PCKh 0.6875\n",
      "Trained batch 85 batch loss 0.545847416 batch mAP 0.507537842 batch PCKh 0.5625\n",
      "Trained batch 86 batch loss 0.487156451 batch mAP 0.515045166 batch PCKh 0.375\n",
      "Trained batch 87 batch loss 0.560900152 batch mAP 0.55758667 batch PCKh 0.375\n",
      "Trained batch 88 batch loss 0.500371933 batch mAP 0.579101562 batch PCKh 0.625\n",
      "Trained batch 89 batch loss 0.509294927 batch mAP 0.639801 batch PCKh 0.8125\n",
      "Trained batch 90 batch loss 0.547688723 batch mAP 0.595672607 batch PCKh 0.4375\n",
      "Trained batch 91 batch loss 0.501951396 batch mAP 0.618560791 batch PCKh 0.5\n",
      "Trained batch 92 batch loss 0.49541378 batch mAP 0.653320312 batch PCKh 0.8125\n",
      "Trained batch 93 batch loss 0.451176047 batch mAP 0.66696167 batch PCKh 0.6875\n",
      "Trained batch 94 batch loss 0.497834921 batch mAP 0.652191162 batch PCKh 0.5\n",
      "Trained batch 95 batch loss 0.444113821 batch mAP 0.654998779 batch PCKh 0.375\n",
      "Trained batch 96 batch loss 0.43614471 batch mAP 0.679901123 batch PCKh 0.625\n",
      "Trained batch 97 batch loss 0.430418819 batch mAP 0.687713623 batch PCKh 0.4375\n",
      "Trained batch 98 batch loss 0.394384354 batch mAP 0.722991943 batch PCKh 0.5\n",
      "Trained batch 99 batch loss 0.399872363 batch mAP 0.70123291 batch PCKh 0.3125\n",
      "Trained batch 100 batch loss 0.589910388 batch mAP 0.603302 batch PCKh 0.25\n",
      "Trained batch 101 batch loss 0.586850762 batch mAP 0.60357666 batch PCKh 0.25\n",
      "Trained batch 102 batch loss 0.572706461 batch mAP 0.571624756 batch PCKh 0.625\n",
      "Trained batch 103 batch loss 0.593742073 batch mAP 0.57913208 batch PCKh 0.375\n",
      "Trained batch 104 batch loss 0.613608658 batch mAP 0.567718506 batch PCKh 0.25\n",
      "Trained batch 105 batch loss 0.607466936 batch mAP 0.535858154 batch PCKh 0.625\n",
      "Trained batch 106 batch loss 0.65787375 batch mAP 0.530181885 batch PCKh 0.0625\n",
      "Trained batch 107 batch loss 0.661853313 batch mAP 0.570739746 batch PCKh 0.1875\n",
      "Trained batch 108 batch loss 0.553832293 batch mAP 0.589569092 batch PCKh 0.3125\n",
      "Trained batch 109 batch loss 0.62494266 batch mAP 0.533630371 batch PCKh 0.625\n",
      "Trained batch 110 batch loss 0.54159677 batch mAP 0.557067871 batch PCKh 0.25\n",
      "Trained batch 111 batch loss 0.605330348 batch mAP 0.56817627 batch PCKh 0.375\n",
      "Trained batch 112 batch loss 0.594524682 batch mAP 0.567321777 batch PCKh 0.4375\n",
      "Trained batch 113 batch loss 0.511667192 batch mAP 0.660888672 batch PCKh 0.4375\n",
      "Trained batch 114 batch loss 0.496880949 batch mAP 0.632232666 batch PCKh 0.625\n",
      "Trained batch 115 batch loss 0.486482739 batch mAP 0.67300415 batch PCKh 0.6875\n",
      "Trained batch 116 batch loss 0.434898973 batch mAP 0.657562256 batch PCKh 0.625\n",
      "Trained batch 117 batch loss 0.597367942 batch mAP 0.608337402 batch PCKh 0.3125\n",
      "Trained batch 118 batch loss 0.486363798 batch mAP 0.554382324 batch PCKh 0.25\n",
      "Trained batch 119 batch loss 0.547184348 batch mAP 0.546234131 batch PCKh 0.375\n",
      "Trained batch 120 batch loss 0.542866588 batch mAP 0.648651123 batch PCKh 0.375\n",
      "Trained batch 121 batch loss 0.577708602 batch mAP 0.618652344 batch PCKh 0.375\n",
      "Trained batch 122 batch loss 0.572676897 batch mAP 0.642059326 batch PCKh 0.375\n",
      "Trained batch 123 batch loss 0.515340507 batch mAP 0.649505615 batch PCKh 0.75\n",
      "Trained batch 124 batch loss 0.580114365 batch mAP 0.653533936 batch PCKh 0.5\n",
      "Trained batch 125 batch loss 0.574665308 batch mAP 0.614593506 batch PCKh 0.875\n",
      "Trained batch 126 batch loss 0.516881704 batch mAP 0.685882568 batch PCKh 0.875\n",
      "Trained batch 127 batch loss 0.589793682 batch mAP 0.6277771 batch PCKh 0.8125\n",
      "Trained batch 128 batch loss 0.510216773 batch mAP 0.64050293 batch PCKh 0.25\n",
      "Trained batch 129 batch loss 0.568413734 batch mAP 0.640716553 batch PCKh 0.625\n",
      "Trained batch 130 batch loss 0.541601777 batch mAP 0.581237793 batch PCKh 0.6875\n",
      "Trained batch 131 batch loss 0.631682873 batch mAP 0.501739502 batch PCKh 0.4375\n",
      "Trained batch 132 batch loss 0.603652239 batch mAP 0.550598145 batch PCKh 0.1875\n",
      "Trained batch 133 batch loss 0.593092859 batch mAP 0.590515137 batch PCKh 0.6875\n",
      "Trained batch 134 batch loss 0.572182298 batch mAP 0.59487915 batch PCKh 0.625\n",
      "Trained batch 135 batch loss 0.54634 batch mAP 0.576538086 batch PCKh 0.5\n",
      "Trained batch 136 batch loss 0.60747093 batch mAP 0.530578613 batch PCKh 0.5625\n",
      "Trained batch 137 batch loss 0.543996751 batch mAP 0.629516602 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 138 batch loss 0.477837205 batch mAP 0.629425049 batch PCKh 0.5625\n",
      "Trained batch 139 batch loss 0.472028852 batch mAP 0.5730896 batch PCKh 0.5625\n",
      "Trained batch 140 batch loss 0.516274214 batch mAP 0.504638672 batch PCKh 0.1875\n",
      "Trained batch 141 batch loss 0.532680809 batch mAP 0.55090332 batch PCKh 0.75\n",
      "Trained batch 142 batch loss 0.499943018 batch mAP 0.505737305 batch PCKh 0.375\n",
      "Trained batch 143 batch loss 0.518765271 batch mAP 0.529205322 batch PCKh 0.6875\n",
      "Trained batch 144 batch loss 0.500242352 batch mAP 0.489624023 batch PCKh 0.4375\n",
      "Trained batch 145 batch loss 0.490658164 batch mAP 0.554199219 batch PCKh 0.5625\n",
      "Trained batch 146 batch loss 0.59668082 batch mAP 0.564361572 batch PCKh 0.5625\n",
      "Trained batch 147 batch loss 0.558442175 batch mAP 0.583648682 batch PCKh 0.375\n",
      "Trained batch 148 batch loss 0.51763916 batch mAP 0.652130127 batch PCKh 0.4375\n",
      "Trained batch 149 batch loss 0.521887958 batch mAP 0.614929199 batch PCKh 0.625\n",
      "Trained batch 150 batch loss 0.577418864 batch mAP 0.606994629 batch PCKh 0.8125\n",
      "Trained batch 151 batch loss 0.515867 batch mAP 0.626312256 batch PCKh 0\n",
      "Trained batch 152 batch loss 0.541187763 batch mAP 0.649017334 batch PCKh 0.4375\n",
      "Trained batch 153 batch loss 0.507770717 batch mAP 0.6690979 batch PCKh 0.5\n",
      "Trained batch 154 batch loss 0.462052613 batch mAP 0.642059326 batch PCKh 0.625\n",
      "Trained batch 155 batch loss 0.473105907 batch mAP 0.664245605 batch PCKh 0.5625\n",
      "Trained batch 156 batch loss 0.552594721 batch mAP 0.668121338 batch PCKh 0.625\n",
      "Trained batch 157 batch loss 0.547439 batch mAP 0.661895752 batch PCKh 0.75\n",
      "Trained batch 158 batch loss 0.539619625 batch mAP 0.585968 batch PCKh 0.25\n",
      "Trained batch 159 batch loss 0.527439475 batch mAP 0.536743164 batch PCKh 0.75\n",
      "Trained batch 160 batch loss 0.478845835 batch mAP 0.588867188 batch PCKh 0.625\n",
      "Trained batch 161 batch loss 0.407289028 batch mAP 0.534057617 batch PCKh 0.5625\n",
      "Trained batch 162 batch loss 0.469938129 batch mAP 0.541015625 batch PCKh 0.6875\n",
      "Trained batch 163 batch loss 0.650815 batch mAP 0.529663086 batch PCKh 0.75\n",
      "Trained batch 164 batch loss 0.482167035 batch mAP 0.666748047 batch PCKh 0.875\n",
      "Trained batch 165 batch loss 0.523300469 batch mAP 0.651001 batch PCKh 0.4375\n",
      "Trained batch 166 batch loss 0.527926743 batch mAP 0.626495361 batch PCKh 0.4375\n",
      "Trained batch 167 batch loss 0.568690777 batch mAP 0.651641846 batch PCKh 0.25\n",
      "Trained batch 168 batch loss 0.461297691 batch mAP 0.663116455 batch PCKh 0.375\n",
      "Trained batch 169 batch loss 0.532837749 batch mAP 0.664794922 batch PCKh 0.4375\n",
      "Trained batch 170 batch loss 0.539061785 batch mAP 0.662017822 batch PCKh 0.125\n",
      "Trained batch 171 batch loss 0.497453749 batch mAP 0.675323486 batch PCKh 0.3125\n",
      "Trained batch 172 batch loss 0.454578221 batch mAP 0.707214355 batch PCKh 0.4375\n",
      "Trained batch 173 batch loss 0.377566785 batch mAP 0.718261719 batch PCKh 0.5\n",
      "Trained batch 174 batch loss 0.468717694 batch mAP 0.690002441 batch PCKh 0.375\n",
      "Trained batch 175 batch loss 0.513435066 batch mAP 0.616973877 batch PCKh 0.3125\n",
      "Trained batch 176 batch loss 0.468389511 batch mAP 0.655059814 batch PCKh 0.5625\n",
      "Trained batch 177 batch loss 0.455565125 batch mAP 0.643066406 batch PCKh 0.4375\n",
      "Trained batch 178 batch loss 0.453524649 batch mAP 0.673370361 batch PCKh 0.3125\n",
      "Trained batch 179 batch loss 0.476777673 batch mAP 0.676605225 batch PCKh 0.4375\n",
      "Trained batch 180 batch loss 0.43213591 batch mAP 0.702087402 batch PCKh 0.75\n",
      "Trained batch 181 batch loss 0.451123923 batch mAP 0.598632812 batch PCKh 0.1875\n",
      "Trained batch 182 batch loss 0.447715908 batch mAP 0.682983398 batch PCKh 0.1875\n",
      "Trained batch 183 batch loss 0.48741734 batch mAP 0.701141357 batch PCKh 0.4375\n",
      "Trained batch 184 batch loss 0.471830577 batch mAP 0.70916748 batch PCKh 0.4375\n",
      "Trained batch 185 batch loss 0.434866071 batch mAP 0.707397461 batch PCKh 0.375\n",
      "Trained batch 186 batch loss 0.41318962 batch mAP 0.723144531 batch PCKh 0.375\n",
      "Trained batch 187 batch loss 0.442407221 batch mAP 0.641967773 batch PCKh 0.3125\n",
      "Trained batch 188 batch loss 0.479338706 batch mAP 0.614898682 batch PCKh 0.4375\n",
      "Trained batch 189 batch loss 0.518402576 batch mAP 0.612884521 batch PCKh 0.4375\n",
      "Trained batch 190 batch loss 0.510388196 batch mAP 0.619110107 batch PCKh 0.75\n",
      "Trained batch 191 batch loss 0.497057736 batch mAP 0.634002686 batch PCKh 0.8125\n",
      "Trained batch 192 batch loss 0.463458061 batch mAP 0.633544922 batch PCKh 0.5\n",
      "Trained batch 193 batch loss 0.48132205 batch mAP 0.647522 batch PCKh 0.0625\n",
      "Trained batch 194 batch loss 0.429209083 batch mAP 0.677703857 batch PCKh 0.375\n",
      "Trained batch 195 batch loss 0.477706879 batch mAP 0.643035889 batch PCKh 0.4375\n",
      "Trained batch 196 batch loss 0.48038584 batch mAP 0.644348145 batch PCKh 0.25\n",
      "Trained batch 197 batch loss 0.499679863 batch mAP 0.63381958 batch PCKh 0.375\n",
      "Trained batch 198 batch loss 0.432822317 batch mAP 0.685699463 batch PCKh 0.5\n",
      "Trained batch 199 batch loss 0.476043284 batch mAP 0.490905762 batch PCKh 0\n",
      "Trained batch 200 batch loss 0.584567308 batch mAP 0.560668945 batch PCKh 0.5\n",
      "Trained batch 201 batch loss 0.596984088 batch mAP 0.552032471 batch PCKh 0.5625\n",
      "Trained batch 202 batch loss 0.70231986 batch mAP 0.488372803 batch PCKh 0.6875\n",
      "Trained batch 203 batch loss 0.62410593 batch mAP 0.504455566 batch PCKh 0.0625\n",
      "Trained batch 204 batch loss 0.521215 batch mAP 0.604736328 batch PCKh 0.1875\n",
      "Trained batch 205 batch loss 0.505654037 batch mAP 0.614318848 batch PCKh 0.625\n",
      "Trained batch 206 batch loss 0.556159496 batch mAP 0.662139893 batch PCKh 0.4375\n",
      "Trained batch 207 batch loss 0.539758444 batch mAP 0.57220459 batch PCKh 0.1875\n",
      "Trained batch 208 batch loss 0.511748374 batch mAP 0.611145 batch PCKh 0.3125\n",
      "Trained batch 209 batch loss 0.646939278 batch mAP 0.540649414 batch PCKh 0.1875\n",
      "Trained batch 210 batch loss 0.56282407 batch mAP 0.542327881 batch PCKh 0.25\n",
      "Trained batch 211 batch loss 0.501312375 batch mAP 0.603881836 batch PCKh 0.625\n",
      "Trained batch 212 batch loss 0.576764882 batch mAP 0.58996582 batch PCKh 0.375\n",
      "Trained batch 213 batch loss 0.567635357 batch mAP 0.552032471 batch PCKh 0.25\n",
      "Trained batch 214 batch loss 0.482870162 batch mAP 0.662109375 batch PCKh 0.3125\n",
      "Trained batch 215 batch loss 0.521140873 batch mAP 0.594177246 batch PCKh 0.875\n",
      "Trained batch 216 batch loss 0.445081592 batch mAP 0.593261719 batch PCKh 0.8125\n",
      "Trained batch 217 batch loss 0.508485198 batch mAP 0.586517334 batch PCKh 0.6875\n",
      "Trained batch 218 batch loss 0.47320953 batch mAP 0.614868164 batch PCKh 0.75\n",
      "Trained batch 219 batch loss 0.516670465 batch mAP 0.593414307 batch PCKh 0.875\n",
      "Trained batch 220 batch loss 0.556583881 batch mAP 0.646209717 batch PCKh 0.3125\n",
      "Trained batch 221 batch loss 0.486143142 batch mAP 0.660125732 batch PCKh 0.4375\n",
      "Trained batch 222 batch loss 0.451221615 batch mAP 0.678588867 batch PCKh 0.6875\n",
      "Trained batch 223 batch loss 0.429623961 batch mAP 0.700164795 batch PCKh 0.5625\n",
      "Trained batch 224 batch loss 0.470036983 batch mAP 0.668640137 batch PCKh 0.5625\n",
      "Trained batch 225 batch loss 0.57906425 batch mAP 0.606964111 batch PCKh 0.4375\n",
      "Trained batch 226 batch loss 0.571621537 batch mAP 0.584320068 batch PCKh 0.4375\n",
      "Trained batch 227 batch loss 0.53382349 batch mAP 0.622558594 batch PCKh 0.625\n",
      "Trained batch 228 batch loss 0.557990432 batch mAP 0.626403809 batch PCKh 0.375\n",
      "Trained batch 229 batch loss 0.564891219 batch mAP 0.566314697 batch PCKh 0.375\n",
      "Trained batch 230 batch loss 0.573502243 batch mAP 0.603179932 batch PCKh 0.1875\n",
      "Trained batch 231 batch loss 0.510410786 batch mAP 0.629180908 batch PCKh 0.6875\n",
      "Trained batch 232 batch loss 0.488130808 batch mAP 0.663085938 batch PCKh 0.875\n",
      "Trained batch 233 batch loss 0.534664631 batch mAP 0.634521484 batch PCKh 0.75\n",
      "Trained batch 234 batch loss 0.526457369 batch mAP 0.593963623 batch PCKh 0.625\n",
      "Trained batch 235 batch loss 0.5410074 batch mAP 0.593261719 batch PCKh 0.375\n",
      "Trained batch 236 batch loss 0.569021344 batch mAP 0.602417 batch PCKh 0.375\n",
      "Trained batch 237 batch loss 0.579007745 batch mAP 0.525360107 batch PCKh 0.125\n",
      "Trained batch 238 batch loss 0.577411532 batch mAP 0.512298584 batch PCKh 0.375\n",
      "Trained batch 239 batch loss 0.536640048 batch mAP 0.596344 batch PCKh 0.5\n",
      "Trained batch 240 batch loss 0.49369061 batch mAP 0.533721924 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 241 batch loss 0.459223747 batch mAP 0.598297119 batch PCKh 0.5\n",
      "Trained batch 242 batch loss 0.524996758 batch mAP 0.491882324 batch PCKh 0.5\n",
      "Trained batch 243 batch loss 0.450891495 batch mAP 0.643981934 batch PCKh 0.25\n",
      "Trained batch 244 batch loss 0.467340678 batch mAP 0.650756836 batch PCKh 0.375\n",
      "Trained batch 245 batch loss 0.514639854 batch mAP 0.51071167 batch PCKh 0.75\n",
      "Trained batch 246 batch loss 0.517 batch mAP 0.517456055 batch PCKh 0.5\n",
      "Trained batch 247 batch loss 0.450653 batch mAP 0.513336182 batch PCKh 0.375\n",
      "Trained batch 248 batch loss 0.426869541 batch mAP 0.594970703 batch PCKh 0.3125\n",
      "Trained batch 249 batch loss 0.480451256 batch mAP 0.564025879 batch PCKh 0.4375\n",
      "Trained batch 250 batch loss 0.477810621 batch mAP 0.573455811 batch PCKh 0.125\n",
      "Trained batch 251 batch loss 0.526399374 batch mAP 0.556427 batch PCKh 0.4375\n",
      "Trained batch 252 batch loss 0.524915934 batch mAP 0.549560547 batch PCKh 0.75\n",
      "Trained batch 253 batch loss 0.459041595 batch mAP 0.64956665 batch PCKh 0.5625\n",
      "Trained batch 254 batch loss 0.428942084 batch mAP 0.63571167 batch PCKh 0.625\n",
      "Trained batch 255 batch loss 0.501914322 batch mAP 0.62298584 batch PCKh 0.4375\n",
      "Trained batch 256 batch loss 0.481460929 batch mAP 0.633972168 batch PCKh 0.6875\n",
      "Trained batch 257 batch loss 0.406931102 batch mAP 0.627105713 batch PCKh 0.625\n",
      "Trained batch 258 batch loss 0.446208596 batch mAP 0.617858887 batch PCKh 0.75\n",
      "Trained batch 259 batch loss 0.533281088 batch mAP 0.575439453 batch PCKh 0.75\n",
      "Trained batch 260 batch loss 0.438928872 batch mAP 0.605255127 batch PCKh 0.5625\n",
      "Trained batch 261 batch loss 0.573799 batch mAP 0.583435059 batch PCKh 0.5\n",
      "Trained batch 262 batch loss 0.543299794 batch mAP 0.553161621 batch PCKh 0.5625\n",
      "Trained batch 263 batch loss 0.499335408 batch mAP 0.645690918 batch PCKh 0.5\n",
      "Trained batch 264 batch loss 0.574514806 batch mAP 0.573425293 batch PCKh 0.5625\n",
      "Trained batch 265 batch loss 0.590035677 batch mAP 0.543609619 batch PCKh 0.75\n",
      "Trained batch 266 batch loss 0.639921904 batch mAP 0.558105469 batch PCKh 0.25\n",
      "Trained batch 267 batch loss 0.594581604 batch mAP 0.538238525 batch PCKh 0\n",
      "Trained batch 268 batch loss 0.552204 batch mAP 0.617584229 batch PCKh 0.5\n",
      "Trained batch 269 batch loss 0.510917187 batch mAP 0.624603271 batch PCKh 0.625\n",
      "Trained batch 270 batch loss 0.543172061 batch mAP 0.56729126 batch PCKh 0.6875\n",
      "Trained batch 271 batch loss 0.537478864 batch mAP 0.611755371 batch PCKh 0.875\n",
      "Trained batch 272 batch loss 0.527631402 batch mAP 0.492523193 batch PCKh 0.4375\n",
      "Trained batch 273 batch loss 0.438154936 batch mAP 0.550109863 batch PCKh 0.625\n",
      "Trained batch 274 batch loss 0.532141 batch mAP 0.534301758 batch PCKh 0.4375\n",
      "Trained batch 275 batch loss 0.544417 batch mAP 0.4659729 batch PCKh 0.125\n",
      "Trained batch 276 batch loss 0.50594759 batch mAP 0.548492432 batch PCKh 0.4375\n",
      "Trained batch 277 batch loss 0.435588479 batch mAP 0.629547119 batch PCKh 0.5\n",
      "Trained batch 278 batch loss 0.415533066 batch mAP 0.669616699 batch PCKh 0.4375\n",
      "Trained batch 279 batch loss 0.589790106 batch mAP 0.604095459 batch PCKh 0.625\n",
      "Trained batch 280 batch loss 0.514735878 batch mAP 0.631744385 batch PCKh 0.625\n",
      "Trained batch 281 batch loss 0.486091673 batch mAP 0.657806396 batch PCKh 0.875\n",
      "Trained batch 282 batch loss 0.483114898 batch mAP 0.620147705 batch PCKh 0.5625\n",
      "Trained batch 283 batch loss 0.419124246 batch mAP 0.586395264 batch PCKh 0.4375\n",
      "Trained batch 284 batch loss 0.485531092 batch mAP 0.611694336 batch PCKh 0.125\n",
      "Trained batch 285 batch loss 0.498029 batch mAP 0.623413086 batch PCKh 0.25\n",
      "Trained batch 286 batch loss 0.364469469 batch mAP 0.671478271 batch PCKh 0.3125\n",
      "Trained batch 287 batch loss 0.394342661 batch mAP 0.706970215 batch PCKh 0.25\n",
      "Trained batch 288 batch loss 0.438914239 batch mAP 0.737060547 batch PCKh 0.4375\n",
      "Trained batch 289 batch loss 0.38473475 batch mAP 0.721221924 batch PCKh 0.375\n",
      "Trained batch 290 batch loss 0.391129613 batch mAP 0.703155518 batch PCKh 0.4375\n",
      "Trained batch 291 batch loss 0.51431036 batch mAP 0.659515381 batch PCKh 0.625\n",
      "Trained batch 292 batch loss 0.551867723 batch mAP 0.639068604 batch PCKh 0.4375\n",
      "Trained batch 293 batch loss 0.654921591 batch mAP 0.573791504 batch PCKh 0.25\n",
      "Trained batch 294 batch loss 0.60658592 batch mAP 0.534912109 batch PCKh 0.75\n",
      "Trained batch 295 batch loss 0.616528153 batch mAP 0.54385376 batch PCKh 0.5\n",
      "Trained batch 296 batch loss 0.605392575 batch mAP 0.570373535 batch PCKh 0.625\n",
      "Trained batch 297 batch loss 0.687204182 batch mAP 0.519958496 batch PCKh 0.875\n",
      "Trained batch 298 batch loss 0.546086788 batch mAP 0.572052 batch PCKh 0.5\n",
      "Trained batch 299 batch loss 0.535824 batch mAP 0.490966797 batch PCKh 0.6875\n",
      "Trained batch 300 batch loss 0.530270219 batch mAP 0.500061035 batch PCKh 0.75\n",
      "Trained batch 301 batch loss 0.595454216 batch mAP 0.536438 batch PCKh 0.75\n",
      "Trained batch 302 batch loss 0.611228 batch mAP 0.53302 batch PCKh 0.875\n",
      "Trained batch 303 batch loss 0.591759086 batch mAP 0.585723877 batch PCKh 0.1875\n",
      "Trained batch 304 batch loss 0.730446815 batch mAP 0.472229 batch PCKh 0.25\n",
      "Trained batch 305 batch loss 0.582356632 batch mAP 0.547668457 batch PCKh 0.5625\n",
      "Trained batch 306 batch loss 0.62512362 batch mAP 0.524658203 batch PCKh 0.625\n",
      "Trained batch 307 batch loss 0.645837307 batch mAP 0.54196167 batch PCKh 0.75\n",
      "Trained batch 308 batch loss 0.494007021 batch mAP 0.517425537 batch PCKh 0.0625\n",
      "Trained batch 309 batch loss 0.471001267 batch mAP 0.572540283 batch PCKh 0.1875\n",
      "Trained batch 310 batch loss 0.549142599 batch mAP 0.571624756 batch PCKh 0.5625\n",
      "Trained batch 311 batch loss 0.469304413 batch mAP 0.539703369 batch PCKh 0.3125\n",
      "Trained batch 312 batch loss 0.441328645 batch mAP 0.567321777 batch PCKh 0\n",
      "Trained batch 313 batch loss 0.557531059 batch mAP 0.532012939 batch PCKh 0\n",
      "Trained batch 314 batch loss 0.590670943 batch mAP 0.531158447 batch PCKh 0.25\n",
      "Trained batch 315 batch loss 0.635686696 batch mAP 0.512207031 batch PCKh 0.1875\n",
      "Trained batch 316 batch loss 0.615055323 batch mAP 0.560913086 batch PCKh 0.5\n",
      "Trained batch 317 batch loss 0.657077074 batch mAP 0.512023926 batch PCKh 0.1875\n",
      "Trained batch 318 batch loss 0.636775255 batch mAP 0.528930664 batch PCKh 0.5625\n",
      "Trained batch 319 batch loss 0.568087816 batch mAP 0.519012451 batch PCKh 0.5625\n",
      "Trained batch 320 batch loss 0.512553 batch mAP 0.511077881 batch PCKh 0.1875\n",
      "Trained batch 321 batch loss 0.553403854 batch mAP 0.483642578 batch PCKh 0.6875\n",
      "Trained batch 322 batch loss 0.583878756 batch mAP 0.567565918 batch PCKh 0.25\n",
      "Trained batch 323 batch loss 0.571588278 batch mAP 0.591552734 batch PCKh 0.4375\n",
      "Trained batch 324 batch loss 0.586306632 batch mAP 0.572235107 batch PCKh 0.75\n",
      "Trained batch 325 batch loss 0.515404522 batch mAP 0.539642334 batch PCKh 0.3125\n",
      "Trained batch 326 batch loss 0.536646545 batch mAP 0.571014404 batch PCKh 0.1875\n",
      "Trained batch 327 batch loss 0.564498425 batch mAP 0.496185303 batch PCKh 0.4375\n",
      "Trained batch 328 batch loss 0.529375792 batch mAP 0.482086182 batch PCKh 0.1875\n",
      "Trained batch 329 batch loss 0.622683108 batch mAP 0.42477417 batch PCKh 0.5\n",
      "Trained batch 330 batch loss 0.475386143 batch mAP 0.489868164 batch PCKh 0.0625\n",
      "Trained batch 331 batch loss 0.565507531 batch mAP 0.53237915 batch PCKh 0.375\n",
      "Trained batch 332 batch loss 0.521462917 batch mAP 0.572937 batch PCKh 0.5\n",
      "Trained batch 333 batch loss 0.49829486 batch mAP 0.55480957 batch PCKh 0.5625\n",
      "Trained batch 334 batch loss 0.585838079 batch mAP 0.550872803 batch PCKh 0.8125\n",
      "Trained batch 335 batch loss 0.513122082 batch mAP 0.594940186 batch PCKh 0.875\n",
      "Trained batch 336 batch loss 0.573398 batch mAP 0.549682617 batch PCKh 0.75\n",
      "Trained batch 337 batch loss 0.555666566 batch mAP 0.54397583 batch PCKh 0.8125\n",
      "Trained batch 338 batch loss 0.561131299 batch mAP 0.563781738 batch PCKh 0.4375\n",
      "Trained batch 339 batch loss 0.546115935 batch mAP 0.542419434 batch PCKh 0.3125\n",
      "Trained batch 340 batch loss 0.565547466 batch mAP 0.565856934 batch PCKh 0.8125\n",
      "Trained batch 341 batch loss 0.561920643 batch mAP 0.536468506 batch PCKh 0.125\n",
      "Trained batch 342 batch loss 0.605720758 batch mAP 0.525390625 batch PCKh 0.6875\n",
      "Trained batch 343 batch loss 0.6085729 batch mAP 0.561676 batch PCKh 0.375\n",
      "Trained batch 344 batch loss 0.583767235 batch mAP 0.571289062 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 345 batch loss 0.548582792 batch mAP 0.630767822 batch PCKh 0.8125\n",
      "Trained batch 346 batch loss 0.583525062 batch mAP 0.614563 batch PCKh 0.3125\n",
      "Trained batch 347 batch loss 0.564090729 batch mAP 0.573944092 batch PCKh 0.625\n",
      "Trained batch 348 batch loss 0.562943459 batch mAP 0.618011475 batch PCKh 0.625\n",
      "Trained batch 349 batch loss 0.448294371 batch mAP 0.633667 batch PCKh 0.75\n",
      "Trained batch 350 batch loss 0.525650203 batch mAP 0.605834961 batch PCKh 0.375\n",
      "Trained batch 351 batch loss 0.554882348 batch mAP 0.603210449 batch PCKh 0.3125\n",
      "Trained batch 352 batch loss 0.486857355 batch mAP 0.600860596 batch PCKh 0.6875\n",
      "Trained batch 353 batch loss 0.628256202 batch mAP 0.52130127 batch PCKh 0.1875\n",
      "Trained batch 354 batch loss 0.513710678 batch mAP 0.569793701 batch PCKh 0.4375\n",
      "Trained batch 355 batch loss 0.491731822 batch mAP 0.578369141 batch PCKh 0.4375\n",
      "Trained batch 356 batch loss 0.458711922 batch mAP 0.612915039 batch PCKh 0.4375\n",
      "Trained batch 357 batch loss 0.502210081 batch mAP 0.57232666 batch PCKh 0.1875\n",
      "Trained batch 358 batch loss 0.53350234 batch mAP 0.580108643 batch PCKh 0.625\n",
      "Trained batch 359 batch loss 0.558183134 batch mAP 0.555480957 batch PCKh 0.375\n",
      "Trained batch 360 batch loss 0.480597347 batch mAP 0.615783691 batch PCKh 0.3125\n",
      "Trained batch 361 batch loss 0.448054612 batch mAP 0.586914062 batch PCKh 0.4375\n",
      "Trained batch 362 batch loss 0.554354191 batch mAP 0.542755127 batch PCKh 0.75\n",
      "Trained batch 363 batch loss 0.51635617 batch mAP 0.608795166 batch PCKh 0.6875\n",
      "Trained batch 364 batch loss 0.453282177 batch mAP 0.605255127 batch PCKh 0.5625\n",
      "Trained batch 365 batch loss 0.550080419 batch mAP 0.587402344 batch PCKh 0.625\n",
      "Trained batch 366 batch loss 0.496867776 batch mAP 0.637298584 batch PCKh 0.625\n",
      "Trained batch 367 batch loss 0.536533475 batch mAP 0.615997314 batch PCKh 0.3125\n",
      "Trained batch 368 batch loss 0.58765018 batch mAP 0.632171631 batch PCKh 0.5\n",
      "Trained batch 369 batch loss 0.506647885 batch mAP 0.646759033 batch PCKh 0.4375\n",
      "Trained batch 370 batch loss 0.489434481 batch mAP 0.631500244 batch PCKh 0.125\n",
      "Trained batch 371 batch loss 0.458542228 batch mAP 0.653045654 batch PCKh 0\n",
      "Trained batch 372 batch loss 0.531439245 batch mAP 0.603424072 batch PCKh 0.3125\n",
      "Trained batch 373 batch loss 0.545538783 batch mAP 0.523620605 batch PCKh 0.3125\n",
      "Trained batch 374 batch loss 0.492910564 batch mAP 0.64050293 batch PCKh 0.4375\n",
      "Trained batch 375 batch loss 0.516622543 batch mAP 0.643585205 batch PCKh 0.5625\n",
      "Trained batch 376 batch loss 0.423515588 batch mAP 0.674163818 batch PCKh 0.625\n",
      "Trained batch 377 batch loss 0.571136951 batch mAP 0.607971191 batch PCKh 0.3125\n",
      "Trained batch 378 batch loss 0.499259323 batch mAP 0.650939941 batch PCKh 0.375\n",
      "Trained batch 379 batch loss 0.563615441 batch mAP 0.61428833 batch PCKh 0.6875\n",
      "Trained batch 380 batch loss 0.603575826 batch mAP 0.598754883 batch PCKh 0.8125\n",
      "Trained batch 381 batch loss 0.56677 batch mAP 0.597503662 batch PCKh 0.4375\n",
      "Trained batch 382 batch loss 0.519369125 batch mAP 0.568573 batch PCKh 0.375\n",
      "Trained batch 383 batch loss 0.509725928 batch mAP 0.595184326 batch PCKh 0.3125\n",
      "Trained batch 384 batch loss 0.492440522 batch mAP 0.610137939 batch PCKh 0.6875\n",
      "Trained batch 385 batch loss 0.572422206 batch mAP 0.564422607 batch PCKh 0.5625\n",
      "Trained batch 386 batch loss 0.500497639 batch mAP 0.569335938 batch PCKh 0.375\n",
      "Trained batch 387 batch loss 0.45913887 batch mAP 0.585754395 batch PCKh 0\n",
      "Trained batch 388 batch loss 0.497603834 batch mAP 0.573547363 batch PCKh 0.6875\n",
      "Trained batch 389 batch loss 0.507363081 batch mAP 0.619537354 batch PCKh 0.6875\n",
      "Trained batch 390 batch loss 0.509905457 batch mAP 0.622436523 batch PCKh 0.875\n",
      "Trained batch 391 batch loss 0.561783493 batch mAP 0.629547119 batch PCKh 0.3125\n",
      "Trained batch 392 batch loss 0.540535331 batch mAP 0.569946289 batch PCKh 0.875\n",
      "Trained batch 393 batch loss 0.510910034 batch mAP 0.630065918 batch PCKh 0.6875\n",
      "Trained batch 394 batch loss 0.561724 batch mAP 0.58190918 batch PCKh 0.625\n",
      "Trained batch 395 batch loss 0.565023363 batch mAP 0.602294922 batch PCKh 0.75\n",
      "Trained batch 396 batch loss 0.516346514 batch mAP 0.585235596 batch PCKh 0.75\n",
      "Trained batch 397 batch loss 0.499814332 batch mAP 0.627838135 batch PCKh 0.5625\n",
      "Trained batch 398 batch loss 0.54724431 batch mAP 0.638183594 batch PCKh 0.5625\n",
      "Trained batch 399 batch loss 0.492253244 batch mAP 0.731750488 batch PCKh 0.4375\n",
      "Trained batch 400 batch loss 0.455299318 batch mAP 0.674865723 batch PCKh 0.4375\n",
      "Trained batch 401 batch loss 0.452884555 batch mAP 0.679840088 batch PCKh 0.75\n",
      "Trained batch 402 batch loss 0.449979395 batch mAP 0.659179688 batch PCKh 0.75\n",
      "Trained batch 403 batch loss 0.456918269 batch mAP 0.628814697 batch PCKh 0.6875\n",
      "Trained batch 404 batch loss 0.505102158 batch mAP 0.641418457 batch PCKh 0.875\n",
      "Trained batch 405 batch loss 0.481814504 batch mAP 0.691986084 batch PCKh 0.5\n",
      "Trained batch 406 batch loss 0.441048801 batch mAP 0.685455322 batch PCKh 0.5625\n",
      "Trained batch 407 batch loss 0.459479332 batch mAP 0.662506104 batch PCKh 0.5\n",
      "Trained batch 408 batch loss 0.487666965 batch mAP 0.64465332 batch PCKh 0.125\n",
      "Trained batch 409 batch loss 0.523584723 batch mAP 0.682006836 batch PCKh 0.3125\n",
      "Trained batch 410 batch loss 0.425675929 batch mAP 0.707397461 batch PCKh 0.5625\n",
      "Trained batch 411 batch loss 0.403609663 batch mAP 0.681152344 batch PCKh 0.25\n",
      "Trained batch 412 batch loss 0.463907629 batch mAP 0.649475098 batch PCKh 0.8125\n",
      "Trained batch 413 batch loss 0.504557669 batch mAP 0.671600342 batch PCKh 0.3125\n",
      "Trained batch 414 batch loss 0.406806201 batch mAP 0.689880371 batch PCKh 0.75\n",
      "Trained batch 415 batch loss 0.47091645 batch mAP 0.667175293 batch PCKh 0.4375\n",
      "Trained batch 416 batch loss 0.446015805 batch mAP 0.660064697 batch PCKh 0.25\n",
      "Trained batch 417 batch loss 0.604524195 batch mAP 0.585022 batch PCKh 0.375\n",
      "Trained batch 418 batch loss 0.547855139 batch mAP 0.542938232 batch PCKh 0.125\n",
      "Trained batch 419 batch loss 0.508347034 batch mAP 0.664672852 batch PCKh 0.875\n",
      "Trained batch 420 batch loss 0.566619515 batch mAP 0.563446045 batch PCKh 0.125\n",
      "Trained batch 421 batch loss 0.500792861 batch mAP 0.586120605 batch PCKh 0.625\n",
      "Trained batch 422 batch loss 0.558445215 batch mAP 0.590148926 batch PCKh 0.1875\n",
      "Trained batch 423 batch loss 0.419405639 batch mAP 0.500854492 batch PCKh 0\n",
      "Trained batch 424 batch loss 0.515714943 batch mAP 0.592865 batch PCKh 0.125\n",
      "Trained batch 425 batch loss 0.510508776 batch mAP 0.496032715 batch PCKh 0.125\n",
      "Trained batch 426 batch loss 0.527768552 batch mAP 0.545013428 batch PCKh 0.0625\n",
      "Trained batch 427 batch loss 0.537155867 batch mAP 0.599456787 batch PCKh 0.375\n",
      "Trained batch 428 batch loss 0.477078617 batch mAP 0.663238525 batch PCKh 0.625\n",
      "Trained batch 429 batch loss 0.435197502 batch mAP 0.667388916 batch PCKh 0.6875\n",
      "Trained batch 430 batch loss 0.471427888 batch mAP 0.61239624 batch PCKh 0.75\n",
      "Trained batch 431 batch loss 0.486299753 batch mAP 0.623535156 batch PCKh 0.3125\n",
      "Trained batch 432 batch loss 0.562362731 batch mAP 0.589996338 batch PCKh 0.5\n",
      "Trained batch 433 batch loss 0.597237 batch mAP 0.561981201 batch PCKh 0.3125\n",
      "Trained batch 434 batch loss 0.4346219 batch mAP 0.699157715 batch PCKh 0.875\n",
      "Trained batch 435 batch loss 0.512346 batch mAP 0.731506348 batch PCKh 0.3125\n",
      "Trained batch 436 batch loss 0.473461747 batch mAP 0.667053223 batch PCKh 0.5625\n",
      "Trained batch 437 batch loss 0.494482398 batch mAP 0.725585938 batch PCKh 0.375\n",
      "Trained batch 438 batch loss 0.451676846 batch mAP 0.737091064 batch PCKh 0.4375\n",
      "Trained batch 439 batch loss 0.503424883 batch mAP 0.69543457 batch PCKh 0.625\n",
      "Trained batch 440 batch loss 0.454054892 batch mAP 0.6612854 batch PCKh 0.4375\n",
      "Trained batch 441 batch loss 0.484000742 batch mAP 0.603637695 batch PCKh 0.6875\n",
      "Trained batch 442 batch loss 0.517190218 batch mAP 0.644287109 batch PCKh 0.375\n",
      "Trained batch 443 batch loss 0.506329834 batch mAP 0.619110107 batch PCKh 0.3125\n",
      "Trained batch 444 batch loss 0.521291673 batch mAP 0.593841553 batch PCKh 0.5625\n",
      "Trained batch 445 batch loss 0.557472765 batch mAP 0.603302 batch PCKh 0.5625\n",
      "Trained batch 446 batch loss 0.561365 batch mAP 0.61529541 batch PCKh 0.6875\n",
      "Trained batch 447 batch loss 0.615431786 batch mAP 0.563873291 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 448 batch loss 0.606942832 batch mAP 0.479705811 batch PCKh 0.75\n",
      "Trained batch 449 batch loss 0.568407536 batch mAP 0.490142822 batch PCKh 0.75\n",
      "Trained batch 450 batch loss 0.541539 batch mAP 0.528839111 batch PCKh 0.5625\n",
      "Trained batch 451 batch loss 0.573084831 batch mAP 0.553833 batch PCKh 0\n",
      "Trained batch 452 batch loss 0.515694261 batch mAP 0.646148682 batch PCKh 0.3125\n",
      "Trained batch 453 batch loss 0.519572258 batch mAP 0.628082275 batch PCKh 0.3125\n",
      "Trained batch 454 batch loss 0.53813231 batch mAP 0.616241455 batch PCKh 0.4375\n",
      "Trained batch 455 batch loss 0.583723903 batch mAP 0.572753906 batch PCKh 0.5\n",
      "Trained batch 456 batch loss 0.509571195 batch mAP 0.609283447 batch PCKh 0.375\n",
      "Trained batch 457 batch loss 0.516433358 batch mAP 0.647064209 batch PCKh 0.4375\n",
      "Trained batch 458 batch loss 0.516400158 batch mAP 0.674865723 batch PCKh 0.5\n",
      "Trained batch 459 batch loss 0.537421107 batch mAP 0.617126465 batch PCKh 0.375\n",
      "Trained batch 460 batch loss 0.564468741 batch mAP 0.578491211 batch PCKh 0.5625\n",
      "Trained batch 461 batch loss 0.585289657 batch mAP 0.570159912 batch PCKh 0.375\n",
      "Trained batch 462 batch loss 0.616331577 batch mAP 0.521057129 batch PCKh 0.625\n",
      "Trained batch 463 batch loss 0.572401345 batch mAP 0.567932129 batch PCKh 0.4375\n",
      "Trained batch 464 batch loss 0.572719455 batch mAP 0.588378906 batch PCKh 0.3125\n",
      "Trained batch 465 batch loss 0.53166604 batch mAP 0.606658936 batch PCKh 0.4375\n",
      "Trained batch 466 batch loss 0.516710043 batch mAP 0.564819336 batch PCKh 0.3125\n",
      "Trained batch 467 batch loss 0.51499 batch mAP 0.527740479 batch PCKh 0.75\n",
      "Trained batch 468 batch loss 0.550036669 batch mAP 0.522277832 batch PCKh 0.4375\n",
      "Trained batch 469 batch loss 0.600981116 batch mAP 0.55279541 batch PCKh 0.3125\n",
      "Trained batch 470 batch loss 0.568794966 batch mAP 0.567199707 batch PCKh 0.1875\n",
      "Trained batch 471 batch loss 0.589801908 batch mAP 0.538787842 batch PCKh 0.75\n",
      "Trained batch 472 batch loss 0.601621747 batch mAP 0.527374268 batch PCKh 0.5625\n",
      "Trained batch 473 batch loss 0.610477686 batch mAP 0.550445557 batch PCKh 0.6875\n",
      "Trained batch 474 batch loss 0.546213329 batch mAP 0.644104 batch PCKh 0.625\n",
      "Trained batch 475 batch loss 0.463000119 batch mAP 0.603881836 batch PCKh 0.25\n",
      "Trained batch 476 batch loss 0.5337286 batch mAP 0.613678 batch PCKh 0.6875\n",
      "Trained batch 477 batch loss 0.466696262 batch mAP 0.626861572 batch PCKh 0.625\n",
      "Trained batch 478 batch loss 0.524540961 batch mAP 0.593109131 batch PCKh 0.5625\n",
      "Trained batch 479 batch loss 0.492490888 batch mAP 0.59387207 batch PCKh 0.625\n",
      "Trained batch 480 batch loss 0.474087894 batch mAP 0.623260498 batch PCKh 0.4375\n",
      "Trained batch 481 batch loss 0.509926498 batch mAP 0.616790771 batch PCKh 0.625\n",
      "Trained batch 482 batch loss 0.509906471 batch mAP 0.645568848 batch PCKh 0.6875\n",
      "Trained batch 483 batch loss 0.463775069 batch mAP 0.601806641 batch PCKh 0.5\n",
      "Trained batch 484 batch loss 0.488661468 batch mAP 0.639404297 batch PCKh 0.6875\n",
      "Trained batch 485 batch loss 0.500858724 batch mAP 0.665740967 batch PCKh 0.4375\n",
      "Trained batch 486 batch loss 0.516788125 batch mAP 0.684295654 batch PCKh 0.25\n",
      "Trained batch 487 batch loss 0.509896159 batch mAP 0.663116455 batch PCKh 0.625\n",
      "Trained batch 488 batch loss 0.538286567 batch mAP 0.621307373 batch PCKh 0.8125\n",
      "Trained batch 489 batch loss 0.514403939 batch mAP 0.606079102 batch PCKh 0.5\n",
      "Trained batch 490 batch loss 0.509244 batch mAP 0.567077637 batch PCKh 0\n",
      "Trained batch 491 batch loss 0.640087783 batch mAP 0.507568359 batch PCKh 0.125\n",
      "Trained batch 492 batch loss 0.609176576 batch mAP 0.57522583 batch PCKh 0.375\n",
      "Trained batch 493 batch loss 0.567558765 batch mAP 0.627655 batch PCKh 0.75\n",
      "Trained batch 494 batch loss 0.575383425 batch mAP 0.626556396 batch PCKh 0.6875\n",
      "Trained batch 495 batch loss 0.614149332 batch mAP 0.590911865 batch PCKh 0.3125\n",
      "Trained batch 496 batch loss 0.523709 batch mAP 0.64730835 batch PCKh 0.5\n",
      "Trained batch 497 batch loss 0.574685514 batch mAP 0.623901367 batch PCKh 0.6875\n",
      "Trained batch 498 batch loss 0.536254585 batch mAP 0.672546387 batch PCKh 0.5\n",
      "Trained batch 499 batch loss 0.530685544 batch mAP 0.650146484 batch PCKh 0.4375\n",
      "Trained batch 500 batch loss 0.535051107 batch mAP 0.558502197 batch PCKh 0.625\n",
      "Trained batch 501 batch loss 0.490186512 batch mAP 0.543334961 batch PCKh 0.75\n",
      "Trained batch 502 batch loss 0.414077699 batch mAP 0.573883057 batch PCKh 0.5625\n",
      "Trained batch 503 batch loss 0.438715577 batch mAP 0.579681396 batch PCKh 0.75\n",
      "Trained batch 504 batch loss 0.520500541 batch mAP 0.525909424 batch PCKh 0.625\n",
      "Trained batch 505 batch loss 0.486057639 batch mAP 0.517120361 batch PCKh 0.6875\n",
      "Trained batch 506 batch loss 0.54738909 batch mAP 0.549041748 batch PCKh 0.5\n",
      "Trained batch 507 batch loss 0.616223216 batch mAP 0.489532471 batch PCKh 0.75\n",
      "Trained batch 508 batch loss 0.583080292 batch mAP 0.488555908 batch PCKh 0.75\n",
      "Trained batch 509 batch loss 0.658594131 batch mAP 0.574493408 batch PCKh 0.875\n",
      "Trained batch 510 batch loss 0.55425483 batch mAP 0.5909729 batch PCKh 0.625\n",
      "Trained batch 511 batch loss 0.516418934 batch mAP 0.550689697 batch PCKh 0.5625\n",
      "Trained batch 512 batch loss 0.52903 batch mAP 0.619720459 batch PCKh 0.625\n",
      "Trained batch 513 batch loss 0.519832075 batch mAP 0.556945801 batch PCKh 0.5625\n",
      "Trained batch 514 batch loss 0.496864021 batch mAP 0.579406738 batch PCKh 0.1875\n",
      "Trained batch 515 batch loss 0.527111 batch mAP 0.546020508 batch PCKh 0.125\n",
      "Trained batch 516 batch loss 0.50933671 batch mAP 0.629150391 batch PCKh 0.4375\n",
      "Trained batch 517 batch loss 0.545413911 batch mAP 0.619995117 batch PCKh 0.625\n",
      "Trained batch 518 batch loss 0.588723302 batch mAP 0.60369873 batch PCKh 0.75\n",
      "Trained batch 519 batch loss 0.428107 batch mAP 0.601196289 batch PCKh 0.25\n",
      "Trained batch 520 batch loss 0.435328245 batch mAP 0.6668396 batch PCKh 0.5625\n",
      "Trained batch 521 batch loss 0.413945913 batch mAP 0.590148926 batch PCKh 0.4375\n",
      "Trained batch 522 batch loss 0.380930603 batch mAP 0.63848877 batch PCKh 0.1875\n",
      "Trained batch 523 batch loss 0.436248183 batch mAP 0.576782227 batch PCKh 0.125\n",
      "Trained batch 524 batch loss 0.383092046 batch mAP 0.578460693 batch PCKh 0\n",
      "Trained batch 525 batch loss 0.387852669 batch mAP 0.598999 batch PCKh 0\n",
      "Trained batch 526 batch loss 0.484993547 batch mAP 0.592132568 batch PCKh 0.625\n",
      "Trained batch 527 batch loss 0.534226596 batch mAP 0.57131958 batch PCKh 0.3125\n",
      "Trained batch 528 batch loss 0.575420499 batch mAP 0.59387207 batch PCKh 0.8125\n",
      "Trained batch 529 batch loss 0.650733 batch mAP 0.542297363 batch PCKh 0.5\n",
      "Trained batch 530 batch loss 0.602674901 batch mAP 0.589294434 batch PCKh 0.25\n",
      "Trained batch 531 batch loss 0.717020571 batch mAP 0.543670654 batch PCKh 0.1875\n",
      "Trained batch 532 batch loss 0.679253936 batch mAP 0.560241699 batch PCKh 0.1875\n",
      "Trained batch 533 batch loss 0.556037307 batch mAP 0.587219238 batch PCKh 0\n",
      "Trained batch 534 batch loss 0.521514833 batch mAP 0.553497314 batch PCKh 0.1875\n",
      "Trained batch 535 batch loss 0.562050223 batch mAP 0.488189697 batch PCKh 0.4375\n",
      "Trained batch 536 batch loss 0.453498304 batch mAP 0.546051 batch PCKh 0.6875\n",
      "Trained batch 537 batch loss 0.511467874 batch mAP 0.518981934 batch PCKh 0.625\n",
      "Trained batch 538 batch loss 0.580484509 batch mAP 0.502166748 batch PCKh 0.25\n",
      "Trained batch 539 batch loss 0.558164537 batch mAP 0.558776855 batch PCKh 0.375\n",
      "Trained batch 540 batch loss 0.502860427 batch mAP 0.526367188 batch PCKh 0.5625\n",
      "Trained batch 541 batch loss 0.536090195 batch mAP 0.558807373 batch PCKh 0.3125\n",
      "Trained batch 542 batch loss 0.57961303 batch mAP 0.548675537 batch PCKh 0.5\n",
      "Trained batch 543 batch loss 0.531816959 batch mAP 0.555297852 batch PCKh 0.375\n",
      "Trained batch 544 batch loss 0.573583782 batch mAP 0.557647705 batch PCKh 0.625\n",
      "Trained batch 545 batch loss 0.513078213 batch mAP 0.618347168 batch PCKh 0.4375\n",
      "Trained batch 546 batch loss 0.509282887 batch mAP 0.625640869 batch PCKh 0.6875\n",
      "Trained batch 547 batch loss 0.575738847 batch mAP 0.623901367 batch PCKh 0.6875\n",
      "Trained batch 548 batch loss 0.561792731 batch mAP 0.562561035 batch PCKh 0.625\n",
      "Trained batch 549 batch loss 0.599280715 batch mAP 0.553772 batch PCKh 0.875\n",
      "Trained batch 550 batch loss 0.480196834 batch mAP 0.643768311 batch PCKh 0.375\n",
      "Trained batch 551 batch loss 0.516027451 batch mAP 0.620849609 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 552 batch loss 0.54007715 batch mAP 0.625549316 batch PCKh 0.5625\n",
      "Trained batch 553 batch loss 0.480886191 batch mAP 0.651367188 batch PCKh 0.375\n",
      "Trained batch 554 batch loss 0.486591876 batch mAP 0.582946777 batch PCKh 0.25\n",
      "Trained batch 555 batch loss 0.426603615 batch mAP 0.587341309 batch PCKh 0.5625\n",
      "Trained batch 556 batch loss 0.460992604 batch mAP 0.627227783 batch PCKh 0.5\n",
      "Trained batch 557 batch loss 0.520263791 batch mAP 0.615509033 batch PCKh 0.4375\n",
      "Trained batch 558 batch loss 0.557525277 batch mAP 0.653686523 batch PCKh 0.1875\n",
      "Trained batch 559 batch loss 0.616394758 batch mAP 0.624450684 batch PCKh 0.75\n",
      "Trained batch 560 batch loss 0.553713858 batch mAP 0.62121582 batch PCKh 0.75\n",
      "Trained batch 561 batch loss 0.46123606 batch mAP 0.63684082 batch PCKh 0.625\n",
      "Trained batch 562 batch loss 0.444995761 batch mAP 0.607971191 batch PCKh 0.4375\n",
      "Trained batch 563 batch loss 0.397027254 batch mAP 0.609466553 batch PCKh 0.3125\n",
      "Trained batch 564 batch loss 0.501677752 batch mAP 0.572479248 batch PCKh 0.1875\n",
      "Trained batch 565 batch loss 0.539419293 batch mAP 0.569000244 batch PCKh 0.1875\n",
      "Trained batch 566 batch loss 0.566433787 batch mAP 0.621887207 batch PCKh 0.5625\n",
      "Trained batch 567 batch loss 0.569043279 batch mAP 0.578887939 batch PCKh 0.1875\n",
      "Trained batch 568 batch loss 0.532352209 batch mAP 0.506530762 batch PCKh 0.4375\n",
      "Trained batch 569 batch loss 0.492393494 batch mAP 0.563232422 batch PCKh 0.5625\n",
      "Trained batch 570 batch loss 0.459263802 batch mAP 0.630950928 batch PCKh 0.5\n",
      "Trained batch 571 batch loss 0.46470502 batch mAP 0.572967529 batch PCKh 0.625\n",
      "Trained batch 572 batch loss 0.408519745 batch mAP 0.61605835 batch PCKh 0.3125\n",
      "Trained batch 573 batch loss 0.472551078 batch mAP 0.633209229 batch PCKh 0.625\n",
      "Trained batch 574 batch loss 0.434429646 batch mAP 0.562438965 batch PCKh 0.5\n",
      "Trained batch 575 batch loss 0.489013314 batch mAP 0.710632324 batch PCKh 0.125\n",
      "Trained batch 576 batch loss 0.62689507 batch mAP 0.602081299 batch PCKh 0.5\n",
      "Trained batch 577 batch loss 0.680036247 batch mAP 0.525939941 batch PCKh 0\n",
      "Trained batch 578 batch loss 0.585162222 batch mAP 0.644012451 batch PCKh 0.25\n",
      "Trained batch 579 batch loss 0.573535204 batch mAP 0.65133667 batch PCKh 0.5\n",
      "Trained batch 580 batch loss 0.527775049 batch mAP 0.643035889 batch PCKh 0.4375\n",
      "Trained batch 581 batch loss 0.498529732 batch mAP 0.649261475 batch PCKh 0.25\n",
      "Trained batch 582 batch loss 0.464528382 batch mAP 0.598388672 batch PCKh 0.5\n",
      "Trained batch 583 batch loss 0.440618783 batch mAP 0.681945801 batch PCKh 0.75\n",
      "Trained batch 584 batch loss 0.532639086 batch mAP 0.65737915 batch PCKh 0.4375\n",
      "Trained batch 585 batch loss 0.568879604 batch mAP 0.625640869 batch PCKh 0.125\n",
      "Trained batch 586 batch loss 0.517770231 batch mAP 0.667602539 batch PCKh 0.4375\n",
      "Trained batch 587 batch loss 0.494537771 batch mAP 0.646057129 batch PCKh 0.3125\n",
      "Trained batch 588 batch loss 0.501159906 batch mAP 0.66998291 batch PCKh 0.6875\n",
      "Trained batch 589 batch loss 0.447147787 batch mAP 0.664550781 batch PCKh 0.8125\n",
      "Trained batch 590 batch loss 0.487455517 batch mAP 0.622650146 batch PCKh 0.625\n",
      "Trained batch 591 batch loss 0.471727371 batch mAP 0.627319336 batch PCKh 0.75\n",
      "Trained batch 592 batch loss 0.509658098 batch mAP 0.657196045 batch PCKh 0.5625\n",
      "Trained batch 593 batch loss 0.514574289 batch mAP 0.649414062 batch PCKh 0.4375\n",
      "Trained batch 594 batch loss 0.507922709 batch mAP 0.598144531 batch PCKh 0.5\n",
      "Trained batch 595 batch loss 0.55583173 batch mAP 0.646331787 batch PCKh 0.8125\n",
      "Trained batch 596 batch loss 0.533688426 batch mAP 0.660705566 batch PCKh 0.5\n",
      "Trained batch 597 batch loss 0.586498082 batch mAP 0.590087891 batch PCKh 0.375\n",
      "Trained batch 598 batch loss 0.515159965 batch mAP 0.672699 batch PCKh 0.625\n",
      "Trained batch 599 batch loss 0.533734322 batch mAP 0.650695801 batch PCKh 0.4375\n",
      "Trained batch 600 batch loss 0.54896915 batch mAP 0.662445068 batch PCKh 0.6875\n",
      "Trained batch 601 batch loss 0.503863931 batch mAP 0.627349854 batch PCKh 0\n",
      "Trained batch 602 batch loss 0.471448362 batch mAP 0.649871826 batch PCKh 0.5\n",
      "Trained batch 603 batch loss 0.49047631 batch mAP 0.690216064 batch PCKh 0.375\n",
      "Trained batch 604 batch loss 0.470208168 batch mAP 0.700897217 batch PCKh 0.3125\n",
      "Trained batch 605 batch loss 0.508288622 batch mAP 0.657470703 batch PCKh 0.1875\n",
      "Trained batch 606 batch loss 0.552981853 batch mAP 0.699737549 batch PCKh 0.4375\n",
      "Trained batch 607 batch loss 0.542952299 batch mAP 0.635040283 batch PCKh 0.5\n",
      "Trained batch 608 batch loss 0.601909757 batch mAP 0.586090088 batch PCKh 0.75\n",
      "Trained batch 609 batch loss 0.495029449 batch mAP 0.57824707 batch PCKh 0.5\n",
      "Trained batch 610 batch loss 0.478499621 batch mAP 0.685089111 batch PCKh 0.4375\n",
      "Trained batch 611 batch loss 0.58208096 batch mAP 0.667327881 batch PCKh 0.4375\n",
      "Trained batch 612 batch loss 0.48838985 batch mAP 0.635437 batch PCKh 0.6875\n",
      "Trained batch 613 batch loss 0.480929255 batch mAP 0.67678833 batch PCKh 0.4375\n",
      "Trained batch 614 batch loss 0.470247179 batch mAP 0.656097412 batch PCKh 0.4375\n",
      "Trained batch 615 batch loss 0.494996965 batch mAP 0.656768799 batch PCKh 0.75\n",
      "Trained batch 616 batch loss 0.535446286 batch mAP 0.640136719 batch PCKh 0.25\n",
      "Trained batch 617 batch loss 0.468918204 batch mAP 0.743377686 batch PCKh 0.75\n",
      "Trained batch 618 batch loss 0.435127765 batch mAP 0.697723389 batch PCKh 0.5\n",
      "Trained batch 619 batch loss 0.520305157 batch mAP 0.566986084 batch PCKh 0.25\n",
      "Trained batch 620 batch loss 0.582236767 batch mAP 0.541687 batch PCKh 0.375\n",
      "Trained batch 621 batch loss 0.483452857 batch mAP 0.642486572 batch PCKh 0.3125\n",
      "Trained batch 622 batch loss 0.466056675 batch mAP 0.573486328 batch PCKh 0.1875\n",
      "Trained batch 623 batch loss 0.506662965 batch mAP 0.513153076 batch PCKh 0.25\n",
      "Trained batch 624 batch loss 0.498869717 batch mAP 0.430267334 batch PCKh 0.1875\n",
      "Trained batch 625 batch loss 0.482590497 batch mAP 0.523223877 batch PCKh 0.5\n",
      "Trained batch 626 batch loss 0.410611421 batch mAP 0.494873047 batch PCKh 0.25\n",
      "Trained batch 627 batch loss 0.346292108 batch mAP 0.581817627 batch PCKh 0\n",
      "Trained batch 628 batch loss 0.416740477 batch mAP 0.583129883 batch PCKh 0.1875\n",
      "Trained batch 629 batch loss 0.455379248 batch mAP 0.608398438 batch PCKh 0.125\n",
      "Trained batch 630 batch loss 0.532460213 batch mAP 0.575317383 batch PCKh 0\n",
      "Trained batch 631 batch loss 0.532714725 batch mAP 0.596557617 batch PCKh 0.375\n",
      "Trained batch 632 batch loss 0.634837329 batch mAP 0.558441162 batch PCKh 0.25\n",
      "Trained batch 633 batch loss 0.512387097 batch mAP 0.604370117 batch PCKh 0.5625\n",
      "Trained batch 634 batch loss 0.555847406 batch mAP 0.579589844 batch PCKh 0.5625\n",
      "Trained batch 635 batch loss 0.462406665 batch mAP 0.548217773 batch PCKh 0.0625\n",
      "Trained batch 636 batch loss 0.483191 batch mAP 0.476898193 batch PCKh 0.0625\n",
      "Trained batch 637 batch loss 0.515649199 batch mAP 0.468475342 batch PCKh 0.75\n",
      "Trained batch 638 batch loss 0.555361748 batch mAP 0.538238525 batch PCKh 0.8125\n",
      "Trained batch 639 batch loss 0.455751956 batch mAP 0.625732422 batch PCKh 0.625\n",
      "Trained batch 640 batch loss 0.469023645 batch mAP 0.628601074 batch PCKh 0.6875\n",
      "Trained batch 641 batch loss 0.409785688 batch mAP 0.639526367 batch PCKh 0.6875\n",
      "Trained batch 642 batch loss 0.430093169 batch mAP 0.561706543 batch PCKh 0.125\n",
      "Trained batch 643 batch loss 0.479037583 batch mAP 0.50994873 batch PCKh 0.4375\n",
      "Trained batch 644 batch loss 0.524919868 batch mAP 0.495147705 batch PCKh 0.75\n",
      "Trained batch 645 batch loss 0.526724339 batch mAP 0.544128418 batch PCKh 0.1875\n",
      "Trained batch 646 batch loss 0.486600369 batch mAP 0.538970947 batch PCKh 0.625\n",
      "Trained batch 647 batch loss 0.575327873 batch mAP 0.439361572 batch PCKh 0.3125\n",
      "Trained batch 648 batch loss 0.474029183 batch mAP 0.523986816 batch PCKh 0\n",
      "Trained batch 649 batch loss 0.559784055 batch mAP 0.479064941 batch PCKh 0.625\n",
      "Trained batch 650 batch loss 0.48085016 batch mAP 0.606414795 batch PCKh 0.625\n",
      "Trained batch 651 batch loss 0.549032807 batch mAP 0.612854 batch PCKh 0.875\n",
      "Trained batch 652 batch loss 0.575366437 batch mAP 0.56918335 batch PCKh 0\n",
      "Trained batch 653 batch loss 0.559374928 batch mAP 0.580932617 batch PCKh 0.5625\n",
      "Trained batch 654 batch loss 0.550780356 batch mAP 0.567138672 batch PCKh 0.375\n",
      "Trained batch 655 batch loss 0.520393431 batch mAP 0.552703857 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 656 batch loss 0.547779858 batch mAP 0.531341553 batch PCKh 0.625\n",
      "Trained batch 657 batch loss 0.624293864 batch mAP 0.507751465 batch PCKh 0.25\n",
      "Trained batch 658 batch loss 0.624612331 batch mAP 0.589202881 batch PCKh 0.5625\n",
      "Trained batch 659 batch loss 0.480450451 batch mAP 0.600158691 batch PCKh 0.75\n",
      "Trained batch 660 batch loss 0.584972441 batch mAP 0.532806396 batch PCKh 0.5625\n",
      "Trained batch 661 batch loss 0.489487052 batch mAP 0.526367188 batch PCKh 0.5\n",
      "Trained batch 662 batch loss 0.50481391 batch mAP 0.513946533 batch PCKh 0.625\n",
      "Trained batch 663 batch loss 0.61499548 batch mAP 0.489807129 batch PCKh 0.5625\n",
      "Trained batch 664 batch loss 0.495974839 batch mAP 0.632568359 batch PCKh 0.8125\n",
      "Trained batch 665 batch loss 0.57404691 batch mAP 0.552429199 batch PCKh 0.4375\n",
      "Trained batch 666 batch loss 0.584492564 batch mAP 0.601196289 batch PCKh 0.375\n",
      "Trained batch 667 batch loss 0.605065346 batch mAP 0.591766357 batch PCKh 0.3125\n",
      "Trained batch 668 batch loss 0.711718619 batch mAP 0.54699707 batch PCKh 0.4375\n",
      "Trained batch 669 batch loss 0.489329219 batch mAP 0.596954346 batch PCKh 0.25\n",
      "Trained batch 670 batch loss 0.38599515 batch mAP 0.592346191 batch PCKh 0.6875\n",
      "Trained batch 671 batch loss 0.532546818 batch mAP 0.568695068 batch PCKh 0.625\n",
      "Trained batch 672 batch loss 0.34475258 batch mAP 0.557159424 batch PCKh 0.375\n",
      "Trained batch 673 batch loss 0.471919477 batch mAP 0.564880371 batch PCKh 0.625\n",
      "Trained batch 674 batch loss 0.533841 batch mAP 0.557159424 batch PCKh 0.1875\n",
      "Trained batch 675 batch loss 0.519919515 batch mAP 0.559173584 batch PCKh 0.25\n",
      "Trained batch 676 batch loss 0.538591921 batch mAP 0.553192139 batch PCKh 0.1875\n",
      "Trained batch 677 batch loss 0.466601163 batch mAP 0.611999512 batch PCKh 0.75\n",
      "Trained batch 678 batch loss 0.557863 batch mAP 0.620697 batch PCKh 0.1875\n",
      "Trained batch 679 batch loss 0.515352428 batch mAP 0.558410645 batch PCKh 0.4375\n",
      "Trained batch 680 batch loss 0.52254349 batch mAP 0.63168335 batch PCKh 0.3125\n",
      "Trained batch 681 batch loss 0.574587166 batch mAP 0.560424805 batch PCKh 0.5625\n",
      "Trained batch 682 batch loss 0.587244511 batch mAP 0.572235107 batch PCKh 0.5\n",
      "Trained batch 683 batch loss 0.51528579 batch mAP 0.607086182 batch PCKh 0.8125\n",
      "Trained batch 684 batch loss 0.551576 batch mAP 0.564788818 batch PCKh 0.5625\n",
      "Trained batch 685 batch loss 0.604024887 batch mAP 0.581451416 batch PCKh 0.5625\n",
      "Trained batch 686 batch loss 0.560609281 batch mAP 0.584960938 batch PCKh 0.5625\n",
      "Trained batch 687 batch loss 0.606686592 batch mAP 0.504638672 batch PCKh 0.25\n",
      "Trained batch 688 batch loss 0.523551583 batch mAP 0.519989 batch PCKh 0.75\n",
      "Trained batch 689 batch loss 0.470702827 batch mAP 0.599273682 batch PCKh 0.5625\n",
      "Trained batch 690 batch loss 0.633303046 batch mAP 0.500396729 batch PCKh 0.75\n",
      "Trained batch 691 batch loss 0.616658866 batch mAP 0.599212646 batch PCKh 0.4375\n",
      "Trained batch 692 batch loss 0.619446754 batch mAP 0.577423096 batch PCKh 0.125\n",
      "Trained batch 693 batch loss 0.654358745 batch mAP 0.598388672 batch PCKh 0.375\n",
      "Trained batch 694 batch loss 0.666618407 batch mAP 0.567382812 batch PCKh 0.5625\n",
      "Trained batch 695 batch loss 0.671455264 batch mAP 0.401245117 batch PCKh 0.5\n",
      "Trained batch 696 batch loss 0.634194374 batch mAP 0.533844 batch PCKh 0.3125\n",
      "Trained batch 697 batch loss 0.639287949 batch mAP 0.533874512 batch PCKh 0.25\n",
      "Trained batch 698 batch loss 0.5699085 batch mAP 0.54119873 batch PCKh 0.1875\n",
      "Trained batch 699 batch loss 0.556031704 batch mAP 0.585418701 batch PCKh 0.125\n",
      "Trained batch 700 batch loss 0.587584 batch mAP 0.526153564 batch PCKh 0.25\n",
      "Trained batch 701 batch loss 0.523952961 batch mAP 0.57925415 batch PCKh 0.75\n",
      "Trained batch 702 batch loss 0.486359656 batch mAP 0.54574585 batch PCKh 0.3125\n",
      "Trained batch 703 batch loss 0.532309651 batch mAP 0.563201904 batch PCKh 0.625\n",
      "Trained batch 704 batch loss 0.560135484 batch mAP 0.56149292 batch PCKh 0.3125\n",
      "Trained batch 705 batch loss 0.514960647 batch mAP 0.594543457 batch PCKh 0.125\n",
      "Trained batch 706 batch loss 0.543805718 batch mAP 0.586669922 batch PCKh 0.625\n",
      "Trained batch 707 batch loss 0.562266469 batch mAP 0.633056641 batch PCKh 0.5625\n",
      "Trained batch 708 batch loss 0.463238686 batch mAP 0.621521 batch PCKh 0.375\n",
      "Trained batch 709 batch loss 0.517547131 batch mAP 0.641418457 batch PCKh 0.625\n",
      "Trained batch 710 batch loss 0.46170485 batch mAP 0.608459473 batch PCKh 0.625\n",
      "Trained batch 711 batch loss 0.433812946 batch mAP 0.578491211 batch PCKh 0.6875\n",
      "Trained batch 712 batch loss 0.433290929 batch mAP 0.551544189 batch PCKh 0.5625\n",
      "Trained batch 713 batch loss 0.430775166 batch mAP 0.567993164 batch PCKh 0.5625\n",
      "Trained batch 714 batch loss 0.50056684 batch mAP 0.620697 batch PCKh 0.75\n",
      "Trained batch 715 batch loss 0.513384104 batch mAP 0.581542969 batch PCKh 0.75\n",
      "Trained batch 716 batch loss 0.496666312 batch mAP 0.60043335 batch PCKh 0.625\n",
      "Trained batch 717 batch loss 0.529452085 batch mAP 0.625793457 batch PCKh 0.75\n",
      "Trained batch 718 batch loss 0.426477611 batch mAP 0.61239624 batch PCKh 0.4375\n",
      "Trained batch 719 batch loss 0.421521723 batch mAP 0.656982422 batch PCKh 0.5625\n",
      "Trained batch 720 batch loss 0.4644683 batch mAP 0.642120361 batch PCKh 0.625\n",
      "Trained batch 721 batch loss 0.538123369 batch mAP 0.632568359 batch PCKh 0.25\n",
      "Trained batch 722 batch loss 0.479451299 batch mAP 0.649475098 batch PCKh 0.375\n",
      "Trained batch 723 batch loss 0.542758167 batch mAP 0.542236328 batch PCKh 0.625\n",
      "Trained batch 724 batch loss 0.562769771 batch mAP 0.502532959 batch PCKh 0.4375\n",
      "Trained batch 725 batch loss 0.55781281 batch mAP 0.529876709 batch PCKh 0.75\n",
      "Trained batch 726 batch loss 0.619080961 batch mAP 0.499755859 batch PCKh 0.375\n",
      "Trained batch 727 batch loss 0.509658158 batch mAP 0.57913208 batch PCKh 0.75\n",
      "Trained batch 728 batch loss 0.524808109 batch mAP 0.589111328 batch PCKh 0.8125\n",
      "Trained batch 729 batch loss 0.444926322 batch mAP 0.623138428 batch PCKh 0.6875\n",
      "Trained batch 730 batch loss 0.559056818 batch mAP 0.605621338 batch PCKh 0.6875\n",
      "Trained batch 731 batch loss 0.548460841 batch mAP 0.631286621 batch PCKh 0.75\n",
      "Trained batch 732 batch loss 0.605172276 batch mAP 0.537384033 batch PCKh 0.3125\n",
      "Trained batch 733 batch loss 0.663738668 batch mAP 0.619293213 batch PCKh 0.3125\n",
      "Trained batch 734 batch loss 0.57200253 batch mAP 0.619842529 batch PCKh 0.1875\n",
      "Trained batch 735 batch loss 0.553215325 batch mAP 0.618225098 batch PCKh 0.25\n",
      "Trained batch 736 batch loss 0.507349253 batch mAP 0.554382324 batch PCKh 0.375\n",
      "Trained batch 737 batch loss 0.560878694 batch mAP 0.541626 batch PCKh 0.375\n",
      "Trained batch 738 batch loss 0.482292593 batch mAP 0.548980713 batch PCKh 0.875\n",
      "Trained batch 739 batch loss 0.564648509 batch mAP 0.487731934 batch PCKh 0.875\n",
      "Trained batch 740 batch loss 0.630109727 batch mAP 0.458435059 batch PCKh 0.6875\n",
      "Trained batch 741 batch loss 0.580174208 batch mAP 0.465698242 batch PCKh 0.6875\n",
      "Trained batch 742 batch loss 0.5111233 batch mAP 0.595611572 batch PCKh 0.375\n",
      "Trained batch 743 batch loss 0.526653826 batch mAP 0.600616455 batch PCKh 0.5625\n",
      "Trained batch 744 batch loss 0.491577744 batch mAP 0.641998291 batch PCKh 0.5\n",
      "Trained batch 745 batch loss 0.472762197 batch mAP 0.686920166 batch PCKh 0.375\n",
      "Trained batch 746 batch loss 0.487075806 batch mAP 0.66027832 batch PCKh 0.25\n",
      "Trained batch 747 batch loss 0.463023901 batch mAP 0.694244385 batch PCKh 0.625\n",
      "Trained batch 748 batch loss 0.530050397 batch mAP 0.663513184 batch PCKh 0.5\n",
      "Trained batch 749 batch loss 0.532504797 batch mAP 0.575927734 batch PCKh 0.75\n",
      "Trained batch 750 batch loss 0.586071968 batch mAP 0.665802 batch PCKh 0.3125\n",
      "Trained batch 751 batch loss 0.535188317 batch mAP 0.639129639 batch PCKh 0.25\n",
      "Trained batch 752 batch loss 0.532327056 batch mAP 0.535003662 batch PCKh 0.625\n",
      "Trained batch 753 batch loss 0.549697697 batch mAP 0.571014404 batch PCKh 0.625\n",
      "Trained batch 754 batch loss 0.491860271 batch mAP 0.598724365 batch PCKh 0.5625\n",
      "Trained batch 755 batch loss 0.552751958 batch mAP 0.584198 batch PCKh 0.625\n",
      "Trained batch 756 batch loss 0.526336551 batch mAP 0.585083 batch PCKh 0.5\n",
      "Trained batch 757 batch loss 0.536945164 batch mAP 0.568206787 batch PCKh 0.375\n",
      "Trained batch 758 batch loss 0.562181473 batch mAP 0.564178467 batch PCKh 0.5625\n",
      "Trained batch 759 batch loss 0.539183795 batch mAP 0.614563 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 760 batch loss 0.691429317 batch mAP 0.588928223 batch PCKh 0.125\n",
      "Trained batch 761 batch loss 0.588264227 batch mAP 0.600769043 batch PCKh 0.3125\n",
      "Trained batch 762 batch loss 0.521439612 batch mAP 0.575927734 batch PCKh 0.5\n",
      "Trained batch 763 batch loss 0.571324 batch mAP 0.593536377 batch PCKh 0.5625\n",
      "Trained batch 764 batch loss 0.581181407 batch mAP 0.507171631 batch PCKh 0.5625\n",
      "Trained batch 765 batch loss 0.603216469 batch mAP 0.568084717 batch PCKh 0.25\n",
      "Trained batch 766 batch loss 0.653550625 batch mAP 0.58493042 batch PCKh 0.375\n",
      "Trained batch 767 batch loss 0.566655278 batch mAP 0.510406494 batch PCKh 0.1875\n",
      "Trained batch 768 batch loss 0.496841431 batch mAP 0.51361084 batch PCKh 0.5\n",
      "Trained batch 769 batch loss 0.472639859 batch mAP 0.531738281 batch PCKh 0.125\n",
      "Trained batch 770 batch loss 0.491839379 batch mAP 0.602081299 batch PCKh 0.5\n",
      "Trained batch 771 batch loss 0.490542293 batch mAP 0.570922852 batch PCKh 0.375\n",
      "Trained batch 772 batch loss 0.540822685 batch mAP 0.558685303 batch PCKh 0.4375\n",
      "Trained batch 773 batch loss 0.478237748 batch mAP 0.546905518 batch PCKh 0.25\n",
      "Trained batch 774 batch loss 0.557177663 batch mAP 0.535553 batch PCKh 0.4375\n",
      "Trained batch 775 batch loss 0.452357 batch mAP 0.647766113 batch PCKh 0.5625\n",
      "Trained batch 776 batch loss 0.454547256 batch mAP 0.644012451 batch PCKh 0.125\n",
      "Trained batch 777 batch loss 0.3643682 batch mAP 0.62210083 batch PCKh 0.4375\n",
      "Trained batch 778 batch loss 0.593163073 batch mAP 0.572570801 batch PCKh 0.5\n",
      "Trained batch 779 batch loss 0.528835475 batch mAP 0.617492676 batch PCKh 0.6875\n",
      "Trained batch 780 batch loss 0.541168809 batch mAP 0.601501465 batch PCKh 0.375\n",
      "Trained batch 781 batch loss 0.546308517 batch mAP 0.606079102 batch PCKh 0.5625\n",
      "Trained batch 782 batch loss 0.573459864 batch mAP 0.554534912 batch PCKh 0.75\n",
      "Trained batch 783 batch loss 0.569447637 batch mAP 0.570220947 batch PCKh 0.75\n",
      "Trained batch 784 batch loss 0.548449218 batch mAP 0.640136719 batch PCKh 0.3125\n",
      "Trained batch 785 batch loss 0.528833628 batch mAP 0.600036621 batch PCKh 0.5\n",
      "Trained batch 786 batch loss 0.569036484 batch mAP 0.547210693 batch PCKh 0.4375\n",
      "Trained batch 787 batch loss 0.557837546 batch mAP 0.560272217 batch PCKh 0.625\n",
      "Trained batch 788 batch loss 0.542696834 batch mAP 0.627807617 batch PCKh 0.8125\n",
      "Trained batch 789 batch loss 0.592104 batch mAP 0.555725098 batch PCKh 0.4375\n",
      "Trained batch 790 batch loss 0.540480912 batch mAP 0.588348389 batch PCKh 0.5625\n",
      "Trained batch 791 batch loss 0.616150677 batch mAP 0.606140137 batch PCKh 0.5625\n",
      "Trained batch 792 batch loss 0.627778053 batch mAP 0.59564209 batch PCKh 0.8125\n",
      "Trained batch 793 batch loss 0.488711506 batch mAP 0.632995605 batch PCKh 0.6875\n",
      "Trained batch 794 batch loss 0.514296591 batch mAP 0.617736816 batch PCKh 0.5\n",
      "Trained batch 795 batch loss 0.526859343 batch mAP 0.656066895 batch PCKh 0.4375\n",
      "Trained batch 796 batch loss 0.495362788 batch mAP 0.611816406 batch PCKh 0.4375\n",
      "Trained batch 797 batch loss 0.486743093 batch mAP 0.622161865 batch PCKh 0.25\n",
      "Trained batch 798 batch loss 0.46516031 batch mAP 0.541626 batch PCKh 0.1875\n",
      "Trained batch 799 batch loss 0.464733452 batch mAP 0.582733154 batch PCKh 0.375\n",
      "Trained batch 800 batch loss 0.43827039 batch mAP 0.623626709 batch PCKh 0.625\n",
      "Trained batch 801 batch loss 0.458317876 batch mAP 0.60244751 batch PCKh 0.5625\n",
      "Trained batch 802 batch loss 0.476630747 batch mAP 0.598968506 batch PCKh 0.75\n",
      "Trained batch 803 batch loss 0.436457694 batch mAP 0.59967041 batch PCKh 0.6875\n",
      "Trained batch 804 batch loss 0.494863451 batch mAP 0.566680908 batch PCKh 0.875\n",
      "Trained batch 805 batch loss 0.527249873 batch mAP 0.57220459 batch PCKh 0.5625\n",
      "Trained batch 806 batch loss 0.478591561 batch mAP 0.521026611 batch PCKh 0.75\n",
      "Trained batch 807 batch loss 0.375515312 batch mAP 0.601409912 batch PCKh 0.375\n",
      "Trained batch 808 batch loss 0.360410064 batch mAP 0.633392334 batch PCKh 0\n",
      "Trained batch 809 batch loss 0.355773628 batch mAP 0.642364502 batch PCKh 0.75\n",
      "Trained batch 810 batch loss 0.433391631 batch mAP 0.588104248 batch PCKh 0\n",
      "Trained batch 811 batch loss 0.409152806 batch mAP 0.619903564 batch PCKh 0\n",
      "Trained batch 812 batch loss 0.348477542 batch mAP 0.655822754 batch PCKh 0.625\n",
      "Trained batch 813 batch loss 0.354261041 batch mAP 0.661499 batch PCKh 0.375\n",
      "Trained batch 814 batch loss 0.396708906 batch mAP 0.629303 batch PCKh 0.5625\n",
      "Trained batch 815 batch loss 0.553788781 batch mAP 0.627258301 batch PCKh 0.375\n",
      "Trained batch 816 batch loss 0.52211535 batch mAP 0.622772217 batch PCKh 0.625\n",
      "Trained batch 817 batch loss 0.509522259 batch mAP 0.669647217 batch PCKh 0.5\n",
      "Trained batch 818 batch loss 0.462411 batch mAP 0.69229126 batch PCKh 0.4375\n",
      "Trained batch 819 batch loss 0.501992524 batch mAP 0.669799805 batch PCKh 0.4375\n",
      "Trained batch 820 batch loss 0.503079414 batch mAP 0.650817871 batch PCKh 0.375\n",
      "Trained batch 821 batch loss 0.560480952 batch mAP 0.633605957 batch PCKh 0.25\n",
      "Trained batch 822 batch loss 0.491251767 batch mAP 0.66418457 batch PCKh 0.375\n",
      "Trained batch 823 batch loss 0.562319338 batch mAP 0.601776123 batch PCKh 0.5625\n",
      "Trained batch 824 batch loss 0.539199829 batch mAP 0.656951904 batch PCKh 0.6875\n",
      "Trained batch 825 batch loss 0.50308007 batch mAP 0.669769287 batch PCKh 0.375\n",
      "Trained batch 826 batch loss 0.522132277 batch mAP 0.641998291 batch PCKh 0.75\n",
      "Trained batch 827 batch loss 0.510702252 batch mAP 0.630340576 batch PCKh 0.1875\n",
      "Trained batch 828 batch loss 0.483774126 batch mAP 0.580474854 batch PCKh 0.5625\n",
      "Trained batch 829 batch loss 0.477409631 batch mAP 0.643188477 batch PCKh 0.3125\n",
      "Trained batch 830 batch loss 0.486177057 batch mAP 0.602203369 batch PCKh 0.375\n",
      "Trained batch 831 batch loss 0.462502539 batch mAP 0.5831604 batch PCKh 0.625\n",
      "Trained batch 832 batch loss 0.447409242 batch mAP 0.60736084 batch PCKh 0.375\n",
      "Trained batch 833 batch loss 0.510728061 batch mAP 0.562438965 batch PCKh 0.625\n",
      "Trained batch 834 batch loss 0.587863445 batch mAP 0.602966309 batch PCKh 0.4375\n",
      "Trained batch 835 batch loss 0.50750345 batch mAP 0.609313965 batch PCKh 0\n",
      "Trained batch 836 batch loss 0.537593961 batch mAP 0.648864746 batch PCKh 0.4375\n",
      "Trained batch 837 batch loss 0.634315789 batch mAP 0.606842041 batch PCKh 0\n",
      "Trained batch 838 batch loss 0.644180417 batch mAP 0.552276611 batch PCKh 0.4375\n",
      "Trained batch 839 batch loss 0.557127118 batch mAP 0.578887939 batch PCKh 0.625\n",
      "Trained batch 840 batch loss 0.529848754 batch mAP 0.614837646 batch PCKh 0.75\n",
      "Trained batch 841 batch loss 0.585437775 batch mAP 0.576080322 batch PCKh 0\n",
      "Trained batch 842 batch loss 0.598753 batch mAP 0.473266602 batch PCKh 0.375\n",
      "Trained batch 843 batch loss 0.553689301 batch mAP 0.487579346 batch PCKh 0.875\n",
      "Trained batch 844 batch loss 0.495369136 batch mAP 0.484649658 batch PCKh 0.75\n",
      "Trained batch 845 batch loss 0.573897302 batch mAP 0.524200439 batch PCKh 0.6875\n",
      "Trained batch 846 batch loss 0.600052178 batch mAP 0.56640625 batch PCKh 0.875\n",
      "Trained batch 847 batch loss 0.496049076 batch mAP 0.570709229 batch PCKh 0.5\n",
      "Trained batch 848 batch loss 0.565831661 batch mAP 0.562469482 batch PCKh 0.5625\n",
      "Trained batch 849 batch loss 0.56423223 batch mAP 0.57699585 batch PCKh 0.625\n",
      "Trained batch 850 batch loss 0.51743865 batch mAP 0.587097168 batch PCKh 0.75\n",
      "Trained batch 851 batch loss 0.564005733 batch mAP 0.614654541 batch PCKh 0.75\n",
      "Trained batch 852 batch loss 0.502989709 batch mAP 0.608856201 batch PCKh 0.6875\n",
      "Trained batch 853 batch loss 0.504703283 batch mAP 0.652984619 batch PCKh 0.875\n",
      "Trained batch 854 batch loss 0.450672507 batch mAP 0.668823242 batch PCKh 0.25\n",
      "Trained batch 855 batch loss 0.456050754 batch mAP 0.671020508 batch PCKh 0.375\n",
      "Trained batch 856 batch loss 0.49028185 batch mAP 0.703735352 batch PCKh 0.5\n",
      "Trained batch 857 batch loss 0.507403314 batch mAP 0.637634277 batch PCKh 0.25\n",
      "Trained batch 858 batch loss 0.567048788 batch mAP 0.608276367 batch PCKh 0.4375\n",
      "Trained batch 859 batch loss 0.569414 batch mAP 0.657440186 batch PCKh 0.5625\n",
      "Trained batch 860 batch loss 0.478315443 batch mAP 0.626281738 batch PCKh 0.625\n",
      "Trained batch 861 batch loss 0.495271087 batch mAP 0.626556396 batch PCKh 0.625\n",
      "Trained batch 862 batch loss 0.499501318 batch mAP 0.550262451 batch PCKh 0.3125\n",
      "Trained batch 863 batch loss 0.476763606 batch mAP 0.510803223 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 864 batch loss 0.417054057 batch mAP 0.611785889 batch PCKh 0.8125\n",
      "Trained batch 865 batch loss 0.536210597 batch mAP 0.563415527 batch PCKh 0.5\n",
      "Trained batch 866 batch loss 0.533009827 batch mAP 0.576293945 batch PCKh 0.5\n",
      "Trained batch 867 batch loss 0.411031753 batch mAP 0.590942383 batch PCKh 0.625\n",
      "Trained batch 868 batch loss 0.455261588 batch mAP 0.589935303 batch PCKh 0.5\n",
      "Trained batch 869 batch loss 0.502266347 batch mAP 0.616363525 batch PCKh 0.75\n",
      "Trained batch 870 batch loss 0.507148087 batch mAP 0.540344238 batch PCKh 0.625\n",
      "Trained batch 871 batch loss 0.46340239 batch mAP 0.57019043 batch PCKh 0.625\n",
      "Trained batch 872 batch loss 0.583225 batch mAP 0.51348877 batch PCKh 0.75\n",
      "Trained batch 873 batch loss 0.509457469 batch mAP 0.566497803 batch PCKh 0.5\n",
      "Trained batch 874 batch loss 0.512234926 batch mAP 0.569488525 batch PCKh 0.5625\n",
      "Trained batch 875 batch loss 0.66499424 batch mAP 0.463317871 batch PCKh 0.75\n",
      "Trained batch 876 batch loss 0.519053698 batch mAP 0.605926514 batch PCKh 0.3125\n",
      "Trained batch 877 batch loss 0.465654343 batch mAP 0.612792969 batch PCKh 0.25\n",
      "Trained batch 878 batch loss 0.606214821 batch mAP 0.483642578 batch PCKh 0.4375\n",
      "Trained batch 879 batch loss 0.629442215 batch mAP 0.532318115 batch PCKh 0.75\n",
      "Trained batch 880 batch loss 0.488357782 batch mAP 0.510376 batch PCKh 0.625\n",
      "Trained batch 881 batch loss 0.52130723 batch mAP 0.492279053 batch PCKh 0.75\n",
      "Trained batch 882 batch loss 0.500876725 batch mAP 0.487304688 batch PCKh 0.6875\n",
      "Trained batch 883 batch loss 0.513192713 batch mAP 0.433197021 batch PCKh 0.3125\n",
      "Trained batch 884 batch loss 0.53259 batch mAP 0.459442139 batch PCKh 0.75\n",
      "Trained batch 885 batch loss 0.604030967 batch mAP 0.545806885 batch PCKh 0.75\n",
      "Trained batch 886 batch loss 0.512815595 batch mAP 0.628967285 batch PCKh 0.4375\n",
      "Trained batch 887 batch loss 0.526644111 batch mAP 0.630615234 batch PCKh 0.5\n",
      "Trained batch 888 batch loss 0.470376 batch mAP 0.672454834 batch PCKh 0.5625\n",
      "Trained batch 889 batch loss 0.51898551 batch mAP 0.659484863 batch PCKh 0.6875\n",
      "Trained batch 890 batch loss 0.452481925 batch mAP 0.719696045 batch PCKh 0.5625\n",
      "Trained batch 891 batch loss 0.482782811 batch mAP 0.665710449 batch PCKh 0.8125\n",
      "Trained batch 892 batch loss 0.399547726 batch mAP 0.642822266 batch PCKh 0.1875\n",
      "Trained batch 893 batch loss 0.507878542 batch mAP 0.580963135 batch PCKh 0.5\n",
      "Trained batch 894 batch loss 0.518814564 batch mAP 0.660919189 batch PCKh 0.3125\n",
      "Trained batch 895 batch loss 0.534345269 batch mAP 0.543426514 batch PCKh 0.5\n",
      "Trained batch 896 batch loss 0.531777143 batch mAP 0.565582275 batch PCKh 0.5625\n",
      "Trained batch 897 batch loss 0.519426465 batch mAP 0.526306152 batch PCKh 0.3125\n",
      "Trained batch 898 batch loss 0.601120591 batch mAP 0.533294678 batch PCKh 0.4375\n",
      "Trained batch 899 batch loss 0.573409 batch mAP 0.553161621 batch PCKh 0.625\n",
      "Trained batch 900 batch loss 0.492443 batch mAP 0.562438965 batch PCKh 0.875\n",
      "Trained batch 901 batch loss 0.510238051 batch mAP 0.550079346 batch PCKh 0.75\n",
      "Trained batch 902 batch loss 0.403560579 batch mAP 0.66506958 batch PCKh 0.875\n",
      "Trained batch 903 batch loss 0.408555686 batch mAP 0.691070557 batch PCKh 0.5625\n",
      "Trained batch 904 batch loss 0.475619495 batch mAP 0.651031494 batch PCKh 0.4375\n",
      "Trained batch 905 batch loss 0.381218433 batch mAP 0.677825928 batch PCKh 0.8125\n",
      "Trained batch 906 batch loss 0.353726774 batch mAP 0.696990967 batch PCKh 0.875\n",
      "Trained batch 907 batch loss 0.372496039 batch mAP 0.702514648 batch PCKh 0.625\n",
      "Trained batch 908 batch loss 0.41642496 batch mAP 0.696350098 batch PCKh 0.625\n",
      "Trained batch 909 batch loss 0.439414233 batch mAP 0.627502441 batch PCKh 0.8125\n",
      "Trained batch 910 batch loss 0.438068092 batch mAP 0.607513428 batch PCKh 0.3125\n",
      "Trained batch 911 batch loss 0.484524 batch mAP 0.603973389 batch PCKh 0.625\n",
      "Trained batch 912 batch loss 0.538541317 batch mAP 0.579315186 batch PCKh 0.375\n",
      "Trained batch 913 batch loss 0.567265451 batch mAP 0.573272705 batch PCKh 0.6875\n",
      "Trained batch 914 batch loss 0.526964188 batch mAP 0.605896 batch PCKh 0.5\n",
      "Trained batch 915 batch loss 0.491575837 batch mAP 0.593231201 batch PCKh 0.6875\n",
      "Trained batch 916 batch loss 0.482432365 batch mAP 0.586029053 batch PCKh 0.75\n",
      "Trained batch 917 batch loss 0.528771043 batch mAP 0.596710205 batch PCKh 0.4375\n",
      "Trained batch 918 batch loss 0.60033983 batch mAP 0.514556885 batch PCKh 0.0625\n",
      "Trained batch 919 batch loss 0.558015466 batch mAP 0.526886 batch PCKh 0.6875\n",
      "Trained batch 920 batch loss 0.568703175 batch mAP 0.552703857 batch PCKh 0.6875\n",
      "Trained batch 921 batch loss 0.44323796 batch mAP 0.555389404 batch PCKh 0.1875\n",
      "Trained batch 922 batch loss 0.427354217 batch mAP 0.563201904 batch PCKh 0.6875\n",
      "Trained batch 923 batch loss 0.423178613 batch mAP 0.602111816 batch PCKh 0.4375\n",
      "Trained batch 924 batch loss 0.419378579 batch mAP 0.686401367 batch PCKh 0.75\n",
      "Trained batch 925 batch loss 0.39630872 batch mAP 0.723480225 batch PCKh 0.6875\n",
      "Trained batch 926 batch loss 0.513292372 batch mAP 0.66595459 batch PCKh 0.5625\n",
      "Trained batch 927 batch loss 0.627862871 batch mAP 0.658111572 batch PCKh 0.4375\n",
      "Trained batch 928 batch loss 0.531381488 batch mAP 0.6328125 batch PCKh 0.375\n",
      "Trained batch 929 batch loss 0.552350163 batch mAP 0.552337646 batch PCKh 0.6875\n",
      "Trained batch 930 batch loss 0.640286565 batch mAP 0.47845459 batch PCKh 0.3125\n",
      "Trained batch 931 batch loss 0.61853826 batch mAP 0.461090088 batch PCKh 0.1875\n",
      "Trained batch 932 batch loss 0.544313967 batch mAP 0.608337402 batch PCKh 0.5625\n",
      "Trained batch 933 batch loss 0.497846186 batch mAP 0.636810303 batch PCKh 0.375\n",
      "Trained batch 934 batch loss 0.615988851 batch mAP 0.529083252 batch PCKh 0.375\n",
      "Trained batch 935 batch loss 0.654824853 batch mAP 0.533691406 batch PCKh 0.3125\n",
      "Trained batch 936 batch loss 0.54801 batch mAP 0.6065979 batch PCKh 0.3125\n",
      "Trained batch 937 batch loss 0.534172058 batch mAP 0.623748779 batch PCKh 0.25\n",
      "Trained batch 938 batch loss 0.537607789 batch mAP 0.59262085 batch PCKh 0.4375\n",
      "Trained batch 939 batch loss 0.589169145 batch mAP 0.604217529 batch PCKh 0.625\n",
      "Trained batch 940 batch loss 0.589942813 batch mAP 0.467041016 batch PCKh 0.5625\n",
      "Trained batch 941 batch loss 0.539451 batch mAP 0.570709229 batch PCKh 0.6875\n",
      "Trained batch 942 batch loss 0.568132281 batch mAP 0.539733887 batch PCKh 0.75\n",
      "Trained batch 943 batch loss 0.538091421 batch mAP 0.567901611 batch PCKh 0.5625\n",
      "Trained batch 944 batch loss 0.609158754 batch mAP 0.602020264 batch PCKh 0.125\n",
      "Trained batch 945 batch loss 0.560132623 batch mAP 0.559906 batch PCKh 0.5625\n",
      "Trained batch 946 batch loss 0.563592613 batch mAP 0.518219 batch PCKh 0.4375\n",
      "Trained batch 947 batch loss 0.63575089 batch mAP 0.518737793 batch PCKh 0.625\n",
      "Trained batch 948 batch loss 0.571128964 batch mAP 0.614532471 batch PCKh 0.75\n",
      "Trained batch 949 batch loss 0.564207315 batch mAP 0.534362793 batch PCKh 0.1875\n",
      "Trained batch 950 batch loss 0.610837221 batch mAP 0.551025391 batch PCKh 0.25\n",
      "Trained batch 951 batch loss 0.548472524 batch mAP 0.542419434 batch PCKh 0.5625\n",
      "Trained batch 952 batch loss 0.476793528 batch mAP 0.572937 batch PCKh 0.125\n",
      "Trained batch 953 batch loss 0.547142863 batch mAP 0.56652832 batch PCKh 0.375\n",
      "Trained batch 954 batch loss 0.556461 batch mAP 0.543243408 batch PCKh 0.5625\n",
      "Trained batch 955 batch loss 0.557922065 batch mAP 0.599029541 batch PCKh 0.125\n",
      "Trained batch 956 batch loss 0.575128138 batch mAP 0.582244873 batch PCKh 0.4375\n",
      "Trained batch 957 batch loss 0.504424 batch mAP 0.655456543 batch PCKh 0.5\n",
      "Trained batch 958 batch loss 0.436383158 batch mAP 0.637878418 batch PCKh 0.1875\n",
      "Trained batch 959 batch loss 0.500220239 batch mAP 0.641357422 batch PCKh 0.375\n",
      "Trained batch 960 batch loss 0.570919096 batch mAP 0.627380371 batch PCKh 0.5625\n",
      "Trained batch 961 batch loss 0.43797946 batch mAP 0.627349854 batch PCKh 0.375\n",
      "Trained batch 962 batch loss 0.488607943 batch mAP 0.605224609 batch PCKh 0.3125\n",
      "Trained batch 963 batch loss 0.521379828 batch mAP 0.604980469 batch PCKh 0.3125\n",
      "Trained batch 964 batch loss 0.508806 batch mAP 0.538665771 batch PCKh 0.6875\n",
      "Trained batch 965 batch loss 0.494902313 batch mAP 0.515838623 batch PCKh 0.375\n",
      "Trained batch 966 batch loss 0.549658298 batch mAP 0.531494141 batch PCKh 0.25\n",
      "Trained batch 967 batch loss 0.572318196 batch mAP 0.485656738 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 968 batch loss 0.520368814 batch mAP 0.620666504 batch PCKh 0.5625\n",
      "Trained batch 969 batch loss 0.493409097 batch mAP 0.601226807 batch PCKh 0.5\n",
      "Trained batch 970 batch loss 0.45222351 batch mAP 0.613067627 batch PCKh 0.25\n",
      "Trained batch 971 batch loss 0.513399184 batch mAP 0.555480957 batch PCKh 0.375\n",
      "Trained batch 972 batch loss 0.502025 batch mAP 0.521118164 batch PCKh 0.4375\n",
      "Trained batch 973 batch loss 0.526139081 batch mAP 0.499816895 batch PCKh 0.125\n",
      "Trained batch 974 batch loss 0.521144211 batch mAP 0.525482178 batch PCKh 0.5\n",
      "Trained batch 975 batch loss 0.540270805 batch mAP 0.55267334 batch PCKh 0.25\n",
      "Trained batch 976 batch loss 0.497376412 batch mAP 0.603088379 batch PCKh 0.5625\n",
      "Trained batch 977 batch loss 0.425288558 batch mAP 0.601013184 batch PCKh 0.25\n",
      "Trained batch 978 batch loss 0.484416962 batch mAP 0.570465088 batch PCKh 0\n",
      "Trained batch 979 batch loss 0.488834381 batch mAP 0.602783203 batch PCKh 0.375\n",
      "Trained batch 980 batch loss 0.484576583 batch mAP 0.582397461 batch PCKh 0.5\n",
      "Trained batch 981 batch loss 0.48447144 batch mAP 0.557403564 batch PCKh 0\n",
      "Trained batch 982 batch loss 0.512894869 batch mAP 0.591918945 batch PCKh 0.5\n",
      "Trained batch 983 batch loss 0.577070594 batch mAP 0.591491699 batch PCKh 0.25\n",
      "Trained batch 984 batch loss 0.551501155 batch mAP 0.668792725 batch PCKh 0.6875\n",
      "Trained batch 985 batch loss 0.584302425 batch mAP 0.609161377 batch PCKh 0.25\n",
      "Trained batch 986 batch loss 0.543952167 batch mAP 0.626983643 batch PCKh 0.8125\n",
      "Trained batch 987 batch loss 0.540791631 batch mAP 0.661621094 batch PCKh 0.3125\n",
      "Trained batch 988 batch loss 0.522963643 batch mAP 0.666290283 batch PCKh 0.625\n",
      "Trained batch 989 batch loss 0.605892301 batch mAP 0.644714355 batch PCKh 0.3125\n",
      "Trained batch 990 batch loss 0.54165709 batch mAP 0.643157959 batch PCKh 0.4375\n",
      "Trained batch 991 batch loss 0.551797628 batch mAP 0.637146 batch PCKh 0.25\n",
      "Trained batch 992 batch loss 0.520025194 batch mAP 0.61920166 batch PCKh 0.5\n",
      "Trained batch 993 batch loss 0.596587181 batch mAP 0.600128174 batch PCKh 0.3125\n",
      "Trained batch 994 batch loss 0.641466737 batch mAP 0.595367432 batch PCKh 0.3125\n",
      "Trained batch 995 batch loss 0.529208064 batch mAP 0.639923096 batch PCKh 0.4375\n",
      "Trained batch 996 batch loss 0.58581841 batch mAP 0.664093 batch PCKh 0.4375\n",
      "Trained batch 997 batch loss 0.558948338 batch mAP 0.620758057 batch PCKh 0.3125\n",
      "Trained batch 998 batch loss 0.562746167 batch mAP 0.602844238 batch PCKh 0.5\n",
      "Trained batch 999 batch loss 0.499313891 batch mAP 0.533081055 batch PCKh 0.25\n",
      "Trained batch 1000 batch loss 0.632478237 batch mAP 0.58392334 batch PCKh 0.25\n",
      "Trained batch 1001 batch loss 0.550258815 batch mAP 0.55279541 batch PCKh 0.1875\n",
      "Trained batch 1002 batch loss 0.56362772 batch mAP 0.660339355 batch PCKh 0.3125\n",
      "Trained batch 1003 batch loss 0.550167203 batch mAP 0.542449951 batch PCKh 0.5\n",
      "Trained batch 1004 batch loss 0.521284938 batch mAP 0.605163574 batch PCKh 0.3125\n",
      "Trained batch 1005 batch loss 0.562961459 batch mAP 0.5574646 batch PCKh 0.4375\n",
      "Trained batch 1006 batch loss 0.541623 batch mAP 0.608520508 batch PCKh 0.6875\n",
      "Trained batch 1007 batch loss 0.53760308 batch mAP 0.584411621 batch PCKh 0.5625\n",
      "Trained batch 1008 batch loss 0.532658696 batch mAP 0.642211914 batch PCKh 0.5625\n",
      "Trained batch 1009 batch loss 0.58981657 batch mAP 0.601257324 batch PCKh 0.5\n",
      "Trained batch 1010 batch loss 0.673013 batch mAP 0.541503906 batch PCKh 0.6875\n",
      "Trained batch 1011 batch loss 0.558112741 batch mAP 0.507324219 batch PCKh 0.3125\n",
      "Trained batch 1012 batch loss 0.637733459 batch mAP 0.483795166 batch PCKh 0.375\n",
      "Trained batch 1013 batch loss 0.573606968 batch mAP 0.555511475 batch PCKh 0.5\n",
      "Trained batch 1014 batch loss 0.518070817 batch mAP 0.594207764 batch PCKh 0.25\n",
      "Trained batch 1015 batch loss 0.546807647 batch mAP 0.652313232 batch PCKh 0.6875\n",
      "Trained batch 1016 batch loss 0.524808764 batch mAP 0.599365234 batch PCKh 0.375\n",
      "Trained batch 1017 batch loss 0.556167662 batch mAP 0.534912109 batch PCKh 0.0625\n",
      "Trained batch 1018 batch loss 0.508947253 batch mAP 0.520141602 batch PCKh 0.5\n",
      "Trained batch 1019 batch loss 0.599728584 batch mAP 0.602172852 batch PCKh 0.375\n",
      "Trained batch 1020 batch loss 0.549901903 batch mAP 0.552032471 batch PCKh 0.5625\n",
      "Trained batch 1021 batch loss 0.474854 batch mAP 0.482696533 batch PCKh 0.6875\n",
      "Trained batch 1022 batch loss 0.629996 batch mAP 0.535003662 batch PCKh 0.8125\n",
      "Trained batch 1023 batch loss 0.521914363 batch mAP 0.543640137 batch PCKh 0.4375\n",
      "Trained batch 1024 batch loss 0.535647631 batch mAP 0.543518066 batch PCKh 0.75\n",
      "Trained batch 1025 batch loss 0.483299285 batch mAP 0.577728271 batch PCKh 0.6875\n",
      "Trained batch 1026 batch loss 0.511784673 batch mAP 0.56741333 batch PCKh 0.375\n",
      "Trained batch 1027 batch loss 0.550308108 batch mAP 0.58480835 batch PCKh 0.5625\n",
      "Trained batch 1028 batch loss 0.616142273 batch mAP 0.551269531 batch PCKh 0.25\n",
      "Trained batch 1029 batch loss 0.606777847 batch mAP 0.554595947 batch PCKh 0.75\n",
      "Trained batch 1030 batch loss 0.528671622 batch mAP 0.501647949 batch PCKh 0.5\n",
      "Trained batch 1031 batch loss 0.681306243 batch mAP 0.529937744 batch PCKh 0.5\n",
      "Trained batch 1032 batch loss 0.576529 batch mAP 0.566589355 batch PCKh 0.5\n",
      "Trained batch 1033 batch loss 0.511489451 batch mAP 0.578796387 batch PCKh 0.875\n",
      "Trained batch 1034 batch loss 0.540417194 batch mAP 0.567382812 batch PCKh 0.5625\n",
      "Trained batch 1035 batch loss 0.654667795 batch mAP 0.506134033 batch PCKh 0.125\n",
      "Trained batch 1036 batch loss 0.563643277 batch mAP 0.536621094 batch PCKh 0.3125\n",
      "Trained batch 1037 batch loss 0.570113897 batch mAP 0.533874512 batch PCKh 0.4375\n",
      "Trained batch 1038 batch loss 0.590869665 batch mAP 0.472717285 batch PCKh 0.6875\n",
      "Trained batch 1039 batch loss 0.544675887 batch mAP 0.467132568 batch PCKh 0.625\n",
      "Trained batch 1040 batch loss 0.599515676 batch mAP 0.470611572 batch PCKh 0.5\n",
      "Trained batch 1041 batch loss 0.497448266 batch mAP 0.516601562 batch PCKh 0.75\n",
      "Trained batch 1042 batch loss 0.497180492 batch mAP 0.505096436 batch PCKh 0.75\n",
      "Trained batch 1043 batch loss 0.466124028 batch mAP 0.526489258 batch PCKh 0.75\n",
      "Trained batch 1044 batch loss 0.504185 batch mAP 0.563934326 batch PCKh 0.8125\n",
      "Trained batch 1045 batch loss 0.607462406 batch mAP 0.495636 batch PCKh 0.625\n",
      "Trained batch 1046 batch loss 0.548757 batch mAP 0.473907471 batch PCKh 0.75\n",
      "Trained batch 1047 batch loss 0.535848618 batch mAP 0.527099609 batch PCKh 0.75\n",
      "Trained batch 1048 batch loss 0.587067306 batch mAP 0.567901611 batch PCKh 0.3125\n",
      "Trained batch 1049 batch loss 0.555199385 batch mAP 0.597961426 batch PCKh 0.3125\n",
      "Trained batch 1050 batch loss 0.633443475 batch mAP 0.521118164 batch PCKh 0.3125\n",
      "Trained batch 1051 batch loss 0.679803073 batch mAP 0.538482666 batch PCKh 0.5625\n",
      "Trained batch 1052 batch loss 0.619853616 batch mAP 0.564239502 batch PCKh 0.25\n",
      "Trained batch 1053 batch loss 0.679459095 batch mAP 0.532501221 batch PCKh 0.6875\n",
      "Trained batch 1054 batch loss 0.699686944 batch mAP 0.511535645 batch PCKh 0\n",
      "Trained batch 1055 batch loss 0.6804986 batch mAP 0.553772 batch PCKh 0\n",
      "Trained batch 1056 batch loss 0.609751105 batch mAP 0.540252686 batch PCKh 0.125\n",
      "Trained batch 1057 batch loss 0.469136536 batch mAP 0.441772461 batch PCKh 0.1875\n",
      "Trained batch 1058 batch loss 0.446775436 batch mAP 0.509735107 batch PCKh 0.25\n",
      "Trained batch 1059 batch loss 0.434241652 batch mAP 0.557861328 batch PCKh 0.625\n",
      "Trained batch 1060 batch loss 0.567596078 batch mAP 0.51184082 batch PCKh 0.3125\n",
      "Trained batch 1061 batch loss 0.514179289 batch mAP 0.608703613 batch PCKh 0.0625\n",
      "Trained batch 1062 batch loss 0.509950757 batch mAP 0.607696533 batch PCKh 0.5625\n",
      "Trained batch 1063 batch loss 0.523868442 batch mAP 0.577667236 batch PCKh 0.6875\n",
      "Trained batch 1064 batch loss 0.457198441 batch mAP 0.605560303 batch PCKh 0.4375\n",
      "Trained batch 1065 batch loss 0.483516335 batch mAP 0.584686279 batch PCKh 0.3125\n",
      "Trained batch 1066 batch loss 0.40576598 batch mAP 0.656646729 batch PCKh 0.0625\n",
      "Trained batch 1067 batch loss 0.502710402 batch mAP 0.652435303 batch PCKh 0.6875\n",
      "Trained batch 1068 batch loss 0.485836953 batch mAP 0.662384033 batch PCKh 0.6875\n",
      "Trained batch 1069 batch loss 0.449433506 batch mAP 0.705627441 batch PCKh 0.375\n",
      "Trained batch 1070 batch loss 0.430780172 batch mAP 0.69430542 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1071 batch loss 0.528429627 batch mAP 0.614532471 batch PCKh 0.25\n",
      "Trained batch 1072 batch loss 0.400618315 batch mAP 0.681945801 batch PCKh 0.3125\n",
      "Trained batch 1073 batch loss 0.41519618 batch mAP 0.702819824 batch PCKh 0.375\n",
      "Trained batch 1074 batch loss 0.436047077 batch mAP 0.643707275 batch PCKh 0.5\n",
      "Trained batch 1075 batch loss 0.456613243 batch mAP 0.676635742 batch PCKh 0.3125\n",
      "Trained batch 1076 batch loss 0.490949333 batch mAP 0.656555176 batch PCKh 0.4375\n",
      "Trained batch 1077 batch loss 0.516681433 batch mAP 0.584625244 batch PCKh 0.25\n",
      "Trained batch 1078 batch loss 0.512785673 batch mAP 0.639434814 batch PCKh 0.3125\n",
      "Trained batch 1079 batch loss 0.552780867 batch mAP 0.649505615 batch PCKh 0.3125\n",
      "Trained batch 1080 batch loss 0.537507951 batch mAP 0.631103516 batch PCKh 0.3125\n",
      "Trained batch 1081 batch loss 0.429259419 batch mAP 0.688262939 batch PCKh 0.25\n",
      "Trained batch 1082 batch loss 0.437366039 batch mAP 0.646514893 batch PCKh 0.4375\n",
      "Trained batch 1083 batch loss 0.398998559 batch mAP 0.646575928 batch PCKh 0.25\n",
      "Trained batch 1084 batch loss 0.446062535 batch mAP 0.632110596 batch PCKh 0.4375\n",
      "Trained batch 1085 batch loss 0.474864483 batch mAP 0.636230469 batch PCKh 0.375\n",
      "Trained batch 1086 batch loss 0.564654827 batch mAP 0.59198 batch PCKh 0.75\n",
      "Trained batch 1087 batch loss 0.587352335 batch mAP 0.603607178 batch PCKh 0.8125\n",
      "Trained batch 1088 batch loss 0.532213748 batch mAP 0.69821167 batch PCKh 0.8125\n",
      "Trained batch 1089 batch loss 0.553792834 batch mAP 0.663543701 batch PCKh 0.625\n",
      "Trained batch 1090 batch loss 0.494400859 batch mAP 0.739135742 batch PCKh 0.375\n",
      "Trained batch 1091 batch loss 0.536785424 batch mAP 0.698761 batch PCKh 0.375\n",
      "Trained batch 1092 batch loss 0.512273 batch mAP 0.725372314 batch PCKh 0.375\n",
      "Trained batch 1093 batch loss 0.609252572 batch mAP 0.618743896 batch PCKh 0.625\n",
      "Trained batch 1094 batch loss 0.668547392 batch mAP 0.487060547 batch PCKh 0.4375\n",
      "Trained batch 1095 batch loss 0.602859437 batch mAP 0.5050354 batch PCKh 0.625\n",
      "Trained batch 1096 batch loss 0.638494492 batch mAP 0.569122314 batch PCKh 0.375\n",
      "Trained batch 1097 batch loss 0.579581141 batch mAP 0.59576416 batch PCKh 0.5625\n",
      "Trained batch 1098 batch loss 0.668455541 batch mAP 0.560638428 batch PCKh 0.625\n",
      "Trained batch 1099 batch loss 0.496741563 batch mAP 0.544433594 batch PCKh 0.6875\n",
      "Trained batch 1100 batch loss 0.496007 batch mAP 0.582611084 batch PCKh 0.875\n",
      "Trained batch 1101 batch loss 0.513630509 batch mAP 0.555328369 batch PCKh 0.8125\n",
      "Trained batch 1102 batch loss 0.522657096 batch mAP 0.504089355 batch PCKh 0.875\n",
      "Trained batch 1103 batch loss 0.537454247 batch mAP 0.53302 batch PCKh 0.5625\n",
      "Trained batch 1104 batch loss 0.504985571 batch mAP 0.520446777 batch PCKh 0.125\n",
      "Trained batch 1105 batch loss 0.535784364 batch mAP 0.555633545 batch PCKh 0.75\n",
      "Trained batch 1106 batch loss 0.584483743 batch mAP 0.536743164 batch PCKh 0.4375\n",
      "Trained batch 1107 batch loss 0.615807533 batch mAP 0.541046143 batch PCKh 0.375\n",
      "Trained batch 1108 batch loss 0.569920599 batch mAP 0.597015381 batch PCKh 0.25\n",
      "Trained batch 1109 batch loss 0.57472831 batch mAP 0.544891357 batch PCKh 0.6875\n",
      "Trained batch 1110 batch loss 0.617392 batch mAP 0.500457764 batch PCKh 0.3125\n",
      "Trained batch 1111 batch loss 0.643696547 batch mAP 0.508880615 batch PCKh 0.25\n",
      "Trained batch 1112 batch loss 0.672981083 batch mAP 0.493591309 batch PCKh 0.5625\n",
      "Trained batch 1113 batch loss 0.626834512 batch mAP 0.537811279 batch PCKh 0.0625\n",
      "Trained batch 1114 batch loss 0.502200365 batch mAP 0.631195068 batch PCKh 0.4375\n",
      "Trained batch 1115 batch loss 0.550702274 batch mAP 0.579376221 batch PCKh 0.3125\n",
      "Trained batch 1116 batch loss 0.456191927 batch mAP 0.607025146 batch PCKh 0.375\n",
      "Trained batch 1117 batch loss 0.696841776 batch mAP 0.456970215 batch PCKh 0.125\n",
      "Trained batch 1118 batch loss 0.529222429 batch mAP 0.639709473 batch PCKh 0.3125\n",
      "Trained batch 1119 batch loss 0.526994705 batch mAP 0.618042 batch PCKh 0.25\n",
      "Trained batch 1120 batch loss 0.538158059 batch mAP 0.544006348 batch PCKh 0.4375\n",
      "Trained batch 1121 batch loss 0.590715051 batch mAP 0.578582764 batch PCKh 0.1875\n",
      "Trained batch 1122 batch loss 0.67101264 batch mAP 0.511627197 batch PCKh 0.0625\n",
      "Trained batch 1123 batch loss 0.571414232 batch mAP 0.545806885 batch PCKh 0.375\n",
      "Trained batch 1124 batch loss 0.592774868 batch mAP 0.554992676 batch PCKh 0.1875\n",
      "Trained batch 1125 batch loss 0.541168809 batch mAP 0.627349854 batch PCKh 0.375\n",
      "Trained batch 1126 batch loss 0.43368721 batch mAP 0.640594482 batch PCKh 0.3125\n",
      "Trained batch 1127 batch loss 0.439726591 batch mAP 0.616394043 batch PCKh 0.5\n",
      "Trained batch 1128 batch loss 0.454408318 batch mAP 0.585357666 batch PCKh 0.75\n",
      "Trained batch 1129 batch loss 0.566862583 batch mAP 0.474395752 batch PCKh 0.75\n",
      "Trained batch 1130 batch loss 0.500826478 batch mAP 0.503723145 batch PCKh 0.5625\n",
      "Trained batch 1131 batch loss 0.540209651 batch mAP 0.525634766 batch PCKh 0.875\n",
      "Trained batch 1132 batch loss 0.51486212 batch mAP 0.546722412 batch PCKh 0.8125\n",
      "Trained batch 1133 batch loss 0.593654513 batch mAP 0.507751465 batch PCKh 0.8125\n",
      "Trained batch 1134 batch loss 0.643497944 batch mAP 0.487579346 batch PCKh 0.625\n",
      "Trained batch 1135 batch loss 0.575985789 batch mAP 0.495880127 batch PCKh 0.3125\n",
      "Trained batch 1136 batch loss 0.504363656 batch mAP 0.547088623 batch PCKh 0.4375\n",
      "Trained batch 1137 batch loss 0.469379187 batch mAP 0.580474854 batch PCKh 0.3125\n",
      "Trained batch 1138 batch loss 0.540116727 batch mAP 0.606658936 batch PCKh 0.6875\n",
      "Trained batch 1139 batch loss 0.673729479 batch mAP 0.441741943 batch PCKh 0.0625\n",
      "Trained batch 1140 batch loss 0.548703492 batch mAP 0.646026611 batch PCKh 0.4375\n",
      "Trained batch 1141 batch loss 0.506995559 batch mAP 0.614196777 batch PCKh 0.875\n",
      "Trained batch 1142 batch loss 0.53038764 batch mAP 0.590637207 batch PCKh 0.5625\n",
      "Trained batch 1143 batch loss 0.517194629 batch mAP 0.58001709 batch PCKh 0.75\n",
      "Trained batch 1144 batch loss 0.43790561 batch mAP 0.575683594 batch PCKh 0.6875\n",
      "Trained batch 1145 batch loss 0.484333247 batch mAP 0.649902344 batch PCKh 0.3125\n",
      "Trained batch 1146 batch loss 0.535811663 batch mAP 0.641876221 batch PCKh 0.5625\n",
      "Trained batch 1147 batch loss 0.612561584 batch mAP 0.545715332 batch PCKh 0.25\n",
      "Trained batch 1148 batch loss 0.63485235 batch mAP 0.503570557 batch PCKh 0.25\n",
      "Trained batch 1149 batch loss 0.626163244 batch mAP 0.487701416 batch PCKh 0.3125\n",
      "Trained batch 1150 batch loss 0.617863536 batch mAP 0.570404053 batch PCKh 0.1875\n",
      "Trained batch 1151 batch loss 0.529210925 batch mAP 0.6355896 batch PCKh 0.3125\n",
      "Trained batch 1152 batch loss 0.45925653 batch mAP 0.669464111 batch PCKh 0.1875\n",
      "Trained batch 1153 batch loss 0.580303 batch mAP 0.587036133 batch PCKh 0.5625\n",
      "Trained batch 1154 batch loss 0.466644585 batch mAP 0.674163818 batch PCKh 0.375\n",
      "Trained batch 1155 batch loss 0.574793458 batch mAP 0.627471924 batch PCKh 0.375\n",
      "Trained batch 1156 batch loss 0.60472995 batch mAP 0.612213135 batch PCKh 0.4375\n",
      "Trained batch 1157 batch loss 0.649206102 batch mAP 0.59262085 batch PCKh 0.1875\n",
      "Trained batch 1158 batch loss 0.526593089 batch mAP 0.699951172 batch PCKh 0.4375\n",
      "Trained batch 1159 batch loss 0.501813531 batch mAP 0.560577393 batch PCKh 0.625\n",
      "Trained batch 1160 batch loss 0.524498761 batch mAP 0.56451416 batch PCKh 0.6875\n",
      "Trained batch 1161 batch loss 0.597055376 batch mAP 0.451843262 batch PCKh 0.625\n",
      "Trained batch 1162 batch loss 0.607218385 batch mAP 0.458099365 batch PCKh 0.8125\n",
      "Trained batch 1163 batch loss 0.581910133 batch mAP 0.438385 batch PCKh 0.5625\n",
      "Trained batch 1164 batch loss 0.67563355 batch mAP 0.414703369 batch PCKh 0.3125\n",
      "Trained batch 1165 batch loss 0.543116093 batch mAP 0.52948 batch PCKh 0.6875\n",
      "Trained batch 1166 batch loss 0.540439844 batch mAP 0.540344238 batch PCKh 0.5625\n",
      "Trained batch 1167 batch loss 0.438744366 batch mAP 0.608276367 batch PCKh 0.375\n",
      "Trained batch 1168 batch loss 0.422058821 batch mAP 0.497406 batch PCKh 0.3125\n",
      "Trained batch 1169 batch loss 0.522889793 batch mAP 0.518707275 batch PCKh 0.125\n",
      "Trained batch 1170 batch loss 0.427977741 batch mAP 0.541687 batch PCKh 0.1875\n",
      "Trained batch 1171 batch loss 0.368164688 batch mAP 0.531005859 batch PCKh 0.1875\n",
      "Trained batch 1172 batch loss 0.37814939 batch mAP 0.536834717 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1173 batch loss 0.412581056 batch mAP 0.57144165 batch PCKh 0.25\n",
      "Trained batch 1174 batch loss 0.416835457 batch mAP 0.566253662 batch PCKh 0\n",
      "Trained batch 1175 batch loss 0.491209477 batch mAP 0.612487793 batch PCKh 0.5625\n",
      "Trained batch 1176 batch loss 0.482158303 batch mAP 0.649536133 batch PCKh 0.375\n",
      "Trained batch 1177 batch loss 0.459338158 batch mAP 0.660888672 batch PCKh 0.6875\n",
      "Trained batch 1178 batch loss 0.504219234 batch mAP 0.657196045 batch PCKh 0.75\n",
      "Trained batch 1179 batch loss 0.454700649 batch mAP 0.679168701 batch PCKh 0.4375\n",
      "Trained batch 1180 batch loss 0.49498716 batch mAP 0.640960693 batch PCKh 0.1875\n",
      "Trained batch 1181 batch loss 0.494719923 batch mAP 0.656005859 batch PCKh 0.75\n",
      "Trained batch 1182 batch loss 0.480499506 batch mAP 0.717803955 batch PCKh 0.9375\n",
      "Trained batch 1183 batch loss 0.462972879 batch mAP 0.725585938 batch PCKh 0.4375\n",
      "Trained batch 1184 batch loss 0.402157 batch mAP 0.746734619 batch PCKh 0.5625\n",
      "Trained batch 1185 batch loss 0.39365688 batch mAP 0.720062256 batch PCKh 0.8125\n",
      "Trained batch 1186 batch loss 0.484525293 batch mAP 0.607025146 batch PCKh 0.25\n",
      "Trained batch 1187 batch loss 0.52383256 batch mAP 0.514862061 batch PCKh 0.625\n",
      "Trained batch 1188 batch loss 0.625050724 batch mAP 0.467895508 batch PCKh 0.4375\n",
      "Trained batch 1189 batch loss 0.471787393 batch mAP 0.558074951 batch PCKh 0.625\n",
      "Trained batch 1190 batch loss 0.477036 batch mAP 0.644744873 batch PCKh 0.5\n",
      "Trained batch 1191 batch loss 0.445832193 batch mAP 0.675628662 batch PCKh 0.5\n",
      "Trained batch 1192 batch loss 0.604126215 batch mAP 0.529205322 batch PCKh 0\n",
      "Trained batch 1193 batch loss 0.509108067 batch mAP 0.564697266 batch PCKh 0.625\n",
      "Trained batch 1194 batch loss 0.470042109 batch mAP 0.579040527 batch PCKh 0.375\n",
      "Trained batch 1195 batch loss 0.389867723 batch mAP 0.589996338 batch PCKh 0.3125\n",
      "Trained batch 1196 batch loss 0.491208881 batch mAP 0.598144531 batch PCKh 0.625\n",
      "Trained batch 1197 batch loss 0.50455147 batch mAP 0.618255615 batch PCKh 0.625\n",
      "Trained batch 1198 batch loss 0.560722351 batch mAP 0.535614 batch PCKh 0.3125\n",
      "Trained batch 1199 batch loss 0.478427351 batch mAP 0.603179932 batch PCKh 0.6875\n",
      "Trained batch 1200 batch loss 0.411959589 batch mAP 0.577423096 batch PCKh 0.625\n",
      "Trained batch 1201 batch loss 0.510875046 batch mAP 0.62512207 batch PCKh 0.3125\n",
      "Trained batch 1202 batch loss 0.510623157 batch mAP 0.553039551 batch PCKh 0.5\n",
      "Trained batch 1203 batch loss 0.597158551 batch mAP 0.58001709 batch PCKh 0.4375\n",
      "Trained batch 1204 batch loss 0.669315934 batch mAP 0.575897217 batch PCKh 0.375\n",
      "Trained batch 1205 batch loss 0.634433568 batch mAP 0.525512695 batch PCKh 0.625\n",
      "Trained batch 1206 batch loss 0.674799144 batch mAP 0.572296143 batch PCKh 0.1875\n",
      "Trained batch 1207 batch loss 0.582871675 batch mAP 0.542755127 batch PCKh 0.5625\n",
      "Trained batch 1208 batch loss 0.610362172 batch mAP 0.470458984 batch PCKh 0.5\n",
      "Trained batch 1209 batch loss 0.462918 batch mAP 0.49206543 batch PCKh 0.5\n",
      "Trained batch 1210 batch loss 0.458194673 batch mAP 0.561035156 batch PCKh 0.4375\n",
      "Trained batch 1211 batch loss 0.532479763 batch mAP 0.493499756 batch PCKh 0.75\n",
      "Trained batch 1212 batch loss 0.538255453 batch mAP 0.480011 batch PCKh 0.875\n",
      "Trained batch 1213 batch loss 0.379577577 batch mAP 0.508667 batch PCKh 0.125\n",
      "Trained batch 1214 batch loss 0.636113822 batch mAP 0.451782227 batch PCKh 0.75\n",
      "Trained batch 1215 batch loss 0.636223912 batch mAP 0.553894043 batch PCKh 0.1875\n",
      "Trained batch 1216 batch loss 0.704272389 batch mAP 0.561462402 batch PCKh 0.1875\n",
      "Trained batch 1217 batch loss 0.676658571 batch mAP 0.523040771 batch PCKh 0.0625\n",
      "Trained batch 1218 batch loss 0.660770535 batch mAP 0.572357178 batch PCKh 0.375\n",
      "Trained batch 1219 batch loss 0.671367526 batch mAP 0.520141602 batch PCKh 0.25\n",
      "Trained batch 1220 batch loss 0.55572933 batch mAP 0.589172363 batch PCKh 0.625\n",
      "Trained batch 1221 batch loss 0.454412818 batch mAP 0.573242188 batch PCKh 0.625\n",
      "Trained batch 1222 batch loss 0.452525318 batch mAP 0.610443115 batch PCKh 0.5625\n",
      "Trained batch 1223 batch loss 0.551703691 batch mAP 0.576782227 batch PCKh 0.5\n",
      "Trained batch 1224 batch loss 0.549097776 batch mAP 0.615386963 batch PCKh 0.75\n",
      "Trained batch 1225 batch loss 0.46339792 batch mAP 0.621917725 batch PCKh 0.5\n",
      "Trained batch 1226 batch loss 0.499109209 batch mAP 0.586486816 batch PCKh 0.6875\n",
      "Trained batch 1227 batch loss 0.584021389 batch mAP 0.594482422 batch PCKh 0.75\n",
      "Trained batch 1228 batch loss 0.551900744 batch mAP 0.543884277 batch PCKh 0.4375\n",
      "Trained batch 1229 batch loss 0.585905194 batch mAP 0.504455566 batch PCKh 0.6875\n",
      "Trained batch 1230 batch loss 0.555513322 batch mAP 0.538848877 batch PCKh 0.4375\n",
      "Trained batch 1231 batch loss 0.499527812 batch mAP 0.499633789 batch PCKh 0.125\n",
      "Trained batch 1232 batch loss 0.618040144 batch mAP 0.491485596 batch PCKh 0.875\n",
      "Trained batch 1233 batch loss 0.613149643 batch mAP 0.519104 batch PCKh 0.6875\n",
      "Trained batch 1234 batch loss 0.61581552 batch mAP 0.488372803 batch PCKh 0\n",
      "Trained batch 1235 batch loss 0.621465564 batch mAP 0.541595459 batch PCKh 0\n",
      "Trained batch 1236 batch loss 0.629872322 batch mAP 0.53527832 batch PCKh 0.5625\n",
      "Trained batch 1237 batch loss 0.599644184 batch mAP 0.514556885 batch PCKh 0.6875\n",
      "Trained batch 1238 batch loss 0.630493402 batch mAP 0.580749512 batch PCKh 0.0625\n",
      "Trained batch 1239 batch loss 0.668547869 batch mAP 0.557952881 batch PCKh 0.125\n",
      "Trained batch 1240 batch loss 0.568948269 batch mAP 0.634307861 batch PCKh 0.625\n",
      "Trained batch 1241 batch loss 0.567283452 batch mAP 0.652862549 batch PCKh 0.3125\n",
      "Trained batch 1242 batch loss 0.596079886 batch mAP 0.648071289 batch PCKh 0.4375\n",
      "Trained batch 1243 batch loss 0.520715356 batch mAP 0.632568359 batch PCKh 0.625\n",
      "Trained batch 1244 batch loss 0.487331301 batch mAP 0.62323 batch PCKh 0.5625\n",
      "Trained batch 1245 batch loss 0.491091758 batch mAP 0.608856201 batch PCKh 0.125\n",
      "Trained batch 1246 batch loss 0.531746864 batch mAP 0.632782 batch PCKh 0.625\n",
      "Trained batch 1247 batch loss 0.574632049 batch mAP 0.622497559 batch PCKh 0.3125\n",
      "Trained batch 1248 batch loss 0.566951334 batch mAP 0.600524902 batch PCKh 0.125\n",
      "Trained batch 1249 batch loss 0.582352042 batch mAP 0.562133789 batch PCKh 0.1875\n",
      "Trained batch 1250 batch loss 0.534984708 batch mAP 0.675140381 batch PCKh 0.625\n",
      "Trained batch 1251 batch loss 0.61868906 batch mAP 0.601043701 batch PCKh 0.4375\n",
      "Trained batch 1252 batch loss 0.453021973 batch mAP 0.68270874 batch PCKh 0.375\n",
      "Trained batch 1253 batch loss 0.565646172 batch mAP 0.6484375 batch PCKh 0.75\n",
      "Trained batch 1254 batch loss 0.505446434 batch mAP 0.681030273 batch PCKh 0.6875\n",
      "Trained batch 1255 batch loss 0.470816582 batch mAP 0.625335693 batch PCKh 0.75\n",
      "Trained batch 1256 batch loss 0.554514945 batch mAP 0.629333496 batch PCKh 0.5625\n",
      "Trained batch 1257 batch loss 0.485423237 batch mAP 0.620361328 batch PCKh 0.625\n",
      "Trained batch 1258 batch loss 0.533506751 batch mAP 0.608642578 batch PCKh 0.5625\n",
      "Trained batch 1259 batch loss 0.492238313 batch mAP 0.569397 batch PCKh 0.1875\n",
      "Trained batch 1260 batch loss 0.572999597 batch mAP 0.57901 batch PCKh 0.3125\n",
      "Trained batch 1261 batch loss 0.593090236 batch mAP 0.612731934 batch PCKh 0.8125\n",
      "Trained batch 1262 batch loss 0.512332559 batch mAP 0.580413818 batch PCKh 0.625\n",
      "Trained batch 1263 batch loss 0.587936103 batch mAP 0.593811035 batch PCKh 0.6875\n",
      "Trained batch 1264 batch loss 0.514701128 batch mAP 0.535430908 batch PCKh 0.625\n",
      "Trained batch 1265 batch loss 0.435105145 batch mAP 0.555419922 batch PCKh 0.5\n",
      "Trained batch 1266 batch loss 0.471084535 batch mAP 0.52255249 batch PCKh 0.1875\n",
      "Trained batch 1267 batch loss 0.485610306 batch mAP 0.570983887 batch PCKh 0.5\n",
      "Trained batch 1268 batch loss 0.551619887 batch mAP 0.562316895 batch PCKh 0.5\n",
      "Trained batch 1269 batch loss 0.527350783 batch mAP 0.595092773 batch PCKh 0.75\n",
      "Trained batch 1270 batch loss 0.564363241 batch mAP 0.579467773 batch PCKh 0.6875\n",
      "Trained batch 1271 batch loss 0.473487318 batch mAP 0.586029053 batch PCKh 0.6875\n",
      "Trained batch 1272 batch loss 0.535117388 batch mAP 0.566558838 batch PCKh 0.625\n",
      "Trained batch 1273 batch loss 0.592388868 batch mAP 0.61416626 batch PCKh 0.6875\n",
      "Trained batch 1274 batch loss 0.646725476 batch mAP 0.555755615 batch PCKh 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1275 batch loss 0.50703913 batch mAP 0.653717041 batch PCKh 0.5\n",
      "Trained batch 1276 batch loss 0.548110485 batch mAP 0.631439209 batch PCKh 0.3125\n",
      "Trained batch 1277 batch loss 0.541761518 batch mAP 0.621246338 batch PCKh 0.375\n",
      "Trained batch 1278 batch loss 0.48965168 batch mAP 0.640838623 batch PCKh 0.5625\n",
      "Trained batch 1279 batch loss 0.510629296 batch mAP 0.684783936 batch PCKh 0.75\n",
      "Trained batch 1280 batch loss 0.533123195 batch mAP 0.638183594 batch PCKh 0.1875\n",
      "Trained batch 1281 batch loss 0.60897243 batch mAP 0.55569458 batch PCKh 0.75\n",
      "Trained batch 1282 batch loss 0.635907292 batch mAP 0.589386 batch PCKh 0.375\n",
      "Trained batch 1283 batch loss 0.526955783 batch mAP 0.621063232 batch PCKh 0.8125\n",
      "Trained batch 1284 batch loss 0.576513 batch mAP 0.602172852 batch PCKh 0.8125\n",
      "Trained batch 1285 batch loss 0.553604 batch mAP 0.651092529 batch PCKh 0.625\n",
      "Trained batch 1286 batch loss 0.577967584 batch mAP 0.641876221 batch PCKh 0.75\n",
      "Trained batch 1287 batch loss 0.563290238 batch mAP 0.546875 batch PCKh 0.6875\n",
      "Trained batch 1288 batch loss 0.507046461 batch mAP 0.559082031 batch PCKh 0.625\n",
      "Trained batch 1289 batch loss 0.570404768 batch mAP 0.587280273 batch PCKh 0.1875\n",
      "Trained batch 1290 batch loss 0.538067341 batch mAP 0.585174561 batch PCKh 0.875\n",
      "Trained batch 1291 batch loss 0.533210278 batch mAP 0.539886475 batch PCKh 0\n",
      "Trained batch 1292 batch loss 0.560114563 batch mAP 0.526733398 batch PCKh 0.25\n",
      "Trained batch 1293 batch loss 0.448581219 batch mAP 0.587615967 batch PCKh 0.625\n",
      "Trained batch 1294 batch loss 0.498728544 batch mAP 0.635864258 batch PCKh 0.5\n",
      "Trained batch 1295 batch loss 0.529537559 batch mAP 0.571167 batch PCKh 0.75\n",
      "Trained batch 1296 batch loss 0.567087173 batch mAP 0.585968 batch PCKh 0.4375\n",
      "Trained batch 1297 batch loss 0.563057542 batch mAP 0.599151611 batch PCKh 0.3125\n",
      "Trained batch 1298 batch loss 0.548413217 batch mAP 0.58493042 batch PCKh 0.75\n",
      "Trained batch 1299 batch loss 0.565819621 batch mAP 0.580657959 batch PCKh 0.625\n",
      "Trained batch 1300 batch loss 0.526093185 batch mAP 0.594787598 batch PCKh 0.4375\n",
      "Trained batch 1301 batch loss 0.539681673 batch mAP 0.59173584 batch PCKh 0.75\n",
      "Trained batch 1302 batch loss 0.491391778 batch mAP 0.589477539 batch PCKh 0.5625\n",
      "Trained batch 1303 batch loss 0.537363112 batch mAP 0.58215332 batch PCKh 0.875\n",
      "Trained batch 1304 batch loss 0.556909382 batch mAP 0.524169922 batch PCKh 0.875\n",
      "Trained batch 1305 batch loss 0.551828 batch mAP 0.489013672 batch PCKh 0.875\n",
      "Trained batch 1306 batch loss 0.347327471 batch mAP 0.657196045 batch PCKh 0\n",
      "Trained batch 1307 batch loss 0.589422107 batch mAP 0.616027832 batch PCKh 0.25\n",
      "Trained batch 1308 batch loss 0.41726464 batch mAP 0.685150146 batch PCKh 0.5625\n",
      "Trained batch 1309 batch loss 0.429322958 batch mAP 0.682220459 batch PCKh 0.375\n",
      "Trained batch 1310 batch loss 0.413213432 batch mAP 0.697113037 batch PCKh 0.4375\n",
      "Trained batch 1311 batch loss 0.426068902 batch mAP 0.710296631 batch PCKh 0.4375\n",
      "Trained batch 1312 batch loss 0.495850652 batch mAP 0.675628662 batch PCKh 0.4375\n",
      "Trained batch 1313 batch loss 0.493207932 batch mAP 0.647460938 batch PCKh 0.375\n",
      "Trained batch 1314 batch loss 0.480536759 batch mAP 0.641235352 batch PCKh 0.4375\n",
      "Trained batch 1315 batch loss 0.477492332 batch mAP 0.732788086 batch PCKh 0.4375\n",
      "Trained batch 1316 batch loss 0.487139881 batch mAP 0.718475342 batch PCKh 0.375\n",
      "Trained batch 1317 batch loss 0.576428235 batch mAP 0.592163086 batch PCKh 0.625\n",
      "Trained batch 1318 batch loss 0.466227204 batch mAP 0.594543457 batch PCKh 0.375\n",
      "Trained batch 1319 batch loss 0.449373722 batch mAP 0.593414307 batch PCKh 0.5\n",
      "Trained batch 1320 batch loss 0.545488775 batch mAP 0.692321777 batch PCKh 0.375\n",
      "Trained batch 1321 batch loss 0.584082 batch mAP 0.684448242 batch PCKh 0.625\n",
      "Trained batch 1322 batch loss 0.632404923 batch mAP 0.618164062 batch PCKh 0.4375\n",
      "Trained batch 1323 batch loss 0.573766947 batch mAP 0.620666504 batch PCKh 0.1875\n",
      "Trained batch 1324 batch loss 0.567175508 batch mAP 0.595306396 batch PCKh 0.375\n",
      "Trained batch 1325 batch loss 0.612394691 batch mAP 0.495056152 batch PCKh 0.75\n",
      "Trained batch 1326 batch loss 0.601054728 batch mAP 0.578735352 batch PCKh 0.1875\n",
      "Trained batch 1327 batch loss 0.577469 batch mAP 0.499298096 batch PCKh 0.3125\n",
      "Trained batch 1328 batch loss 0.503950298 batch mAP 0.560791 batch PCKh 0.5625\n",
      "Trained batch 1329 batch loss 0.479707718 batch mAP 0.535888672 batch PCKh 0\n",
      "Trained batch 1330 batch loss 0.444802284 batch mAP 0.637695312 batch PCKh 0.1875\n",
      "Trained batch 1331 batch loss 0.484475732 batch mAP 0.637481689 batch PCKh 0.5\n",
      "Trained batch 1332 batch loss 0.497464895 batch mAP 0.59777832 batch PCKh 0.5625\n",
      "Trained batch 1333 batch loss 0.466409832 batch mAP 0.614715576 batch PCKh 0.875\n",
      "Trained batch 1334 batch loss 0.553340673 batch mAP 0.605072 batch PCKh 0.6875\n",
      "Trained batch 1335 batch loss 0.571443498 batch mAP 0.593383789 batch PCKh 0.5625\n",
      "Trained batch 1336 batch loss 0.562224269 batch mAP 0.581207275 batch PCKh 0.375\n",
      "Trained batch 1337 batch loss 0.626693189 batch mAP 0.588531494 batch PCKh 0.3125\n",
      "Trained batch 1338 batch loss 0.591169715 batch mAP 0.598938 batch PCKh 0.6875\n",
      "Trained batch 1339 batch loss 0.575804412 batch mAP 0.672149658 batch PCKh 0.5625\n",
      "Trained batch 1340 batch loss 0.544803262 batch mAP 0.626586914 batch PCKh 0.4375\n",
      "Trained batch 1341 batch loss 0.592974067 batch mAP 0.621887207 batch PCKh 0.3125\n",
      "Trained batch 1342 batch loss 0.525041461 batch mAP 0.534210205 batch PCKh 0.4375\n",
      "Trained batch 1343 batch loss 0.481527328 batch mAP 0.585571289 batch PCKh 0.5\n",
      "Trained batch 1344 batch loss 0.493400902 batch mAP 0.551879883 batch PCKh 0.1875\n",
      "Trained batch 1345 batch loss 0.474731952 batch mAP 0.582489 batch PCKh 0.5\n",
      "Trained batch 1346 batch loss 0.50839293 batch mAP 0.599945068 batch PCKh 0.375\n",
      "Trained batch 1347 batch loss 0.568226099 batch mAP 0.65145874 batch PCKh 0.375\n",
      "Trained batch 1348 batch loss 0.570923328 batch mAP 0.661010742 batch PCKh 0.3125\n",
      "Trained batch 1349 batch loss 0.64030683 batch mAP 0.648986816 batch PCKh 0.625\n",
      "Trained batch 1350 batch loss 0.614909649 batch mAP 0.632110596 batch PCKh 0.3125\n",
      "Trained batch 1351 batch loss 0.540842295 batch mAP 0.679443359 batch PCKh 0.25\n",
      "Trained batch 1352 batch loss 0.512292 batch mAP 0.680206299 batch PCKh 0.375\n",
      "Trained batch 1353 batch loss 0.51820749 batch mAP 0.668060303 batch PCKh 0.4375\n",
      "Trained batch 1354 batch loss 0.481217951 batch mAP 0.674224854 batch PCKh 0.3125\n",
      "Trained batch 1355 batch loss 0.46612525 batch mAP 0.693847656 batch PCKh 0.625\n",
      "Trained batch 1356 batch loss 0.632701397 batch mAP 0.64440918 batch PCKh 0.4375\n",
      "Trained batch 1357 batch loss 0.496676207 batch mAP 0.676055908 batch PCKh 0.3125\n",
      "Trained batch 1358 batch loss 0.500391066 batch mAP 0.652252197 batch PCKh 0.4375\n",
      "Trained batch 1359 batch loss 0.505013883 batch mAP 0.624389648 batch PCKh 0.4375\n",
      "Trained batch 1360 batch loss 0.508139491 batch mAP 0.629669189 batch PCKh 0.625\n",
      "Trained batch 1361 batch loss 0.546348572 batch mAP 0.571746826 batch PCKh 0.5\n",
      "Trained batch 1362 batch loss 0.544054 batch mAP 0.534240723 batch PCKh 0.75\n",
      "Trained batch 1363 batch loss 0.477831483 batch mAP 0.565246582 batch PCKh 0.5625\n",
      "Trained batch 1364 batch loss 0.534662247 batch mAP 0.539794922 batch PCKh 0.75\n",
      "Trained batch 1365 batch loss 0.464807749 batch mAP 0.58215332 batch PCKh 0.625\n",
      "Trained batch 1366 batch loss 0.480082929 batch mAP 0.564300537 batch PCKh 0.5\n",
      "Trained batch 1367 batch loss 0.517548323 batch mAP 0.588531494 batch PCKh 0.625\n",
      "Trained batch 1368 batch loss 0.477022558 batch mAP 0.617584229 batch PCKh 0.5\n",
      "Trained batch 1369 batch loss 0.511728406 batch mAP 0.581115723 batch PCKh 0.875\n",
      "Trained batch 1370 batch loss 0.53611958 batch mAP 0.551757812 batch PCKh 0.875\n",
      "Trained batch 1371 batch loss 0.487499744 batch mAP 0.653961182 batch PCKh 0.6875\n",
      "Trained batch 1372 batch loss 0.450688571 batch mAP 0.6484375 batch PCKh 0.4375\n",
      "Trained batch 1373 batch loss 0.522516429 batch mAP 0.658721924 batch PCKh 0.4375\n",
      "Trained batch 1374 batch loss 0.528220296 batch mAP 0.630493164 batch PCKh 0.75\n",
      "Trained batch 1375 batch loss 0.493381321 batch mAP 0.581848145 batch PCKh 0.625\n",
      "Trained batch 1376 batch loss 0.527395487 batch mAP 0.588592529 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1377 batch loss 0.502840102 batch mAP 0.606628418 batch PCKh 0.6875\n",
      "Trained batch 1378 batch loss 0.506412864 batch mAP 0.547454834 batch PCKh 0.5\n",
      "Trained batch 1379 batch loss 0.453540683 batch mAP 0.58203125 batch PCKh 0.25\n",
      "Trained batch 1380 batch loss 0.498448074 batch mAP 0.632598877 batch PCKh 0.5625\n",
      "Trained batch 1381 batch loss 0.536901176 batch mAP 0.620727539 batch PCKh 0.4375\n",
      "Trained batch 1382 batch loss 0.521482944 batch mAP 0.652099609 batch PCKh 0.25\n",
      "Trained batch 1383 batch loss 0.467116803 batch mAP 0.642272949 batch PCKh 0.3125\n",
      "Trained batch 1384 batch loss 0.488588423 batch mAP 0.610809326 batch PCKh 0.1875\n",
      "Trained batch 1385 batch loss 0.572627664 batch mAP 0.542053223 batch PCKh 0.5\n",
      "Trained batch 1386 batch loss 0.571153104 batch mAP 0.477264404 batch PCKh 0\n",
      "Trained batch 1387 batch loss 0.581870556 batch mAP 0.572692871 batch PCKh 0.25\n",
      "Trained batch 1388 batch loss 0.578761697 batch mAP 0.631530762 batch PCKh 0.8125\n",
      "Trained batch 1389 batch loss 0.604641438 batch mAP 0.609680176 batch PCKh 0.5625\n",
      "Trained batch 1390 batch loss 0.689797878 batch mAP 0.506317139 batch PCKh 0.3125\n",
      "Trained batch 1391 batch loss 0.624519 batch mAP 0.6222229 batch PCKh 0.25\n",
      "Trained batch 1392 batch loss 0.51012826 batch mAP 0.614776611 batch PCKh 0.625\n",
      "Trained batch 1393 batch loss 0.591368675 batch mAP 0.53817749 batch PCKh 0.875\n",
      "Trained batch 1394 batch loss 0.494192153 batch mAP 0.526947 batch PCKh 0.6875\n",
      "Trained batch 1395 batch loss 0.575668752 batch mAP 0.569610596 batch PCKh 0.6875\n",
      "Trained batch 1396 batch loss 0.668254077 batch mAP 0.425689697 batch PCKh 0.1875\n",
      "Trained batch 1397 batch loss 0.661878586 batch mAP 0.446258545 batch PCKh 0.5625\n",
      "Trained batch 1398 batch loss 0.568447351 batch mAP 0.537109375 batch PCKh 0.4375\n",
      "Trained batch 1399 batch loss 0.642886162 batch mAP 0.554107666 batch PCKh 0.9375\n",
      "Trained batch 1400 batch loss 0.591714263 batch mAP 0.559936523 batch PCKh 0.5\n",
      "Trained batch 1401 batch loss 0.450285137 batch mAP 0.547271729 batch PCKh 0.625\n",
      "Trained batch 1402 batch loss 0.546103537 batch mAP 0.602966309 batch PCKh 0\n",
      "Trained batch 1403 batch loss 0.512397528 batch mAP 0.609222412 batch PCKh 0.375\n",
      "Trained batch 1404 batch loss 0.56573987 batch mAP 0.658935547 batch PCKh 0.25\n",
      "Trained batch 1405 batch loss 0.500918925 batch mAP 0.57220459 batch PCKh 0.25\n",
      "Trained batch 1406 batch loss 0.513928652 batch mAP 0.605621338 batch PCKh 0.5625\n",
      "Trained batch 1407 batch loss 0.555147171 batch mAP 0.559387207 batch PCKh 0.875\n",
      "Trained batch 1408 batch loss 0.520285428 batch mAP 0.559906 batch PCKh 0.25\n",
      "Trained batch 1409 batch loss 0.593686 batch mAP 0.560455322 batch PCKh 0.6875\n",
      "Trained batch 1410 batch loss 0.533147931 batch mAP 0.567901611 batch PCKh 0.8125\n",
      "Trained batch 1411 batch loss 0.591100693 batch mAP 0.550323486 batch PCKh 0.4375\n",
      "Trained batch 1412 batch loss 0.585257113 batch mAP 0.570282 batch PCKh 0.75\n",
      "Trained batch 1413 batch loss 0.587448359 batch mAP 0.568969727 batch PCKh 0.4375\n",
      "Trained batch 1414 batch loss 0.576554179 batch mAP 0.56741333 batch PCKh 0.4375\n",
      "Trained batch 1415 batch loss 0.549332142 batch mAP 0.577972412 batch PCKh 0.4375\n",
      "Trained batch 1416 batch loss 0.538778305 batch mAP 0.571685791 batch PCKh 0.6875\n",
      "Trained batch 1417 batch loss 0.577138722 batch mAP 0.645385742 batch PCKh 0.6875\n",
      "Trained batch 1418 batch loss 0.644954085 batch mAP 0.559295654 batch PCKh 0.0625\n",
      "Trained batch 1419 batch loss 0.608238 batch mAP 0.621124268 batch PCKh 0.1875\n",
      "Trained batch 1420 batch loss 0.50740391 batch mAP 0.639068604 batch PCKh 0.6875\n",
      "Trained batch 1421 batch loss 0.436466187 batch mAP 0.652709961 batch PCKh 0.4375\n",
      "Trained batch 1422 batch loss 0.395911217 batch mAP 0.650390625 batch PCKh 0.5625\n",
      "Trained batch 1423 batch loss 0.494082898 batch mAP 0.632965088 batch PCKh 0.75\n",
      "Trained batch 1424 batch loss 0.504511476 batch mAP 0.524200439 batch PCKh 0.625\n",
      "Trained batch 1425 batch loss 0.51666528 batch mAP 0.565460205 batch PCKh 0.4375\n",
      "Trained batch 1426 batch loss 0.478435218 batch mAP 0.571289062 batch PCKh 0.375\n",
      "Trained batch 1427 batch loss 0.58021 batch mAP 0.541107178 batch PCKh 0.625\n",
      "Trained batch 1428 batch loss 0.544967 batch mAP 0.537139893 batch PCKh 0.375\n",
      "Trained batch 1429 batch loss 0.529036582 batch mAP 0.580688477 batch PCKh 0.375\n",
      "Trained batch 1430 batch loss 0.596141 batch mAP 0.635070801 batch PCKh 0.125\n",
      "Trained batch 1431 batch loss 0.494199723 batch mAP 0.744842529 batch PCKh 0.5625\n",
      "Trained batch 1432 batch loss 0.567413 batch mAP 0.589752197 batch PCKh 0.375\n",
      "Trained batch 1433 batch loss 0.536340594 batch mAP 0.523712158 batch PCKh 0.1875\n",
      "Trained batch 1434 batch loss 0.427734792 batch mAP 0.646209717 batch PCKh 0.1875\n",
      "Trained batch 1435 batch loss 0.511112869 batch mAP 0.587921143 batch PCKh 0.375\n",
      "Trained batch 1436 batch loss 0.565899193 batch mAP 0.520233154 batch PCKh 0.0625\n",
      "Trained batch 1437 batch loss 0.537167072 batch mAP 0.467865 batch PCKh 0.125\n",
      "Trained batch 1438 batch loss 0.507794678 batch mAP 0.542511 batch PCKh 0.5\n",
      "Trained batch 1439 batch loss 0.580850363 batch mAP 0.533874512 batch PCKh 0.625\n",
      "Trained batch 1440 batch loss 0.608708382 batch mAP 0.541687 batch PCKh 0.375\n",
      "Trained batch 1441 batch loss 0.696644664 batch mAP 0.51550293 batch PCKh 0.875\n",
      "Trained batch 1442 batch loss 0.612307847 batch mAP 0.555145264 batch PCKh 0.4375\n",
      "Trained batch 1443 batch loss 0.665811181 batch mAP 0.546630859 batch PCKh 0.6875\n",
      "Trained batch 1444 batch loss 0.645165205 batch mAP 0.532897949 batch PCKh 0.3125\n",
      "Trained batch 1445 batch loss 0.569726467 batch mAP 0.617553711 batch PCKh 0.3125\n",
      "Trained batch 1446 batch loss 0.473446369 batch mAP 0.606933594 batch PCKh 0.75\n",
      "Trained batch 1447 batch loss 0.462416708 batch mAP 0.565094 batch PCKh 0.5625\n",
      "Trained batch 1448 batch loss 0.582422733 batch mAP 0.55670166 batch PCKh 0.6875\n",
      "Trained batch 1449 batch loss 0.554732859 batch mAP 0.659637451 batch PCKh 0.5625\n",
      "Trained batch 1450 batch loss 0.559065402 batch mAP 0.582611084 batch PCKh 0.5\n",
      "Trained batch 1451 batch loss 0.475622743 batch mAP 0.580108643 batch PCKh 0.75\n",
      "Trained batch 1452 batch loss 0.545975864 batch mAP 0.538909912 batch PCKh 0.4375\n",
      "Trained batch 1453 batch loss 0.553624213 batch mAP 0.593658447 batch PCKh 0.3125\n",
      "Trained batch 1454 batch loss 0.555404782 batch mAP 0.638275146 batch PCKh 0.25\n",
      "Trained batch 1455 batch loss 0.592321754 batch mAP 0.656311035 batch PCKh 0.3125\n",
      "Trained batch 1456 batch loss 0.607436538 batch mAP 0.615020752 batch PCKh 0.5625\n",
      "Trained batch 1457 batch loss 0.604974031 batch mAP 0.542816162 batch PCKh 0.5\n",
      "Trained batch 1458 batch loss 0.584145069 batch mAP 0.593841553 batch PCKh 0.375\n",
      "Trained batch 1459 batch loss 0.607454836 batch mAP 0.610809326 batch PCKh 0.5\n",
      "Trained batch 1460 batch loss 0.570967436 batch mAP 0.655914307 batch PCKh 0.3125\n",
      "Trained batch 1461 batch loss 0.611011207 batch mAP 0.618438721 batch PCKh 0.25\n",
      "Trained batch 1462 batch loss 0.657442153 batch mAP 0.60748291 batch PCKh 0.375\n",
      "Trained batch 1463 batch loss 0.585659385 batch mAP 0.551422119 batch PCKh 0.5625\n",
      "Trained batch 1464 batch loss 0.667154 batch mAP 0.597351074 batch PCKh 0.625\n",
      "Trained batch 1465 batch loss 0.624815106 batch mAP 0.577148438 batch PCKh 0.5\n",
      "Trained batch 1466 batch loss 0.637452602 batch mAP 0.619842529 batch PCKh 0.6875\n",
      "Trained batch 1467 batch loss 0.635831773 batch mAP 0.596435547 batch PCKh 0.625\n",
      "Trained batch 1468 batch loss 0.596768379 batch mAP 0.574890137 batch PCKh 0.5625\n",
      "Trained batch 1469 batch loss 0.595122695 batch mAP 0.550048828 batch PCKh 0.375\n",
      "Trained batch 1470 batch loss 0.625162184 batch mAP 0.575775146 batch PCKh 0.75\n",
      "Trained batch 1471 batch loss 0.584458053 batch mAP 0.546386719 batch PCKh 0.5625\n",
      "Trained batch 1472 batch loss 0.588674307 batch mAP 0.567626953 batch PCKh 0.375\n",
      "Trained batch 1473 batch loss 0.522734821 batch mAP 0.564300537 batch PCKh 0.625\n",
      "Trained batch 1474 batch loss 0.613274455 batch mAP 0.517822266 batch PCKh 0.5625\n",
      "Trained batch 1475 batch loss 0.550379872 batch mAP 0.546508789 batch PCKh 0.8125\n",
      "Trained batch 1476 batch loss 0.579366922 batch mAP 0.583679199 batch PCKh 0.6875\n",
      "Trained batch 1477 batch loss 0.602858245 batch mAP 0.595397949 batch PCKh 0.25\n",
      "Trained batch 1478 batch loss 0.518058 batch mAP 0.547149658 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1479 batch loss 0.493581831 batch mAP 0.608398438 batch PCKh 0.75\n",
      "Trained batch 1480 batch loss 0.530512094 batch mAP 0.592651367 batch PCKh 0.4375\n",
      "Trained batch 1481 batch loss 0.55488 batch mAP 0.582672119 batch PCKh 0.1875\n",
      "Trained batch 1482 batch loss 0.554188967 batch mAP 0.623779297 batch PCKh 0.375\n",
      "Trained batch 1483 batch loss 0.532632411 batch mAP 0.621368408 batch PCKh 0.5\n",
      "Trained batch 1484 batch loss 0.550296307 batch mAP 0.557373047 batch PCKh 0.8125\n",
      "Trained batch 1485 batch loss 0.591151595 batch mAP 0.593200684 batch PCKh 0.8125\n",
      "Trained batch 1486 batch loss 0.524328947 batch mAP 0.570983887 batch PCKh 0.75\n",
      "Trained batch 1487 batch loss 0.455668211 batch mAP 0.564209 batch PCKh 0.0625\n",
      "Trained batch 1488 batch loss 0.519675612 batch mAP 0.693206787 batch PCKh 0.5\n",
      "Trained batch 1489 batch loss 0.504964113 batch mAP 0.673339844 batch PCKh 0.875\n",
      "Trained batch 1490 batch loss 0.528823256 batch mAP 0.638916 batch PCKh 0.8125\n",
      "Trained batch 1491 batch loss 0.50221473 batch mAP 0.63104248 batch PCKh 0.4375\n",
      "Trained batch 1492 batch loss 0.445125401 batch mAP 0.689178467 batch PCKh 0.75\n",
      "Trained batch 1493 batch loss 0.491270036 batch mAP 0.642456055 batch PCKh 0.625\n",
      "Trained batch 1494 batch loss 0.449329972 batch mAP 0.71762085 batch PCKh 0.6875\n",
      "Trained batch 1495 batch loss 0.491877913 batch mAP 0.683898926 batch PCKh 0.625\n",
      "Trained batch 1496 batch loss 0.561879098 batch mAP 0.606536865 batch PCKh 0.5\n",
      "Trained batch 1497 batch loss 0.588178933 batch mAP 0.599151611 batch PCKh 0.5625\n",
      "Trained batch 1498 batch loss 0.537898958 batch mAP 0.629058838 batch PCKh 0.6875\n",
      "Trained batch 1499 batch loss 0.565833688 batch mAP 0.617340088 batch PCKh 0.6875\n",
      "Trained batch 1500 batch loss 0.605780244 batch mAP 0.591217041 batch PCKh 0.25\n",
      "Trained batch 1501 batch loss 0.614887953 batch mAP 0.565032959 batch PCKh 0.375\n",
      "Trained batch 1502 batch loss 0.511173844 batch mAP 0.629577637 batch PCKh 0.5625\n",
      "Trained batch 1503 batch loss 0.557792902 batch mAP 0.626709 batch PCKh 0.5\n",
      "Trained batch 1504 batch loss 0.514062643 batch mAP 0.656616211 batch PCKh 0.875\n",
      "Trained batch 1505 batch loss 0.523278236 batch mAP 0.624603271 batch PCKh 0.4375\n",
      "Trained batch 1506 batch loss 0.534383774 batch mAP 0.58505249 batch PCKh 0.6875\n",
      "Trained batch 1507 batch loss 0.567963541 batch mAP 0.584533691 batch PCKh 0.75\n",
      "Trained batch 1508 batch loss 0.540142536 batch mAP 0.603820801 batch PCKh 0.3125\n",
      "Trained batch 1509 batch loss 0.527738214 batch mAP 0.632080078 batch PCKh 0.4375\n",
      "Trained batch 1510 batch loss 0.452529728 batch mAP 0.637786865 batch PCKh 0.375\n",
      "Trained batch 1511 batch loss 0.446171075 batch mAP 0.687835693 batch PCKh 0.5\n",
      "Trained batch 1512 batch loss 0.47217536 batch mAP 0.683288574 batch PCKh 0.6875\n",
      "Trained batch 1513 batch loss 0.533355892 batch mAP 0.63772583 batch PCKh 0.6875\n",
      "Trained batch 1514 batch loss 0.608095586 batch mAP 0.542358398 batch PCKh 0.1875\n",
      "Trained batch 1515 batch loss 0.441612393 batch mAP 0.660064697 batch PCKh 0.5\n",
      "Trained batch 1516 batch loss 0.442664713 batch mAP 0.649169922 batch PCKh 0.625\n",
      "Trained batch 1517 batch loss 0.513841093 batch mAP 0.585571289 batch PCKh 0.875\n",
      "Trained batch 1518 batch loss 0.530007064 batch mAP 0.612579346 batch PCKh 0.625\n",
      "Trained batch 1519 batch loss 0.592405319 batch mAP 0.539764404 batch PCKh 0.75\n",
      "Trained batch 1520 batch loss 0.495346 batch mAP 0.577514648 batch PCKh 0.4375\n",
      "Trained batch 1521 batch loss 0.60717 batch mAP 0.580169678 batch PCKh 0.5\n",
      "Trained batch 1522 batch loss 0.544175446 batch mAP 0.612609863 batch PCKh 0.25\n",
      "Trained batch 1523 batch loss 0.434122175 batch mAP 0.679107666 batch PCKh 0.75\n",
      "Trained batch 1524 batch loss 0.525157213 batch mAP 0.63684082 batch PCKh 0.5625\n",
      "Trained batch 1525 batch loss 0.511755466 batch mAP 0.609832764 batch PCKh 0.5625\n",
      "Trained batch 1526 batch loss 0.589301348 batch mAP 0.594696045 batch PCKh 0.625\n",
      "Trained batch 1527 batch loss 0.570559859 batch mAP 0.555755615 batch PCKh 0.5\n",
      "Trained batch 1528 batch loss 0.561754942 batch mAP 0.573547363 batch PCKh 0.5625\n",
      "Trained batch 1529 batch loss 0.468514562 batch mAP 0.5809021 batch PCKh 0.5\n",
      "Trained batch 1530 batch loss 0.533647895 batch mAP 0.523498535 batch PCKh 0.625\n",
      "Trained batch 1531 batch loss 0.512212634 batch mAP 0.557800293 batch PCKh 0.5\n",
      "Trained batch 1532 batch loss 0.453935802 batch mAP 0.647033691 batch PCKh 0.6875\n",
      "Trained batch 1533 batch loss 0.514412582 batch mAP 0.605804443 batch PCKh 0.0625\n",
      "Trained batch 1534 batch loss 0.52783972 batch mAP 0.605194092 batch PCKh 0.75\n",
      "Trained batch 1535 batch loss 0.510946035 batch mAP 0.570556641 batch PCKh 0.375\n",
      "Trained batch 1536 batch loss 0.503521323 batch mAP 0.61315918 batch PCKh 0.8125\n",
      "Trained batch 1537 batch loss 0.536285877 batch mAP 0.536376953 batch PCKh 0.75\n",
      "Trained batch 1538 batch loss 0.62158668 batch mAP 0.500946045 batch PCKh 0.6875\n",
      "Trained batch 1539 batch loss 0.544733882 batch mAP 0.569763184 batch PCKh 0.625\n",
      "Trained batch 1540 batch loss 0.545092165 batch mAP 0.574432373 batch PCKh 0.1875\n",
      "Trained batch 1541 batch loss 0.493059903 batch mAP 0.630340576 batch PCKh 0.75\n",
      "Trained batch 1542 batch loss 0.481690586 batch mAP 0.621734619 batch PCKh 0.5\n",
      "Trained batch 1543 batch loss 0.474863887 batch mAP 0.601898193 batch PCKh 0.5\n",
      "Trained batch 1544 batch loss 0.474058837 batch mAP 0.604309082 batch PCKh 0.6875\n",
      "Trained batch 1545 batch loss 0.561647534 batch mAP 0.593841553 batch PCKh 0.4375\n",
      "Trained batch 1546 batch loss 0.506928563 batch mAP 0.62789917 batch PCKh 0.5\n",
      "Trained batch 1547 batch loss 0.561094046 batch mAP 0.620513916 batch PCKh 0.4375\n",
      "Trained batch 1548 batch loss 0.540730953 batch mAP 0.63961792 batch PCKh 0.5\n",
      "Trained batch 1549 batch loss 0.496308625 batch mAP 0.625427246 batch PCKh 0.3125\n",
      "Trained batch 1550 batch loss 0.473715514 batch mAP 0.656311035 batch PCKh 0.875\n",
      "Trained batch 1551 batch loss 0.572362721 batch mAP 0.554260254 batch PCKh 0.4375\n",
      "Trained batch 1552 batch loss 0.570546746 batch mAP 0.595428467 batch PCKh 0.375\n",
      "Trained batch 1553 batch loss 0.521351635 batch mAP 0.599273682 batch PCKh 0.3125\n",
      "Trained batch 1554 batch loss 0.526506782 batch mAP 0.629425049 batch PCKh 0.6875\n",
      "Trained batch 1555 batch loss 0.433545172 batch mAP 0.597412109 batch PCKh 0.125\n",
      "Trained batch 1556 batch loss 0.474044621 batch mAP 0.627929688 batch PCKh 0.5\n",
      "Trained batch 1557 batch loss 0.479857683 batch mAP 0.54586792 batch PCKh 0.4375\n",
      "Trained batch 1558 batch loss 0.499352753 batch mAP 0.639953613 batch PCKh 0.75\n",
      "Trained batch 1559 batch loss 0.498612106 batch mAP 0.668762207 batch PCKh 0.625\n",
      "Trained batch 1560 batch loss 0.480312705 batch mAP 0.582580566 batch PCKh 0.25\n",
      "Trained batch 1561 batch loss 0.452853292 batch mAP 0.57232666 batch PCKh 0.5\n",
      "Trained batch 1562 batch loss 0.509661853 batch mAP 0.621459961 batch PCKh 0.375\n",
      "Trained batch 1563 batch loss 0.564040661 batch mAP 0.626098633 batch PCKh 0.5\n",
      "Trained batch 1564 batch loss 0.465281427 batch mAP 0.669586182 batch PCKh 0.375\n",
      "Trained batch 1565 batch loss 0.483660638 batch mAP 0.663879395 batch PCKh 0.4375\n",
      "Trained batch 1566 batch loss 0.525772691 batch mAP 0.681976318 batch PCKh 0.25\n",
      "Trained batch 1567 batch loss 0.592632532 batch mAP 0.608215332 batch PCKh 0.1875\n",
      "Trained batch 1568 batch loss 0.539290905 batch mAP 0.683227539 batch PCKh 0.375\n",
      "Trained batch 1569 batch loss 0.553439438 batch mAP 0.640808105 batch PCKh 0.375\n",
      "Trained batch 1570 batch loss 0.517428458 batch mAP 0.657196045 batch PCKh 0.3125\n",
      "Trained batch 1571 batch loss 0.595850468 batch mAP 0.552612305 batch PCKh 0.25\n",
      "Trained batch 1572 batch loss 0.500222147 batch mAP 0.654571533 batch PCKh 0.375\n",
      "Trained batch 1573 batch loss 0.543188274 batch mAP 0.64352417 batch PCKh 0.25\n",
      "Trained batch 1574 batch loss 0.508609772 batch mAP 0.663421631 batch PCKh 0.875\n",
      "Trained batch 1575 batch loss 0.45917353 batch mAP 0.637237549 batch PCKh 0.5\n",
      "Trained batch 1576 batch loss 0.510111332 batch mAP 0.68560791 batch PCKh 0.3125\n",
      "Trained batch 1577 batch loss 0.605446517 batch mAP 0.659759521 batch PCKh 0.3125\n",
      "Trained batch 1578 batch loss 0.643009126 batch mAP 0.631774902 batch PCKh 0.3125\n",
      "Trained batch 1579 batch loss 0.572759628 batch mAP 0.661499 batch PCKh 0.25\n",
      "Trained batch 1580 batch loss 0.595590115 batch mAP 0.620880127 batch PCKh 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1581 batch loss 0.575886667 batch mAP 0.596130371 batch PCKh 0.625\n",
      "Trained batch 1582 batch loss 0.546762109 batch mAP 0.64440918 batch PCKh 0.4375\n",
      "Trained batch 1583 batch loss 0.526817083 batch mAP 0.589141846 batch PCKh 0.6875\n",
      "Trained batch 1584 batch loss 0.579638362 batch mAP 0.56362915 batch PCKh 0.5\n",
      "Trained batch 1585 batch loss 0.499088138 batch mAP 0.595550537 batch PCKh 0.625\n",
      "Trained batch 1586 batch loss 0.492825568 batch mAP 0.613678 batch PCKh 0.5625\n",
      "Trained batch 1587 batch loss 0.50608623 batch mAP 0.559295654 batch PCKh 0.5\n",
      "Trained batch 1588 batch loss 0.463868856 batch mAP 0.559234619 batch PCKh 0.5\n",
      "Trained batch 1589 batch loss 0.463388264 batch mAP 0.665466309 batch PCKh 0.4375\n",
      "Trained batch 1590 batch loss 0.583594084 batch mAP 0.639556885 batch PCKh 0.5625\n",
      "Trained batch 1591 batch loss 0.597175419 batch mAP 0.578277588 batch PCKh 0.5\n",
      "Trained batch 1592 batch loss 0.552214146 batch mAP 0.557159424 batch PCKh 0.5625\n",
      "Trained batch 1593 batch loss 0.555355608 batch mAP 0.553497314 batch PCKh 0.25\n",
      "Trained batch 1594 batch loss 0.536195815 batch mAP 0.58694458 batch PCKh 0.3125\n",
      "Trained batch 1595 batch loss 0.590506077 batch mAP 0.545135498 batch PCKh 0.375\n",
      "Trained batch 1596 batch loss 0.623154759 batch mAP 0.605682373 batch PCKh 0.6875\n",
      "Trained batch 1597 batch loss 0.587379336 batch mAP 0.598938 batch PCKh 0.8125\n",
      "Trained batch 1598 batch loss 0.583994687 batch mAP 0.580444336 batch PCKh 0.375\n",
      "Trained batch 1599 batch loss 0.606734812 batch mAP 0.592712402 batch PCKh 0.4375\n",
      "Trained batch 1600 batch loss 0.569058418 batch mAP 0.587524414 batch PCKh 0.8125\n",
      "Trained batch 1601 batch loss 0.669523716 batch mAP 0.574127197 batch PCKh 0.125\n",
      "Trained batch 1602 batch loss 0.611697495 batch mAP 0.596954346 batch PCKh 0.75\n",
      "Trained batch 1603 batch loss 0.510644794 batch mAP 0.604003906 batch PCKh 0.3125\n",
      "Trained batch 1604 batch loss 0.480405957 batch mAP 0.631347656 batch PCKh 0.5625\n",
      "Trained batch 1605 batch loss 0.522433639 batch mAP 0.557617188 batch PCKh 0.125\n",
      "Trained batch 1606 batch loss 0.53065896 batch mAP 0.614440918 batch PCKh 0.5625\n",
      "Trained batch 1607 batch loss 0.518339038 batch mAP 0.607116699 batch PCKh 0.3125\n",
      "Trained batch 1608 batch loss 0.607411623 batch mAP 0.545043945 batch PCKh 0.5\n",
      "Trained batch 1609 batch loss 0.544670582 batch mAP 0.550231934 batch PCKh 0.75\n",
      "Trained batch 1610 batch loss 0.496307641 batch mAP 0.579559326 batch PCKh 0.1875\n",
      "Trained batch 1611 batch loss 0.524989963 batch mAP 0.52948 batch PCKh 0.875\n",
      "Trained batch 1612 batch loss 0.546025753 batch mAP 0.607299805 batch PCKh 0.6875\n",
      "Trained batch 1613 batch loss 0.525218964 batch mAP 0.593841553 batch PCKh 0.6875\n",
      "Trained batch 1614 batch loss 0.589582384 batch mAP 0.635376 batch PCKh 0.3125\n",
      "Trained batch 1615 batch loss 0.592026234 batch mAP 0.554168701 batch PCKh 0.625\n",
      "Trained batch 1616 batch loss 0.512547195 batch mAP 0.627258301 batch PCKh 0.5625\n",
      "Trained batch 1617 batch loss 0.531504512 batch mAP 0.608154297 batch PCKh 0.1875\n",
      "Trained batch 1618 batch loss 0.552926123 batch mAP 0.585998535 batch PCKh 0.625\n",
      "Trained batch 1619 batch loss 0.531148136 batch mAP 0.610595703 batch PCKh 0.25\n",
      "Trained batch 1620 batch loss 0.534753501 batch mAP 0.555145264 batch PCKh 0.6875\n",
      "Trained batch 1621 batch loss 0.535819292 batch mAP 0.595459 batch PCKh 0.375\n",
      "Trained batch 1622 batch loss 0.493734568 batch mAP 0.5859375 batch PCKh 0.6875\n",
      "Trained batch 1623 batch loss 0.520997643 batch mAP 0.58291626 batch PCKh 0.25\n",
      "Trained batch 1624 batch loss 0.620269716 batch mAP 0.507659912 batch PCKh 0.6875\n",
      "Trained batch 1625 batch loss 0.478376091 batch mAP 0.58807373 batch PCKh 0.1875\n",
      "Trained batch 1626 batch loss 0.461298347 batch mAP 0.695587158 batch PCKh 0.375\n",
      "Trained batch 1627 batch loss 0.472212642 batch mAP 0.669281 batch PCKh 0.6875\n",
      "Trained batch 1628 batch loss 0.519472837 batch mAP 0.697601318 batch PCKh 0.4375\n",
      "Trained batch 1629 batch loss 0.537648678 batch mAP 0.663787842 batch PCKh 0.5625\n",
      "Trained batch 1630 batch loss 0.56255281 batch mAP 0.603118896 batch PCKh 0.8125\n",
      "Trained batch 1631 batch loss 0.571782351 batch mAP 0.582855225 batch PCKh 0.8125\n",
      "Trained batch 1632 batch loss 0.683516264 batch mAP 0.498809814 batch PCKh 0.1875\n",
      "Trained batch 1633 batch loss 0.602203548 batch mAP 0.552246094 batch PCKh 0.3125\n",
      "Trained batch 1634 batch loss 0.626319408 batch mAP 0.546783447 batch PCKh 0.625\n",
      "Trained batch 1635 batch loss 0.692255259 batch mAP 0.538726807 batch PCKh 0.0625\n",
      "Trained batch 1636 batch loss 0.56829083 batch mAP 0.572052 batch PCKh 0.4375\n",
      "Trained batch 1637 batch loss 0.585941911 batch mAP 0.536224365 batch PCKh 0.5625\n",
      "Trained batch 1638 batch loss 0.572759 batch mAP 0.52166748 batch PCKh 0.6875\n",
      "Trained batch 1639 batch loss 0.597810149 batch mAP 0.496154785 batch PCKh 0.1875\n",
      "Trained batch 1640 batch loss 0.51400429 batch mAP 0.463470459 batch PCKh 0.5\n",
      "Trained batch 1641 batch loss 0.443844736 batch mAP 0.510894775 batch PCKh 0.5\n",
      "Trained batch 1642 batch loss 0.471821696 batch mAP 0.509674072 batch PCKh 0.5625\n",
      "Trained batch 1643 batch loss 0.439458638 batch mAP 0.531219482 batch PCKh 0\n",
      "Trained batch 1644 batch loss 0.415797412 batch mAP 0.576446533 batch PCKh 0.5625\n",
      "Trained batch 1645 batch loss 0.388598025 batch mAP 0.565734863 batch PCKh 0\n",
      "Trained batch 1646 batch loss 0.391109526 batch mAP 0.591674805 batch PCKh 0.6875\n",
      "Trained batch 1647 batch loss 0.402538955 batch mAP 0.585418701 batch PCKh 0.75\n",
      "Trained batch 1648 batch loss 0.450045854 batch mAP 0.606994629 batch PCKh 0.6875\n",
      "Trained batch 1649 batch loss 0.478401154 batch mAP 0.599853516 batch PCKh 0.75\n",
      "Trained batch 1650 batch loss 0.485856414 batch mAP 0.657409668 batch PCKh 0.4375\n",
      "Trained batch 1651 batch loss 0.463858098 batch mAP 0.63961792 batch PCKh 0.4375\n",
      "Trained batch 1652 batch loss 0.434408367 batch mAP 0.671386719 batch PCKh 0.4375\n",
      "Trained batch 1653 batch loss 0.443864465 batch mAP 0.634887695 batch PCKh 0.5625\n",
      "Trained batch 1654 batch loss 0.443479568 batch mAP 0.667114258 batch PCKh 0.4375\n",
      "Trained batch 1655 batch loss 0.50232023 batch mAP 0.656616211 batch PCKh 0.5625\n",
      "Trained batch 1656 batch loss 0.512589455 batch mAP 0.581665039 batch PCKh 0.75\n",
      "Trained batch 1657 batch loss 0.56960094 batch mAP 0.598114 batch PCKh 0.8125\n",
      "Trained batch 1658 batch loss 0.510209084 batch mAP 0.554046631 batch PCKh 0.1875\n",
      "Trained batch 1659 batch loss 0.582928538 batch mAP 0.570465088 batch PCKh 0.75\n",
      "Trained batch 1660 batch loss 0.556390643 batch mAP 0.564666748 batch PCKh 0.375\n",
      "Trained batch 1661 batch loss 0.524918377 batch mAP 0.538330078 batch PCKh 0.75\n",
      "Trained batch 1662 batch loss 0.51128304 batch mAP 0.567657471 batch PCKh 0.5625\n",
      "Trained batch 1663 batch loss 0.545452535 batch mAP 0.585754395 batch PCKh 0.625\n",
      "Trained batch 1664 batch loss 0.522025824 batch mAP 0.580993652 batch PCKh 0.1875\n",
      "Trained batch 1665 batch loss 0.443481088 batch mAP 0.554504395 batch PCKh 0.1875\n",
      "Trained batch 1666 batch loss 0.490769267 batch mAP 0.559295654 batch PCKh 0.25\n",
      "Trained batch 1667 batch loss 0.448192835 batch mAP 0.557830811 batch PCKh 0.3125\n",
      "Trained batch 1668 batch loss 0.496206403 batch mAP 0.569702148 batch PCKh 0.1875\n",
      "Trained batch 1669 batch loss 0.500028133 batch mAP 0.643554688 batch PCKh 0.875\n",
      "Trained batch 1670 batch loss 0.514592648 batch mAP 0.643432617 batch PCKh 0.75\n",
      "Trained batch 1671 batch loss 0.53258 batch mAP 0.665130615 batch PCKh 0.625\n",
      "Trained batch 1672 batch loss 0.508260727 batch mAP 0.640930176 batch PCKh 0.0625\n",
      "Trained batch 1673 batch loss 0.545368731 batch mAP 0.594726562 batch PCKh 0.1875\n",
      "Trained batch 1674 batch loss 0.576279342 batch mAP 0.577972412 batch PCKh 0.5\n",
      "Trained batch 1675 batch loss 0.575383306 batch mAP 0.594177246 batch PCKh 0.3125\n",
      "Trained batch 1676 batch loss 0.575437605 batch mAP 0.593658447 batch PCKh 0.5\n",
      "Trained batch 1677 batch loss 0.592296958 batch mAP 0.623901367 batch PCKh 0.4375\n",
      "Trained batch 1678 batch loss 0.547897935 batch mAP 0.606536865 batch PCKh 0.625\n",
      "Trained batch 1679 batch loss 0.483205676 batch mAP 0.625152588 batch PCKh 0.6875\n",
      "Trained batch 1680 batch loss 0.532700717 batch mAP 0.549255371 batch PCKh 0.625\n",
      "Trained batch 1681 batch loss 0.534642279 batch mAP 0.574707031 batch PCKh 0.5625\n",
      "Trained batch 1682 batch loss 0.529827237 batch mAP 0.556427 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1683 batch loss 0.498539865 batch mAP 0.629699707 batch PCKh 0.5625\n",
      "Trained batch 1684 batch loss 0.509173632 batch mAP 0.622406 batch PCKh 0.125\n",
      "Trained batch 1685 batch loss 0.511217117 batch mAP 0.585418701 batch PCKh 0.3125\n",
      "Trained batch 1686 batch loss 0.631285667 batch mAP 0.546295166 batch PCKh 0.625\n",
      "Trained batch 1687 batch loss 0.456343353 batch mAP 0.548706055 batch PCKh 0.4375\n",
      "Trained batch 1688 batch loss 0.524752498 batch mAP 0.579315186 batch PCKh 0.5\n",
      "Trained batch 1689 batch loss 0.55117631 batch mAP 0.654449463 batch PCKh 0.625\n",
      "Trained batch 1690 batch loss 0.551373 batch mAP 0.589263916 batch PCKh 0.5625\n",
      "Trained batch 1691 batch loss 0.554817 batch mAP 0.590026855 batch PCKh 0.3125\n",
      "Trained batch 1692 batch loss 0.5509094 batch mAP 0.600524902 batch PCKh 0.25\n",
      "Trained batch 1693 batch loss 0.52386 batch mAP 0.58291626 batch PCKh 0.3125\n",
      "Trained batch 1694 batch loss 0.628324032 batch mAP 0.573669434 batch PCKh 0.375\n",
      "Trained batch 1695 batch loss 0.60342443 batch mAP 0.583831787 batch PCKh 0.5625\n",
      "Trained batch 1696 batch loss 0.562569737 batch mAP 0.631652832 batch PCKh 0.375\n",
      "Trained batch 1697 batch loss 0.481348544 batch mAP 0.5965271 batch PCKh 0.375\n",
      "Trained batch 1698 batch loss 0.46542111 batch mAP 0.628601074 batch PCKh 0.25\n",
      "Trained batch 1699 batch loss 0.405018628 batch mAP 0.698150635 batch PCKh 0.5625\n",
      "Trained batch 1700 batch loss 0.434091449 batch mAP 0.689941406 batch PCKh 0.125\n",
      "Trained batch 1701 batch loss 0.542045891 batch mAP 0.6640625 batch PCKh 0.6875\n",
      "Trained batch 1702 batch loss 0.529548943 batch mAP 0.671661377 batch PCKh 0.625\n",
      "Trained batch 1703 batch loss 0.457568049 batch mAP 0.708465576 batch PCKh 0.5\n",
      "Trained batch 1704 batch loss 0.427659392 batch mAP 0.713684082 batch PCKh 0.6875\n",
      "Trained batch 1705 batch loss 0.508137 batch mAP 0.645507812 batch PCKh 0.1875\n",
      "Trained batch 1706 batch loss 0.521939278 batch mAP 0.632598877 batch PCKh 0.75\n",
      "Trained batch 1707 batch loss 0.440508068 batch mAP 0.642700195 batch PCKh 0.3125\n",
      "Trained batch 1708 batch loss 0.51140219 batch mAP 0.642791748 batch PCKh 0.5\n",
      "Trained batch 1709 batch loss 0.600429177 batch mAP 0.537902832 batch PCKh 0.3125\n",
      "Trained batch 1710 batch loss 0.605127096 batch mAP 0.622741699 batch PCKh 0.0625\n",
      "Trained batch 1711 batch loss 0.477579564 batch mAP 0.575622559 batch PCKh 0.375\n",
      "Trained batch 1712 batch loss 0.412387 batch mAP 0.652771 batch PCKh 0\n",
      "Trained batch 1713 batch loss 0.56988883 batch mAP 0.662445068 batch PCKh 0.6875\n",
      "Trained batch 1714 batch loss 0.593533576 batch mAP 0.635162354 batch PCKh 0.5625\n",
      "Trained batch 1715 batch loss 0.508762121 batch mAP 0.622283936 batch PCKh 0.375\n",
      "Trained batch 1716 batch loss 0.49564296 batch mAP 0.627227783 batch PCKh 0.5\n",
      "Trained batch 1717 batch loss 0.536374927 batch mAP 0.607055664 batch PCKh 0.3125\n",
      "Trained batch 1718 batch loss 0.579286873 batch mAP 0.601532 batch PCKh 0.6875\n",
      "Trained batch 1719 batch loss 0.649441242 batch mAP 0.532867432 batch PCKh 0.1875\n",
      "Trained batch 1720 batch loss 0.781062365 batch mAP 0.514282227 batch PCKh 0.4375\n",
      "Trained batch 1721 batch loss 0.626357079 batch mAP 0.584136963 batch PCKh 0\n",
      "Trained batch 1722 batch loss 0.699125946 batch mAP 0.517456055 batch PCKh 0\n",
      "Trained batch 1723 batch loss 0.655405521 batch mAP 0.486694336 batch PCKh 0.4375\n",
      "Trained batch 1724 batch loss 0.54885304 batch mAP 0.51852417 batch PCKh 0.125\n",
      "Trained batch 1725 batch loss 0.512493432 batch mAP 0.44329834 batch PCKh 0.3125\n",
      "Trained batch 1726 batch loss 0.453212678 batch mAP 0.491638184 batch PCKh 0\n",
      "Trained batch 1727 batch loss 0.415109634 batch mAP 0.450531 batch PCKh 0.5625\n",
      "Trained batch 1728 batch loss 0.518713951 batch mAP 0.460906982 batch PCKh 0.125\n",
      "Trained batch 1729 batch loss 0.612373352 batch mAP 0.493103027 batch PCKh 0.1875\n",
      "Trained batch 1730 batch loss 0.547648072 batch mAP 0.490753174 batch PCKh 0.625\n",
      "Trained batch 1731 batch loss 0.613626182 batch mAP 0.464752197 batch PCKh 0.3125\n",
      "Trained batch 1732 batch loss 0.622921526 batch mAP 0.476257324 batch PCKh 0.5625\n",
      "Trained batch 1733 batch loss 0.586801052 batch mAP 0.457580566 batch PCKh 0.125\n",
      "Trained batch 1734 batch loss 0.565431535 batch mAP 0.520324707 batch PCKh 0.8125\n",
      "Trained batch 1735 batch loss 0.586542308 batch mAP 0.496398926 batch PCKh 0.5\n",
      "Trained batch 1736 batch loss 0.517621279 batch mAP 0.51651 batch PCKh 0.6875\n",
      "Trained batch 1737 batch loss 0.522003472 batch mAP 0.550201416 batch PCKh 0.75\n",
      "Trained batch 1738 batch loss 0.48242718 batch mAP 0.557891846 batch PCKh 0.625\n",
      "Trained batch 1739 batch loss 0.478989661 batch mAP 0.547515869 batch PCKh 0.4375\n",
      "Trained batch 1740 batch loss 0.525363684 batch mAP 0.643249512 batch PCKh 0.625\n",
      "Trained batch 1741 batch loss 0.5081141 batch mAP 0.64440918 batch PCKh 0.625\n",
      "Trained batch 1742 batch loss 0.609687924 batch mAP 0.573608398 batch PCKh 0.375\n",
      "Trained batch 1743 batch loss 0.530530334 batch mAP 0.633148193 batch PCKh 0.4375\n",
      "Trained batch 1744 batch loss 0.563935578 batch mAP 0.536132812 batch PCKh 0.4375\n",
      "Trained batch 1745 batch loss 0.545464039 batch mAP 0.648345947 batch PCKh 0\n",
      "Trained batch 1746 batch loss 0.54279387 batch mAP 0.560424805 batch PCKh 0.3125\n",
      "Trained batch 1747 batch loss 0.56111151 batch mAP 0.585296631 batch PCKh 0.3125\n",
      "Trained batch 1748 batch loss 0.552742302 batch mAP 0.579406738 batch PCKh 0.5\n",
      "Trained batch 1749 batch loss 0.614382 batch mAP 0.559967041 batch PCKh 0.5625\n",
      "Trained batch 1750 batch loss 0.62431848 batch mAP 0.577880859 batch PCKh 0.25\n",
      "Trained batch 1751 batch loss 0.6058442 batch mAP 0.537719727 batch PCKh 0.0625\n",
      "Trained batch 1752 batch loss 0.675326407 batch mAP 0.456756592 batch PCKh 0\n",
      "Trained batch 1753 batch loss 0.644033909 batch mAP 0.508667 batch PCKh 0\n",
      "Trained batch 1754 batch loss 0.67539072 batch mAP 0.495636 batch PCKh 0.1875\n",
      "Trained batch 1755 batch loss 0.651887536 batch mAP 0.469085693 batch PCKh 0.25\n",
      "Trained batch 1756 batch loss 0.588254213 batch mAP 0.457641602 batch PCKh 0.0625\n",
      "Trained batch 1757 batch loss 0.474050164 batch mAP 0.64944458 batch PCKh 0.5\n",
      "Trained batch 1758 batch loss 0.52300179 batch mAP 0.606292725 batch PCKh 0.5\n",
      "Trained batch 1759 batch loss 0.491150469 batch mAP 0.607055664 batch PCKh 0.4375\n",
      "Trained batch 1760 batch loss 0.544029772 batch mAP 0.587524414 batch PCKh 0.6875\n",
      "Trained batch 1761 batch loss 0.605391085 batch mAP 0.518981934 batch PCKh 0.25\n",
      "Trained batch 1762 batch loss 0.531647921 batch mAP 0.532043457 batch PCKh 0.5\n",
      "Trained batch 1763 batch loss 0.65497148 batch mAP 0.566101074 batch PCKh 0.25\n",
      "Trained batch 1764 batch loss 0.517238379 batch mAP 0.611663818 batch PCKh 0.4375\n",
      "Trained batch 1765 batch loss 0.53309387 batch mAP 0.631011963 batch PCKh 0.3125\n",
      "Trained batch 1766 batch loss 0.553281665 batch mAP 0.647369385 batch PCKh 0.5\n",
      "Trained batch 1767 batch loss 0.511972606 batch mAP 0.694366455 batch PCKh 0.6875\n",
      "Trained batch 1768 batch loss 0.441774368 batch mAP 0.691955566 batch PCKh 0.6875\n",
      "Trained batch 1769 batch loss 0.519210815 batch mAP 0.61328125 batch PCKh 0.625\n",
      "Trained batch 1770 batch loss 0.50232476 batch mAP 0.66583252 batch PCKh 0.5625\n",
      "Trained batch 1771 batch loss 0.529574215 batch mAP 0.606536865 batch PCKh 0.5\n",
      "Trained batch 1772 batch loss 0.479368508 batch mAP 0.645080566 batch PCKh 0.5\n",
      "Trained batch 1773 batch loss 0.49353987 batch mAP 0.686462402 batch PCKh 0.5\n",
      "Trained batch 1774 batch loss 0.421122968 batch mAP 0.714263916 batch PCKh 0.3125\n",
      "Trained batch 1775 batch loss 0.528554082 batch mAP 0.67980957 batch PCKh 0.3125\n",
      "Trained batch 1776 batch loss 0.545983 batch mAP 0.646972656 batch PCKh 0.3125\n",
      "Trained batch 1777 batch loss 0.503750205 batch mAP 0.674224854 batch PCKh 0.3125\n",
      "Trained batch 1778 batch loss 0.524806201 batch mAP 0.669769287 batch PCKh 0.5\n",
      "Trained batch 1779 batch loss 0.456040084 batch mAP 0.709411621 batch PCKh 0.25\n",
      "Trained batch 1780 batch loss 0.420184582 batch mAP 0.715942383 batch PCKh 0.5625\n",
      "Trained batch 1781 batch loss 0.405310661 batch mAP 0.67666626 batch PCKh 0.5625\n",
      "Trained batch 1782 batch loss 0.482366711 batch mAP 0.619018555 batch PCKh 0.125\n",
      "Trained batch 1783 batch loss 0.575174451 batch mAP 0.61227417 batch PCKh 0.4375\n",
      "Trained batch 1784 batch loss 0.587176681 batch mAP 0.551361084 batch PCKh 0.5625\n",
      "Trained batch 1785 batch loss 0.500674665 batch mAP 0.581268311 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1786 batch loss 0.600897789 batch mAP 0.640960693 batch PCKh 0.5\n",
      "Trained batch 1787 batch loss 0.559788883 batch mAP 0.650909424 batch PCKh 0.4375\n",
      "Trained batch 1788 batch loss 0.520891547 batch mAP 0.611755371 batch PCKh 0.5\n",
      "Trained batch 1789 batch loss 0.538042963 batch mAP 0.65927124 batch PCKh 0.6875\n",
      "Trained batch 1790 batch loss 0.581146538 batch mAP 0.608398438 batch PCKh 0.4375\n",
      "Trained batch 1791 batch loss 0.585536122 batch mAP 0.644317627 batch PCKh 0.375\n",
      "Trained batch 1792 batch loss 0.537299752 batch mAP 0.595336914 batch PCKh 0.5625\n",
      "Trained batch 1793 batch loss 0.537699223 batch mAP 0.636566162 batch PCKh 0.75\n",
      "Trained batch 1794 batch loss 0.412707686 batch mAP 0.673370361 batch PCKh 0.75\n",
      "Trained batch 1795 batch loss 0.449769497 batch mAP 0.593688965 batch PCKh 0.8125\n",
      "Trained batch 1796 batch loss 0.434273481 batch mAP 0.591583252 batch PCKh 0.875\n",
      "Trained batch 1797 batch loss 0.442805231 batch mAP 0.643798828 batch PCKh 0.75\n",
      "Trained batch 1798 batch loss 0.498641729 batch mAP 0.601776123 batch PCKh 0.5\n",
      "Trained batch 1799 batch loss 0.571792483 batch mAP 0.574554443 batch PCKh 0.75\n",
      "Trained batch 1800 batch loss 0.518072 batch mAP 0.603607178 batch PCKh 0.8125\n",
      "Trained batch 1801 batch loss 0.533387542 batch mAP 0.603179932 batch PCKh 0.625\n",
      "Trained batch 1802 batch loss 0.527961433 batch mAP 0.610229492 batch PCKh 0.5\n",
      "Trained batch 1803 batch loss 0.504218757 batch mAP 0.612304688 batch PCKh 0.625\n",
      "Trained batch 1804 batch loss 0.630796254 batch mAP 0.489746094 batch PCKh 0.625\n",
      "Trained batch 1805 batch loss 0.615075111 batch mAP 0.564239502 batch PCKh 0.5\n",
      "Trained batch 1806 batch loss 0.610058308 batch mAP 0.573761 batch PCKh 0.625\n",
      "Trained batch 1807 batch loss 0.520696759 batch mAP 0.545654297 batch PCKh 0.25\n",
      "Trained batch 1808 batch loss 0.48640728 batch mAP 0.529876709 batch PCKh 0.75\n",
      "Trained batch 1809 batch loss 0.633920312 batch mAP 0.512481689 batch PCKh 0.75\n",
      "Trained batch 1810 batch loss 0.547822416 batch mAP 0.509277344 batch PCKh 0.5625\n",
      "Trained batch 1811 batch loss 0.572179794 batch mAP 0.48916626 batch PCKh 0.5625\n",
      "Trained batch 1812 batch loss 0.562666178 batch mAP 0.451416016 batch PCKh 0.625\n",
      "Trained batch 1813 batch loss 0.641780734 batch mAP 0.435180664 batch PCKh 0.5\n",
      "Trained batch 1814 batch loss 0.609975457 batch mAP 0.503936768 batch PCKh 0.875\n",
      "Trained batch 1815 batch loss 0.610386968 batch mAP 0.496704102 batch PCKh 0.625\n",
      "Trained batch 1816 batch loss 0.549228668 batch mAP 0.510101318 batch PCKh 0.75\n",
      "Trained batch 1817 batch loss 0.590132058 batch mAP 0.523376465 batch PCKh 0.625\n",
      "Trained batch 1818 batch loss 0.611199439 batch mAP 0.526916504 batch PCKh 0.625\n",
      "Trained batch 1819 batch loss 0.668047667 batch mAP 0.565734863 batch PCKh 0.5\n",
      "Trained batch 1820 batch loss 0.554266095 batch mAP 0.553283691 batch PCKh 0.5\n",
      "Trained batch 1821 batch loss 0.557085514 batch mAP 0.561920166 batch PCKh 0.4375\n",
      "Trained batch 1822 batch loss 0.556227386 batch mAP 0.631469727 batch PCKh 0.5\n",
      "Trained batch 1823 batch loss 0.570982218 batch mAP 0.635345459 batch PCKh 0.6875\n",
      "Trained batch 1824 batch loss 0.520218134 batch mAP 0.701568604 batch PCKh 0.5\n",
      "Trained batch 1825 batch loss 0.535758 batch mAP 0.652252197 batch PCKh 0.375\n",
      "Trained batch 1826 batch loss 0.51976037 batch mAP 0.585113525 batch PCKh 0.25\n",
      "Trained batch 1827 batch loss 0.54623419 batch mAP 0.589386 batch PCKh 0.25\n",
      "Trained batch 1828 batch loss 0.623790085 batch mAP 0.557342529 batch PCKh 0.1875\n",
      "Trained batch 1829 batch loss 0.570170224 batch mAP 0.602478 batch PCKh 0.3125\n",
      "Trained batch 1830 batch loss 0.56678772 batch mAP 0.535064697 batch PCKh 0.625\n",
      "Trained batch 1831 batch loss 0.594318926 batch mAP 0.557189941 batch PCKh 0.75\n",
      "Trained batch 1832 batch loss 0.610363 batch mAP 0.53314209 batch PCKh 0.0625\n",
      "Trained batch 1833 batch loss 0.568623304 batch mAP 0.56539917 batch PCKh 0.625\n",
      "Trained batch 1834 batch loss 0.557041049 batch mAP 0.574707031 batch PCKh 0.5625\n",
      "Trained batch 1835 batch loss 0.519102931 batch mAP 0.652923584 batch PCKh 0.6875\n",
      "Trained batch 1836 batch loss 0.548689842 batch mAP 0.498840332 batch PCKh 0.8125\n",
      "Trained batch 1837 batch loss 0.597604275 batch mAP 0.587860107 batch PCKh 0.8125\n",
      "Trained batch 1838 batch loss 0.576663136 batch mAP 0.5652771 batch PCKh 0.75\n",
      "Trained batch 1839 batch loss 0.561889291 batch mAP 0.603729248 batch PCKh 0.5\n",
      "Trained batch 1840 batch loss 0.585579872 batch mAP 0.652526855 batch PCKh 0.75\n",
      "Trained batch 1841 batch loss 0.53048563 batch mAP 0.611358643 batch PCKh 0.5625\n",
      "Trained batch 1842 batch loss 0.575682878 batch mAP 0.546966553 batch PCKh 0.25\n",
      "Trained batch 1843 batch loss 0.634373724 batch mAP 0.522583 batch PCKh 0.1875\n",
      "Trained batch 1844 batch loss 0.588707626 batch mAP 0.578674316 batch PCKh 0.5\n",
      "Trained batch 1845 batch loss 0.596847892 batch mAP 0.561859131 batch PCKh 0.375\n",
      "Trained batch 1846 batch loss 0.519143939 batch mAP 0.642120361 batch PCKh 0.8125\n",
      "Trained batch 1847 batch loss 0.600118756 batch mAP 0.614685059 batch PCKh 0.625\n",
      "Trained batch 1848 batch loss 0.557641268 batch mAP 0.556854248 batch PCKh 0.25\n",
      "Trained batch 1849 batch loss 0.641447544 batch mAP 0.516601562 batch PCKh 0.6875\n",
      "Trained batch 1850 batch loss 0.55639863 batch mAP 0.524658203 batch PCKh 0.8125\n",
      "Trained batch 1851 batch loss 0.499644309 batch mAP 0.595733643 batch PCKh 0.625\n",
      "Trained batch 1852 batch loss 0.593429208 batch mAP 0.574523926 batch PCKh 0.8125\n",
      "Trained batch 1853 batch loss 0.444550455 batch mAP 0.602752686 batch PCKh 0.75\n",
      "Trained batch 1854 batch loss 0.499745131 batch mAP 0.54498291 batch PCKh 0.25\n",
      "Trained batch 1855 batch loss 0.554402947 batch mAP 0.486907959 batch PCKh 0.1875\n",
      "Trained batch 1856 batch loss 0.586308 batch mAP 0.519256592 batch PCKh 0.375\n",
      "Trained batch 1857 batch loss 0.550945699 batch mAP 0.516113281 batch PCKh 0.5625\n",
      "Trained batch 1858 batch loss 0.570931256 batch mAP 0.506561279 batch PCKh 0.75\n",
      "Trained batch 1859 batch loss 0.48394233 batch mAP 0.56036377 batch PCKh 0.75\n",
      "Trained batch 1860 batch loss 0.511630297 batch mAP 0.573822 batch PCKh 0.3125\n",
      "Trained batch 1861 batch loss 0.599171638 batch mAP 0.52935791 batch PCKh 0.4375\n",
      "Trained batch 1862 batch loss 0.5010584 batch mAP 0.553192139 batch PCKh 0.5625\n",
      "Trained batch 1863 batch loss 0.477362067 batch mAP 0.607940674 batch PCKh 0.6875\n",
      "Trained batch 1864 batch loss 0.438680768 batch mAP 0.614624 batch PCKh 0.75\n",
      "Trained batch 1865 batch loss 0.4840132 batch mAP 0.588043213 batch PCKh 0\n",
      "Trained batch 1866 batch loss 0.504648864 batch mAP 0.679748535 batch PCKh 0.375\n",
      "Trained batch 1867 batch loss 0.546349 batch mAP 0.633605957 batch PCKh 0.25\n",
      "Trained batch 1868 batch loss 0.558127642 batch mAP 0.620056152 batch PCKh 0.5\n",
      "Trained batch 1869 batch loss 0.509552121 batch mAP 0.653717041 batch PCKh 0.5\n",
      "Trained batch 1870 batch loss 0.522477925 batch mAP 0.590484619 batch PCKh 0.375\n",
      "Trained batch 1871 batch loss 0.547525048 batch mAP 0.555145264 batch PCKh 0.8125\n",
      "Trained batch 1872 batch loss 0.526120484 batch mAP 0.535919189 batch PCKh 0.8125\n",
      "Trained batch 1873 batch loss 0.544797421 batch mAP 0.611694336 batch PCKh 0.875\n",
      "Trained batch 1874 batch loss 0.624823928 batch mAP 0.622619629 batch PCKh 0.5625\n",
      "Trained batch 1875 batch loss 0.650261343 batch mAP 0.590332031 batch PCKh 0.0625\n",
      "Trained batch 1876 batch loss 0.679275632 batch mAP 0.51965332 batch PCKh 0.25\n",
      "Trained batch 1877 batch loss 0.642420411 batch mAP 0.482330322 batch PCKh 0.1875\n",
      "Trained batch 1878 batch loss 0.621218741 batch mAP 0.531982422 batch PCKh 0.5\n",
      "Trained batch 1879 batch loss 0.580340743 batch mAP 0.531768799 batch PCKh 0.625\n",
      "Trained batch 1880 batch loss 0.513644695 batch mAP 0.524688721 batch PCKh 0.5\n",
      "Trained batch 1881 batch loss 0.529571891 batch mAP 0.56451416 batch PCKh 0.75\n",
      "Trained batch 1882 batch loss 0.482348442 batch mAP 0.620910645 batch PCKh 0.5\n",
      "Trained batch 1883 batch loss 0.454968542 batch mAP 0.632171631 batch PCKh 0.75\n",
      "Trained batch 1884 batch loss 0.354441673 batch mAP 0.574676514 batch PCKh 0\n",
      "Trained batch 1885 batch loss 0.357275903 batch mAP 0.639648438 batch PCKh 0.25\n",
      "Trained batch 1886 batch loss 0.407763243 batch mAP 0.644897461 batch PCKh 0.5\n",
      "Trained batch 1887 batch loss 0.470984906 batch mAP 0.600616455 batch PCKh 0.1875\n",
      "Trained batch 1888 batch loss 0.538307428 batch mAP 0.627868652 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1889 batch loss 0.570952892 batch mAP 0.553192139 batch PCKh 0.625\n",
      "Trained batch 1890 batch loss 0.337562382 batch mAP 0.56060791 batch PCKh 0\n",
      "Trained batch 1891 batch loss 0.411179096 batch mAP 0.570495605 batch PCKh 0.0625\n",
      "Trained batch 1892 batch loss 0.34490034 batch mAP 0.623413086 batch PCKh 0.3125\n",
      "Trained batch 1893 batch loss 0.36974445 batch mAP 0.609802246 batch PCKh 0\n",
      "Trained batch 1894 batch loss 0.404464841 batch mAP 0.571105957 batch PCKh 0\n",
      "Trained batch 1895 batch loss 0.406394869 batch mAP 0.564971924 batch PCKh 0.75\n",
      "Trained batch 1896 batch loss 0.418127924 batch mAP 0.55355835 batch PCKh 0.75\n",
      "Trained batch 1897 batch loss 0.471991152 batch mAP 0.581481934 batch PCKh 0.5\n",
      "Trained batch 1898 batch loss 0.457835972 batch mAP 0.545318604 batch PCKh 0.6875\n",
      "Trained batch 1899 batch loss 0.583888412 batch mAP 0.528259277 batch PCKh 0.8125\n",
      "Trained batch 1900 batch loss 0.416598141 batch mAP 0.584533691 batch PCKh 0.4375\n",
      "Trained batch 1901 batch loss 0.446487308 batch mAP 0.555114746 batch PCKh 0.4375\n",
      "Trained batch 1902 batch loss 0.539672 batch mAP 0.607269287 batch PCKh 0.375\n",
      "Trained batch 1903 batch loss 0.516860247 batch mAP 0.610778809 batch PCKh 0.625\n",
      "Trained batch 1904 batch loss 0.511915684 batch mAP 0.601959229 batch PCKh 0.25\n",
      "Trained batch 1905 batch loss 0.488995969 batch mAP 0.612884521 batch PCKh 0.8125\n",
      "Trained batch 1906 batch loss 0.492356241 batch mAP 0.669281 batch PCKh 0.875\n",
      "Trained batch 1907 batch loss 0.554595947 batch mAP 0.599517822 batch PCKh 0.4375\n",
      "Trained batch 1908 batch loss 0.562255621 batch mAP 0.625824 batch PCKh 0.3125\n",
      "Trained batch 1909 batch loss 0.603257596 batch mAP 0.609466553 batch PCKh 0.125\n",
      "Trained batch 1910 batch loss 0.614639938 batch mAP 0.469940186 batch PCKh 0.25\n",
      "Trained batch 1911 batch loss 0.518128 batch mAP 0.547851562 batch PCKh 0.3125\n",
      "Trained batch 1912 batch loss 0.453965127 batch mAP 0.613922119 batch PCKh 0.5\n",
      "Trained batch 1913 batch loss 0.519973159 batch mAP 0.588531494 batch PCKh 0.5\n",
      "Trained batch 1914 batch loss 0.504442096 batch mAP 0.577941895 batch PCKh 0.4375\n",
      "Trained batch 1915 batch loss 0.601682365 batch mAP 0.519775391 batch PCKh 0.625\n",
      "Trained batch 1916 batch loss 0.55660224 batch mAP 0.557525635 batch PCKh 0.375\n",
      "Trained batch 1917 batch loss 0.484729707 batch mAP 0.592865 batch PCKh 0.375\n",
      "Trained batch 1918 batch loss 0.503683746 batch mAP 0.520904541 batch PCKh 0.1875\n",
      "Trained batch 1919 batch loss 0.493802577 batch mAP 0.568786621 batch PCKh 0.5\n",
      "Trained batch 1920 batch loss 0.480834663 batch mAP 0.656677246 batch PCKh 0.4375\n",
      "Trained batch 1921 batch loss 0.562072814 batch mAP 0.703582764 batch PCKh 0.75\n",
      "Trained batch 1922 batch loss 0.508569241 batch mAP 0.653106689 batch PCKh 0.3125\n",
      "Trained batch 1923 batch loss 0.495119482 batch mAP 0.634277344 batch PCKh 0.5625\n",
      "Trained batch 1924 batch loss 0.44923231 batch mAP 0.682678223 batch PCKh 0.6875\n",
      "Trained batch 1925 batch loss 0.510382175 batch mAP 0.652374268 batch PCKh 0.375\n",
      "Trained batch 1926 batch loss 0.461390316 batch mAP 0.645843506 batch PCKh 0.5625\n",
      "Trained batch 1927 batch loss 0.405596644 batch mAP 0.659088135 batch PCKh 0.4375\n",
      "Trained batch 1928 batch loss 0.45376271 batch mAP 0.621826172 batch PCKh 0.625\n",
      "Trained batch 1929 batch loss 0.415140033 batch mAP 0.651580811 batch PCKh 0.5\n",
      "Trained batch 1930 batch loss 0.487906218 batch mAP 0.592651367 batch PCKh 0.875\n",
      "Trained batch 1931 batch loss 0.374390185 batch mAP 0.653564453 batch PCKh 0.375\n",
      "Trained batch 1932 batch loss 0.5052706 batch mAP 0.621459961 batch PCKh 0.6875\n",
      "Trained batch 1933 batch loss 0.556865811 batch mAP 0.550506592 batch PCKh 0.75\n",
      "Trained batch 1934 batch loss 0.439754128 batch mAP 0.619476318 batch PCKh 0.1875\n",
      "Trained batch 1935 batch loss 0.438225627 batch mAP 0.551025391 batch PCKh 0.625\n",
      "Trained batch 1936 batch loss 0.439092487 batch mAP 0.637786865 batch PCKh 0.5625\n",
      "Trained batch 1937 batch loss 0.454833448 batch mAP 0.613586426 batch PCKh 0.6875\n",
      "Trained batch 1938 batch loss 0.498390794 batch mAP 0.586517334 batch PCKh 0.6875\n",
      "Trained batch 1939 batch loss 0.483038515 batch mAP 0.620544434 batch PCKh 0.6875\n",
      "Trained batch 1940 batch loss 0.476352036 batch mAP 0.617462158 batch PCKh 0.6875\n",
      "Trained batch 1941 batch loss 0.475705802 batch mAP 0.591705322 batch PCKh 0.5625\n",
      "Trained batch 1942 batch loss 0.448114246 batch mAP 0.681335449 batch PCKh 0.1875\n",
      "Trained batch 1943 batch loss 0.56959933 batch mAP 0.60849 batch PCKh 0.0625\n",
      "Trained batch 1944 batch loss 0.587959886 batch mAP 0.557739258 batch PCKh 0.3125\n",
      "Trained batch 1945 batch loss 0.548780203 batch mAP 0.595214844 batch PCKh 0.5\n",
      "Trained batch 1946 batch loss 0.545092225 batch mAP 0.638946533 batch PCKh 0.5625\n",
      "Trained batch 1947 batch loss 0.532578707 batch mAP 0.632843 batch PCKh 0.1875\n",
      "Trained batch 1948 batch loss 0.567246318 batch mAP 0.676422119 batch PCKh 0.375\n",
      "Trained batch 1949 batch loss 0.570398033 batch mAP 0.690948486 batch PCKh 0.3125\n",
      "Trained batch 1950 batch loss 0.61258018 batch mAP 0.66104126 batch PCKh 0.3125\n",
      "Trained batch 1951 batch loss 0.520048916 batch mAP 0.630523682 batch PCKh 0.5\n",
      "Trained batch 1952 batch loss 0.537919879 batch mAP 0.572570801 batch PCKh 0.3125\n",
      "Trained batch 1953 batch loss 0.553766191 batch mAP 0.585662842 batch PCKh 0.4375\n",
      "Trained batch 1954 batch loss 0.520159662 batch mAP 0.573699951 batch PCKh 0.6875\n",
      "Trained batch 1955 batch loss 0.579781651 batch mAP 0.575592041 batch PCKh 0.5\n",
      "Trained batch 1956 batch loss 0.598710299 batch mAP 0.562652588 batch PCKh 0.3125\n",
      "Trained batch 1957 batch loss 0.507612705 batch mAP 0.586303711 batch PCKh 0.5\n",
      "Trained batch 1958 batch loss 0.491436183 batch mAP 0.636688232 batch PCKh 0.3125\n",
      "Trained batch 1959 batch loss 0.458156526 batch mAP 0.675354 batch PCKh 0.3125\n",
      "Trained batch 1960 batch loss 0.469962388 batch mAP 0.628509521 batch PCKh 0.4375\n",
      "Trained batch 1961 batch loss 0.423296511 batch mAP 0.66607666 batch PCKh 0.3125\n",
      "Trained batch 1962 batch loss 0.487945318 batch mAP 0.647399902 batch PCKh 0.3125\n",
      "Trained batch 1963 batch loss 0.463179916 batch mAP 0.684997559 batch PCKh 0.25\n",
      "Trained batch 1964 batch loss 0.501291931 batch mAP 0.661865234 batch PCKh 0.4375\n",
      "Trained batch 1965 batch loss 0.454414845 batch mAP 0.670562744 batch PCKh 0.625\n",
      "Trained batch 1966 batch loss 0.485711873 batch mAP 0.638641357 batch PCKh 0.625\n",
      "Trained batch 1967 batch loss 0.430596 batch mAP 0.690185547 batch PCKh 0.8125\n",
      "Trained batch 1968 batch loss 0.488158166 batch mAP 0.661804199 batch PCKh 0.5625\n",
      "Trained batch 1969 batch loss 0.54218936 batch mAP 0.639953613 batch PCKh 0.8125\n",
      "Trained batch 1970 batch loss 0.467171758 batch mAP 0.616699219 batch PCKh 0.25\n",
      "Trained batch 1971 batch loss 0.53648752 batch mAP 0.579284668 batch PCKh 0.1875\n",
      "Trained batch 1972 batch loss 0.480149865 batch mAP 0.6328125 batch PCKh 0.4375\n",
      "Trained batch 1973 batch loss 0.546953619 batch mAP 0.616851807 batch PCKh 0.625\n",
      "Trained batch 1974 batch loss 0.493803471 batch mAP 0.660217285 batch PCKh 0.25\n",
      "Trained batch 1975 batch loss 0.451502085 batch mAP 0.662261963 batch PCKh 0.25\n",
      "Trained batch 1976 batch loss 0.495739579 batch mAP 0.53793335 batch PCKh 0.375\n",
      "Trained batch 1977 batch loss 0.556107759 batch mAP 0.597045898 batch PCKh 0.5625\n",
      "Trained batch 1978 batch loss 0.514006257 batch mAP 0.611877441 batch PCKh 0.3125\n",
      "Trained batch 1979 batch loss 0.576596737 batch mAP 0.592437744 batch PCKh 0.1875\n",
      "Trained batch 1980 batch loss 0.542004466 batch mAP 0.654754639 batch PCKh 0.375\n",
      "Trained batch 1981 batch loss 0.56174618 batch mAP 0.59967041 batch PCKh 0.75\n",
      "Trained batch 1982 batch loss 0.522583365 batch mAP 0.638183594 batch PCKh 0.3125\n",
      "Trained batch 1983 batch loss 0.560681164 batch mAP 0.601470947 batch PCKh 0.3125\n",
      "Trained batch 1984 batch loss 0.568351209 batch mAP 0.606536865 batch PCKh 0.25\n",
      "Trained batch 1985 batch loss 0.62062192 batch mAP 0.567077637 batch PCKh 0.4375\n",
      "Trained batch 1986 batch loss 0.531803727 batch mAP 0.579742432 batch PCKh 0.3125\n",
      "Trained batch 1987 batch loss 0.457068861 batch mAP 0.642181396 batch PCKh 0.125\n",
      "Trained batch 1988 batch loss 0.554160476 batch mAP 0.552093506 batch PCKh 0.4375\n",
      "Trained batch 1989 batch loss 0.482915133 batch mAP 0.574646 batch PCKh 0.3125\n",
      "Trained batch 1990 batch loss 0.523098767 batch mAP 0.581970215 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1991 batch loss 0.546042681 batch mAP 0.608612061 batch PCKh 0.75\n",
      "Trained batch 1992 batch loss 0.554604471 batch mAP 0.582702637 batch PCKh 0.5625\n",
      "Trained batch 1993 batch loss 0.495979846 batch mAP 0.628265381 batch PCKh 0.75\n",
      "Trained batch 1994 batch loss 0.505781829 batch mAP 0.615509033 batch PCKh 0.6875\n",
      "Trained batch 1995 batch loss 0.500333846 batch mAP 0.621765137 batch PCKh 0.625\n",
      "Trained batch 1996 batch loss 0.524138451 batch mAP 0.548492432 batch PCKh 0.0625\n",
      "Trained batch 1997 batch loss 0.528062344 batch mAP 0.620239258 batch PCKh 0.625\n",
      "Trained batch 1998 batch loss 0.465918303 batch mAP 0.549926758 batch PCKh 0.625\n",
      "Trained batch 1999 batch loss 0.470421135 batch mAP 0.493743896 batch PCKh 0.6875\n",
      "Trained batch 2000 batch loss 0.55246073 batch mAP 0.565704346 batch PCKh 0.75\n",
      "Trained batch 2001 batch loss 0.544814408 batch mAP 0.592926 batch PCKh 0.4375\n",
      "Trained batch 2002 batch loss 0.554143667 batch mAP 0.584320068 batch PCKh 0.5\n",
      "Trained batch 2003 batch loss 0.539280534 batch mAP 0.606628418 batch PCKh 0.3125\n",
      "Trained batch 2004 batch loss 0.553867936 batch mAP 0.565734863 batch PCKh 0.25\n",
      "Trained batch 2005 batch loss 0.575831652 batch mAP 0.564880371 batch PCKh 0.3125\n",
      "Trained batch 2006 batch loss 0.606390238 batch mAP 0.517089844 batch PCKh 0.75\n",
      "Trained batch 2007 batch loss 0.587128699 batch mAP 0.495300293 batch PCKh 0.6875\n",
      "Trained batch 2008 batch loss 0.480401486 batch mAP 0.542999268 batch PCKh 0.625\n",
      "Trained batch 2009 batch loss 0.448980838 batch mAP 0.608428955 batch PCKh 0.625\n",
      "Trained batch 2010 batch loss 0.439540356 batch mAP 0.586700439 batch PCKh 0.25\n",
      "Trained batch 2011 batch loss 0.598828793 batch mAP 0.564086914 batch PCKh 0.375\n",
      "Trained batch 2012 batch loss 0.414718747 batch mAP 0.605011 batch PCKh 0.5625\n",
      "Trained batch 2013 batch loss 0.441351801 batch mAP 0.579223633 batch PCKh 0.4375\n",
      "Trained batch 2014 batch loss 0.441647589 batch mAP 0.605712891 batch PCKh 0.6875\n",
      "Trained batch 2015 batch loss 0.415355921 batch mAP 0.594421387 batch PCKh 0.75\n",
      "Trained batch 2016 batch loss 0.392364264 batch mAP 0.599609375 batch PCKh 0.75\n",
      "Trained batch 2017 batch loss 0.506701648 batch mAP 0.602600098 batch PCKh 0.5625\n",
      "Trained batch 2018 batch loss 0.494306296 batch mAP 0.573791504 batch PCKh 0.8125\n",
      "Trained batch 2019 batch loss 0.523563802 batch mAP 0.537109375 batch PCKh 0.75\n",
      "Trained batch 2020 batch loss 0.48636359 batch mAP 0.536651611 batch PCKh 0.5625\n",
      "Trained batch 2021 batch loss 0.546880901 batch mAP 0.622772217 batch PCKh 0.6875\n",
      "Trained batch 2022 batch loss 0.52999723 batch mAP 0.625762939 batch PCKh 0.75\n",
      "Trained batch 2023 batch loss 0.484639168 batch mAP 0.615112305 batch PCKh 0.5625\n",
      "Trained batch 2024 batch loss 0.445330024 batch mAP 0.640808105 batch PCKh 0.3125\n",
      "Trained batch 2025 batch loss 0.487075865 batch mAP 0.642181396 batch PCKh 0.6875\n",
      "Trained batch 2026 batch loss 0.565654695 batch mAP 0.619567871 batch PCKh 0.75\n",
      "Trained batch 2027 batch loss 0.551618457 batch mAP 0.598907471 batch PCKh 0.6875\n",
      "Trained batch 2028 batch loss 0.445665061 batch mAP 0.650146484 batch PCKh 0.75\n",
      "Trained batch 2029 batch loss 0.427304924 batch mAP 0.69543457 batch PCKh 0.4375\n",
      "Trained batch 2030 batch loss 0.526349783 batch mAP 0.592651367 batch PCKh 0.625\n",
      "Trained batch 2031 batch loss 0.531580567 batch mAP 0.632904053 batch PCKh 0.375\n",
      "Trained batch 2032 batch loss 0.589272141 batch mAP 0.653381348 batch PCKh 0.6875\n",
      "Trained batch 2033 batch loss 0.562643945 batch mAP 0.61315918 batch PCKh 0.1875\n",
      "Trained batch 2034 batch loss 0.573488176 batch mAP 0.560668945 batch PCKh 0.6875\n",
      "Trained batch 2035 batch loss 0.588180661 batch mAP 0.616729736 batch PCKh 0.25\n",
      "Trained batch 2036 batch loss 0.543579519 batch mAP 0.642669678 batch PCKh 0.4375\n",
      "Trained batch 2037 batch loss 0.555557609 batch mAP 0.642120361 batch PCKh 0.75\n",
      "Trained batch 2038 batch loss 0.554075599 batch mAP 0.519989 batch PCKh 0.5625\n",
      "Trained batch 2039 batch loss 0.445946813 batch mAP 0.592254639 batch PCKh 0.25\n",
      "Trained batch 2040 batch loss 0.595104754 batch mAP 0.55770874 batch PCKh 0.25\n",
      "Trained batch 2041 batch loss 0.555828393 batch mAP 0.55871582 batch PCKh 0.25\n",
      "Trained batch 2042 batch loss 0.548668087 batch mAP 0.57131958 batch PCKh 0.5\n",
      "Trained batch 2043 batch loss 0.610861659 batch mAP 0.567443848 batch PCKh 0.75\n",
      "Trained batch 2044 batch loss 0.544323325 batch mAP 0.641143799 batch PCKh 0.25\n",
      "Trained batch 2045 batch loss 0.595865071 batch mAP 0.617858887 batch PCKh 0.625\n",
      "Trained batch 2046 batch loss 0.544146657 batch mAP 0.618591309 batch PCKh 0.8125\n",
      "Trained batch 2047 batch loss 0.569510937 batch mAP 0.676208496 batch PCKh 0.75\n",
      "Trained batch 2048 batch loss 0.579041123 batch mAP 0.664978 batch PCKh 0.6875\n",
      "Trained batch 2049 batch loss 0.587354124 batch mAP 0.632171631 batch PCKh 0.5625\n",
      "Trained batch 2050 batch loss 0.523138702 batch mAP 0.65737915 batch PCKh 0.5625\n",
      "Trained batch 2051 batch loss 0.505293608 batch mAP 0.64831543 batch PCKh 0.75\n",
      "Trained batch 2052 batch loss 0.589228034 batch mAP 0.647186279 batch PCKh 0.5\n",
      "Trained batch 2053 batch loss 0.533198714 batch mAP 0.598114 batch PCKh 0.5\n",
      "Trained batch 2054 batch loss 0.502814233 batch mAP 0.612884521 batch PCKh 0.5625\n",
      "Trained batch 2055 batch loss 0.568382 batch mAP 0.592041 batch PCKh 0.0625\n",
      "Trained batch 2056 batch loss 0.640365481 batch mAP 0.56741333 batch PCKh 0.4375\n",
      "Trained batch 2057 batch loss 0.636380434 batch mAP 0.524047852 batch PCKh 0.25\n",
      "Trained batch 2058 batch loss 0.683082 batch mAP 0.542358398 batch PCKh 0.5\n",
      "Trained batch 2059 batch loss 0.537483394 batch mAP 0.599029541 batch PCKh 0.3125\n",
      "Trained batch 2060 batch loss 0.634918094 batch mAP 0.607910156 batch PCKh 0.6875\n",
      "Trained batch 2061 batch loss 0.416086555 batch mAP 0.661132812 batch PCKh 0.5625\n",
      "Trained batch 2062 batch loss 0.566200197 batch mAP 0.538543701 batch PCKh 0.6875\n",
      "Trained batch 2063 batch loss 0.585874856 batch mAP 0.533782959 batch PCKh 0.25\n",
      "Trained batch 2064 batch loss 0.570014536 batch mAP 0.59185791 batch PCKh 0.5\n",
      "Trained batch 2065 batch loss 0.536815107 batch mAP 0.593231201 batch PCKh 0.3125\n",
      "Trained batch 2066 batch loss 0.518901229 batch mAP 0.623260498 batch PCKh 0.4375\n",
      "Trained batch 2067 batch loss 0.605945 batch mAP 0.630615234 batch PCKh 0.25\n",
      "Trained batch 2068 batch loss 0.602906704 batch mAP 0.63180542 batch PCKh 0.3125\n",
      "Trained batch 2069 batch loss 0.514536262 batch mAP 0.65512085 batch PCKh 0.5625\n",
      "Trained batch 2070 batch loss 0.614661276 batch mAP 0.638885498 batch PCKh 0.375\n",
      "Trained batch 2071 batch loss 0.553031504 batch mAP 0.628601074 batch PCKh 0.5625\n",
      "Trained batch 2072 batch loss 0.544694483 batch mAP 0.571777344 batch PCKh 0.4375\n",
      "Trained batch 2073 batch loss 0.473241627 batch mAP 0.591491699 batch PCKh 0.75\n",
      "Trained batch 2074 batch loss 0.63393414 batch mAP 0.542785645 batch PCKh 0.6875\n",
      "Trained batch 2075 batch loss 0.545756578 batch mAP 0.602355957 batch PCKh 0.625\n",
      "Trained batch 2076 batch loss 0.530409753 batch mAP 0.583557129 batch PCKh 0.625\n",
      "Trained batch 2077 batch loss 0.492901415 batch mAP 0.569702148 batch PCKh 0.3125\n",
      "Trained batch 2078 batch loss 0.632865965 batch mAP 0.499450684 batch PCKh 0.5\n",
      "Trained batch 2079 batch loss 0.542003632 batch mAP 0.637146 batch PCKh 0.1875\n",
      "Trained batch 2080 batch loss 0.589642048 batch mAP 0.619171143 batch PCKh 0.4375\n",
      "Trained batch 2081 batch loss 0.627269149 batch mAP 0.555114746 batch PCKh 0.75\n",
      "Trained batch 2082 batch loss 0.517827 batch mAP 0.579437256 batch PCKh 0.4375\n",
      "Trained batch 2083 batch loss 0.460519135 batch mAP 0.646453857 batch PCKh 0.75\n",
      "Trained batch 2084 batch loss 0.570450246 batch mAP 0.610015869 batch PCKh 0.3125\n",
      "Trained batch 2085 batch loss 0.570561647 batch mAP 0.606781 batch PCKh 0.75\n",
      "Trained batch 2086 batch loss 0.483992428 batch mAP 0.627166748 batch PCKh 0.75\n",
      "Trained batch 2087 batch loss 0.558261275 batch mAP 0.587890625 batch PCKh 0.8125\n",
      "Trained batch 2088 batch loss 0.619505167 batch mAP 0.613739 batch PCKh 0.8125\n",
      "Trained batch 2089 batch loss 0.551785707 batch mAP 0.595245361 batch PCKh 0.25\n",
      "Trained batch 2090 batch loss 0.537834406 batch mAP 0.575592041 batch PCKh 0.4375\n",
      "Trained batch 2091 batch loss 0.540829659 batch mAP 0.611022949 batch PCKh 0.875\n",
      "Trained batch 2092 batch loss 0.56642 batch mAP 0.574493408 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2093 batch loss 0.618286192 batch mAP 0.584564209 batch PCKh 0.6875\n",
      "Trained batch 2094 batch loss 0.534758389 batch mAP 0.581970215 batch PCKh 0.8125\n",
      "Trained batch 2095 batch loss 0.552439928 batch mAP 0.528595 batch PCKh 0.8125\n",
      "Trained batch 2096 batch loss 0.530811 batch mAP 0.619262695 batch PCKh 0.4375\n",
      "Trained batch 2097 batch loss 0.561454475 batch mAP 0.555419922 batch PCKh 0.4375\n",
      "Trained batch 2098 batch loss 0.541458905 batch mAP 0.554016113 batch PCKh 0.4375\n",
      "Trained batch 2099 batch loss 0.441098183 batch mAP 0.614624 batch PCKh 0.5625\n",
      "Trained batch 2100 batch loss 0.498938531 batch mAP 0.588989258 batch PCKh 0.625\n",
      "Trained batch 2101 batch loss 0.402147 batch mAP 0.600402832 batch PCKh 0.1875\n",
      "Trained batch 2102 batch loss 0.547772706 batch mAP 0.55267334 batch PCKh 0.75\n",
      "Trained batch 2103 batch loss 0.598394334 batch mAP 0.542236328 batch PCKh 0.6875\n",
      "Trained batch 2104 batch loss 0.569148302 batch mAP 0.575012207 batch PCKh 0.625\n",
      "Trained batch 2105 batch loss 0.587227 batch mAP 0.622833252 batch PCKh 0.75\n",
      "Trained batch 2106 batch loss 0.619733095 batch mAP 0.60067749 batch PCKh 0.8125\n",
      "Trained batch 2107 batch loss 0.592504859 batch mAP 0.508850098 batch PCKh 0.1875\n",
      "Trained batch 2108 batch loss 0.582921445 batch mAP 0.587188721 batch PCKh 0.375\n",
      "Trained batch 2109 batch loss 0.54461813 batch mAP 0.646728516 batch PCKh 0.375\n",
      "Trained batch 2110 batch loss 0.510855317 batch mAP 0.628265381 batch PCKh 0.625\n",
      "Trained batch 2111 batch loss 0.566190183 batch mAP 0.62979126 batch PCKh 0.25\n",
      "Trained batch 2112 batch loss 0.553167462 batch mAP 0.600647 batch PCKh 0.75\n",
      "Trained batch 2113 batch loss 0.554472566 batch mAP 0.623626709 batch PCKh 0.8125\n",
      "Trained batch 2114 batch loss 0.535444379 batch mAP 0.643096924 batch PCKh 0.4375\n",
      "Trained batch 2115 batch loss 0.56368345 batch mAP 0.562286377 batch PCKh 0.75\n",
      "Trained batch 2116 batch loss 0.454095 batch mAP 0.645965576 batch PCKh 0.5625\n",
      "Trained batch 2117 batch loss 0.426352322 batch mAP 0.579284668 batch PCKh 0.5625\n",
      "Trained batch 2118 batch loss 0.512306 batch mAP 0.620147705 batch PCKh 0.625\n",
      "Trained batch 2119 batch loss 0.526160419 batch mAP 0.595947266 batch PCKh 0.8125\n",
      "Trained batch 2120 batch loss 0.543751895 batch mAP 0.552581787 batch PCKh 0.6875\n",
      "Trained batch 2121 batch loss 0.637140751 batch mAP 0.566864 batch PCKh 0.25\n",
      "Trained batch 2122 batch loss 0.629635036 batch mAP 0.511230469 batch PCKh 0.375\n",
      "Trained batch 2123 batch loss 0.545006335 batch mAP 0.597869873 batch PCKh 0.375\n",
      "Trained batch 2124 batch loss 0.555205822 batch mAP 0.556762695 batch PCKh 0.5\n",
      "Trained batch 2125 batch loss 0.530811787 batch mAP 0.58895874 batch PCKh 0.5\n",
      "Trained batch 2126 batch loss 0.48846668 batch mAP 0.588470459 batch PCKh 0.125\n",
      "Trained batch 2127 batch loss 0.450727463 batch mAP 0.604888916 batch PCKh 0.6875\n",
      "Trained batch 2128 batch loss 0.418662041 batch mAP 0.632232666 batch PCKh 0.4375\n",
      "Trained batch 2129 batch loss 0.511800766 batch mAP 0.611053467 batch PCKh 0.4375\n",
      "Trained batch 2130 batch loss 0.545366883 batch mAP 0.655212402 batch PCKh 0.75\n",
      "Trained batch 2131 batch loss 0.600090921 batch mAP 0.587646484 batch PCKh 0.625\n",
      "Trained batch 2132 batch loss 0.644432604 batch mAP 0.591186523 batch PCKh 0.3125\n",
      "Trained batch 2133 batch loss 0.595227361 batch mAP 0.564117432 batch PCKh 0.8125\n",
      "Trained batch 2134 batch loss 0.525359154 batch mAP 0.633667 batch PCKh 0.25\n",
      "Trained batch 2135 batch loss 0.590002537 batch mAP 0.622802734 batch PCKh 0.8125\n",
      "Trained batch 2136 batch loss 0.609217167 batch mAP 0.615600586 batch PCKh 0.25\n",
      "Trained batch 2137 batch loss 0.502569795 batch mAP 0.624908447 batch PCKh 0.8125\n",
      "Trained batch 2138 batch loss 0.558174491 batch mAP 0.613311768 batch PCKh 0.4375\n",
      "Trained batch 2139 batch loss 0.647079229 batch mAP 0.581817627 batch PCKh 0.6875\n",
      "Trained batch 2140 batch loss 0.683295 batch mAP 0.584350586 batch PCKh 0.375\n",
      "Trained batch 2141 batch loss 0.569802284 batch mAP 0.646972656 batch PCKh 0.375\n",
      "Trained batch 2142 batch loss 0.555472 batch mAP 0.638763428 batch PCKh 0.4375\n",
      "Trained batch 2143 batch loss 0.518496871 batch mAP 0.619506836 batch PCKh 0.4375\n",
      "Trained batch 2144 batch loss 0.53958559 batch mAP 0.58694458 batch PCKh 0.75\n",
      "Trained batch 2145 batch loss 0.582996 batch mAP 0.624481201 batch PCKh 0.6875\n",
      "Trained batch 2146 batch loss 0.626368284 batch mAP 0.531799316 batch PCKh 0.5625\n",
      "Trained batch 2147 batch loss 0.607789934 batch mAP 0.529174805 batch PCKh 0.1875\n",
      "Trained batch 2148 batch loss 0.557457685 batch mAP 0.657684326 batch PCKh 0.4375\n",
      "Trained batch 2149 batch loss 0.52076757 batch mAP 0.618103 batch PCKh 0.75\n",
      "Trained batch 2150 batch loss 0.521794081 batch mAP 0.576293945 batch PCKh 0.1875\n",
      "Trained batch 2151 batch loss 0.473892629 batch mAP 0.619934082 batch PCKh 0.1875\n",
      "Trained batch 2152 batch loss 0.430676 batch mAP 0.661132812 batch PCKh 0.5625\n",
      "Trained batch 2153 batch loss 0.488797307 batch mAP 0.598999 batch PCKh 0.75\n",
      "Trained batch 2154 batch loss 0.453517348 batch mAP 0.625030518 batch PCKh 0.375\n",
      "Trained batch 2155 batch loss 0.404576629 batch mAP 0.605438232 batch PCKh 0.1875\n",
      "Trained batch 2156 batch loss 0.418746352 batch mAP 0.585876465 batch PCKh 0.0625\n",
      "Trained batch 2157 batch loss 0.473448783 batch mAP 0.630065918 batch PCKh 0.375\n",
      "Trained batch 2158 batch loss 0.478550255 batch mAP 0.604217529 batch PCKh 0.0625\n",
      "Trained batch 2159 batch loss 0.516521931 batch mAP 0.653686523 batch PCKh 0.25\n",
      "Trained batch 2160 batch loss 0.586702883 batch mAP 0.579864502 batch PCKh 0.625\n",
      "Trained batch 2161 batch loss 0.484505653 batch mAP 0.656524658 batch PCKh 0.375\n",
      "Trained batch 2162 batch loss 0.57635963 batch mAP 0.570922852 batch PCKh 0.125\n",
      "Trained batch 2163 batch loss 0.565686584 batch mAP 0.527984619 batch PCKh 0.75\n",
      "Trained batch 2164 batch loss 0.568425775 batch mAP 0.555511475 batch PCKh 0.375\n",
      "Trained batch 2165 batch loss 0.544914842 batch mAP 0.549072266 batch PCKh 0.5\n",
      "Trained batch 2166 batch loss 0.607981205 batch mAP 0.572143555 batch PCKh 0.875\n",
      "Trained batch 2167 batch loss 0.568305254 batch mAP 0.529815674 batch PCKh 0.625\n",
      "Trained batch 2168 batch loss 0.479312479 batch mAP 0.572479248 batch PCKh 0.5\n",
      "Trained batch 2169 batch loss 0.526095271 batch mAP 0.538848877 batch PCKh 0.625\n",
      "Trained batch 2170 batch loss 0.550603867 batch mAP 0.565582275 batch PCKh 0.625\n",
      "Trained batch 2171 batch loss 0.548372507 batch mAP 0.597564697 batch PCKh 0.5625\n",
      "Trained batch 2172 batch loss 0.515493214 batch mAP 0.635406494 batch PCKh 0.75\n",
      "Trained batch 2173 batch loss 0.516718149 batch mAP 0.607025146 batch PCKh 0.25\n",
      "Trained batch 2174 batch loss 0.447948903 batch mAP 0.605011 batch PCKh 0.6875\n",
      "Trained batch 2175 batch loss 0.46603319 batch mAP 0.613006592 batch PCKh 0.1875\n",
      "Trained batch 2176 batch loss 0.403367162 batch mAP 0.615997314 batch PCKh 0.5\n",
      "Trained batch 2177 batch loss 0.435407072 batch mAP 0.596008301 batch PCKh 0.75\n",
      "Trained batch 2178 batch loss 0.383562654 batch mAP 0.59085083 batch PCKh 0.625\n",
      "Trained batch 2179 batch loss 0.427586 batch mAP 0.582428 batch PCKh 0.4375\n",
      "Trained batch 2180 batch loss 0.531683564 batch mAP 0.569030762 batch PCKh 0.75\n",
      "Trained batch 2181 batch loss 0.634241164 batch mAP 0.54309082 batch PCKh 0.6875\n",
      "Trained batch 2182 batch loss 0.513682961 batch mAP 0.554351807 batch PCKh 0.625\n",
      "Trained batch 2183 batch loss 0.6249488 batch mAP 0.473602295 batch PCKh 0.4375\n",
      "Trained batch 2184 batch loss 0.515797377 batch mAP 0.541656494 batch PCKh 0.375\n",
      "Trained batch 2185 batch loss 0.631844521 batch mAP 0.509429932 batch PCKh 0.1875\n",
      "Trained batch 2186 batch loss 0.568088949 batch mAP 0.550262451 batch PCKh 0.4375\n",
      "Trained batch 2187 batch loss 0.570911527 batch mAP 0.635467529 batch PCKh 0.625\n",
      "Trained batch 2188 batch loss 0.38733539 batch mAP 0.704345703 batch PCKh 0.5\n",
      "Trained batch 2189 batch loss 0.494879127 batch mAP 0.695220947 batch PCKh 0.25\n",
      "Trained batch 2190 batch loss 0.498535097 batch mAP 0.614379883 batch PCKh 0.4375\n",
      "Trained batch 2191 batch loss 0.380300581 batch mAP 0.692077637 batch PCKh 0.4375\n",
      "Trained batch 2192 batch loss 0.622508705 batch mAP 0.528411865 batch PCKh 0.375\n",
      "Trained batch 2193 batch loss 0.504903257 batch mAP 0.612060547 batch PCKh 0.875\n",
      "Trained batch 2194 batch loss 0.574668765 batch mAP 0.592895508 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2195 batch loss 0.499530643 batch mAP 0.662780762 batch PCKh 0.875\n",
      "Trained batch 2196 batch loss 0.538919628 batch mAP 0.623352051 batch PCKh 0.625\n",
      "Trained batch 2197 batch loss 0.602129877 batch mAP 0.586120605 batch PCKh 0.625\n",
      "Trained batch 2198 batch loss 0.541204333 batch mAP 0.597595215 batch PCKh 0.75\n",
      "Trained batch 2199 batch loss 0.413205475 batch mAP 0.632354736 batch PCKh 0.5625\n",
      "Trained batch 2200 batch loss 0.541228771 batch mAP 0.623443604 batch PCKh 0.8125\n",
      "Trained batch 2201 batch loss 0.541621149 batch mAP 0.587615967 batch PCKh 0.4375\n",
      "Trained batch 2202 batch loss 0.66461581 batch mAP 0.540008545 batch PCKh 0.5625\n",
      "Trained batch 2203 batch loss 0.642571807 batch mAP 0.492126465 batch PCKh 0.875\n",
      "Trained batch 2204 batch loss 0.558190644 batch mAP 0.461364746 batch PCKh 0.875\n",
      "Trained batch 2205 batch loss 0.501657546 batch mAP 0.615844727 batch PCKh 0.75\n",
      "Trained batch 2206 batch loss 0.550899446 batch mAP 0.558380127 batch PCKh 0.875\n",
      "Trained batch 2207 batch loss 0.504649699 batch mAP 0.626922607 batch PCKh 0.875\n",
      "Trained batch 2208 batch loss 0.590919197 batch mAP 0.605377197 batch PCKh 0.4375\n",
      "Trained batch 2209 batch loss 0.427518547 batch mAP 0.592041 batch PCKh 0.625\n",
      "Trained batch 2210 batch loss 0.462167889 batch mAP 0.581604 batch PCKh 0.1875\n",
      "Trained batch 2211 batch loss 0.472186208 batch mAP 0.603088379 batch PCKh 0.6875\n",
      "Trained batch 2212 batch loss 0.488959193 batch mAP 0.64175415 batch PCKh 0.5625\n",
      "Trained batch 2213 batch loss 0.432495475 batch mAP 0.602264404 batch PCKh 0.625\n",
      "Trained batch 2214 batch loss 0.536545157 batch mAP 0.587463379 batch PCKh 0.4375\n",
      "Trained batch 2215 batch loss 0.54458189 batch mAP 0.611297607 batch PCKh 0.5\n",
      "Trained batch 2216 batch loss 0.554762125 batch mAP 0.591217041 batch PCKh 0.4375\n",
      "Trained batch 2217 batch loss 0.617937565 batch mAP 0.569091797 batch PCKh 0.0625\n",
      "Trained batch 2218 batch loss 0.604432344 batch mAP 0.576629639 batch PCKh 0.4375\n",
      "Trained batch 2219 batch loss 0.622176945 batch mAP 0.517700195 batch PCKh 0.25\n",
      "Trained batch 2220 batch loss 0.672728062 batch mAP 0.501159668 batch PCKh 0.1875\n",
      "Trained batch 2221 batch loss 0.632256866 batch mAP 0.541442871 batch PCKh 0.6875\n",
      "Trained batch 2222 batch loss 0.640598297 batch mAP 0.566497803 batch PCKh 0.3125\n",
      "Trained batch 2223 batch loss 0.577856421 batch mAP 0.582275391 batch PCKh 0.5625\n",
      "Trained batch 2224 batch loss 0.527180791 batch mAP 0.647216797 batch PCKh 0.4375\n",
      "Trained batch 2225 batch loss 0.60007894 batch mAP 0.537719727 batch PCKh 0.0625\n",
      "Trained batch 2226 batch loss 0.563796163 batch mAP 0.4793396 batch PCKh 0.3125\n",
      "Trained batch 2227 batch loss 0.543602705 batch mAP 0.578399658 batch PCKh 0.75\n",
      "Trained batch 2228 batch loss 0.571522593 batch mAP 0.572692871 batch PCKh 0.5625\n",
      "Trained batch 2229 batch loss 0.500388 batch mAP 0.638183594 batch PCKh 0.5625\n",
      "Trained batch 2230 batch loss 0.493804812 batch mAP 0.610839844 batch PCKh 0.4375\n",
      "Trained batch 2231 batch loss 0.438068569 batch mAP 0.596740723 batch PCKh 0.5625\n",
      "Trained batch 2232 batch loss 0.477608323 batch mAP 0.655395508 batch PCKh 0.5625\n",
      "Trained batch 2233 batch loss 0.521645904 batch mAP 0.579040527 batch PCKh 0.5625\n",
      "Trained batch 2234 batch loss 0.574281454 batch mAP 0.692382812 batch PCKh 0.5625\n",
      "Trained batch 2235 batch loss 0.504870176 batch mAP 0.626464844 batch PCKh 0.375\n",
      "Trained batch 2236 batch loss 0.593208373 batch mAP 0.516296387 batch PCKh 0.1875\n",
      "Trained batch 2237 batch loss 0.605777562 batch mAP 0.607269287 batch PCKh 0.5\n",
      "Trained batch 2238 batch loss 0.526543677 batch mAP 0.585510254 batch PCKh 0.6875\n",
      "Trained batch 2239 batch loss 0.576647222 batch mAP 0.517608643 batch PCKh 0.5625\n",
      "Trained batch 2240 batch loss 0.501210809 batch mAP 0.555450439 batch PCKh 0.6875\n",
      "Trained batch 2241 batch loss 0.474161685 batch mAP 0.602386475 batch PCKh 0.375\n",
      "Trained batch 2242 batch loss 0.562172532 batch mAP 0.549469 batch PCKh 0.25\n",
      "Trained batch 2243 batch loss 0.470316589 batch mAP 0.690551758 batch PCKh 0.625\n",
      "Trained batch 2244 batch loss 0.547883928 batch mAP 0.613555908 batch PCKh 0.4375\n",
      "Trained batch 2245 batch loss 0.410478771 batch mAP 0.700073242 batch PCKh 0.4375\n",
      "Trained batch 2246 batch loss 0.355543554 batch mAP 0.73449707 batch PCKh 0.3125\n",
      "Trained batch 2247 batch loss 0.418540597 batch mAP 0.682067871 batch PCKh 0.375\n",
      "Trained batch 2248 batch loss 0.495917201 batch mAP 0.637390137 batch PCKh 0.6875\n",
      "Trained batch 2249 batch loss 0.44026944 batch mAP 0.674377441 batch PCKh 0.5\n",
      "Trained batch 2250 batch loss 0.541521311 batch mAP 0.60559082 batch PCKh 0.8125\n",
      "Trained batch 2251 batch loss 0.516994178 batch mAP 0.649841309 batch PCKh 0.3125\n",
      "Trained batch 2252 batch loss 0.458732367 batch mAP 0.66204834 batch PCKh 0.75\n",
      "Trained batch 2253 batch loss 0.465728611 batch mAP 0.646209717 batch PCKh 0.3125\n",
      "Trained batch 2254 batch loss 0.511119604 batch mAP 0.673858643 batch PCKh 0.1875\n",
      "Trained batch 2255 batch loss 0.579303205 batch mAP 0.648529053 batch PCKh 0.375\n",
      "Trained batch 2256 batch loss 0.484394461 batch mAP 0.640472412 batch PCKh 0.3125\n",
      "Trained batch 2257 batch loss 0.525185347 batch mAP 0.661407471 batch PCKh 0.375\n",
      "Trained batch 2258 batch loss 0.541276813 batch mAP 0.648925781 batch PCKh 0.5\n",
      "Trained batch 2259 batch loss 0.551695585 batch mAP 0.670349121 batch PCKh 0.75\n",
      "Trained batch 2260 batch loss 0.594371378 batch mAP 0.606445312 batch PCKh 0.875\n",
      "Trained batch 2261 batch loss 0.601019859 batch mAP 0.533630371 batch PCKh 0.375\n",
      "Trained batch 2262 batch loss 0.599810064 batch mAP 0.607513428 batch PCKh 0.875\n",
      "Trained batch 2263 batch loss 0.605302691 batch mAP 0.55581665 batch PCKh 0.3125\n",
      "Trained batch 2264 batch loss 0.531868339 batch mAP 0.573303223 batch PCKh 0.875\n",
      "Trained batch 2265 batch loss 0.537438691 batch mAP 0.582336426 batch PCKh 0.625\n",
      "Trained batch 2266 batch loss 0.5381037 batch mAP 0.539611816 batch PCKh 0.5625\n",
      "Trained batch 2267 batch loss 0.644176364 batch mAP 0.53604126 batch PCKh 0.5\n",
      "Trained batch 2268 batch loss 0.601697087 batch mAP 0.547180176 batch PCKh 0.5625\n",
      "Trained batch 2269 batch loss 0.620838284 batch mAP 0.545928955 batch PCKh 0.625\n",
      "Trained batch 2270 batch loss 0.547522485 batch mAP 0.542877197 batch PCKh 0.75\n",
      "Trained batch 2271 batch loss 0.51212132 batch mAP 0.561584473 batch PCKh 0.6875\n",
      "Trained batch 2272 batch loss 0.567457736 batch mAP 0.496124268 batch PCKh 0.75\n",
      "Trained batch 2273 batch loss 0.580964088 batch mAP 0.483551025 batch PCKh 0.75\n",
      "Trained batch 2274 batch loss 0.572999597 batch mAP 0.490753174 batch PCKh 0.25\n",
      "Trained batch 2275 batch loss 0.590399861 batch mAP 0.54119873 batch PCKh 0.6875\n",
      "Trained batch 2276 batch loss 0.511769652 batch mAP 0.526306152 batch PCKh 0.0625\n",
      "Trained batch 2277 batch loss 0.525915 batch mAP 0.632263184 batch PCKh 0.4375\n",
      "Trained batch 2278 batch loss 0.536091506 batch mAP 0.579406738 batch PCKh 0.5\n",
      "Trained batch 2279 batch loss 0.582801461 batch mAP 0.558105469 batch PCKh 0.75\n",
      "Trained batch 2280 batch loss 0.632687926 batch mAP 0.595306396 batch PCKh 0\n",
      "Trained batch 2281 batch loss 0.591514051 batch mAP 0.599609375 batch PCKh 0.625\n",
      "Trained batch 2282 batch loss 0.641900182 batch mAP 0.590423584 batch PCKh 0.25\n",
      "Trained batch 2283 batch loss 0.516318619 batch mAP 0.618835449 batch PCKh 0.5625\n",
      "Trained batch 2284 batch loss 0.611395359 batch mAP 0.624969482 batch PCKh 0.875\n",
      "Trained batch 2285 batch loss 0.583652377 batch mAP 0.677429199 batch PCKh 0.625\n",
      "Trained batch 2286 batch loss 0.558909178 batch mAP 0.669799805 batch PCKh 0.25\n",
      "Trained batch 2287 batch loss 0.431688517 batch mAP 0.678161621 batch PCKh 0.1875\n",
      "Trained batch 2288 batch loss 0.436974794 batch mAP 0.714477539 batch PCKh 0.5\n",
      "Trained batch 2289 batch loss 0.393569291 batch mAP 0.72454834 batch PCKh 0.25\n",
      "Trained batch 2290 batch loss 0.449694872 batch mAP 0.724731445 batch PCKh 0.4375\n",
      "Trained batch 2291 batch loss 0.524220347 batch mAP 0.661651611 batch PCKh 0.3125\n",
      "Trained batch 2292 batch loss 0.510723114 batch mAP 0.669189453 batch PCKh 0.4375\n",
      "Trained batch 2293 batch loss 0.520623386 batch mAP 0.646698 batch PCKh 0.375\n",
      "Trained batch 2294 batch loss 0.528433323 batch mAP 0.592376709 batch PCKh 0.75\n",
      "Trained batch 2295 batch loss 0.547833443 batch mAP 0.573761 batch PCKh 0\n",
      "Trained batch 2296 batch loss 0.594016552 batch mAP 0.52746582 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2297 batch loss 0.548966169 batch mAP 0.523742676 batch PCKh 0.125\n",
      "Trained batch 2298 batch loss 0.587369859 batch mAP 0.550628662 batch PCKh 0.5\n",
      "Trained batch 2299 batch loss 0.694283128 batch mAP 0.548126221 batch PCKh 0.375\n",
      "Trained batch 2300 batch loss 0.608630538 batch mAP 0.555847168 batch PCKh 0.125\n",
      "Trained batch 2301 batch loss 0.57680583 batch mAP 0.611328125 batch PCKh 0.5\n",
      "Trained batch 2302 batch loss 0.629599214 batch mAP 0.607452393 batch PCKh 0.4375\n",
      "Trained batch 2303 batch loss 0.608398139 batch mAP 0.582275391 batch PCKh 0.375\n",
      "Trained batch 2304 batch loss 0.539767146 batch mAP 0.622558594 batch PCKh 0.5\n",
      "Trained batch 2305 batch loss 0.608793259 batch mAP 0.61819458 batch PCKh 0.75\n",
      "Trained batch 2306 batch loss 0.599986076 batch mAP 0.57611084 batch PCKh 0.625\n",
      "Trained batch 2307 batch loss 0.518205941 batch mAP 0.644775391 batch PCKh 0.1875\n",
      "Trained batch 2308 batch loss 0.483950853 batch mAP 0.566314697 batch PCKh 0\n",
      "Trained batch 2309 batch loss 0.6045295 batch mAP 0.622345 batch PCKh 0.4375\n",
      "Trained batch 2310 batch loss 0.612387 batch mAP 0.520721436 batch PCKh 0.75\n",
      "Trained batch 2311 batch loss 0.549239099 batch mAP 0.545776367 batch PCKh 0.75\n",
      "Trained batch 2312 batch loss 0.541205108 batch mAP 0.624694824 batch PCKh 0.4375\n",
      "Trained batch 2313 batch loss 0.507949352 batch mAP 0.694763184 batch PCKh 0.375\n",
      "Trained batch 2314 batch loss 0.552604675 batch mAP 0.644073486 batch PCKh 0.625\n",
      "Trained batch 2315 batch loss 0.529275656 batch mAP 0.62020874 batch PCKh 0.5625\n",
      "Trained batch 2316 batch loss 0.557716608 batch mAP 0.625366211 batch PCKh 0.75\n",
      "Trained batch 2317 batch loss 0.548935235 batch mAP 0.605926514 batch PCKh 0.375\n",
      "Trained batch 2318 batch loss 0.657948196 batch mAP 0.57131958 batch PCKh 0.75\n",
      "Trained batch 2319 batch loss 0.544532418 batch mAP 0.55291748 batch PCKh 0.375\n",
      "Trained batch 2320 batch loss 0.512207091 batch mAP 0.574737549 batch PCKh 0.3125\n",
      "Trained batch 2321 batch loss 0.570406616 batch mAP 0.536315918 batch PCKh 0.375\n",
      "Trained batch 2322 batch loss 0.498977 batch mAP 0.574584961 batch PCKh 0.75\n",
      "Trained batch 2323 batch loss 0.507920802 batch mAP 0.614654541 batch PCKh 0.5\n",
      "Trained batch 2324 batch loss 0.568151712 batch mAP 0.635162354 batch PCKh 0.125\n",
      "Trained batch 2325 batch loss 0.536833048 batch mAP 0.618713379 batch PCKh 0.375\n",
      "Trained batch 2326 batch loss 0.584159493 batch mAP 0.613464355 batch PCKh 0.375\n",
      "Trained batch 2327 batch loss 0.475605249 batch mAP 0.678192139 batch PCKh 0.625\n",
      "Trained batch 2328 batch loss 0.455633074 batch mAP 0.692108154 batch PCKh 0.4375\n",
      "Trained batch 2329 batch loss 0.481761426 batch mAP 0.666229248 batch PCKh 0.5\n",
      "Trained batch 2330 batch loss 0.517556906 batch mAP 0.668701172 batch PCKh 0.5625\n",
      "Trained batch 2331 batch loss 0.567933798 batch mAP 0.607910156 batch PCKh 0.625\n",
      "Trained batch 2332 batch loss 0.514772654 batch mAP 0.675933838 batch PCKh 0.75\n",
      "Trained batch 2333 batch loss 0.50774163 batch mAP 0.683441162 batch PCKh 0.6875\n",
      "Trained batch 2334 batch loss 0.482934177 batch mAP 0.704101562 batch PCKh 0.4375\n",
      "Trained batch 2335 batch loss 0.422392607 batch mAP 0.731781 batch PCKh 0.5\n",
      "Trained batch 2336 batch loss 0.491646945 batch mAP 0.707214355 batch PCKh 0.4375\n",
      "Trained batch 2337 batch loss 0.465102673 batch mAP 0.716796875 batch PCKh 0.4375\n",
      "Trained batch 2338 batch loss 0.440632224 batch mAP 0.723175049 batch PCKh 0.625\n",
      "Trained batch 2339 batch loss 0.546001613 batch mAP 0.664215088 batch PCKh 0.625\n",
      "Trained batch 2340 batch loss 0.464062661 batch mAP 0.692169189 batch PCKh 0.875\n",
      "Trained batch 2341 batch loss 0.558398247 batch mAP 0.605438232 batch PCKh 0.8125\n",
      "Trained batch 2342 batch loss 0.594451308 batch mAP 0.549316406 batch PCKh 0.1875\n",
      "Trained batch 2343 batch loss 0.482121021 batch mAP 0.549865723 batch PCKh 0\n",
      "Trained batch 2344 batch loss 0.488428205 batch mAP 0.574279785 batch PCKh 0.1875\n",
      "Trained batch 2345 batch loss 0.48945564 batch mAP 0.587982178 batch PCKh 0.1875\n",
      "Trained batch 2346 batch loss 0.606802762 batch mAP 0.506835938 batch PCKh 0.25\n",
      "Trained batch 2347 batch loss 0.635971785 batch mAP 0.467437744 batch PCKh 0\n",
      "Trained batch 2348 batch loss 0.735989332 batch mAP 0.472930908 batch PCKh 0.5\n",
      "Trained batch 2349 batch loss 0.570922 batch mAP 0.595031738 batch PCKh 0.375\n",
      "Trained batch 2350 batch loss 0.461065978 batch mAP 0.677276611 batch PCKh 0.25\n",
      "Trained batch 2351 batch loss 0.534013748 batch mAP 0.649719238 batch PCKh 0\n",
      "Trained batch 2352 batch loss 0.447544307 batch mAP 0.640472412 batch PCKh 0.4375\n",
      "Trained batch 2353 batch loss 0.414426565 batch mAP 0.706390381 batch PCKh 0.375\n",
      "Trained batch 2354 batch loss 0.357366353 batch mAP 0.680938721 batch PCKh 0.5\n",
      "Trained batch 2355 batch loss 0.494966328 batch mAP 0.598327637 batch PCKh 0.6875\n",
      "Trained batch 2356 batch loss 0.458531857 batch mAP 0.656768799 batch PCKh 0.375\n",
      "Trained batch 2357 batch loss 0.562657714 batch mAP 0.620697 batch PCKh 0.1875\n",
      "Trained batch 2358 batch loss 0.512148857 batch mAP 0.590484619 batch PCKh 0.0625\n",
      "Trained batch 2359 batch loss 0.641232133 batch mAP 0.583526611 batch PCKh 0.1875\n",
      "Trained batch 2360 batch loss 0.567883849 batch mAP 0.578063965 batch PCKh 0.8125\n",
      "Trained batch 2361 batch loss 0.547823429 batch mAP 0.618255615 batch PCKh 0.75\n",
      "Trained batch 2362 batch loss 0.582457244 batch mAP 0.56930542 batch PCKh 0.375\n",
      "Trained batch 2363 batch loss 0.566438317 batch mAP 0.596221924 batch PCKh 0.25\n",
      "Trained batch 2364 batch loss 0.540674686 batch mAP 0.622589111 batch PCKh 0.1875\n",
      "Trained batch 2365 batch loss 0.539210141 batch mAP 0.641998291 batch PCKh 0.625\n",
      "Trained batch 2366 batch loss 0.567751169 batch mAP 0.589416504 batch PCKh 0.25\n",
      "Trained batch 2367 batch loss 0.453691185 batch mAP 0.688812256 batch PCKh 0.375\n",
      "Trained batch 2368 batch loss 0.48269558 batch mAP 0.680664062 batch PCKh 0.375\n",
      "Trained batch 2369 batch loss 0.550599337 batch mAP 0.565734863 batch PCKh 0\n",
      "Trained batch 2370 batch loss 0.4807477 batch mAP 0.599884033 batch PCKh 0.375\n",
      "Trained batch 2371 batch loss 0.536924958 batch mAP 0.590698242 batch PCKh 0.75\n",
      "Trained batch 2372 batch loss 0.510727525 batch mAP 0.627258301 batch PCKh 0.375\n",
      "Trained batch 2373 batch loss 0.568111598 batch mAP 0.570953369 batch PCKh 0.3125\n",
      "Trained batch 2374 batch loss 0.668646634 batch mAP 0.584228516 batch PCKh 0.125\n",
      "Trained batch 2375 batch loss 0.589695811 batch mAP 0.602508545 batch PCKh 0\n",
      "Trained batch 2376 batch loss 0.614423156 batch mAP 0.611846924 batch PCKh 0.3125\n",
      "Trained batch 2377 batch loss 0.533201694 batch mAP 0.602874756 batch PCKh 0.8125\n",
      "Trained batch 2378 batch loss 0.486130178 batch mAP 0.623321533 batch PCKh 0.875\n",
      "Trained batch 2379 batch loss 0.445488 batch mAP 0.567749 batch PCKh 0.5625\n",
      "Trained batch 2380 batch loss 0.457354188 batch mAP 0.553344727 batch PCKh 0.625\n",
      "Trained batch 2381 batch loss 0.480629116 batch mAP 0.545074463 batch PCKh 0.375\n",
      "Trained batch 2382 batch loss 0.366833 batch mAP 0.629730225 batch PCKh 0.1875\n",
      "Trained batch 2383 batch loss 0.481634468 batch mAP 0.561248779 batch PCKh 0.375\n",
      "Trained batch 2384 batch loss 0.467842698 batch mAP 0.567199707 batch PCKh 0.625\n",
      "Trained batch 2385 batch loss 0.556492507 batch mAP 0.58706665 batch PCKh 0.5625\n",
      "Trained batch 2386 batch loss 0.617375255 batch mAP 0.587646484 batch PCKh 0.75\n",
      "Trained batch 2387 batch loss 0.60275054 batch mAP 0.556243896 batch PCKh 0.6875\n",
      "Trained batch 2388 batch loss 0.456113696 batch mAP 0.582305908 batch PCKh 0.25\n",
      "Trained batch 2389 batch loss 0.504322946 batch mAP 0.564666748 batch PCKh 0.1875\n",
      "Trained batch 2390 batch loss 0.487065881 batch mAP 0.528289795 batch PCKh 0.375\n",
      "Trained batch 2391 batch loss 0.605097115 batch mAP 0.509429932 batch PCKh 0\n",
      "Trained batch 2392 batch loss 0.646669209 batch mAP 0.557067871 batch PCKh 0\n",
      "Trained batch 2393 batch loss 0.570713103 batch mAP 0.647460938 batch PCKh 0.5625\n",
      "Trained batch 2394 batch loss 0.567987919 batch mAP 0.606231689 batch PCKh 0.625\n",
      "Trained batch 2395 batch loss 0.538969696 batch mAP 0.586181641 batch PCKh 0\n",
      "Trained batch 2396 batch loss 0.556453407 batch mAP 0.54699707 batch PCKh 0.125\n",
      "Trained batch 2397 batch loss 0.473955244 batch mAP 0.651428223 batch PCKh 0.5625\n",
      "Trained batch 2398 batch loss 0.502464056 batch mAP 0.642974854 batch PCKh 0.75\n",
      "Trained batch 2399 batch loss 0.500660837 batch mAP 0.652679443 batch PCKh 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2400 batch loss 0.486841381 batch mAP 0.643371582 batch PCKh 0.5625\n",
      "Trained batch 2401 batch loss 0.517062783 batch mAP 0.605987549 batch PCKh 0.75\n",
      "Trained batch 2402 batch loss 0.393819451 batch mAP 0.584747314 batch PCKh 0.3125\n",
      "Trained batch 2403 batch loss 0.446636558 batch mAP 0.597076416 batch PCKh 0.6875\n",
      "Trained batch 2404 batch loss 0.54754734 batch mAP 0.525390625 batch PCKh 0.875\n",
      "Trained batch 2405 batch loss 0.510574 batch mAP 0.577178955 batch PCKh 0.75\n",
      "Trained batch 2406 batch loss 0.51971209 batch mAP 0.63092041 batch PCKh 0.5625\n",
      "Trained batch 2407 batch loss 0.5256598 batch mAP 0.6328125 batch PCKh 0.625\n",
      "Trained batch 2408 batch loss 0.532112122 batch mAP 0.642700195 batch PCKh 0.4375\n",
      "Trained batch 2409 batch loss 0.467168033 batch mAP 0.701629639 batch PCKh 0.4375\n",
      "Trained batch 2410 batch loss 0.455686629 batch mAP 0.629211426 batch PCKh 0.0625\n",
      "Trained batch 2411 batch loss 0.487552494 batch mAP 0.611114502 batch PCKh 0.75\n",
      "Trained batch 2412 batch loss 0.446057647 batch mAP 0.553375244 batch PCKh 0.5\n",
      "Trained batch 2413 batch loss 0.464577258 batch mAP 0.563018799 batch PCKh 0.75\n",
      "Trained batch 2414 batch loss 0.520544291 batch mAP 0.588317871 batch PCKh 0.75\n",
      "Trained batch 2415 batch loss 0.533621907 batch mAP 0.575500488 batch PCKh 0.5\n",
      "Trained batch 2416 batch loss 0.679500937 batch mAP 0.49887085 batch PCKh 0.4375\n",
      "Trained batch 2417 batch loss 0.637756705 batch mAP 0.511108398 batch PCKh 0.4375\n",
      "Trained batch 2418 batch loss 0.528202415 batch mAP 0.614196777 batch PCKh 0.5625\n",
      "Trained batch 2419 batch loss 0.462029099 batch mAP 0.57208252 batch PCKh 0.6875\n",
      "Trained batch 2420 batch loss 0.503249407 batch mAP 0.559295654 batch PCKh 0.25\n",
      "Trained batch 2421 batch loss 0.576167464 batch mAP 0.493865967 batch PCKh 0.8125\n",
      "Trained batch 2422 batch loss 0.465833783 batch mAP 0.503753662 batch PCKh 0.75\n",
      "Trained batch 2423 batch loss 0.403653383 batch mAP 0.603973389 batch PCKh 0.6875\n",
      "Trained batch 2424 batch loss 0.328751594 batch mAP 0.749450684 batch PCKh 0.625\n",
      "Trained batch 2425 batch loss 0.304372698 batch mAP 0.751068115 batch PCKh 0.625\n",
      "Trained batch 2426 batch loss 0.297924191 batch mAP 0.715606689 batch PCKh 0.625\n",
      "Trained batch 2427 batch loss 0.332695216 batch mAP 0.742950439 batch PCKh 0.6875\n",
      "Trained batch 2428 batch loss 0.407140464 batch mAP 0.718383789 batch PCKh 0.625\n",
      "Trained batch 2429 batch loss 0.46541205 batch mAP 0.633911133 batch PCKh 0.375\n",
      "Trained batch 2430 batch loss 0.638114393 batch mAP 0.579071045 batch PCKh 0.625\n",
      "Trained batch 2431 batch loss 0.492726147 batch mAP 0.655700684 batch PCKh 0.625\n",
      "Trained batch 2432 batch loss 0.550245762 batch mAP 0.663269043 batch PCKh 0.3125\n",
      "Trained batch 2433 batch loss 0.52510643 batch mAP 0.674987793 batch PCKh 0.8125\n",
      "Trained batch 2434 batch loss 0.563748717 batch mAP 0.617034912 batch PCKh 0.375\n",
      "Trained batch 2435 batch loss 0.572598338 batch mAP 0.66394043 batch PCKh 0.5625\n",
      "Trained batch 2436 batch loss 0.54813385 batch mAP 0.624053955 batch PCKh 0.1875\n",
      "Trained batch 2437 batch loss 0.525476098 batch mAP 0.652832031 batch PCKh 0.5625\n",
      "Trained batch 2438 batch loss 0.593068659 batch mAP 0.60458374 batch PCKh 0.75\n",
      "Trained batch 2439 batch loss 0.594718814 batch mAP 0.621673584 batch PCKh 0.6875\n",
      "Trained batch 2440 batch loss 0.569754958 batch mAP 0.664123535 batch PCKh 0.375\n",
      "Trained batch 2441 batch loss 0.573554397 batch mAP 0.627105713 batch PCKh 0.3125\n",
      "Trained batch 2442 batch loss 0.552552819 batch mAP 0.591827393 batch PCKh 0.4375\n",
      "Trained batch 2443 batch loss 0.534027696 batch mAP 0.669494629 batch PCKh 0.3125\n",
      "Trained batch 2444 batch loss 0.561189473 batch mAP 0.606628418 batch PCKh 0.3125\n",
      "Trained batch 2445 batch loss 0.498719811 batch mAP 0.634368896 batch PCKh 0.75\n",
      "Trained batch 2446 batch loss 0.556102097 batch mAP 0.558136 batch PCKh 0.75\n",
      "Trained batch 2447 batch loss 0.485239506 batch mAP 0.64074707 batch PCKh 0.4375\n",
      "Trained batch 2448 batch loss 0.55155921 batch mAP 0.644073486 batch PCKh 0.5625\n",
      "Trained batch 2449 batch loss 0.472680062 batch mAP 0.639251709 batch PCKh 0.625\n",
      "Trained batch 2450 batch loss 0.53009367 batch mAP 0.639709473 batch PCKh 0.5\n",
      "Trained batch 2451 batch loss 0.485802561 batch mAP 0.644042969 batch PCKh 0.5625\n",
      "Trained batch 2452 batch loss 0.56028825 batch mAP 0.629516602 batch PCKh 0.5625\n",
      "Trained batch 2453 batch loss 0.465909868 batch mAP 0.676483154 batch PCKh 0.1875\n",
      "Trained batch 2454 batch loss 0.569483 batch mAP 0.628295898 batch PCKh 0.4375\n",
      "Trained batch 2455 batch loss 0.521822512 batch mAP 0.649169922 batch PCKh 0.4375\n",
      "Trained batch 2456 batch loss 0.489825308 batch mAP 0.578155518 batch PCKh 0.1875\n",
      "Trained batch 2457 batch loss 0.506334424 batch mAP 0.650085449 batch PCKh 0.75\n",
      "Trained batch 2458 batch loss 0.4794617 batch mAP 0.674926758 batch PCKh 0.75\n",
      "Trained batch 2459 batch loss 0.443448305 batch mAP 0.720184326 batch PCKh 0.5\n",
      "Trained batch 2460 batch loss 0.412794352 batch mAP 0.74331665 batch PCKh 0.5625\n",
      "Trained batch 2461 batch loss 0.441943318 batch mAP 0.622711182 batch PCKh 0.6875\n",
      "Trained batch 2462 batch loss 0.466546714 batch mAP 0.604919434 batch PCKh 0.5\n",
      "Trained batch 2463 batch loss 0.566999078 batch mAP 0.586578369 batch PCKh 0.5625\n",
      "Trained batch 2464 batch loss 0.521618724 batch mAP 0.603515625 batch PCKh 0.5\n",
      "Trained batch 2465 batch loss 0.528420568 batch mAP 0.539703369 batch PCKh 0.625\n",
      "Trained batch 2466 batch loss 0.567024767 batch mAP 0.574676514 batch PCKh 0.8125\n",
      "Trained batch 2467 batch loss 0.525832057 batch mAP 0.569915771 batch PCKh 0.6875\n",
      "Trained batch 2468 batch loss 0.532678723 batch mAP 0.57043457 batch PCKh 0\n",
      "Trained batch 2469 batch loss 0.51401329 batch mAP 0.545257568 batch PCKh 0.4375\n",
      "Trained batch 2470 batch loss 0.630761147 batch mAP 0.4921875 batch PCKh 0.875\n",
      "Trained batch 2471 batch loss 0.55808866 batch mAP 0.541046143 batch PCKh 0.5\n",
      "Trained batch 2472 batch loss 0.545179605 batch mAP 0.551849365 batch PCKh 0.3125\n",
      "Trained batch 2473 batch loss 0.561526 batch mAP 0.596099854 batch PCKh 0.8125\n",
      "Trained batch 2474 batch loss 0.589421272 batch mAP 0.591827393 batch PCKh 0.3125\n",
      "Trained batch 2475 batch loss 0.563566327 batch mAP 0.642272949 batch PCKh 0.5\n",
      "Trained batch 2476 batch loss 0.503853202 batch mAP 0.677978516 batch PCKh 0.25\n",
      "Trained batch 2477 batch loss 0.591848 batch mAP 0.632141113 batch PCKh 0.6875\n",
      "Trained batch 2478 batch loss 0.537177 batch mAP 0.696899414 batch PCKh 0.25\n",
      "Trained batch 2479 batch loss 0.60510236 batch mAP 0.66192627 batch PCKh 0.25\n",
      "Trained batch 2480 batch loss 0.544609427 batch mAP 0.619842529 batch PCKh 0.25\n",
      "Trained batch 2481 batch loss 0.572811 batch mAP 0.635406494 batch PCKh 0.25\n",
      "Trained batch 2482 batch loss 0.616379 batch mAP 0.585907 batch PCKh 0.3125\n",
      "Trained batch 2483 batch loss 0.693054676 batch mAP 0.495788574 batch PCKh 0\n",
      "Trained batch 2484 batch loss 0.667739391 batch mAP 0.495849609 batch PCKh 0.25\n",
      "Trained batch 2485 batch loss 0.506674111 batch mAP 0.550170898 batch PCKh 0.375\n",
      "Trained batch 2486 batch loss 0.480107158 batch mAP 0.524841309 batch PCKh 0.25\n",
      "Trained batch 2487 batch loss 0.491054565 batch mAP 0.464569092 batch PCKh 0.4375\n",
      "Trained batch 2488 batch loss 0.601155579 batch mAP 0.386352539 batch PCKh 0.375\n",
      "Trained batch 2489 batch loss 0.625594735 batch mAP 0.339996338 batch PCKh 0.125\n",
      "Trained batch 2490 batch loss 0.618472099 batch mAP 0.440612793 batch PCKh 0.375\n",
      "Trained batch 2491 batch loss 0.592849731 batch mAP 0.431182861 batch PCKh 0.8125\n",
      "Trained batch 2492 batch loss 0.615207732 batch mAP 0.442047119 batch PCKh 0.5\n",
      "Trained batch 2493 batch loss 0.533032596 batch mAP 0.534606934 batch PCKh 0.375\n",
      "Trained batch 2494 batch loss 0.600932837 batch mAP 0.498535156 batch PCKh 0.625\n",
      "Trained batch 2495 batch loss 0.58344543 batch mAP 0.551849365 batch PCKh 0.5625\n",
      "Trained batch 2496 batch loss 0.457795858 batch mAP 0.578735352 batch PCKh 0.4375\n",
      "Trained batch 2497 batch loss 0.479075909 batch mAP 0.593566895 batch PCKh 0.4375\n",
      "Trained batch 2498 batch loss 0.516809583 batch mAP 0.642608643 batch PCKh 0.6875\n",
      "Trained batch 2499 batch loss 0.514641702 batch mAP 0.627532959 batch PCKh 0.625\n",
      "Trained batch 2500 batch loss 0.530314445 batch mAP 0.571929932 batch PCKh 0.125\n",
      "Trained batch 2501 batch loss 0.545314133 batch mAP 0.566467285 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2502 batch loss 0.543485105 batch mAP 0.551696777 batch PCKh 0.6875\n",
      "Trained batch 2503 batch loss 0.578964233 batch mAP 0.649017334 batch PCKh 0.4375\n",
      "Trained batch 2504 batch loss 0.525535941 batch mAP 0.664764404 batch PCKh 0.5\n",
      "Trained batch 2505 batch loss 0.627915 batch mAP 0.557830811 batch PCKh 0.625\n",
      "Trained batch 2506 batch loss 0.578759313 batch mAP 0.546966553 batch PCKh 0.1875\n",
      "Trained batch 2507 batch loss 0.545073867 batch mAP 0.664245605 batch PCKh 0.5625\n",
      "Trained batch 2508 batch loss 0.455214858 batch mAP 0.623321533 batch PCKh 0.125\n",
      "Trained batch 2509 batch loss 0.493403971 batch mAP 0.687530518 batch PCKh 0.5625\n",
      "Trained batch 2510 batch loss 0.549010873 batch mAP 0.692230225 batch PCKh 0.375\n",
      "Trained batch 2511 batch loss 0.597122252 batch mAP 0.672668457 batch PCKh 0.25\n",
      "Trained batch 2512 batch loss 0.480062962 batch mAP 0.692871094 batch PCKh 0.25\n",
      "Trained batch 2513 batch loss 0.541837096 batch mAP 0.643127441 batch PCKh 0.8125\n",
      "Trained batch 2514 batch loss 0.534725547 batch mAP 0.546173096 batch PCKh 0.5625\n",
      "Trained batch 2515 batch loss 0.555269241 batch mAP 0.539520264 batch PCKh 0.5\n",
      "Trained batch 2516 batch loss 0.55883 batch mAP 0.607055664 batch PCKh 0.5625\n",
      "Trained batch 2517 batch loss 0.536682844 batch mAP 0.626739502 batch PCKh 0.4375\n",
      "Trained batch 2518 batch loss 0.434560657 batch mAP 0.622680664 batch PCKh 0.4375\n",
      "Trained batch 2519 batch loss 0.510259211 batch mAP 0.616821289 batch PCKh 0.3125\n",
      "Trained batch 2520 batch loss 0.535810053 batch mAP 0.582122803 batch PCKh 0.75\n",
      "Trained batch 2521 batch loss 0.52965492 batch mAP 0.61340332 batch PCKh 0.25\n",
      "Trained batch 2522 batch loss 0.521603286 batch mAP 0.617584229 batch PCKh 0.5\n",
      "Trained batch 2523 batch loss 0.439590633 batch mAP 0.643493652 batch PCKh 0.6875\n",
      "Trained batch 2524 batch loss 0.562692881 batch mAP 0.587249756 batch PCKh 0.5625\n",
      "Trained batch 2525 batch loss 0.621507 batch mAP 0.623687744 batch PCKh 0.125\n",
      "Trained batch 2526 batch loss 0.550700426 batch mAP 0.567047119 batch PCKh 0.625\n",
      "Trained batch 2527 batch loss 0.587594271 batch mAP 0.609466553 batch PCKh 0.375\n",
      "Trained batch 2528 batch loss 0.546828926 batch mAP 0.651824951 batch PCKh 0.3125\n",
      "Trained batch 2529 batch loss 0.54286474 batch mAP 0.561126709 batch PCKh 0.3125\n",
      "Trained batch 2530 batch loss 0.507617652 batch mAP 0.61831665 batch PCKh 0.375\n",
      "Trained batch 2531 batch loss 0.544249535 batch mAP 0.569732666 batch PCKh 0.4375\n",
      "Trained batch 2532 batch loss 0.563297868 batch mAP 0.578521729 batch PCKh 0.3125\n",
      "Trained batch 2533 batch loss 0.60725 batch mAP 0.554901123 batch PCKh 0.5\n",
      "Trained batch 2534 batch loss 0.622052968 batch mAP 0.575683594 batch PCKh 0\n",
      "Trained batch 2535 batch loss 0.550846457 batch mAP 0.588104248 batch PCKh 0.5\n",
      "Trained batch 2536 batch loss 0.472895324 batch mAP 0.560150146 batch PCKh 0.6875\n",
      "Trained batch 2537 batch loss 0.594913483 batch mAP 0.561920166 batch PCKh 0.3125\n",
      "Trained batch 2538 batch loss 0.678184152 batch mAP 0.508056641 batch PCKh 0.875\n",
      "Trained batch 2539 batch loss 0.554719746 batch mAP 0.560424805 batch PCKh 0.6875\n",
      "Trained batch 2540 batch loss 0.560779691 batch mAP 0.565948486 batch PCKh 0.625\n",
      "Trained batch 2541 batch loss 0.540625751 batch mAP 0.572265625 batch PCKh 0.75\n",
      "Trained batch 2542 batch loss 0.432916343 batch mAP 0.650848389 batch PCKh 0.5625\n",
      "Trained batch 2543 batch loss 0.487550437 batch mAP 0.599487305 batch PCKh 0.5\n",
      "Trained batch 2544 batch loss 0.453029275 batch mAP 0.592010498 batch PCKh 0.1875\n",
      "Trained batch 2545 batch loss 0.53762114 batch mAP 0.600616455 batch PCKh 0.375\n",
      "Trained batch 2546 batch loss 0.614773154 batch mAP 0.639099121 batch PCKh 0.1875\n",
      "Trained batch 2547 batch loss 0.535722733 batch mAP 0.608734131 batch PCKh 0.6875\n",
      "Trained batch 2548 batch loss 0.508387208 batch mAP 0.594146729 batch PCKh 0.125\n",
      "Trained batch 2549 batch loss 0.664603353 batch mAP 0.562530518 batch PCKh 0.625\n",
      "Trained batch 2550 batch loss 0.632855 batch mAP 0.612823486 batch PCKh 0.1875\n",
      "Trained batch 2551 batch loss 0.593951285 batch mAP 0.656463623 batch PCKh 0.4375\n",
      "Trained batch 2552 batch loss 0.613578618 batch mAP 0.654388428 batch PCKh 0.75\n",
      "Trained batch 2553 batch loss 0.61584866 batch mAP 0.530883789 batch PCKh 0.5625\n",
      "Trained batch 2554 batch loss 0.58858037 batch mAP 0.557647705 batch PCKh 0.6875\n",
      "Trained batch 2555 batch loss 0.405687451 batch mAP 0.606140137 batch PCKh 0\n",
      "Trained batch 2556 batch loss 0.390603244 batch mAP 0.561798096 batch PCKh 0.1875\n",
      "Trained batch 2557 batch loss 0.565216959 batch mAP 0.626220703 batch PCKh 0.625\n",
      "Trained batch 2558 batch loss 0.534359574 batch mAP 0.616821289 batch PCKh 0.5\n",
      "Trained batch 2559 batch loss 0.545179784 batch mAP 0.641998291 batch PCKh 0.5625\n",
      "Trained batch 2560 batch loss 0.506491303 batch mAP 0.674224854 batch PCKh 0.875\n",
      "Trained batch 2561 batch loss 0.51733166 batch mAP 0.725463867 batch PCKh 0.5\n",
      "Trained batch 2562 batch loss 0.553368926 batch mAP 0.693969727 batch PCKh 0.5\n",
      "Trained batch 2563 batch loss 0.526160061 batch mAP 0.67074585 batch PCKh 0.4375\n",
      "Trained batch 2564 batch loss 0.449628234 batch mAP 0.644989 batch PCKh 0.5625\n",
      "Trained batch 2565 batch loss 0.442195833 batch mAP 0.651062 batch PCKh 0.25\n",
      "Trained batch 2566 batch loss 0.47347641 batch mAP 0.631408691 batch PCKh 0.4375\n",
      "Trained batch 2567 batch loss 0.418078631 batch mAP 0.635559082 batch PCKh 0\n",
      "Trained batch 2568 batch loss 0.538923502 batch mAP 0.651245117 batch PCKh 0.25\n",
      "Trained batch 2569 batch loss 0.538296819 batch mAP 0.620117188 batch PCKh 0.625\n",
      "Trained batch 2570 batch loss 0.538412929 batch mAP 0.630126953 batch PCKh 0.25\n",
      "Trained batch 2571 batch loss 0.520501316 batch mAP 0.640472412 batch PCKh 0.5\n",
      "Trained batch 2572 batch loss 0.568163633 batch mAP 0.61340332 batch PCKh 0.125\n",
      "Trained batch 2573 batch loss 0.623462498 batch mAP 0.581787109 batch PCKh 0.3125\n",
      "Trained batch 2574 batch loss 0.538882196 batch mAP 0.650970459 batch PCKh 0.375\n",
      "Trained batch 2575 batch loss 0.585197926 batch mAP 0.583343506 batch PCKh 0.3125\n",
      "Trained batch 2576 batch loss 0.539010763 batch mAP 0.564086914 batch PCKh 0.375\n",
      "Trained batch 2577 batch loss 0.658651 batch mAP 0.543212891 batch PCKh 0.375\n",
      "Trained batch 2578 batch loss 0.60756743 batch mAP 0.480194092 batch PCKh 0\n",
      "Trained batch 2579 batch loss 0.479452729 batch mAP 0.603088379 batch PCKh 0.625\n",
      "Trained batch 2580 batch loss 0.501989722 batch mAP 0.613220215 batch PCKh 0.25\n",
      "Trained batch 2581 batch loss 0.605645597 batch mAP 0.555908203 batch PCKh 0.625\n",
      "Trained batch 2582 batch loss 0.48477453 batch mAP 0.620025635 batch PCKh 0.1875\n",
      "Trained batch 2583 batch loss 0.514697552 batch mAP 0.647491455 batch PCKh 0.4375\n",
      "Trained batch 2584 batch loss 0.538538694 batch mAP 0.658691406 batch PCKh 0.3125\n",
      "Trained batch 2585 batch loss 0.546433628 batch mAP 0.606231689 batch PCKh 0.25\n",
      "Trained batch 2586 batch loss 0.556550741 batch mAP 0.630340576 batch PCKh 0.6875\n",
      "Trained batch 2587 batch loss 0.563593626 batch mAP 0.643096924 batch PCKh 0.5625\n",
      "Trained batch 2588 batch loss 0.612773538 batch mAP 0.58078 batch PCKh 0.3125\n",
      "Trained batch 2589 batch loss 0.397190392 batch mAP 0.597747803 batch PCKh 0.1875\n",
      "Trained batch 2590 batch loss 0.475319058 batch mAP 0.653656 batch PCKh 0.1875\n",
      "Trained batch 2591 batch loss 0.428131163 batch mAP 0.612487793 batch PCKh 0\n",
      "Trained batch 2592 batch loss 0.342621893 batch mAP 0.506988525 batch PCKh 0\n",
      "Trained batch 2593 batch loss 0.402442098 batch mAP 0.533691406 batch PCKh 0.3125\n",
      "Trained batch 2594 batch loss 0.48090744 batch mAP 0.733398438 batch PCKh 0.5625\n",
      "Trained batch 2595 batch loss 0.588278294 batch mAP 0.717895508 batch PCKh 0.875\n",
      "Trained batch 2596 batch loss 0.547434151 batch mAP 0.730316162 batch PCKh 0.75\n",
      "Trained batch 2597 batch loss 0.598918438 batch mAP 0.677886963 batch PCKh 0.125\n",
      "Trained batch 2598 batch loss 0.591391563 batch mAP 0.665374756 batch PCKh 0.5\n",
      "Trained batch 2599 batch loss 0.670068 batch mAP 0.620819092 batch PCKh 0.0625\n",
      "Trained batch 2600 batch loss 0.600252211 batch mAP 0.563385 batch PCKh 0.5\n",
      "Trained batch 2601 batch loss 0.631035328 batch mAP 0.562255859 batch PCKh 0.6875\n",
      "Trained batch 2602 batch loss 0.575443149 batch mAP 0.640380859 batch PCKh 0.4375\n",
      "Trained batch 2603 batch loss 0.580736458 batch mAP 0.634490967 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2604 batch loss 0.582818747 batch mAP 0.642852783 batch PCKh 0.4375\n",
      "Trained batch 2605 batch loss 0.487874746 batch mAP 0.677001953 batch PCKh 0.25\n",
      "Trained batch 2606 batch loss 0.571540833 batch mAP 0.648681641 batch PCKh 0.6875\n",
      "Trained batch 2607 batch loss 0.596256614 batch mAP 0.574920654 batch PCKh 0.25\n",
      "Trained batch 2608 batch loss 0.517690778 batch mAP 0.581451416 batch PCKh 0.25\n",
      "Trained batch 2609 batch loss 0.592985034 batch mAP 0.576751709 batch PCKh 0.875\n",
      "Trained batch 2610 batch loss 0.536937952 batch mAP 0.573699951 batch PCKh 0.5625\n",
      "Trained batch 2611 batch loss 0.450540364 batch mAP 0.593383789 batch PCKh 0.625\n",
      "Trained batch 2612 batch loss 0.552133262 batch mAP 0.574157715 batch PCKh 0.75\n",
      "Trained batch 2613 batch loss 0.548086941 batch mAP 0.592193604 batch PCKh 0.5625\n",
      "Trained batch 2614 batch loss 0.555844188 batch mAP 0.67401123 batch PCKh 0.75\n",
      "Trained batch 2615 batch loss 0.49758476 batch mAP 0.586517334 batch PCKh 0.5\n",
      "Trained batch 2616 batch loss 0.487219721 batch mAP 0.612182617 batch PCKh 0.0625\n",
      "Trained batch 2617 batch loss 0.461476684 batch mAP 0.643554688 batch PCKh 0.5625\n",
      "Trained batch 2618 batch loss 0.531561196 batch mAP 0.607605 batch PCKh 0.8125\n",
      "Trained batch 2619 batch loss 0.484596163 batch mAP 0.635437 batch PCKh 0.625\n",
      "Trained batch 2620 batch loss 0.562499881 batch mAP 0.586242676 batch PCKh 0.5625\n",
      "Trained batch 2621 batch loss 0.658539712 batch mAP 0.486358643 batch PCKh 0.5625\n",
      "Trained batch 2622 batch loss 0.631469965 batch mAP 0.528198242 batch PCKh 0.0625\n",
      "Trained batch 2623 batch loss 0.552133083 batch mAP 0.6065979 batch PCKh 0.75\n",
      "Trained batch 2624 batch loss 0.538415074 batch mAP 0.565063477 batch PCKh 0.75\n",
      "Trained batch 2625 batch loss 0.646027386 batch mAP 0.494537354 batch PCKh 0.0625\n",
      "Trained batch 2626 batch loss 0.542979062 batch mAP 0.602905273 batch PCKh 0.5625\n",
      "Trained batch 2627 batch loss 0.60972923 batch mAP 0.482635498 batch PCKh 0.8125\n",
      "Trained batch 2628 batch loss 0.579103827 batch mAP 0.541534424 batch PCKh 0.625\n",
      "Trained batch 2629 batch loss 0.504566729 batch mAP 0.639221191 batch PCKh 0.75\n",
      "Trained batch 2630 batch loss 0.598310053 batch mAP 0.584320068 batch PCKh 0.4375\n",
      "Trained batch 2631 batch loss 0.559900284 batch mAP 0.618164062 batch PCKh 0.6875\n",
      "Trained batch 2632 batch loss 0.614791691 batch mAP 0.552063 batch PCKh 0.375\n",
      "Trained batch 2633 batch loss 0.581430674 batch mAP 0.636199951 batch PCKh 0.75\n",
      "Trained batch 2634 batch loss 0.570629537 batch mAP 0.641265869 batch PCKh 0.5\n",
      "Trained batch 2635 batch loss 0.556378067 batch mAP 0.611053467 batch PCKh 0.6875\n",
      "Trained batch 2636 batch loss 0.547977567 batch mAP 0.525756836 batch PCKh 0.4375\n",
      "Trained batch 2637 batch loss 0.561821938 batch mAP 0.603546143 batch PCKh 0.1875\n",
      "Trained batch 2638 batch loss 0.551672816 batch mAP 0.549591064 batch PCKh 0.5625\n",
      "Trained batch 2639 batch loss 0.47670877 batch mAP 0.59387207 batch PCKh 0.6875\n",
      "Trained batch 2640 batch loss 0.587629795 batch mAP 0.552948 batch PCKh 0.1875\n",
      "Trained batch 2641 batch loss 0.490324169 batch mAP 0.602386475 batch PCKh 0.25\n",
      "Trained batch 2642 batch loss 0.442800462 batch mAP 0.64465332 batch PCKh 0.75\n",
      "Trained batch 2643 batch loss 0.47009176 batch mAP 0.602172852 batch PCKh 0.5625\n",
      "Trained batch 2644 batch loss 0.483121902 batch mAP 0.54989624 batch PCKh 0.75\n",
      "Trained batch 2645 batch loss 0.519184947 batch mAP 0.536682129 batch PCKh 0.5\n",
      "Trained batch 2646 batch loss 0.462863147 batch mAP 0.654937744 batch PCKh 0.75\n",
      "Trained batch 2647 batch loss 0.599565208 batch mAP 0.564331055 batch PCKh 0.5625\n",
      "Trained batch 2648 batch loss 0.642967701 batch mAP 0.599029541 batch PCKh 0.5625\n",
      "Trained batch 2649 batch loss 0.543191314 batch mAP 0.538269043 batch PCKh 0.625\n",
      "Trained batch 2650 batch loss 0.534633815 batch mAP 0.554138184 batch PCKh 0.375\n",
      "Trained batch 2651 batch loss 0.582364202 batch mAP 0.596282959 batch PCKh 0.5\n",
      "Trained batch 2652 batch loss 0.608944893 batch mAP 0.606506348 batch PCKh 0.6875\n",
      "Trained batch 2653 batch loss 0.534351945 batch mAP 0.635376 batch PCKh 0.4375\n",
      "Trained batch 2654 batch loss 0.562682748 batch mAP 0.556671143 batch PCKh 0.5\n",
      "Trained batch 2655 batch loss 0.546322584 batch mAP 0.514007568 batch PCKh 0.625\n",
      "Trained batch 2656 batch loss 0.47901845 batch mAP 0.597076416 batch PCKh 0.4375\n",
      "Trained batch 2657 batch loss 0.469979286 batch mAP 0.68737793 batch PCKh 0.75\n",
      "Trained batch 2658 batch loss 0.444811285 batch mAP 0.61050415 batch PCKh 0\n",
      "Trained batch 2659 batch loss 0.547695279 batch mAP 0.615997314 batch PCKh 0.1875\n",
      "Trained batch 2660 batch loss 0.527209282 batch mAP 0.599121094 batch PCKh 0.6875\n",
      "Trained batch 2661 batch loss 0.510089397 batch mAP 0.631713867 batch PCKh 0.5\n",
      "Trained batch 2662 batch loss 0.577222943 batch mAP 0.56628418 batch PCKh 0.4375\n",
      "Trained batch 2663 batch loss 0.580065608 batch mAP 0.553955078 batch PCKh 0.6875\n",
      "Trained batch 2664 batch loss 0.519008279 batch mAP 0.626678467 batch PCKh 0.1875\n",
      "Trained batch 2665 batch loss 0.568961 batch mAP 0.541931152 batch PCKh 0.375\n",
      "Trained batch 2666 batch loss 0.505964756 batch mAP 0.606140137 batch PCKh 0.6875\n",
      "Trained batch 2667 batch loss 0.468100429 batch mAP 0.647338867 batch PCKh 0.5625\n",
      "Trained batch 2668 batch loss 0.585016489 batch mAP 0.667053223 batch PCKh 0.875\n",
      "Trained batch 2669 batch loss 0.627598226 batch mAP 0.641113281 batch PCKh 0.8125\n",
      "Trained batch 2670 batch loss 0.567211092 batch mAP 0.620056152 batch PCKh 0.375\n",
      "Trained batch 2671 batch loss 0.551300824 batch mAP 0.476867676 batch PCKh 0.6875\n",
      "Trained batch 2672 batch loss 0.492029339 batch mAP 0.536804199 batch PCKh 0.625\n",
      "Trained batch 2673 batch loss 0.403453 batch mAP 0.589782715 batch PCKh 0.6875\n",
      "Trained batch 2674 batch loss 0.536867 batch mAP 0.544464111 batch PCKh 0.0625\n",
      "Trained batch 2675 batch loss 0.512950242 batch mAP 0.563415527 batch PCKh 0.8125\n",
      "Trained batch 2676 batch loss 0.587813735 batch mAP 0.465423584 batch PCKh 0.5\n",
      "Trained batch 2677 batch loss 0.564235032 batch mAP 0.539489746 batch PCKh 0.625\n",
      "Trained batch 2678 batch loss 0.510410547 batch mAP 0.605621338 batch PCKh 0.3125\n",
      "Trained batch 2679 batch loss 0.545545816 batch mAP 0.550354 batch PCKh 0.5625\n",
      "Trained batch 2680 batch loss 0.510917544 batch mAP 0.615875244 batch PCKh 0.3125\n",
      "Trained batch 2681 batch loss 0.5947299 batch mAP 0.563598633 batch PCKh 0.5\n",
      "Trained batch 2682 batch loss 0.554456592 batch mAP 0.587219238 batch PCKh 0.25\n",
      "Trained batch 2683 batch loss 0.480599701 batch mAP 0.627044678 batch PCKh 0.375\n",
      "Trained batch 2684 batch loss 0.491306841 batch mAP 0.632141113 batch PCKh 0.75\n",
      "Trained batch 2685 batch loss 0.376006246 batch mAP 0.670806885 batch PCKh 0.75\n",
      "Trained batch 2686 batch loss 0.503380179 batch mAP 0.522613525 batch PCKh 0.0625\n",
      "Trained batch 2687 batch loss 0.566929579 batch mAP 0.492980957 batch PCKh 0.5\n",
      "Trained batch 2688 batch loss 0.607705712 batch mAP 0.53024292 batch PCKh 0.0625\n",
      "Trained batch 2689 batch loss 0.550653 batch mAP 0.528137207 batch PCKh 0.1875\n",
      "Trained batch 2690 batch loss 0.444852173 batch mAP 0.564941406 batch PCKh 0.1875\n",
      "Trained batch 2691 batch loss 0.534053266 batch mAP 0.563232422 batch PCKh 0\n",
      "Trained batch 2692 batch loss 0.680473328 batch mAP 0.542755127 batch PCKh 0\n",
      "Trained batch 2693 batch loss 0.690536499 batch mAP 0.585449219 batch PCKh 0.25\n",
      "Trained batch 2694 batch loss 0.572819233 batch mAP 0.568664551 batch PCKh 0.25\n",
      "Trained batch 2695 batch loss 0.63406539 batch mAP 0.620361328 batch PCKh 0.4375\n",
      "Trained batch 2696 batch loss 0.667004228 batch mAP 0.673034668 batch PCKh 0.3125\n",
      "Trained batch 2697 batch loss 0.579836667 batch mAP 0.612243652 batch PCKh 0.4375\n",
      "Trained batch 2698 batch loss 0.639562964 batch mAP 0.498199463 batch PCKh 0.5\n",
      "Trained batch 2699 batch loss 0.612883151 batch mAP 0.565795898 batch PCKh 0.5625\n",
      "Trained batch 2700 batch loss 0.585861444 batch mAP 0.450286865 batch PCKh 0.75\n",
      "Trained batch 2701 batch loss 0.56056273 batch mAP 0.478607178 batch PCKh 0.875\n",
      "Trained batch 2702 batch loss 0.603285432 batch mAP 0.551178 batch PCKh 0.5\n",
      "Trained batch 2703 batch loss 0.461029053 batch mAP 0.577148438 batch PCKh 0.5\n",
      "Trained batch 2704 batch loss 0.587119699 batch mAP 0.527954102 batch PCKh 0.625\n",
      "Trained batch 2705 batch loss 0.596714377 batch mAP 0.507782 batch PCKh 0.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2706 batch loss 0.513597786 batch mAP 0.509094238 batch PCKh 0.5\n",
      "Trained batch 2707 batch loss 0.590530038 batch mAP 0.537994385 batch PCKh 0.3125\n",
      "Trained batch 2708 batch loss 0.497226596 batch mAP 0.598053 batch PCKh 0.375\n",
      "Trained batch 2709 batch loss 0.552903891 batch mAP 0.601013184 batch PCKh 0.5\n",
      "Trained batch 2710 batch loss 0.579853117 batch mAP 0.624115 batch PCKh 0.6875\n",
      "Trained batch 2711 batch loss 0.547108114 batch mAP 0.629455566 batch PCKh 0.75\n",
      "Trained batch 2712 batch loss 0.564204693 batch mAP 0.632080078 batch PCKh 0.125\n",
      "Trained batch 2713 batch loss 0.537753582 batch mAP 0.595275879 batch PCKh 0.25\n",
      "Trained batch 2714 batch loss 0.54812336 batch mAP 0.584136963 batch PCKh 0.5625\n",
      "Trained batch 2715 batch loss 0.565775156 batch mAP 0.585662842 batch PCKh 0.8125\n",
      "Trained batch 2716 batch loss 0.494693816 batch mAP 0.514678955 batch PCKh 0.6875\n",
      "Trained batch 2717 batch loss 0.521696687 batch mAP 0.488250732 batch PCKh 0.5\n",
      "Trained batch 2718 batch loss 0.496123582 batch mAP 0.533569336 batch PCKh 0.1875\n",
      "Trained batch 2719 batch loss 0.544786453 batch mAP 0.604003906 batch PCKh 0.625\n",
      "Trained batch 2720 batch loss 0.43689245 batch mAP 0.581817627 batch PCKh 0.5625\n",
      "Trained batch 2721 batch loss 0.412374228 batch mAP 0.578552246 batch PCKh 0.625\n",
      "Trained batch 2722 batch loss 0.46516031 batch mAP 0.53894043 batch PCKh 0.75\n",
      "Trained batch 2723 batch loss 0.464865893 batch mAP 0.571075439 batch PCKh 0.75\n",
      "Trained batch 2724 batch loss 0.471924365 batch mAP 0.582763672 batch PCKh 0.75\n",
      "Trained batch 2725 batch loss 0.535990357 batch mAP 0.494384766 batch PCKh 0.6875\n",
      "Trained batch 2726 batch loss 0.569082081 batch mAP 0.580078125 batch PCKh 0.6875\n",
      "Trained batch 2727 batch loss 0.552655101 batch mAP 0.505065918 batch PCKh 0.875\n",
      "Trained batch 2728 batch loss 0.545399725 batch mAP 0.484313965 batch PCKh 0.8125\n",
      "Trained batch 2729 batch loss 0.619113803 batch mAP 0.50302124 batch PCKh 0.875\n",
      "Trained batch 2730 batch loss 0.617910922 batch mAP 0.510986328 batch PCKh 0.6875\n",
      "Trained batch 2731 batch loss 0.637734294 batch mAP 0.496795654 batch PCKh 0.25\n",
      "Trained batch 2732 batch loss 0.558709443 batch mAP 0.551727295 batch PCKh 0.4375\n",
      "Trained batch 2733 batch loss 0.623452902 batch mAP 0.510406494 batch PCKh 0.5\n",
      "Trained batch 2734 batch loss 0.597551584 batch mAP 0.567840576 batch PCKh 0.375\n",
      "Trained batch 2735 batch loss 0.58678478 batch mAP 0.621948242 batch PCKh 0.625\n",
      "Trained batch 2736 batch loss 0.578780651 batch mAP 0.61505127 batch PCKh 0.5\n",
      "Trained batch 2737 batch loss 0.555783212 batch mAP 0.622345 batch PCKh 0.375\n",
      "Trained batch 2738 batch loss 0.51206553 batch mAP 0.637573242 batch PCKh 0.5625\n",
      "Trained batch 2739 batch loss 0.508596659 batch mAP 0.659179688 batch PCKh 0.5625\n",
      "Trained batch 2740 batch loss 0.580650091 batch mAP 0.586242676 batch PCKh 0.625\n",
      "Trained batch 2741 batch loss 0.618353605 batch mAP 0.534484863 batch PCKh 0.625\n",
      "Trained batch 2742 batch loss 0.58164835 batch mAP 0.497619629 batch PCKh 0.625\n",
      "Trained batch 2743 batch loss 0.529592395 batch mAP 0.567382812 batch PCKh 0.625\n",
      "Trained batch 2744 batch loss 0.546921611 batch mAP 0.550537109 batch PCKh 0.5625\n",
      "Trained batch 2745 batch loss 0.581235051 batch mAP 0.561645508 batch PCKh 0.75\n",
      "Trained batch 2746 batch loss 0.549236059 batch mAP 0.528564453 batch PCKh 0.625\n",
      "Trained batch 2747 batch loss 0.515099049 batch mAP 0.558868408 batch PCKh 0.6875\n",
      "Trained batch 2748 batch loss 0.540058255 batch mAP 0.603393555 batch PCKh 0.625\n",
      "Trained batch 2749 batch loss 0.594877601 batch mAP 0.554931641 batch PCKh 0.875\n",
      "Trained batch 2750 batch loss 0.547316432 batch mAP 0.577453613 batch PCKh 0.5625\n",
      "Trained batch 2751 batch loss 0.552956939 batch mAP 0.553436279 batch PCKh 0.3125\n",
      "Trained batch 2752 batch loss 0.470246315 batch mAP 0.580444336 batch PCKh 0.5\n",
      "Trained batch 2753 batch loss 0.533771813 batch mAP 0.616149902 batch PCKh 0.6875\n",
      "Trained batch 2754 batch loss 0.529112816 batch mAP 0.651062 batch PCKh 0.6875\n",
      "Trained batch 2755 batch loss 0.525328159 batch mAP 0.627349854 batch PCKh 0.8125\n",
      "Trained batch 2756 batch loss 0.495940298 batch mAP 0.620025635 batch PCKh 0.75\n",
      "Trained batch 2757 batch loss 0.582759 batch mAP 0.54599 batch PCKh 0.5625\n",
      "Trained batch 2758 batch loss 0.544980526 batch mAP 0.546051 batch PCKh 0.625\n",
      "Trained batch 2759 batch loss 0.577534795 batch mAP 0.534484863 batch PCKh 0.5\n",
      "Trained batch 2760 batch loss 0.605456471 batch mAP 0.527954102 batch PCKh 0.5625\n",
      "Trained batch 2761 batch loss 0.502946496 batch mAP 0.58984375 batch PCKh 0.5625\n",
      "Trained batch 2762 batch loss 0.493994474 batch mAP 0.602783203 batch PCKh 0.75\n",
      "Trained batch 2763 batch loss 0.440441 batch mAP 0.640533447 batch PCKh 0.625\n",
      "Trained batch 2764 batch loss 0.475672 batch mAP 0.682281494 batch PCKh 0.3125\n",
      "Trained batch 2765 batch loss 0.426920056 batch mAP 0.622833252 batch PCKh 0.5625\n",
      "Trained batch 2766 batch loss 0.385567158 batch mAP 0.647186279 batch PCKh 0.875\n",
      "Trained batch 2767 batch loss 0.493586421 batch mAP 0.617614746 batch PCKh 0.6875\n",
      "Trained batch 2768 batch loss 0.515888214 batch mAP 0.683563232 batch PCKh 0.6875\n",
      "Trained batch 2769 batch loss 0.471233487 batch mAP 0.672424316 batch PCKh 0.6875\n",
      "Trained batch 2770 batch loss 0.441966832 batch mAP 0.668518066 batch PCKh 0.75\n",
      "Trained batch 2771 batch loss 0.509278953 batch mAP 0.631713867 batch PCKh 0.6875\n",
      "Trained batch 2772 batch loss 0.507252038 batch mAP 0.584411621 batch PCKh 0.5625\n",
      "Trained batch 2773 batch loss 0.448888719 batch mAP 0.645874 batch PCKh 0.375\n",
      "Trained batch 2774 batch loss 0.400535226 batch mAP 0.678283691 batch PCKh 0.3125\n",
      "Trained batch 2775 batch loss 0.467609584 batch mAP 0.624023438 batch PCKh 0.75\n",
      "Trained batch 2776 batch loss 0.418212354 batch mAP 0.618774414 batch PCKh 0.5625\n",
      "Epoch 7 train loss 0.5313138365745544 train mAP 0.5933347344398499 train PCKh\n",
      "Validated batch 1 batch loss 0.522548676 batch mAP 0.61517334 batch PCKh 0.4375\n",
      "Validated batch 2 batch loss 0.67023921 batch mAP 0.62512207 batch PCKh 0.5625\n",
      "Validated batch 3 batch loss 0.559701502 batch mAP 0.683197 batch PCKh 0.5\n",
      "Validated batch 4 batch loss 0.549489617 batch mAP 0.635559082 batch PCKh 0.5\n",
      "Validated batch 5 batch loss 0.469645262 batch mAP 0.720184326 batch PCKh 0.375\n",
      "Validated batch 6 batch loss 0.631926298 batch mAP 0.533355713 batch PCKh 0.125\n",
      "Validated batch 7 batch loss 0.524873614 batch mAP 0.616027832 batch PCKh 0.3125\n",
      "Validated batch 8 batch loss 0.596094 batch mAP 0.524810791 batch PCKh 0.5625\n",
      "Validated batch 9 batch loss 0.598177075 batch mAP 0.635253906 batch PCKh 0.75\n",
      "Validated batch 10 batch loss 0.509413779 batch mAP 0.657104492 batch PCKh 0.125\n",
      "Validated batch 11 batch loss 0.647885919 batch mAP 0.551757812 batch PCKh 0.75\n",
      "Validated batch 12 batch loss 0.536121309 batch mAP 0.669769287 batch PCKh 0.4375\n",
      "Validated batch 13 batch loss 0.628331 batch mAP 0.625610352 batch PCKh 0.375\n",
      "Validated batch 14 batch loss 0.510057509 batch mAP 0.647491455 batch PCKh 0.25\n",
      "Validated batch 15 batch loss 0.579448342 batch mAP 0.5652771 batch PCKh 0.5625\n",
      "Validated batch 16 batch loss 0.53986454 batch mAP 0.651672363 batch PCKh 0.75\n",
      "Validated batch 17 batch loss 0.548256159 batch mAP 0.622741699 batch PCKh 0.4375\n",
      "Validated batch 18 batch loss 0.692979217 batch mAP 0.645263672 batch PCKh 0.3125\n",
      "Validated batch 19 batch loss 0.601708472 batch mAP 0.656463623 batch PCKh 0.25\n",
      "Validated batch 20 batch loss 0.559297204 batch mAP 0.672851562 batch PCKh 0.1875\n",
      "Validated batch 21 batch loss 0.529733777 batch mAP 0.692749 batch PCKh 0.1875\n",
      "Validated batch 22 batch loss 0.529585302 batch mAP 0.6121521 batch PCKh 0.5\n",
      "Validated batch 23 batch loss 0.449646294 batch mAP 0.671691895 batch PCKh 0.125\n",
      "Validated batch 24 batch loss 0.536616743 batch mAP 0.566436768 batch PCKh 0.625\n",
      "Validated batch 25 batch loss 0.528538048 batch mAP 0.61505127 batch PCKh 0.3125\n",
      "Validated batch 26 batch loss 0.638519287 batch mAP 0.61831665 batch PCKh 0.6875\n",
      "Validated batch 27 batch loss 0.666348815 batch mAP 0.61730957 batch PCKh 0.5\n",
      "Validated batch 28 batch loss 0.5641976 batch mAP 0.509674072 batch PCKh 0.0625\n",
      "Validated batch 29 batch loss 0.691672444 batch mAP 0.479736328 batch PCKh 0.1875\n",
      "Validated batch 30 batch loss 0.680315852 batch mAP 0.484954834 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 31 batch loss 0.69910121 batch mAP 0.489807129 batch PCKh 0.4375\n",
      "Validated batch 32 batch loss 0.620860636 batch mAP 0.585907 batch PCKh 0.75\n",
      "Validated batch 33 batch loss 0.557562 batch mAP 0.635894775 batch PCKh 0.75\n",
      "Validated batch 34 batch loss 0.68391 batch mAP 0.634674072 batch PCKh 0.25\n",
      "Validated batch 35 batch loss 0.714745581 batch mAP 0.645782471 batch PCKh 0.8125\n",
      "Validated batch 36 batch loss 0.566180646 batch mAP 0.562988281 batch PCKh 0.0625\n",
      "Validated batch 37 batch loss 0.55388236 batch mAP 0.630157471 batch PCKh 0.4375\n",
      "Validated batch 38 batch loss 0.623668194 batch mAP 0.666107178 batch PCKh 0.625\n",
      "Validated batch 39 batch loss 0.615451097 batch mAP 0.570617676 batch PCKh 0.4375\n",
      "Validated batch 40 batch loss 0.50638473 batch mAP 0.578643799 batch PCKh 0.25\n",
      "Validated batch 41 batch loss 0.634139538 batch mAP 0.610351562 batch PCKh 0.5625\n",
      "Validated batch 42 batch loss 0.516447306 batch mAP 0.608276367 batch PCKh 0.6875\n",
      "Validated batch 43 batch loss 0.418999642 batch mAP 0.67300415 batch PCKh 0.1875\n",
      "Validated batch 44 batch loss 0.485683143 batch mAP 0.632995605 batch PCKh 0.625\n",
      "Validated batch 45 batch loss 0.521139085 batch mAP 0.591278076 batch PCKh 0.125\n",
      "Validated batch 46 batch loss 0.598902345 batch mAP 0.657409668 batch PCKh 0.5625\n",
      "Validated batch 47 batch loss 0.529009461 batch mAP 0.594238281 batch PCKh 0.625\n",
      "Validated batch 48 batch loss 0.497665107 batch mAP 0.633087158 batch PCKh 0.75\n",
      "Validated batch 49 batch loss 0.516223252 batch mAP 0.631652832 batch PCKh 0.625\n",
      "Validated batch 50 batch loss 0.555755854 batch mAP 0.644348145 batch PCKh 0.75\n",
      "Validated batch 51 batch loss 0.612273157 batch mAP 0.580932617 batch PCKh 0.8125\n",
      "Validated batch 52 batch loss 0.55120492 batch mAP 0.609619141 batch PCKh 0.375\n",
      "Validated batch 53 batch loss 0.582686245 batch mAP 0.65826416 batch PCKh 0.3125\n",
      "Validated batch 54 batch loss 0.488494217 batch mAP 0.681365967 batch PCKh 0.5625\n",
      "Validated batch 55 batch loss 0.577160537 batch mAP 0.606384277 batch PCKh 0.6875\n",
      "Validated batch 56 batch loss 0.645603061 batch mAP 0.6015625 batch PCKh 0.3125\n",
      "Validated batch 57 batch loss 0.708617568 batch mAP 0.566040039 batch PCKh 0.6875\n",
      "Validated batch 58 batch loss 0.642316222 batch mAP 0.621368408 batch PCKh 0.6875\n",
      "Validated batch 59 batch loss 0.458955973 batch mAP 0.666809082 batch PCKh 0.8125\n",
      "Validated batch 60 batch loss 0.535135388 batch mAP 0.675354 batch PCKh 0.875\n",
      "Validated batch 61 batch loss 0.571408868 batch mAP 0.689056396 batch PCKh 0.25\n",
      "Validated batch 62 batch loss 0.58683908 batch mAP 0.654693604 batch PCKh 0.375\n",
      "Validated batch 63 batch loss 0.562546611 batch mAP 0.566467285 batch PCKh 0.0625\n",
      "Validated batch 64 batch loss 0.520925164 batch mAP 0.55960083 batch PCKh 0.5625\n",
      "Validated batch 65 batch loss 0.636659682 batch mAP 0.59854126 batch PCKh 0.6875\n",
      "Validated batch 66 batch loss 0.602470636 batch mAP 0.609558105 batch PCKh 0.5\n",
      "Validated batch 67 batch loss 0.571425319 batch mAP 0.674987793 batch PCKh 0.375\n",
      "Validated batch 68 batch loss 0.58427161 batch mAP 0.620239258 batch PCKh 0.5\n",
      "Validated batch 69 batch loss 0.511319399 batch mAP 0.623718262 batch PCKh 0.75\n",
      "Validated batch 70 batch loss 0.434290737 batch mAP 0.595367432 batch PCKh 0.75\n",
      "Validated batch 71 batch loss 0.353744745 batch mAP 0.699005127 batch PCKh 0.375\n",
      "Validated batch 72 batch loss 0.595123172 batch mAP 0.651397705 batch PCKh 0.75\n",
      "Validated batch 73 batch loss 0.538329184 batch mAP 0.670166 batch PCKh 0.625\n",
      "Validated batch 74 batch loss 0.486914814 batch mAP 0.6640625 batch PCKh 0.5625\n",
      "Validated batch 75 batch loss 0.531489372 batch mAP 0.695129395 batch PCKh 0.375\n",
      "Validated batch 76 batch loss 0.548974633 batch mAP 0.635406494 batch PCKh 0.5625\n",
      "Validated batch 77 batch loss 0.519221961 batch mAP 0.610015869 batch PCKh 0.5625\n",
      "Validated batch 78 batch loss 0.579637 batch mAP 0.551269531 batch PCKh 0.375\n",
      "Validated batch 79 batch loss 0.637161434 batch mAP 0.640350342 batch PCKh 0.6875\n",
      "Validated batch 80 batch loss 0.497484922 batch mAP 0.714324951 batch PCKh 0.8125\n",
      "Validated batch 81 batch loss 0.489702195 batch mAP 0.717834473 batch PCKh 0.375\n",
      "Validated batch 82 batch loss 0.47324574 batch mAP 0.680023193 batch PCKh 0.875\n",
      "Validated batch 83 batch loss 0.635221303 batch mAP 0.665374756 batch PCKh 0.125\n",
      "Validated batch 84 batch loss 0.54990077 batch mAP 0.627288818 batch PCKh 0.0625\n",
      "Validated batch 85 batch loss 0.714943886 batch mAP 0.524536133 batch PCKh 0\n",
      "Validated batch 86 batch loss 0.561937511 batch mAP 0.685882568 batch PCKh 0.375\n",
      "Validated batch 87 batch loss 0.639594436 batch mAP 0.665283203 batch PCKh 0.5625\n",
      "Validated batch 88 batch loss 0.597064435 batch mAP 0.716156 batch PCKh 0.5\n",
      "Validated batch 89 batch loss 0.595716238 batch mAP 0.653503418 batch PCKh 0.6875\n",
      "Validated batch 90 batch loss 0.526695728 batch mAP 0.617279053 batch PCKh 0.75\n",
      "Validated batch 91 batch loss 0.524906337 batch mAP 0.580169678 batch PCKh 0.3125\n",
      "Validated batch 92 batch loss 0.519048214 batch mAP 0.645629883 batch PCKh 0.6875\n",
      "Validated batch 93 batch loss 0.554771066 batch mAP 0.619689941 batch PCKh 0.6875\n",
      "Validated batch 94 batch loss 0.611833155 batch mAP 0.603057861 batch PCKh 0.4375\n",
      "Validated batch 95 batch loss 0.593293726 batch mAP 0.608398438 batch PCKh 0.6875\n",
      "Validated batch 96 batch loss 0.567643881 batch mAP 0.602355957 batch PCKh 0.6875\n",
      "Validated batch 97 batch loss 0.541250706 batch mAP 0.624542236 batch PCKh 0.75\n",
      "Validated batch 98 batch loss 0.575241446 batch mAP 0.624542236 batch PCKh 0.5625\n",
      "Validated batch 99 batch loss 0.514210105 batch mAP 0.644500732 batch PCKh 0.5625\n",
      "Validated batch 100 batch loss 0.630546212 batch mAP 0.588653564 batch PCKh 0.625\n",
      "Validated batch 101 batch loss 0.586535275 batch mAP 0.569000244 batch PCKh 0.1875\n",
      "Validated batch 102 batch loss 0.626269817 batch mAP 0.57925415 batch PCKh 0.6875\n",
      "Validated batch 103 batch loss 0.71898824 batch mAP 0.555633545 batch PCKh 0.6875\n",
      "Validated batch 104 batch loss 0.633686781 batch mAP 0.606658936 batch PCKh 0.3125\n",
      "Validated batch 105 batch loss 0.537618756 batch mAP 0.664581299 batch PCKh 0.375\n",
      "Validated batch 106 batch loss 0.598384142 batch mAP 0.700073242 batch PCKh 0.25\n",
      "Validated batch 107 batch loss 0.579609156 batch mAP 0.612915039 batch PCKh 0.8125\n",
      "Validated batch 108 batch loss 0.635570645 batch mAP 0.637634277 batch PCKh 0.6875\n",
      "Validated batch 109 batch loss 0.54783535 batch mAP 0.639099121 batch PCKh 0.1875\n",
      "Validated batch 110 batch loss 0.584638536 batch mAP 0.639068604 batch PCKh 0.5\n",
      "Validated batch 111 batch loss 0.598390102 batch mAP 0.609893799 batch PCKh 0.1875\n",
      "Validated batch 112 batch loss 0.599939942 batch mAP 0.632415771 batch PCKh 0.875\n",
      "Validated batch 113 batch loss 0.573244214 batch mAP 0.697784424 batch PCKh 0.625\n",
      "Validated batch 114 batch loss 0.559497952 batch mAP 0.623291 batch PCKh 0.875\n",
      "Validated batch 115 batch loss 0.557286859 batch mAP 0.61203 batch PCKh 0.625\n",
      "Validated batch 116 batch loss 0.515486836 batch mAP 0.703887939 batch PCKh 0.5\n",
      "Validated batch 117 batch loss 0.614310443 batch mAP 0.601654053 batch PCKh 0.75\n",
      "Validated batch 118 batch loss 0.715743184 batch mAP 0.631713867 batch PCKh 0.25\n",
      "Validated batch 119 batch loss 0.648255944 batch mAP 0.661621094 batch PCKh 0.625\n",
      "Validated batch 120 batch loss 0.471390307 batch mAP 0.653564453 batch PCKh 0.5625\n",
      "Validated batch 121 batch loss 0.636683702 batch mAP 0.60357666 batch PCKh 0.375\n",
      "Validated batch 122 batch loss 0.657183647 batch mAP 0.631713867 batch PCKh 0.375\n",
      "Validated batch 123 batch loss 0.54311645 batch mAP 0.67855835 batch PCKh 0\n",
      "Validated batch 124 batch loss 0.609235168 batch mAP 0.685821533 batch PCKh 0.75\n",
      "Validated batch 125 batch loss 0.660858393 batch mAP 0.631713867 batch PCKh 0.875\n",
      "Validated batch 126 batch loss 0.599405527 batch mAP 0.534820557 batch PCKh 0.375\n",
      "Validated batch 127 batch loss 0.606412232 batch mAP 0.64050293 batch PCKh 0.6875\n",
      "Validated batch 128 batch loss 0.443409801 batch mAP 0.625061035 batch PCKh 0.4375\n",
      "Validated batch 129 batch loss 0.603335679 batch mAP 0.596710205 batch PCKh 0.875\n",
      "Validated batch 130 batch loss 0.574290693 batch mAP 0.52053833 batch PCKh 0.625\n",
      "Validated batch 131 batch loss 0.579426587 batch mAP 0.692749 batch PCKh 0.1875\n",
      "Validated batch 132 batch loss 0.513468 batch mAP 0.66015625 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 133 batch loss 0.591314793 batch mAP 0.66217041 batch PCKh 0.625\n",
      "Validated batch 134 batch loss 0.558954775 batch mAP 0.637084961 batch PCKh 0.5625\n",
      "Validated batch 135 batch loss 0.612830043 batch mAP 0.608276367 batch PCKh 0.625\n",
      "Validated batch 136 batch loss 0.583850503 batch mAP 0.650299072 batch PCKh 0.25\n",
      "Validated batch 137 batch loss 0.520742297 batch mAP 0.658508301 batch PCKh 0.625\n",
      "Validated batch 138 batch loss 0.609917104 batch mAP 0.55960083 batch PCKh 0.625\n",
      "Validated batch 139 batch loss 0.600113 batch mAP 0.621276855 batch PCKh 0.375\n",
      "Validated batch 140 batch loss 0.602152109 batch mAP 0.607208252 batch PCKh 0.875\n",
      "Validated batch 141 batch loss 0.57917279 batch mAP 0.588592529 batch PCKh 0.75\n",
      "Validated batch 142 batch loss 0.591994822 batch mAP 0.623809814 batch PCKh 0.75\n",
      "Validated batch 143 batch loss 0.537405491 batch mAP 0.645965576 batch PCKh 0.0625\n",
      "Validated batch 144 batch loss 0.548359275 batch mAP 0.619812 batch PCKh 0.375\n",
      "Validated batch 145 batch loss 0.503341198 batch mAP 0.637390137 batch PCKh 0.5\n",
      "Validated batch 146 batch loss 0.55837512 batch mAP 0.662536621 batch PCKh 0.6875\n",
      "Validated batch 147 batch loss 0.571691811 batch mAP 0.64666748 batch PCKh 0.4375\n",
      "Validated batch 148 batch loss 0.601825595 batch mAP 0.67276 batch PCKh 0.375\n",
      "Validated batch 149 batch loss 0.589616895 batch mAP 0.653015137 batch PCKh 0.75\n",
      "Validated batch 150 batch loss 0.646203756 batch mAP 0.739135742 batch PCKh 0.9375\n",
      "Validated batch 151 batch loss 0.611528933 batch mAP 0.592651367 batch PCKh 0.5\n",
      "Validated batch 152 batch loss 0.62747097 batch mAP 0.599823 batch PCKh 0.25\n",
      "Validated batch 153 batch loss 0.615803123 batch mAP 0.584442139 batch PCKh 0.3125\n",
      "Validated batch 154 batch loss 0.642220795 batch mAP 0.577819824 batch PCKh 0.375\n",
      "Validated batch 155 batch loss 0.619660318 batch mAP 0.586883545 batch PCKh 0.125\n",
      "Validated batch 156 batch loss 0.577341557 batch mAP 0.617492676 batch PCKh 0.125\n",
      "Validated batch 157 batch loss 0.54946512 batch mAP 0.617218 batch PCKh 0.6875\n",
      "Validated batch 158 batch loss 0.546227098 batch mAP 0.716156 batch PCKh 0.625\n",
      "Validated batch 159 batch loss 0.617176414 batch mAP 0.667419434 batch PCKh 0.5\n",
      "Validated batch 160 batch loss 0.57495445 batch mAP 0.636047363 batch PCKh 0.5\n",
      "Validated batch 161 batch loss 0.605965614 batch mAP 0.616485596 batch PCKh 0.3125\n",
      "Validated batch 162 batch loss 0.519915879 batch mAP 0.591278076 batch PCKh 0.5\n",
      "Validated batch 163 batch loss 0.541196883 batch mAP 0.517181396 batch PCKh 0.25\n",
      "Validated batch 164 batch loss 0.584765732 batch mAP 0.609863281 batch PCKh 0.625\n",
      "Validated batch 165 batch loss 0.66497916 batch mAP 0.57901 batch PCKh 0.6875\n",
      "Validated batch 166 batch loss 0.619136 batch mAP 0.568634033 batch PCKh 0.3125\n",
      "Validated batch 167 batch loss 0.658423126 batch mAP 0.537841797 batch PCKh 0\n",
      "Validated batch 168 batch loss 0.606868923 batch mAP 0.559204102 batch PCKh 0.0625\n",
      "Validated batch 169 batch loss 0.574195 batch mAP 0.608215332 batch PCKh 0.5625\n",
      "Validated batch 170 batch loss 0.623517871 batch mAP 0.641479492 batch PCKh 0.25\n",
      "Validated batch 171 batch loss 0.588299453 batch mAP 0.66607666 batch PCKh 0.5625\n",
      "Validated batch 172 batch loss 0.725965738 batch mAP 0.511383057 batch PCKh 0\n",
      "Validated batch 173 batch loss 0.527970672 batch mAP 0.552978516 batch PCKh 0.5\n",
      "Validated batch 174 batch loss 0.535407484 batch mAP 0.624359131 batch PCKh 0.125\n",
      "Validated batch 175 batch loss 0.576964 batch mAP 0.624237061 batch PCKh 0.1875\n",
      "Validated batch 176 batch loss 0.634373 batch mAP 0.548431396 batch PCKh 0.5\n",
      "Validated batch 177 batch loss 0.540891469 batch mAP 0.679870605 batch PCKh 0.5\n",
      "Validated batch 178 batch loss 0.469272077 batch mAP 0.694488525 batch PCKh 0.4375\n",
      "Validated batch 179 batch loss 0.469908565 batch mAP 0.651641846 batch PCKh 0.5625\n",
      "Validated batch 180 batch loss 0.612336338 batch mAP 0.559967041 batch PCKh 0.5625\n",
      "Validated batch 181 batch loss 0.580275416 batch mAP 0.691314697 batch PCKh 0.375\n",
      "Validated batch 182 batch loss 0.610348165 batch mAP 0.623016357 batch PCKh 0.5625\n",
      "Validated batch 183 batch loss 0.513047457 batch mAP 0.625152588 batch PCKh 0.625\n",
      "Validated batch 184 batch loss 0.523353 batch mAP 0.668945312 batch PCKh 0.75\n",
      "Validated batch 185 batch loss 0.554205477 batch mAP 0.574035645 batch PCKh 0.4375\n",
      "Validated batch 186 batch loss 0.558770478 batch mAP 0.552764893 batch PCKh 0.125\n",
      "Validated batch 187 batch loss 0.533312917 batch mAP 0.61541748 batch PCKh 0.4375\n",
      "Validated batch 188 batch loss 0.510956883 batch mAP 0.588806152 batch PCKh 0.3125\n",
      "Validated batch 189 batch loss 0.5278157 batch mAP 0.662506104 batch PCKh 0.75\n",
      "Validated batch 190 batch loss 0.565200686 batch mAP 0.601928711 batch PCKh 0.5625\n",
      "Validated batch 191 batch loss 0.587187886 batch mAP 0.672515869 batch PCKh 0.3125\n",
      "Validated batch 192 batch loss 0.499052405 batch mAP 0.712036133 batch PCKh 0.5625\n",
      "Validated batch 193 batch loss 0.584025145 batch mAP 0.599731445 batch PCKh 0.75\n",
      "Validated batch 194 batch loss 0.490041494 batch mAP 0.630249 batch PCKh 0.6875\n",
      "Validated batch 195 batch loss 0.552556396 batch mAP 0.636199951 batch PCKh 0.3125\n",
      "Validated batch 196 batch loss 0.629460692 batch mAP 0.551269531 batch PCKh 0.8125\n",
      "Validated batch 197 batch loss 0.504461 batch mAP 0.623382568 batch PCKh 0.6875\n",
      "Validated batch 198 batch loss 0.630557477 batch mAP 0.557373047 batch PCKh 0.125\n",
      "Validated batch 199 batch loss 0.576779127 batch mAP 0.602111816 batch PCKh 0.375\n",
      "Validated batch 200 batch loss 0.602724075 batch mAP 0.598297119 batch PCKh 0.4375\n",
      "Validated batch 201 batch loss 0.580595613 batch mAP 0.525543213 batch PCKh 0.75\n",
      "Validated batch 202 batch loss 0.555103481 batch mAP 0.677246094 batch PCKh 0.5625\n",
      "Validated batch 203 batch loss 0.607928455 batch mAP 0.612060547 batch PCKh 0.375\n",
      "Validated batch 204 batch loss 0.523454368 batch mAP 0.528564453 batch PCKh 0.4375\n",
      "Validated batch 205 batch loss 0.575406313 batch mAP 0.65145874 batch PCKh 0.25\n",
      "Validated batch 206 batch loss 0.621283412 batch mAP 0.649230957 batch PCKh 0.75\n",
      "Validated batch 207 batch loss 0.68824172 batch mAP 0.566009521 batch PCKh 0.6875\n",
      "Validated batch 208 batch loss 0.689034343 batch mAP 0.547149658 batch PCKh 0.0625\n",
      "Validated batch 209 batch loss 0.613994777 batch mAP 0.637451172 batch PCKh 0.6875\n",
      "Validated batch 210 batch loss 0.60159719 batch mAP 0.479125977 batch PCKh 0.5\n",
      "Validated batch 211 batch loss 0.6254794 batch mAP 0.6456604 batch PCKh 0.5\n",
      "Validated batch 212 batch loss 0.613813281 batch mAP 0.586700439 batch PCKh 0.0625\n",
      "Validated batch 213 batch loss 0.642632961 batch mAP 0.648407 batch PCKh 0.6875\n",
      "Validated batch 214 batch loss 0.611090302 batch mAP 0.606781 batch PCKh 0.6875\n",
      "Validated batch 215 batch loss 0.664142132 batch mAP 0.571929932 batch PCKh 0.75\n",
      "Validated batch 216 batch loss 0.633570492 batch mAP 0.657867432 batch PCKh 0.75\n",
      "Validated batch 217 batch loss 0.63931179 batch mAP 0.551971436 batch PCKh 0.5\n",
      "Validated batch 218 batch loss 0.598017573 batch mAP 0.63961792 batch PCKh 0.25\n",
      "Validated batch 219 batch loss 0.6088745 batch mAP 0.685333252 batch PCKh 0.6875\n",
      "Validated batch 220 batch loss 0.47586754 batch mAP 0.583526611 batch PCKh 0.25\n",
      "Validated batch 221 batch loss 0.542719424 batch mAP 0.658538818 batch PCKh 0.5\n",
      "Validated batch 222 batch loss 0.607596755 batch mAP 0.634246826 batch PCKh 0.375\n",
      "Validated batch 223 batch loss 0.615984321 batch mAP 0.557739258 batch PCKh 0.1875\n",
      "Validated batch 224 batch loss 0.583136082 batch mAP 0.68270874 batch PCKh 0.3125\n",
      "Validated batch 225 batch loss 0.642752 batch mAP 0.611785889 batch PCKh 0.75\n",
      "Validated batch 226 batch loss 0.571182668 batch mAP 0.585388184 batch PCKh 0.625\n",
      "Validated batch 227 batch loss 0.526015 batch mAP 0.694366455 batch PCKh 0.3125\n",
      "Validated batch 228 batch loss 0.571151137 batch mAP 0.597137451 batch PCKh 0.5625\n",
      "Validated batch 229 batch loss 0.622835934 batch mAP 0.585632324 batch PCKh 0.375\n",
      "Validated batch 230 batch loss 0.528558 batch mAP 0.589752197 batch PCKh 0.5\n",
      "Validated batch 231 batch loss 0.615743 batch mAP 0.557251 batch PCKh 0.5\n",
      "Validated batch 232 batch loss 0.662350655 batch mAP 0.580749512 batch PCKh 0.1875\n",
      "Validated batch 233 batch loss 0.545863628 batch mAP 0.621002197 batch PCKh 0.25\n",
      "Validated batch 234 batch loss 0.590192318 batch mAP 0.627807617 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 235 batch loss 0.584193766 batch mAP 0.628662109 batch PCKh 0.625\n",
      "Validated batch 236 batch loss 0.495492727 batch mAP 0.684509277 batch PCKh 0.5625\n",
      "Validated batch 237 batch loss 0.579105675 batch mAP 0.697235107 batch PCKh 0.5\n",
      "Validated batch 238 batch loss 0.580269337 batch mAP 0.711151123 batch PCKh 0.625\n",
      "Validated batch 239 batch loss 0.535849333 batch mAP 0.633270264 batch PCKh 0.875\n",
      "Validated batch 240 batch loss 0.522593319 batch mAP 0.707305908 batch PCKh 0.625\n",
      "Validated batch 241 batch loss 0.586010575 batch mAP 0.624328613 batch PCKh 0.375\n",
      "Validated batch 242 batch loss 0.559120357 batch mAP 0.703155518 batch PCKh 0.8125\n",
      "Validated batch 243 batch loss 0.620372355 batch mAP 0.657562256 batch PCKh 0.5625\n",
      "Validated batch 244 batch loss 0.590697765 batch mAP 0.613952637 batch PCKh 0.6875\n",
      "Validated batch 245 batch loss 0.60406369 batch mAP 0.685211182 batch PCKh 0.125\n",
      "Validated batch 246 batch loss 0.485519588 batch mAP 0.604126 batch PCKh 0.5\n",
      "Validated batch 247 batch loss 0.521509945 batch mAP 0.653656 batch PCKh 0.5625\n",
      "Validated batch 248 batch loss 0.551690459 batch mAP 0.688079834 batch PCKh 0.25\n",
      "Validated batch 249 batch loss 0.619176865 batch mAP 0.651519775 batch PCKh 0.5625\n",
      "Validated batch 250 batch loss 0.564894259 batch mAP 0.644622803 batch PCKh 0.4375\n",
      "Validated batch 251 batch loss 0.604199409 batch mAP 0.691497803 batch PCKh 0.5625\n",
      "Validated batch 252 batch loss 0.602882 batch mAP 0.599365234 batch PCKh 0.125\n",
      "Validated batch 253 batch loss 0.668844163 batch mAP 0.540863037 batch PCKh 0.5\n",
      "Validated batch 254 batch loss 0.587367892 batch mAP 0.575805664 batch PCKh 0.5625\n",
      "Validated batch 255 batch loss 0.555815101 batch mAP 0.612518311 batch PCKh 0.4375\n",
      "Validated batch 256 batch loss 0.581327736 batch mAP 0.593933105 batch PCKh 0.375\n",
      "Validated batch 257 batch loss 0.467435658 batch mAP 0.599700928 batch PCKh 0.3125\n",
      "Validated batch 258 batch loss 0.553811133 batch mAP 0.571655273 batch PCKh 0.6875\n",
      "Validated batch 259 batch loss 0.616657376 batch mAP 0.604553223 batch PCKh 0.1875\n",
      "Validated batch 260 batch loss 0.47663492 batch mAP 0.693817139 batch PCKh 0.3125\n",
      "Validated batch 261 batch loss 0.587065876 batch mAP 0.571777344 batch PCKh 0.625\n",
      "Validated batch 262 batch loss 0.536612809 batch mAP 0.631530762 batch PCKh 0.625\n",
      "Validated batch 263 batch loss 0.50353688 batch mAP 0.666473389 batch PCKh 0.3125\n",
      "Validated batch 264 batch loss 0.640407085 batch mAP 0.592163086 batch PCKh 0.5\n",
      "Validated batch 265 batch loss 0.553970933 batch mAP 0.5675354 batch PCKh 0.25\n",
      "Validated batch 266 batch loss 0.584530771 batch mAP 0.637634277 batch PCKh 0.375\n",
      "Validated batch 267 batch loss 0.590551794 batch mAP 0.648864746 batch PCKh 0.625\n",
      "Validated batch 268 batch loss 0.585099578 batch mAP 0.656921387 batch PCKh 0.375\n",
      "Validated batch 269 batch loss 0.63815695 batch mAP 0.644287109 batch PCKh 0.3125\n",
      "Validated batch 270 batch loss 0.692288876 batch mAP 0.555114746 batch PCKh 0.3125\n",
      "Validated batch 271 batch loss 0.71373415 batch mAP 0.578369141 batch PCKh 0.3125\n",
      "Validated batch 272 batch loss 0.583257556 batch mAP 0.69052124 batch PCKh 0.625\n",
      "Validated batch 273 batch loss 0.506350219 batch mAP 0.666687 batch PCKh 0.625\n",
      "Validated batch 274 batch loss 0.625930071 batch mAP 0.596923828 batch PCKh 0.5625\n",
      "Validated batch 275 batch loss 0.516460478 batch mAP 0.582214355 batch PCKh 0.5\n",
      "Validated batch 276 batch loss 0.508985519 batch mAP 0.661468506 batch PCKh 0.125\n",
      "Validated batch 277 batch loss 0.441612184 batch mAP 0.68939209 batch PCKh 0.6875\n",
      "Validated batch 278 batch loss 0.551243722 batch mAP 0.617004395 batch PCKh 0.5625\n",
      "Validated batch 279 batch loss 0.578108311 batch mAP 0.621551514 batch PCKh 0.125\n",
      "Validated batch 280 batch loss 0.571644127 batch mAP 0.590240479 batch PCKh 0.5\n",
      "Validated batch 281 batch loss 0.540260077 batch mAP 0.568450928 batch PCKh 0.625\n",
      "Validated batch 282 batch loss 0.54153049 batch mAP 0.551971436 batch PCKh 0.125\n",
      "Validated batch 283 batch loss 0.471011817 batch mAP 0.668182373 batch PCKh 0.1875\n",
      "Validated batch 284 batch loss 0.5645 batch mAP 0.616577148 batch PCKh 0\n",
      "Validated batch 285 batch loss 0.567029 batch mAP 0.68157959 batch PCKh 0.875\n",
      "Validated batch 286 batch loss 0.516725719 batch mAP 0.649353 batch PCKh 0.1875\n",
      "Validated batch 287 batch loss 0.542036891 batch mAP 0.657409668 batch PCKh 0.4375\n",
      "Validated batch 288 batch loss 0.721471429 batch mAP 0.573486328 batch PCKh 0\n",
      "Validated batch 289 batch loss 0.527148068 batch mAP 0.663879395 batch PCKh 0.5625\n",
      "Validated batch 290 batch loss 0.487641513 batch mAP 0.649475098 batch PCKh 0.5625\n",
      "Validated batch 291 batch loss 0.603766382 batch mAP 0.602600098 batch PCKh 0.5\n",
      "Validated batch 292 batch loss 0.429955751 batch mAP 0.682525635 batch PCKh 0.375\n",
      "Validated batch 293 batch loss 0.499921381 batch mAP 0.71774292 batch PCKh 0.4375\n",
      "Validated batch 294 batch loss 0.566946447 batch mAP 0.662414551 batch PCKh 0.75\n",
      "Validated batch 295 batch loss 0.547563791 batch mAP 0.69619751 batch PCKh 0.1875\n",
      "Validated batch 296 batch loss 0.576250315 batch mAP 0.567840576 batch PCKh 0.125\n",
      "Validated batch 297 batch loss 0.479795337 batch mAP 0.706848145 batch PCKh 0.5\n",
      "Validated batch 298 batch loss 0.512669623 batch mAP 0.666900635 batch PCKh 0.625\n",
      "Validated batch 299 batch loss 0.579633057 batch mAP 0.559356689 batch PCKh 0.125\n",
      "Validated batch 300 batch loss 0.578812659 batch mAP 0.581573486 batch PCKh 0\n",
      "Validated batch 301 batch loss 0.493057102 batch mAP 0.558776855 batch PCKh 0\n",
      "Validated batch 302 batch loss 0.509299219 batch mAP 0.635467529 batch PCKh 0.4375\n",
      "Validated batch 303 batch loss 0.617649376 batch mAP 0.589141846 batch PCKh 0.6875\n",
      "Validated batch 304 batch loss 0.48280865 batch mAP 0.670898438 batch PCKh 0.3125\n",
      "Validated batch 305 batch loss 0.600663424 batch mAP 0.651001 batch PCKh 0.875\n",
      "Validated batch 306 batch loss 0.636077404 batch mAP 0.583831787 batch PCKh 0.6875\n",
      "Validated batch 307 batch loss 0.645936608 batch mAP 0.56741333 batch PCKh 0.1875\n",
      "Validated batch 308 batch loss 0.623765647 batch mAP 0.627105713 batch PCKh 0.3125\n",
      "Validated batch 309 batch loss 0.520898342 batch mAP 0.658050537 batch PCKh 0.5625\n",
      "Validated batch 310 batch loss 0.534456849 batch mAP 0.605133057 batch PCKh 0.75\n",
      "Validated batch 311 batch loss 0.622172356 batch mAP 0.618713379 batch PCKh 0.25\n",
      "Validated batch 312 batch loss 0.629966617 batch mAP 0.566589355 batch PCKh 0.0625\n",
      "Validated batch 313 batch loss 0.486042857 batch mAP 0.683776855 batch PCKh 0.3125\n",
      "Validated batch 314 batch loss 0.443998486 batch mAP 0.711578369 batch PCKh 0.375\n",
      "Validated batch 315 batch loss 0.551642179 batch mAP 0.551269531 batch PCKh 0\n",
      "Validated batch 316 batch loss 0.457772017 batch mAP 0.687255859 batch PCKh 0.3125\n",
      "Validated batch 317 batch loss 0.602299392 batch mAP 0.53137207 batch PCKh 0.5\n",
      "Validated batch 318 batch loss 0.474514842 batch mAP 0.698303223 batch PCKh 0.6875\n",
      "Validated batch 319 batch loss 0.474786282 batch mAP 0.677886963 batch PCKh 0.5625\n",
      "Validated batch 320 batch loss 0.597347856 batch mAP 0.624603271 batch PCKh 0.3125\n",
      "Validated batch 321 batch loss 0.600285888 batch mAP 0.566680908 batch PCKh 0.5625\n",
      "Validated batch 322 batch loss 0.613392472 batch mAP 0.56741333 batch PCKh 0.625\n",
      "Validated batch 323 batch loss 0.615382195 batch mAP 0.534179688 batch PCKh 0.1875\n",
      "Validated batch 324 batch loss 0.545606375 batch mAP 0.672699 batch PCKh 0.4375\n",
      "Validated batch 325 batch loss 0.542278051 batch mAP 0.556549072 batch PCKh 0.375\n",
      "Validated batch 326 batch loss 0.494310528 batch mAP 0.637908936 batch PCKh 0.5625\n",
      "Validated batch 327 batch loss 0.586381793 batch mAP 0.590271 batch PCKh 0.25\n",
      "Validated batch 328 batch loss 0.548568845 batch mAP 0.519897461 batch PCKh 0.5625\n",
      "Validated batch 329 batch loss 0.529812694 batch mAP 0.520263672 batch PCKh 0.125\n",
      "Validated batch 330 batch loss 0.53001 batch mAP 0.606628418 batch PCKh 0.625\n",
      "Validated batch 331 batch loss 0.473186493 batch mAP 0.585418701 batch PCKh 0.75\n",
      "Validated batch 332 batch loss 0.525654197 batch mAP 0.643859863 batch PCKh 0.75\n",
      "Validated batch 333 batch loss 0.574416518 batch mAP 0.586181641 batch PCKh 0.5625\n",
      "Validated batch 334 batch loss 0.631557 batch mAP 0.549926758 batch PCKh 0.375\n",
      "Validated batch 335 batch loss 0.565301716 batch mAP 0.627502441 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 336 batch loss 0.499715805 batch mAP 0.623260498 batch PCKh 0.5\n",
      "Validated batch 337 batch loss 0.621472716 batch mAP 0.438903809 batch PCKh 0.25\n",
      "Validated batch 338 batch loss 0.535997748 batch mAP 0.703430176 batch PCKh 0.6875\n",
      "Validated batch 339 batch loss 0.63780272 batch mAP 0.542144775 batch PCKh 0.5\n",
      "Validated batch 340 batch loss 0.570717096 batch mAP 0.56729126 batch PCKh 0.6875\n",
      "Validated batch 341 batch loss 0.475590527 batch mAP 0.695831299 batch PCKh 0.25\n",
      "Validated batch 342 batch loss 0.480861187 batch mAP 0.710632324 batch PCKh 0.375\n",
      "Validated batch 343 batch loss 0.661669254 batch mAP 0.647216797 batch PCKh 0.4375\n",
      "Validated batch 344 batch loss 0.567068815 batch mAP 0.605987549 batch PCKh 0.8125\n",
      "Validated batch 345 batch loss 0.539765239 batch mAP 0.639373779 batch PCKh 0.125\n",
      "Validated batch 346 batch loss 0.605744183 batch mAP 0.611175537 batch PCKh 0.375\n",
      "Validated batch 347 batch loss 0.483061969 batch mAP 0.610961914 batch PCKh 0.4375\n",
      "Validated batch 348 batch loss 0.518439353 batch mAP 0.651153564 batch PCKh 0.1875\n",
      "Validated batch 349 batch loss 0.629401088 batch mAP 0.609008789 batch PCKh 0.1875\n",
      "Validated batch 350 batch loss 0.521656334 batch mAP 0.605102539 batch PCKh 0.5\n",
      "Validated batch 351 batch loss 0.624762297 batch mAP 0.60736084 batch PCKh 0.6875\n",
      "Validated batch 352 batch loss 0.562417448 batch mAP 0.593841553 batch PCKh 0.3125\n",
      "Validated batch 353 batch loss 0.563217 batch mAP 0.639984131 batch PCKh 0.25\n",
      "Validated batch 354 batch loss 0.511000395 batch mAP 0.624176 batch PCKh 0.5\n",
      "Validated batch 355 batch loss 0.6047436 batch mAP 0.640930176 batch PCKh 0.375\n",
      "Validated batch 356 batch loss 0.65951 batch mAP 0.52835083 batch PCKh 0.0625\n",
      "Validated batch 357 batch loss 0.625878692 batch mAP 0.602142334 batch PCKh 0.625\n",
      "Validated batch 358 batch loss 0.546715319 batch mAP 0.605712891 batch PCKh 0.25\n",
      "Validated batch 359 batch loss 0.668412805 batch mAP 0.53793335 batch PCKh 0.375\n",
      "Validated batch 360 batch loss 0.562498033 batch mAP 0.684082031 batch PCKh 0.75\n",
      "Validated batch 361 batch loss 0.429483414 batch mAP 0.697418213 batch PCKh 0.5\n",
      "Validated batch 362 batch loss 0.581815064 batch mAP 0.471008301 batch PCKh 0.4375\n",
      "Validated batch 363 batch loss 0.629798532 batch mAP 0.50112915 batch PCKh 0.375\n",
      "Validated batch 364 batch loss 0.569988549 batch mAP 0.650695801 batch PCKh 0.5\n",
      "Validated batch 365 batch loss 0.59537369 batch mAP 0.622894287 batch PCKh 0.25\n",
      "Validated batch 366 batch loss 0.545397937 batch mAP 0.489074707 batch PCKh 0.625\n",
      "Validated batch 367 batch loss 0.457383156 batch mAP 0.699066162 batch PCKh 0.6875\n",
      "Validated batch 368 batch loss 0.569371343 batch mAP 0.605316162 batch PCKh 0.5625\n",
      "Validated batch 369 batch loss 0.588325858 batch mAP 0.633575439 batch PCKh 0.5625\n",
      "Epoch 7 val loss 0.5719015598297119 val mAP 0.620590090751648 val PCKh\n",
      "Epoch 7 completed in 763.23 seconds\n",
      "Model /aiffel/aiffel/model_weight/GD08/y_model-epoch-7-loss-0.5719.h5 saved.\n",
      "Start epoch 8 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.681192875 batch mAP 0.590393066 batch PCKh 0.1875\n",
      "Trained batch 2 batch loss 0.585269928 batch mAP 0.556610107 batch PCKh 0.0625\n",
      "Trained batch 3 batch loss 0.578491271 batch mAP 0.620880127 batch PCKh 0.625\n",
      "Trained batch 4 batch loss 0.529022455 batch mAP 0.645935059 batch PCKh 0.6875\n",
      "Trained batch 5 batch loss 0.531750202 batch mAP 0.600280762 batch PCKh 0.75\n",
      "Trained batch 6 batch loss 0.530022144 batch mAP 0.595214844 batch PCKh 0.6875\n",
      "Trained batch 7 batch loss 0.594515443 batch mAP 0.646270752 batch PCKh 0.4375\n",
      "Trained batch 8 batch loss 0.639567852 batch mAP 0.574890137 batch PCKh 0.5625\n",
      "Trained batch 9 batch loss 0.547465324 batch mAP 0.610687256 batch PCKh 0.5625\n",
      "Trained batch 10 batch loss 0.554322302 batch mAP 0.569549561 batch PCKh 0.6875\n",
      "Trained batch 11 batch loss 0.633478403 batch mAP 0.597167969 batch PCKh 0.25\n",
      "Trained batch 12 batch loss 0.480383694 batch mAP 0.588409424 batch PCKh 0.1875\n",
      "Trained batch 13 batch loss 0.598434 batch mAP 0.623443604 batch PCKh 0.75\n",
      "Trained batch 14 batch loss 0.517071903 batch mAP 0.643463135 batch PCKh 0.3125\n",
      "Trained batch 15 batch loss 0.464278102 batch mAP 0.610168457 batch PCKh 0.4375\n",
      "Trained batch 16 batch loss 0.506221533 batch mAP 0.637573242 batch PCKh 0.125\n",
      "Trained batch 17 batch loss 0.552878261 batch mAP 0.587219238 batch PCKh 0.4375\n",
      "Trained batch 18 batch loss 0.47457993 batch mAP 0.618255615 batch PCKh 0.6875\n",
      "Trained batch 19 batch loss 0.487184465 batch mAP 0.657531738 batch PCKh 0.625\n",
      "Trained batch 20 batch loss 0.535612166 batch mAP 0.596588135 batch PCKh 0.125\n",
      "Trained batch 21 batch loss 0.576128244 batch mAP 0.574066162 batch PCKh 0.6875\n",
      "Trained batch 22 batch loss 0.544592142 batch mAP 0.596374512 batch PCKh 0.5\n",
      "Trained batch 23 batch loss 0.550288141 batch mAP 0.620605469 batch PCKh 0.8125\n",
      "Trained batch 24 batch loss 0.567251384 batch mAP 0.617126465 batch PCKh 0.5\n",
      "Trained batch 25 batch loss 0.564786136 batch mAP 0.599945068 batch PCKh 0.625\n",
      "Trained batch 26 batch loss 0.617485642 batch mAP 0.589874268 batch PCKh 0.25\n",
      "Trained batch 27 batch loss 0.585676908 batch mAP 0.519378662 batch PCKh 0.4375\n",
      "Trained batch 28 batch loss 0.485434294 batch mAP 0.608917236 batch PCKh 0.625\n",
      "Trained batch 29 batch loss 0.527408123 batch mAP 0.667572 batch PCKh 0.6875\n",
      "Trained batch 30 batch loss 0.59600389 batch mAP 0.613830566 batch PCKh 0.5\n",
      "Trained batch 31 batch loss 0.602018058 batch mAP 0.592926 batch PCKh 0.0625\n",
      "Trained batch 32 batch loss 0.441981167 batch mAP 0.668670654 batch PCKh 0.625\n",
      "Trained batch 33 batch loss 0.456636786 batch mAP 0.673614502 batch PCKh 0.5\n",
      "Trained batch 34 batch loss 0.424287796 batch mAP 0.679107666 batch PCKh 0.4375\n",
      "Trained batch 35 batch loss 0.438457221 batch mAP 0.657440186 batch PCKh 0.5\n",
      "Trained batch 36 batch loss 0.469358623 batch mAP 0.60144043 batch PCKh 0.3125\n",
      "Trained batch 37 batch loss 0.453816235 batch mAP 0.580474854 batch PCKh 0.3125\n",
      "Trained batch 38 batch loss 0.48933965 batch mAP 0.551574707 batch PCKh 0.4375\n",
      "Trained batch 39 batch loss 0.485168278 batch mAP 0.586395264 batch PCKh 0.5\n",
      "Trained batch 40 batch loss 0.582795262 batch mAP 0.518371582 batch PCKh 0.25\n",
      "Trained batch 41 batch loss 0.53450489 batch mAP 0.578033447 batch PCKh 0.3125\n",
      "Trained batch 42 batch loss 0.589125156 batch mAP 0.642028809 batch PCKh 0.4375\n",
      "Trained batch 43 batch loss 0.502765417 batch mAP 0.716888428 batch PCKh 0.4375\n",
      "Trained batch 44 batch loss 0.58096242 batch mAP 0.655029297 batch PCKh 0.5625\n",
      "Trained batch 45 batch loss 0.501924574 batch mAP 0.621612549 batch PCKh 0.125\n",
      "Trained batch 46 batch loss 0.566446841 batch mAP 0.593170166 batch PCKh 0.125\n",
      "Trained batch 47 batch loss 0.548814535 batch mAP 0.669250488 batch PCKh 0.3125\n",
      "Trained batch 48 batch loss 0.45795548 batch mAP 0.681152344 batch PCKh 0.375\n",
      "Trained batch 49 batch loss 0.55064404 batch mAP 0.627868652 batch PCKh 0.5625\n",
      "Trained batch 50 batch loss 0.535003185 batch mAP 0.574584961 batch PCKh 0.5625\n",
      "Trained batch 51 batch loss 0.553775489 batch mAP 0.63797 batch PCKh 0.4375\n",
      "Trained batch 52 batch loss 0.479217798 batch mAP 0.647064209 batch PCKh 0.625\n",
      "Trained batch 53 batch loss 0.527162552 batch mAP 0.651123047 batch PCKh 0.8125\n",
      "Trained batch 54 batch loss 0.550575376 batch mAP 0.605712891 batch PCKh 0.5\n",
      "Trained batch 55 batch loss 0.579816759 batch mAP 0.594146729 batch PCKh 0.875\n",
      "Trained batch 56 batch loss 0.58665061 batch mAP 0.492828369 batch PCKh 0.0625\n",
      "Trained batch 57 batch loss 0.596564889 batch mAP 0.532562256 batch PCKh 0.5625\n",
      "Trained batch 58 batch loss 0.627136946 batch mAP 0.499969482 batch PCKh 0.4375\n",
      "Trained batch 59 batch loss 0.5649243 batch mAP 0.544677734 batch PCKh 0.5625\n",
      "Trained batch 60 batch loss 0.481055379 batch mAP 0.653717041 batch PCKh 0.6875\n",
      "Trained batch 61 batch loss 0.466537863 batch mAP 0.613616943 batch PCKh 0\n",
      "Trained batch 62 batch loss 0.562494099 batch mAP 0.66317749 batch PCKh 0.375\n",
      "Trained batch 63 batch loss 0.533545673 batch mAP 0.449462891 batch PCKh 0.375\n",
      "Trained batch 64 batch loss 0.545894146 batch mAP 0.554779053 batch PCKh 0.125\n",
      "Trained batch 65 batch loss 0.559006929 batch mAP 0.537689209 batch PCKh 0.75\n",
      "Trained batch 66 batch loss 0.566012502 batch mAP 0.531402588 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 67 batch loss 0.48933658 batch mAP 0.526123047 batch PCKh 0.8125\n",
      "Trained batch 68 batch loss 0.584681392 batch mAP 0.505493164 batch PCKh 0.4375\n",
      "Trained batch 69 batch loss 0.54109472 batch mAP 0.501983643 batch PCKh 0.5625\n",
      "Trained batch 70 batch loss 0.539164245 batch mAP 0.583679199 batch PCKh 0.5625\n",
      "Trained batch 71 batch loss 0.466397554 batch mAP 0.574798584 batch PCKh 0.375\n",
      "Trained batch 72 batch loss 0.508814752 batch mAP 0.608734131 batch PCKh 0.75\n",
      "Trained batch 73 batch loss 0.58498472 batch mAP 0.509674072 batch PCKh 0.5625\n",
      "Trained batch 74 batch loss 0.575936258 batch mAP 0.597167969 batch PCKh 0.75\n",
      "Trained batch 75 batch loss 0.585734129 batch mAP 0.538757324 batch PCKh 0.375\n",
      "Trained batch 76 batch loss 0.573979259 batch mAP 0.530059814 batch PCKh 0.3125\n",
      "Trained batch 77 batch loss 0.603983283 batch mAP 0.54675293 batch PCKh 0.1875\n",
      "Trained batch 78 batch loss 0.578890502 batch mAP 0.600769043 batch PCKh 0.875\n",
      "Trained batch 79 batch loss 0.572027504 batch mAP 0.566833496 batch PCKh 0.4375\n",
      "Trained batch 80 batch loss 0.51678741 batch mAP 0.56060791 batch PCKh 0.6875\n",
      "Trained batch 81 batch loss 0.547423422 batch mAP 0.530731201 batch PCKh 0.4375\n",
      "Trained batch 82 batch loss 0.617524147 batch mAP 0.576355 batch PCKh 0.1875\n",
      "Trained batch 83 batch loss 0.560373604 batch mAP 0.536132812 batch PCKh 0.4375\n",
      "Trained batch 84 batch loss 0.614262462 batch mAP 0.516265869 batch PCKh 0.5625\n",
      "Trained batch 85 batch loss 0.574271202 batch mAP 0.554626465 batch PCKh 0.6875\n",
      "Trained batch 86 batch loss 0.484217048 batch mAP 0.488098145 batch PCKh 0.75\n",
      "Trained batch 87 batch loss 0.51574713 batch mAP 0.538604736 batch PCKh 0.5\n",
      "Trained batch 88 batch loss 0.46334517 batch mAP 0.49810791 batch PCKh 0.5\n",
      "Trained batch 89 batch loss 0.45776087 batch mAP 0.498687744 batch PCKh 0.75\n",
      "Trained batch 90 batch loss 0.437319547 batch mAP 0.503723145 batch PCKh 0.375\n",
      "Trained batch 91 batch loss 0.51255 batch mAP 0.497497559 batch PCKh 0.75\n",
      "Trained batch 92 batch loss 0.505657315 batch mAP 0.517578125 batch PCKh 0.6875\n",
      "Trained batch 93 batch loss 0.553213 batch mAP 0.511138916 batch PCKh 0.125\n",
      "Trained batch 94 batch loss 0.617233276 batch mAP 0.496734619 batch PCKh 0.375\n",
      "Trained batch 95 batch loss 0.520376682 batch mAP 0.580169678 batch PCKh 0.25\n",
      "Trained batch 96 batch loss 0.464929968 batch mAP 0.63760376 batch PCKh 0.6875\n",
      "Trained batch 97 batch loss 0.525168419 batch mAP 0.600952148 batch PCKh 0.3125\n",
      "Trained batch 98 batch loss 0.566320777 batch mAP 0.548126221 batch PCKh 0.75\n",
      "Trained batch 99 batch loss 0.575997472 batch mAP 0.553863525 batch PCKh 0.5\n",
      "Trained batch 100 batch loss 0.485088289 batch mAP 0.514831543 batch PCKh 0.8125\n",
      "Trained batch 101 batch loss 0.494191825 batch mAP 0.508422852 batch PCKh 0.375\n",
      "Trained batch 102 batch loss 0.487669647 batch mAP 0.505493164 batch PCKh 0.75\n",
      "Trained batch 103 batch loss 0.530228853 batch mAP 0.465026855 batch PCKh 0.625\n",
      "Trained batch 104 batch loss 0.544974566 batch mAP 0.562469482 batch PCKh 0.6875\n",
      "Trained batch 105 batch loss 0.541124523 batch mAP 0.641449 batch PCKh 0.375\n",
      "Trained batch 106 batch loss 0.508356214 batch mAP 0.607910156 batch PCKh 0.375\n",
      "Trained batch 107 batch loss 0.54095912 batch mAP 0.659423828 batch PCKh 0.375\n",
      "Trained batch 108 batch loss 0.455542803 batch mAP 0.690063477 batch PCKh 0.6875\n",
      "Trained batch 109 batch loss 0.527938366 batch mAP 0.697540283 batch PCKh 0.5\n",
      "Trained batch 110 batch loss 0.428191543 batch mAP 0.68560791 batch PCKh 0.625\n",
      "Trained batch 111 batch loss 0.484595597 batch mAP 0.65032959 batch PCKh 0.75\n",
      "Trained batch 112 batch loss 0.512115836 batch mAP 0.610534668 batch PCKh 0.5625\n",
      "Trained batch 113 batch loss 0.483546078 batch mAP 0.640258789 batch PCKh 0.1875\n",
      "Trained batch 114 batch loss 0.518843055 batch mAP 0.594940186 batch PCKh 0.5\n",
      "Trained batch 115 batch loss 0.630341887 batch mAP 0.566650391 batch PCKh 0.75\n",
      "Trained batch 116 batch loss 0.415628374 batch mAP 0.582366943 batch PCKh 0.4375\n",
      "Trained batch 117 batch loss 0.555216 batch mAP 0.581848145 batch PCKh 0\n",
      "Trained batch 118 batch loss 0.530655622 batch mAP 0.581970215 batch PCKh 0.75\n",
      "Trained batch 119 batch loss 0.450793236 batch mAP 0.613067627 batch PCKh 0.75\n",
      "Trained batch 120 batch loss 0.460761428 batch mAP 0.621551514 batch PCKh 0.875\n",
      "Trained batch 121 batch loss 0.415837914 batch mAP 0.64944458 batch PCKh 0.75\n",
      "Trained batch 122 batch loss 0.358828902 batch mAP 0.756561279 batch PCKh 0.5\n",
      "Trained batch 123 batch loss 0.417464316 batch mAP 0.655181885 batch PCKh 0.75\n",
      "Trained batch 124 batch loss 0.382626057 batch mAP 0.701171875 batch PCKh 0.875\n",
      "Trained batch 125 batch loss 0.383294821 batch mAP 0.692047119 batch PCKh 0.625\n",
      "Trained batch 126 batch loss 0.38657698 batch mAP 0.68447876 batch PCKh 0.8125\n",
      "Trained batch 127 batch loss 0.377995193 batch mAP 0.732574463 batch PCKh 0.75\n",
      "Trained batch 128 batch loss 0.463120639 batch mAP 0.613861084 batch PCKh 0.875\n",
      "Trained batch 129 batch loss 0.438559 batch mAP 0.609588623 batch PCKh 0.5625\n",
      "Trained batch 130 batch loss 0.40195033 batch mAP 0.66217041 batch PCKh 0.375\n",
      "Trained batch 131 batch loss 0.401037365 batch mAP 0.626373291 batch PCKh 0.4375\n",
      "Trained batch 132 batch loss 0.489985108 batch mAP 0.600830078 batch PCKh 0.125\n",
      "Trained batch 133 batch loss 0.560824513 batch mAP 0.524017334 batch PCKh 0.5625\n",
      "Trained batch 134 batch loss 0.581945777 batch mAP 0.602203369 batch PCKh 0.6875\n",
      "Trained batch 135 batch loss 0.617087 batch mAP 0.569793701 batch PCKh 0.125\n",
      "Trained batch 136 batch loss 0.549143851 batch mAP 0.644470215 batch PCKh 0.75\n",
      "Trained batch 137 batch loss 0.530243635 batch mAP 0.653564453 batch PCKh 0.6875\n",
      "Trained batch 138 batch loss 0.527272582 batch mAP 0.65234375 batch PCKh 0.625\n",
      "Trained batch 139 batch loss 0.497675687 batch mAP 0.684204102 batch PCKh 0.8125\n",
      "Trained batch 140 batch loss 0.62744236 batch mAP 0.628723145 batch PCKh 0.6875\n",
      "Trained batch 141 batch loss 0.50405705 batch mAP 0.668792725 batch PCKh 0.8125\n",
      "Trained batch 142 batch loss 0.484567434 batch mAP 0.679077148 batch PCKh 0.8125\n",
      "Trained batch 143 batch loss 0.506077409 batch mAP 0.644348145 batch PCKh 0.4375\n",
      "Trained batch 144 batch loss 0.560666323 batch mAP 0.637939453 batch PCKh 0.4375\n",
      "Trained batch 145 batch loss 0.532486796 batch mAP 0.589599609 batch PCKh 0.3125\n",
      "Trained batch 146 batch loss 0.587637663 batch mAP 0.588104248 batch PCKh 0.75\n",
      "Trained batch 147 batch loss 0.613768578 batch mAP 0.557128906 batch PCKh 0.625\n",
      "Trained batch 148 batch loss 0.569307268 batch mAP 0.591461182 batch PCKh 0.5\n",
      "Trained batch 149 batch loss 0.603719234 batch mAP 0.577606201 batch PCKh 0.4375\n",
      "Trained batch 150 batch loss 0.649503469 batch mAP 0.531951904 batch PCKh 0.5625\n",
      "Trained batch 151 batch loss 0.532693505 batch mAP 0.546875 batch PCKh 0.25\n",
      "Trained batch 152 batch loss 0.470446885 batch mAP 0.588226318 batch PCKh 0.375\n",
      "Trained batch 153 batch loss 0.554980695 batch mAP 0.551483154 batch PCKh 0.6875\n",
      "Trained batch 154 batch loss 0.606811643 batch mAP 0.489929199 batch PCKh 0.3125\n",
      "Trained batch 155 batch loss 0.522138834 batch mAP 0.594268799 batch PCKh 0.25\n",
      "Trained batch 156 batch loss 0.524235725 batch mAP 0.591339111 batch PCKh 0.8125\n",
      "Trained batch 157 batch loss 0.465296358 batch mAP 0.613769531 batch PCKh 0.4375\n",
      "Trained batch 158 batch loss 0.529503763 batch mAP 0.641296387 batch PCKh 0.375\n",
      "Trained batch 159 batch loss 0.53650403 batch mAP 0.622253418 batch PCKh 0.0625\n",
      "Trained batch 160 batch loss 0.562921703 batch mAP 0.638031 batch PCKh 0.625\n",
      "Trained batch 161 batch loss 0.536210179 batch mAP 0.667755127 batch PCKh 0.6875\n",
      "Trained batch 162 batch loss 0.530294299 batch mAP 0.668945312 batch PCKh 0.25\n",
      "Trained batch 163 batch loss 0.529862523 batch mAP 0.597412109 batch PCKh 0.625\n",
      "Trained batch 164 batch loss 0.467912674 batch mAP 0.571563721 batch PCKh 0.6875\n",
      "Trained batch 165 batch loss 0.584834 batch mAP 0.549499512 batch PCKh 0.75\n",
      "Trained batch 166 batch loss 0.509407103 batch mAP 0.609436035 batch PCKh 0.6875\n",
      "Trained batch 167 batch loss 0.490198493 batch mAP 0.57131958 batch PCKh 0.6875\n",
      "Trained batch 168 batch loss 0.527274489 batch mAP 0.556304932 batch PCKh 0.5625\n",
      "Trained batch 169 batch loss 0.538688064 batch mAP 0.550750732 batch PCKh 0.125\n",
      "Trained batch 170 batch loss 0.591376185 batch mAP 0.520751953 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 171 batch loss 0.565800726 batch mAP 0.624115 batch PCKh 0.5\n",
      "Trained batch 172 batch loss 0.471966922 batch mAP 0.661193848 batch PCKh 0.25\n",
      "Trained batch 173 batch loss 0.503360152 batch mAP 0.588012695 batch PCKh 0.5625\n",
      "Trained batch 174 batch loss 0.436510384 batch mAP 0.633148193 batch PCKh 0.375\n",
      "Trained batch 175 batch loss 0.559597969 batch mAP 0.613952637 batch PCKh 0.5625\n",
      "Trained batch 176 batch loss 0.471165717 batch mAP 0.643737793 batch PCKh 0.3125\n",
      "Trained batch 177 batch loss 0.521301568 batch mAP 0.628082275 batch PCKh 0.75\n",
      "Trained batch 178 batch loss 0.547438145 batch mAP 0.594665527 batch PCKh 0.6875\n",
      "Trained batch 179 batch loss 0.570373654 batch mAP 0.601013184 batch PCKh 0.875\n",
      "Trained batch 180 batch loss 0.57986474 batch mAP 0.574890137 batch PCKh 0.6875\n",
      "Trained batch 181 batch loss 0.535671532 batch mAP 0.590789795 batch PCKh 0.625\n",
      "Trained batch 182 batch loss 0.473736107 batch mAP 0.611328125 batch PCKh 0.75\n",
      "Trained batch 183 batch loss 0.562156796 batch mAP 0.556396484 batch PCKh 0.75\n",
      "Trained batch 184 batch loss 0.486879 batch mAP 0.565643311 batch PCKh 0.5625\n",
      "Trained batch 185 batch loss 0.530290723 batch mAP 0.577941895 batch PCKh 0.3125\n",
      "Trained batch 186 batch loss 0.635232806 batch mAP 0.574249268 batch PCKh 0.875\n",
      "Trained batch 187 batch loss 0.583374858 batch mAP 0.641052246 batch PCKh 0.5625\n",
      "Trained batch 188 batch loss 0.553135276 batch mAP 0.593078613 batch PCKh 0.625\n",
      "Trained batch 189 batch loss 0.556245685 batch mAP 0.558441162 batch PCKh 0.75\n",
      "Trained batch 190 batch loss 0.51609236 batch mAP 0.59753418 batch PCKh 0.5625\n",
      "Trained batch 191 batch loss 0.596539259 batch mAP 0.549377441 batch PCKh 0\n",
      "Trained batch 192 batch loss 0.506149471 batch mAP 0.604919434 batch PCKh 0.75\n",
      "Trained batch 193 batch loss 0.506759822 batch mAP 0.597381592 batch PCKh 0.5\n",
      "Trained batch 194 batch loss 0.504581332 batch mAP 0.578033447 batch PCKh 0.5\n",
      "Trained batch 195 batch loss 0.482694715 batch mAP 0.606872559 batch PCKh 0.375\n",
      "Trained batch 196 batch loss 0.519416571 batch mAP 0.632720947 batch PCKh 0.6875\n",
      "Trained batch 197 batch loss 0.549667954 batch mAP 0.623535156 batch PCKh 0.625\n",
      "Trained batch 198 batch loss 0.598651707 batch mAP 0.579223633 batch PCKh 0.8125\n",
      "Trained batch 199 batch loss 0.569706202 batch mAP 0.592865 batch PCKh 0.625\n",
      "Trained batch 200 batch loss 0.614574373 batch mAP 0.612518311 batch PCKh 0.3125\n",
      "Trained batch 201 batch loss 0.505521894 batch mAP 0.619262695 batch PCKh 0.625\n",
      "Trained batch 202 batch loss 0.583999276 batch mAP 0.62210083 batch PCKh 0.6875\n",
      "Trained batch 203 batch loss 0.554962635 batch mAP 0.667114258 batch PCKh 0.75\n",
      "Trained batch 204 batch loss 0.554644525 batch mAP 0.663452148 batch PCKh 0.875\n",
      "Trained batch 205 batch loss 0.435378611 batch mAP 0.701293945 batch PCKh 0.4375\n",
      "Trained batch 206 batch loss 0.377270401 batch mAP 0.720581055 batch PCKh 0.125\n",
      "Trained batch 207 batch loss 0.34127298 batch mAP 0.749084473 batch PCKh 0.1875\n",
      "Trained batch 208 batch loss 0.497964203 batch mAP 0.694763184 batch PCKh 0.3125\n",
      "Trained batch 209 batch loss 0.513018608 batch mAP 0.672668457 batch PCKh 0.1875\n",
      "Trained batch 210 batch loss 0.463353872 batch mAP 0.676574707 batch PCKh 0.1875\n",
      "Trained batch 211 batch loss 0.46945554 batch mAP 0.717407227 batch PCKh 0.3125\n",
      "Trained batch 212 batch loss 0.525189459 batch mAP 0.60546875 batch PCKh 0.625\n",
      "Trained batch 213 batch loss 0.492287874 batch mAP 0.608642578 batch PCKh 0.1875\n",
      "Trained batch 214 batch loss 0.596918285 batch mAP 0.560516357 batch PCKh 0.6875\n",
      "Trained batch 215 batch loss 0.608598828 batch mAP 0.532135 batch PCKh 0.375\n",
      "Trained batch 216 batch loss 0.477139711 batch mAP 0.608642578 batch PCKh 0.1875\n",
      "Trained batch 217 batch loss 0.530396938 batch mAP 0.662475586 batch PCKh 0.25\n",
      "Trained batch 218 batch loss 0.579917729 batch mAP 0.633117676 batch PCKh 0.5625\n",
      "Trained batch 219 batch loss 0.579769492 batch mAP 0.643035889 batch PCKh 0.3125\n",
      "Trained batch 220 batch loss 0.55491972 batch mAP 0.693847656 batch PCKh 0.75\n",
      "Trained batch 221 batch loss 0.575587034 batch mAP 0.592773438 batch PCKh 0.4375\n",
      "Trained batch 222 batch loss 0.502811134 batch mAP 0.658782959 batch PCKh 0.75\n",
      "Trained batch 223 batch loss 0.464524925 batch mAP 0.630157471 batch PCKh 0.6875\n",
      "Trained batch 224 batch loss 0.478284597 batch mAP 0.591796875 batch PCKh 0.5625\n",
      "Trained batch 225 batch loss 0.486878604 batch mAP 0.51651 batch PCKh 0.375\n",
      "Trained batch 226 batch loss 0.419123441 batch mAP 0.546142578 batch PCKh 0\n",
      "Trained batch 227 batch loss 0.521857679 batch mAP 0.578155518 batch PCKh 0.75\n",
      "Trained batch 228 batch loss 0.551712096 batch mAP 0.586181641 batch PCKh 0.75\n",
      "Trained batch 229 batch loss 0.578373671 batch mAP 0.497467041 batch PCKh 0.75\n",
      "Trained batch 230 batch loss 0.506560206 batch mAP 0.548522949 batch PCKh 0.625\n",
      "Trained batch 231 batch loss 0.583212793 batch mAP 0.521942139 batch PCKh 0.0625\n",
      "Trained batch 232 batch loss 0.581610918 batch mAP 0.531707764 batch PCKh 0.5\n",
      "Trained batch 233 batch loss 0.56897682 batch mAP 0.582519531 batch PCKh 0.4375\n",
      "Trained batch 234 batch loss 0.524535775 batch mAP 0.584777832 batch PCKh 0.8125\n",
      "Trained batch 235 batch loss 0.478763878 batch mAP 0.622375488 batch PCKh 0.75\n",
      "Trained batch 236 batch loss 0.558669806 batch mAP 0.664703369 batch PCKh 0.3125\n",
      "Trained batch 237 batch loss 0.535880446 batch mAP 0.617736816 batch PCKh 0.75\n",
      "Trained batch 238 batch loss 0.439155817 batch mAP 0.696228 batch PCKh 0.75\n",
      "Trained batch 239 batch loss 0.482159078 batch mAP 0.680114746 batch PCKh 0.4375\n",
      "Trained batch 240 batch loss 0.561920285 batch mAP 0.551178 batch PCKh 0.625\n",
      "Trained batch 241 batch loss 0.582589746 batch mAP 0.588470459 batch PCKh 0.4375\n",
      "Trained batch 242 batch loss 0.573362112 batch mAP 0.654022217 batch PCKh 0.3125\n",
      "Trained batch 243 batch loss 0.5354141 batch mAP 0.629119873 batch PCKh 0.875\n",
      "Trained batch 244 batch loss 0.556381345 batch mAP 0.632354736 batch PCKh 0.75\n",
      "Trained batch 245 batch loss 0.571221948 batch mAP 0.611175537 batch PCKh 0.375\n",
      "Trained batch 246 batch loss 0.550798416 batch mAP 0.59085083 batch PCKh 0.75\n",
      "Trained batch 247 batch loss 0.513604522 batch mAP 0.563171387 batch PCKh 0.375\n",
      "Trained batch 248 batch loss 0.598541617 batch mAP 0.612121582 batch PCKh 0.25\n",
      "Trained batch 249 batch loss 0.499717176 batch mAP 0.501037598 batch PCKh 0.5\n",
      "Trained batch 250 batch loss 0.482923687 batch mAP 0.629272461 batch PCKh 0.1875\n",
      "Trained batch 251 batch loss 0.54086107 batch mAP 0.569854736 batch PCKh 0.75\n",
      "Trained batch 252 batch loss 0.397531211 batch mAP 0.574584961 batch PCKh 0.25\n",
      "Trained batch 253 batch loss 0.452967 batch mAP 0.64855957 batch PCKh 0.4375\n",
      "Trained batch 254 batch loss 0.525984585 batch mAP 0.568939209 batch PCKh 0.3125\n",
      "Trained batch 255 batch loss 0.570994258 batch mAP 0.573242188 batch PCKh 0.875\n",
      "Trained batch 256 batch loss 0.539294541 batch mAP 0.565216064 batch PCKh 0.4375\n",
      "Trained batch 257 batch loss 0.573446512 batch mAP 0.575592041 batch PCKh 0.75\n",
      "Trained batch 258 batch loss 0.505423486 batch mAP 0.592102051 batch PCKh 0.625\n",
      "Trained batch 259 batch loss 0.493747056 batch mAP 0.625274658 batch PCKh 0.625\n",
      "Trained batch 260 batch loss 0.497386247 batch mAP 0.577148438 batch PCKh 0.625\n",
      "Trained batch 261 batch loss 0.559579134 batch mAP 0.570953369 batch PCKh 0.25\n",
      "Trained batch 262 batch loss 0.533410072 batch mAP 0.645172119 batch PCKh 0.3125\n",
      "Trained batch 263 batch loss 0.46473366 batch mAP 0.554534912 batch PCKh 0.3125\n",
      "Trained batch 264 batch loss 0.562397361 batch mAP 0.553375244 batch PCKh 0.5625\n",
      "Trained batch 265 batch loss 0.554920375 batch mAP 0.57800293 batch PCKh 0.25\n",
      "Trained batch 266 batch loss 0.422432601 batch mAP 0.646484375 batch PCKh 0.5625\n",
      "Trained batch 267 batch loss 0.436743319 batch mAP 0.663330078 batch PCKh 0.5\n",
      "Trained batch 268 batch loss 0.494915545 batch mAP 0.643432617 batch PCKh 0.6875\n",
      "Trained batch 269 batch loss 0.382369161 batch mAP 0.667938232 batch PCKh 0.1875\n",
      "Trained batch 270 batch loss 0.541078269 batch mAP 0.659576416 batch PCKh 0.75\n",
      "Trained batch 271 batch loss 0.444352508 batch mAP 0.691894531 batch PCKh 0.375\n",
      "Trained batch 272 batch loss 0.429862946 batch mAP 0.738861084 batch PCKh 0.4375\n",
      "Trained batch 273 batch loss 0.441734016 batch mAP 0.698699951 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 274 batch loss 0.46444121 batch mAP 0.684539795 batch PCKh 0.5625\n",
      "Trained batch 275 batch loss 0.410516679 batch mAP 0.706268311 batch PCKh 0.875\n",
      "Trained batch 276 batch loss 0.449285865 batch mAP 0.680725098 batch PCKh 0.625\n",
      "Trained batch 277 batch loss 0.441108793 batch mAP 0.673431396 batch PCKh 0.375\n",
      "Trained batch 278 batch loss 0.522115111 batch mAP 0.647796631 batch PCKh 0.5625\n",
      "Trained batch 279 batch loss 0.46649456 batch mAP 0.654205322 batch PCKh 0.3125\n",
      "Trained batch 280 batch loss 0.469013721 batch mAP 0.635681152 batch PCKh 0.25\n",
      "Trained batch 281 batch loss 0.519649088 batch mAP 0.611480713 batch PCKh 0.75\n",
      "Trained batch 282 batch loss 0.502579689 batch mAP 0.667602539 batch PCKh 0.1875\n",
      "Trained batch 283 batch loss 0.417673141 batch mAP 0.678955078 batch PCKh 0.125\n",
      "Trained batch 284 batch loss 0.459568262 batch mAP 0.643371582 batch PCKh 0.4375\n",
      "Trained batch 285 batch loss 0.463341415 batch mAP 0.560180664 batch PCKh 0.4375\n",
      "Trained batch 286 batch loss 0.479328752 batch mAP 0.635040283 batch PCKh 0.0625\n",
      "Trained batch 287 batch loss 0.50891608 batch mAP 0.640869141 batch PCKh 0.875\n",
      "Trained batch 288 batch loss 0.517838 batch mAP 0.626709 batch PCKh 0.5625\n",
      "Trained batch 289 batch loss 0.495669544 batch mAP 0.655670166 batch PCKh 0.25\n",
      "Trained batch 290 batch loss 0.525142431 batch mAP 0.627624512 batch PCKh 0.1875\n",
      "Trained batch 291 batch loss 0.514636397 batch mAP 0.636932373 batch PCKh 0.4375\n",
      "Trained batch 292 batch loss 0.52886343 batch mAP 0.627716064 batch PCKh 0.375\n",
      "Trained batch 293 batch loss 0.5693838 batch mAP 0.566741943 batch PCKh 0.4375\n",
      "Trained batch 294 batch loss 0.488334 batch mAP 0.59664917 batch PCKh 0.3125\n",
      "Trained batch 295 batch loss 0.468895882 batch mAP 0.645629883 batch PCKh 0.25\n",
      "Trained batch 296 batch loss 0.464508861 batch mAP 0.55456543 batch PCKh 0.4375\n",
      "Trained batch 297 batch loss 0.546524346 batch mAP 0.586853 batch PCKh 0.25\n",
      "Trained batch 298 batch loss 0.434515238 batch mAP 0.631378174 batch PCKh 0.25\n",
      "Trained batch 299 batch loss 0.451671869 batch mAP 0.62689209 batch PCKh 0.6875\n",
      "Trained batch 300 batch loss 0.536610067 batch mAP 0.555023193 batch PCKh 0.4375\n",
      "Trained batch 301 batch loss 0.541787207 batch mAP 0.585968 batch PCKh 0.875\n",
      "Trained batch 302 batch loss 0.489282787 batch mAP 0.652618408 batch PCKh 0.375\n",
      "Trained batch 303 batch loss 0.48641625 batch mAP 0.647613525 batch PCKh 0.375\n",
      "Trained batch 304 batch loss 0.437415063 batch mAP 0.619049072 batch PCKh 0.75\n",
      "Trained batch 305 batch loss 0.561321497 batch mAP 0.649871826 batch PCKh 0.4375\n",
      "Trained batch 306 batch loss 0.603765488 batch mAP 0.632385254 batch PCKh 0.5\n",
      "Trained batch 307 batch loss 0.51193732 batch mAP 0.627166748 batch PCKh 0.8125\n",
      "Trained batch 308 batch loss 0.475146294 batch mAP 0.653106689 batch PCKh 0.5625\n",
      "Trained batch 309 batch loss 0.505745351 batch mAP 0.658569336 batch PCKh 0.75\n",
      "Trained batch 310 batch loss 0.629333496 batch mAP 0.606506348 batch PCKh 0.75\n",
      "Trained batch 311 batch loss 0.569506884 batch mAP 0.596588135 batch PCKh 0.625\n",
      "Trained batch 312 batch loss 0.58263278 batch mAP 0.586029053 batch PCKh 0.8125\n",
      "Trained batch 313 batch loss 0.477448046 batch mAP 0.625030518 batch PCKh 0.4375\n",
      "Trained batch 314 batch loss 0.539189339 batch mAP 0.583129883 batch PCKh 0.8125\n",
      "Trained batch 315 batch loss 0.595855832 batch mAP 0.534637451 batch PCKh 0.8125\n",
      "Trained batch 316 batch loss 0.507986128 batch mAP 0.559173584 batch PCKh 0.75\n",
      "Trained batch 317 batch loss 0.486863643 batch mAP 0.504699707 batch PCKh 0.875\n",
      "Trained batch 318 batch loss 0.506879687 batch mAP 0.569549561 batch PCKh 0.5625\n",
      "Trained batch 319 batch loss 0.490868956 batch mAP 0.551849365 batch PCKh 0.5625\n",
      "Trained batch 320 batch loss 0.451212853 batch mAP 0.573150635 batch PCKh 0.625\n",
      "Trained batch 321 batch loss 0.444303811 batch mAP 0.627502441 batch PCKh 0.5625\n",
      "Trained batch 322 batch loss 0.457919955 batch mAP 0.597625732 batch PCKh 0.5625\n",
      "Trained batch 323 batch loss 0.442751765 batch mAP 0.574615479 batch PCKh 0.5\n",
      "Trained batch 324 batch loss 0.534256101 batch mAP 0.528930664 batch PCKh 0.5\n",
      "Trained batch 325 batch loss 0.578234196 batch mAP 0.555023193 batch PCKh 0.875\n",
      "Trained batch 326 batch loss 0.534268 batch mAP 0.523284912 batch PCKh 0.75\n",
      "Trained batch 327 batch loss 0.554693 batch mAP 0.597595215 batch PCKh 0.375\n",
      "Trained batch 328 batch loss 0.61087507 batch mAP 0.537872314 batch PCKh 0.625\n",
      "Trained batch 329 batch loss 0.566386 batch mAP 0.504943848 batch PCKh 0.5\n",
      "Trained batch 330 batch loss 0.475164115 batch mAP 0.573547363 batch PCKh 0.4375\n",
      "Trained batch 331 batch loss 0.505041718 batch mAP 0.59954834 batch PCKh 0.375\n",
      "Trained batch 332 batch loss 0.471146047 batch mAP 0.648864746 batch PCKh 0.4375\n",
      "Trained batch 333 batch loss 0.504782557 batch mAP 0.657501221 batch PCKh 0.375\n",
      "Trained batch 334 batch loss 0.585774243 batch mAP 0.585479736 batch PCKh 0.75\n",
      "Trained batch 335 batch loss 0.466008723 batch mAP 0.64074707 batch PCKh 0.625\n",
      "Trained batch 336 batch loss 0.580153584 batch mAP 0.561065674 batch PCKh 0.8125\n",
      "Trained batch 337 batch loss 0.481528133 batch mAP 0.608184814 batch PCKh 0.625\n",
      "Trained batch 338 batch loss 0.460897118 batch mAP 0.576080322 batch PCKh 0.1875\n",
      "Trained batch 339 batch loss 0.464899898 batch mAP 0.589782715 batch PCKh 0.0625\n",
      "Trained batch 340 batch loss 0.50273788 batch mAP 0.609313965 batch PCKh 0.75\n",
      "Trained batch 341 batch loss 0.511131644 batch mAP 0.580535889 batch PCKh 0.5\n",
      "Trained batch 342 batch loss 0.55855155 batch mAP 0.592407227 batch PCKh 0.625\n",
      "Trained batch 343 batch loss 0.546666563 batch mAP 0.552703857 batch PCKh 0.3125\n",
      "Trained batch 344 batch loss 0.569974542 batch mAP 0.552337646 batch PCKh 0.125\n",
      "Trained batch 345 batch loss 0.5935812 batch mAP 0.553436279 batch PCKh 0.6875\n",
      "Trained batch 346 batch loss 0.540116787 batch mAP 0.582763672 batch PCKh 0.5\n",
      "Trained batch 347 batch loss 0.527388513 batch mAP 0.624145508 batch PCKh 0.25\n",
      "Trained batch 348 batch loss 0.501416087 batch mAP 0.697967529 batch PCKh 0.5625\n",
      "Trained batch 349 batch loss 0.544844449 batch mAP 0.611236572 batch PCKh 0.1875\n",
      "Trained batch 350 batch loss 0.456634164 batch mAP 0.684021 batch PCKh 0.5\n",
      "Trained batch 351 batch loss 0.580925465 batch mAP 0.543548584 batch PCKh 0.375\n",
      "Trained batch 352 batch loss 0.571428955 batch mAP 0.603759766 batch PCKh 0.6875\n",
      "Trained batch 353 batch loss 0.571760356 batch mAP 0.605407715 batch PCKh 0.375\n",
      "Trained batch 354 batch loss 0.502120495 batch mAP 0.543914795 batch PCKh 0.6875\n",
      "Trained batch 355 batch loss 0.425637931 batch mAP 0.599578857 batch PCKh 0.75\n",
      "Trained batch 356 batch loss 0.485496044 batch mAP 0.595275879 batch PCKh 0.25\n",
      "Trained batch 357 batch loss 0.590290785 batch mAP 0.572296143 batch PCKh 0.625\n",
      "Trained batch 358 batch loss 0.487025589 batch mAP 0.620330811 batch PCKh 0.5625\n",
      "Trained batch 359 batch loss 0.499180853 batch mAP 0.616790771 batch PCKh 0.5\n",
      "Trained batch 360 batch loss 0.491319209 batch mAP 0.631500244 batch PCKh 0.25\n",
      "Trained batch 361 batch loss 0.455082536 batch mAP 0.606811523 batch PCKh 0.5625\n",
      "Trained batch 362 batch loss 0.567371964 batch mAP 0.61831665 batch PCKh 0.1875\n",
      "Trained batch 363 batch loss 0.509216726 batch mAP 0.635101318 batch PCKh 0.75\n",
      "Trained batch 364 batch loss 0.448387563 batch mAP 0.647644043 batch PCKh 0.75\n",
      "Trained batch 365 batch loss 0.587442517 batch mAP 0.512084961 batch PCKh 0.375\n",
      "Trained batch 366 batch loss 0.648433864 batch mAP 0.550872803 batch PCKh 0.5625\n",
      "Trained batch 367 batch loss 0.492854893 batch mAP 0.5625 batch PCKh 0.625\n",
      "Trained batch 368 batch loss 0.617694 batch mAP 0.501678467 batch PCKh 0.6875\n",
      "Trained batch 369 batch loss 0.579191506 batch mAP 0.524993896 batch PCKh 0.375\n",
      "Trained batch 370 batch loss 0.484946549 batch mAP 0.605377197 batch PCKh 0.75\n",
      "Trained batch 371 batch loss 0.514957607 batch mAP 0.539672852 batch PCKh 0.75\n",
      "Trained batch 372 batch loss 0.576294661 batch mAP 0.568023682 batch PCKh 0.8125\n",
      "Trained batch 373 batch loss 0.488287568 batch mAP 0.609832764 batch PCKh 0.5\n",
      "Trained batch 374 batch loss 0.514592707 batch mAP 0.64453125 batch PCKh 0.75\n",
      "Trained batch 375 batch loss 0.539951921 batch mAP 0.647399902 batch PCKh 0.25\n",
      "Trained batch 376 batch loss 0.568924 batch mAP 0.528625488 batch PCKh 0.6875\n",
      "Trained batch 377 batch loss 0.598251164 batch mAP 0.628173828 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 378 batch loss 0.476641595 batch mAP 0.655639648 batch PCKh 0.6875\n",
      "Trained batch 379 batch loss 0.472571015 batch mAP 0.658050537 batch PCKh 0.75\n",
      "Trained batch 380 batch loss 0.519135535 batch mAP 0.570922852 batch PCKh 0.5\n",
      "Trained batch 381 batch loss 0.588409543 batch mAP 0.518737793 batch PCKh 0.5\n",
      "Trained batch 382 batch loss 0.577624679 batch mAP 0.584381104 batch PCKh 0.3125\n",
      "Trained batch 383 batch loss 0.477142751 batch mAP 0.596923828 batch PCKh 0.6875\n",
      "Trained batch 384 batch loss 0.481174052 batch mAP 0.621398926 batch PCKh 0.25\n",
      "Trained batch 385 batch loss 0.509739041 batch mAP 0.626342773 batch PCKh 0.375\n",
      "Trained batch 386 batch loss 0.514902532 batch mAP 0.629303 batch PCKh 0.6875\n",
      "Trained batch 387 batch loss 0.417762607 batch mAP 0.642791748 batch PCKh 0.625\n",
      "Trained batch 388 batch loss 0.408737659 batch mAP 0.599609375 batch PCKh 0.5\n",
      "Trained batch 389 batch loss 0.403617233 batch mAP 0.592956543 batch PCKh 0.625\n",
      "Trained batch 390 batch loss 0.546646953 batch mAP 0.605072 batch PCKh 0.875\n",
      "Trained batch 391 batch loss 0.509393573 batch mAP 0.628570557 batch PCKh 0.625\n",
      "Trained batch 392 batch loss 0.477223516 batch mAP 0.636047363 batch PCKh 0.25\n",
      "Trained batch 393 batch loss 0.498034894 batch mAP 0.601989746 batch PCKh 0.4375\n",
      "Trained batch 394 batch loss 0.402639508 batch mAP 0.597198486 batch PCKh 0.1875\n",
      "Trained batch 395 batch loss 0.549459457 batch mAP 0.521911621 batch PCKh 0.125\n",
      "Trained batch 396 batch loss 0.539084315 batch mAP 0.569030762 batch PCKh 0\n",
      "Trained batch 397 batch loss 0.606240571 batch mAP 0.581207275 batch PCKh 0.3125\n",
      "Trained batch 398 batch loss 0.509632409 batch mAP 0.634796143 batch PCKh 0.75\n",
      "Trained batch 399 batch loss 0.543089807 batch mAP 0.617004395 batch PCKh 0.625\n",
      "Trained batch 400 batch loss 0.492120862 batch mAP 0.613494873 batch PCKh 0.125\n",
      "Trained batch 401 batch loss 0.511446357 batch mAP 0.594421387 batch PCKh 0.25\n",
      "Trained batch 402 batch loss 0.504811823 batch mAP 0.639312744 batch PCKh 0.6875\n",
      "Trained batch 403 batch loss 0.470128238 batch mAP 0.623657227 batch PCKh 0.8125\n",
      "Trained batch 404 batch loss 0.441910416 batch mAP 0.66229248 batch PCKh 0.8125\n",
      "Trained batch 405 batch loss 0.444699287 batch mAP 0.661376953 batch PCKh 0.5\n",
      "Trained batch 406 batch loss 0.38662231 batch mAP 0.571105957 batch PCKh 0.5625\n",
      "Trained batch 407 batch loss 0.444172621 batch mAP 0.57409668 batch PCKh 0.75\n",
      "Trained batch 408 batch loss 0.465346754 batch mAP 0.530822754 batch PCKh 0.5\n",
      "Trained batch 409 batch loss 0.510622561 batch mAP 0.53817749 batch PCKh 0.875\n",
      "Trained batch 410 batch loss 0.52807039 batch mAP 0.607818604 batch PCKh 0.5625\n",
      "Trained batch 411 batch loss 0.465129435 batch mAP 0.668792725 batch PCKh 0.5\n",
      "Trained batch 412 batch loss 0.57204783 batch mAP 0.620239258 batch PCKh 0.5\n",
      "Trained batch 413 batch loss 0.570208132 batch mAP 0.645111084 batch PCKh 0.75\n",
      "Trained batch 414 batch loss 0.367588878 batch mAP 0.722747803 batch PCKh 0.3125\n",
      "Trained batch 415 batch loss 0.475929081 batch mAP 0.668579102 batch PCKh 0.5\n",
      "Trained batch 416 batch loss 0.424340487 batch mAP 0.604949951 batch PCKh 0.6875\n",
      "Trained batch 417 batch loss 0.387657046 batch mAP 0.63458252 batch PCKh 0.5\n",
      "Trained batch 418 batch loss 0.413655281 batch mAP 0.617492676 batch PCKh 0.75\n",
      "Trained batch 419 batch loss 0.492142439 batch mAP 0.610595703 batch PCKh 0.25\n",
      "Trained batch 420 batch loss 0.604012251 batch mAP 0.570404053 batch PCKh 0.6875\n",
      "Trained batch 421 batch loss 0.663441539 batch mAP 0.513580322 batch PCKh 0.5\n",
      "Trained batch 422 batch loss 0.584426343 batch mAP 0.559845 batch PCKh 0.6875\n",
      "Trained batch 423 batch loss 0.448076487 batch mAP 0.620849609 batch PCKh 0.4375\n",
      "Trained batch 424 batch loss 0.44808042 batch mAP 0.610656738 batch PCKh 0.75\n",
      "Trained batch 425 batch loss 0.568782687 batch mAP 0.584106445 batch PCKh 0.8125\n",
      "Trained batch 426 batch loss 0.519753218 batch mAP 0.480743408 batch PCKh 0.75\n",
      "Trained batch 427 batch loss 0.358286738 batch mAP 0.576049805 batch PCKh 0.375\n",
      "Trained batch 428 batch loss 0.358590543 batch mAP 0.68850708 batch PCKh 0.5625\n",
      "Trained batch 429 batch loss 0.315879166 batch mAP 0.762634277 batch PCKh 0.5625\n",
      "Trained batch 430 batch loss 0.343667895 batch mAP 0.731536865 batch PCKh 0.5\n",
      "Trained batch 431 batch loss 0.299051434 batch mAP 0.758636475 batch PCKh 0.6875\n",
      "Trained batch 432 batch loss 0.350309581 batch mAP 0.775268555 batch PCKh 0.5625\n",
      "Trained batch 433 batch loss 0.369159102 batch mAP 0.730285645 batch PCKh 0.5\n",
      "Trained batch 434 batch loss 0.451625049 batch mAP 0.660308838 batch PCKh 0.5\n",
      "Trained batch 435 batch loss 0.622238815 batch mAP 0.559814453 batch PCKh 0.875\n",
      "Trained batch 436 batch loss 0.454368651 batch mAP 0.609039307 batch PCKh 0.6875\n",
      "Trained batch 437 batch loss 0.503888369 batch mAP 0.570526123 batch PCKh 0.6875\n",
      "Trained batch 438 batch loss 0.547669828 batch mAP 0.553741455 batch PCKh 0.75\n",
      "Trained batch 439 batch loss 0.619860113 batch mAP 0.53994751 batch PCKh 0.8125\n",
      "Trained batch 440 batch loss 0.58393842 batch mAP 0.570831299 batch PCKh 0.5\n",
      "Trained batch 441 batch loss 0.541375339 batch mAP 0.583648682 batch PCKh 0.375\n",
      "Trained batch 442 batch loss 0.484616816 batch mAP 0.603302 batch PCKh 0.375\n",
      "Trained batch 443 batch loss 0.507128954 batch mAP 0.59954834 batch PCKh 0.5\n",
      "Trained batch 444 batch loss 0.591387928 batch mAP 0.576873779 batch PCKh 0.4375\n",
      "Trained batch 445 batch loss 0.629649103 batch mAP 0.5887146 batch PCKh 0.25\n",
      "Trained batch 446 batch loss 0.571359873 batch mAP 0.576385498 batch PCKh 0.3125\n",
      "Trained batch 447 batch loss 0.424379766 batch mAP 0.661621094 batch PCKh 0.875\n",
      "Trained batch 448 batch loss 0.463519275 batch mAP 0.62600708 batch PCKh 0.875\n",
      "Trained batch 449 batch loss 0.592833638 batch mAP 0.58581543 batch PCKh 0.6875\n",
      "Trained batch 450 batch loss 0.399622321 batch mAP 0.63482666 batch PCKh 0.5625\n",
      "Trained batch 451 batch loss 0.5059 batch mAP 0.657897949 batch PCKh 0.3125\n",
      "Trained batch 452 batch loss 0.53609556 batch mAP 0.574279785 batch PCKh 0.625\n",
      "Trained batch 453 batch loss 0.59710741 batch mAP 0.578125 batch PCKh 0.3125\n",
      "Trained batch 454 batch loss 0.598243594 batch mAP 0.528717041 batch PCKh 0.3125\n",
      "Trained batch 455 batch loss 0.557563543 batch mAP 0.563324 batch PCKh 0.3125\n",
      "Trained batch 456 batch loss 0.585330367 batch mAP 0.609375 batch PCKh 0.4375\n",
      "Trained batch 457 batch loss 0.488843262 batch mAP 0.659301758 batch PCKh 0.25\n",
      "Trained batch 458 batch loss 0.502827346 batch mAP 0.618103 batch PCKh 0.375\n",
      "Trained batch 459 batch loss 0.543998361 batch mAP 0.679046631 batch PCKh 0.5625\n",
      "Trained batch 460 batch loss 0.433118105 batch mAP 0.679870605 batch PCKh 0.3125\n",
      "Trained batch 461 batch loss 0.480981708 batch mAP 0.69039917 batch PCKh 0.4375\n",
      "Trained batch 462 batch loss 0.617612 batch mAP 0.579162598 batch PCKh 0.4375\n",
      "Trained batch 463 batch loss 0.539055407 batch mAP 0.640136719 batch PCKh 0.25\n",
      "Trained batch 464 batch loss 0.485605776 batch mAP 0.652313232 batch PCKh 0.5\n",
      "Trained batch 465 batch loss 0.496241689 batch mAP 0.636322 batch PCKh 0.3125\n",
      "Trained batch 466 batch loss 0.617012322 batch mAP 0.562530518 batch PCKh 0.875\n",
      "Trained batch 467 batch loss 0.543919563 batch mAP 0.536315918 batch PCKh 0.8125\n",
      "Trained batch 468 batch loss 0.624394417 batch mAP 0.491577148 batch PCKh 0.6875\n",
      "Trained batch 469 batch loss 0.603532195 batch mAP 0.469207764 batch PCKh 0.3125\n",
      "Trained batch 470 batch loss 0.626783669 batch mAP 0.509918213 batch PCKh 0.3125\n",
      "Trained batch 471 batch loss 0.532087684 batch mAP 0.596313477 batch PCKh 0.625\n",
      "Trained batch 472 batch loss 0.525854349 batch mAP 0.582428 batch PCKh 0.75\n",
      "Trained batch 473 batch loss 0.453139305 batch mAP 0.606567383 batch PCKh 0.375\n",
      "Trained batch 474 batch loss 0.3986817 batch mAP 0.606018066 batch PCKh 0.25\n",
      "Trained batch 475 batch loss 0.452702582 batch mAP 0.530517578 batch PCKh 0.0625\n",
      "Trained batch 476 batch loss 0.422189087 batch mAP 0.555847168 batch PCKh 0.3125\n",
      "Trained batch 477 batch loss 0.404922575 batch mAP 0.522674561 batch PCKh 0.375\n",
      "Trained batch 478 batch loss 0.661106288 batch mAP 0.578399658 batch PCKh 0.3125\n",
      "Trained batch 479 batch loss 0.659019887 batch mAP 0.61541748 batch PCKh 0.1875\n",
      "Trained batch 480 batch loss 0.620177388 batch mAP 0.5965271 batch PCKh 0.1875\n",
      "Trained batch 481 batch loss 0.69664079 batch mAP 0.59475708 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 482 batch loss 0.584861636 batch mAP 0.630340576 batch PCKh 0.3125\n",
      "Trained batch 483 batch loss 0.50160116 batch mAP 0.651977539 batch PCKh 0.75\n",
      "Trained batch 484 batch loss 0.490804434 batch mAP 0.674163818 batch PCKh 0.5625\n",
      "Trained batch 485 batch loss 0.525527656 batch mAP 0.642272949 batch PCKh 0.375\n",
      "Trained batch 486 batch loss 0.533372581 batch mAP 0.633575439 batch PCKh 0.5\n",
      "Trained batch 487 batch loss 0.408061743 batch mAP 0.673584 batch PCKh 0.625\n",
      "Trained batch 488 batch loss 0.493899941 batch mAP 0.650787354 batch PCKh 0.625\n",
      "Trained batch 489 batch loss 0.605912566 batch mAP 0.593444824 batch PCKh 0.75\n",
      "Trained batch 490 batch loss 0.584376037 batch mAP 0.511352539 batch PCKh 0.5\n",
      "Trained batch 491 batch loss 0.586915731 batch mAP 0.522064209 batch PCKh 0.8125\n",
      "Trained batch 492 batch loss 0.520782113 batch mAP 0.533294678 batch PCKh 0.1875\n",
      "Trained batch 493 batch loss 0.465543896 batch mAP 0.520629883 batch PCKh 0.125\n",
      "Trained batch 494 batch loss 0.53493154 batch mAP 0.442169189 batch PCKh 0.625\n",
      "Trained batch 495 batch loss 0.581892 batch mAP 0.452667236 batch PCKh 0.875\n",
      "Trained batch 496 batch loss 0.612471402 batch mAP 0.468261719 batch PCKh 0\n",
      "Trained batch 497 batch loss 0.58551836 batch mAP 0.471282959 batch PCKh 0.4375\n",
      "Trained batch 498 batch loss 0.608847857 batch mAP 0.494689941 batch PCKh 0.125\n",
      "Trained batch 499 batch loss 0.626358151 batch mAP 0.492462158 batch PCKh 0.25\n",
      "Trained batch 500 batch loss 0.625467241 batch mAP 0.474853516 batch PCKh 0.5625\n",
      "Trained batch 501 batch loss 0.667084 batch mAP 0.493499756 batch PCKh 0.5625\n",
      "Trained batch 502 batch loss 0.548286259 batch mAP 0.60446167 batch PCKh 0.3125\n",
      "Trained batch 503 batch loss 0.546007931 batch mAP 0.632171631 batch PCKh 0.5\n",
      "Trained batch 504 batch loss 0.559340239 batch mAP 0.655944824 batch PCKh 0.625\n",
      "Trained batch 505 batch loss 0.535821319 batch mAP 0.598907471 batch PCKh 0.3125\n",
      "Trained batch 506 batch loss 0.483330101 batch mAP 0.620330811 batch PCKh 0.3125\n",
      "Trained batch 507 batch loss 0.516641319 batch mAP 0.603179932 batch PCKh 0.625\n",
      "Trained batch 508 batch loss 0.511850953 batch mAP 0.661956787 batch PCKh 0.5625\n",
      "Trained batch 509 batch loss 0.491437435 batch mAP 0.633361816 batch PCKh 0.125\n",
      "Trained batch 510 batch loss 0.582381129 batch mAP 0.601532 batch PCKh 0.8125\n",
      "Trained batch 511 batch loss 0.566009641 batch mAP 0.590179443 batch PCKh 0.1875\n",
      "Trained batch 512 batch loss 0.531429231 batch mAP 0.673278809 batch PCKh 0.1875\n",
      "Trained batch 513 batch loss 0.524229884 batch mAP 0.658538818 batch PCKh 0.75\n",
      "Trained batch 514 batch loss 0.480428398 batch mAP 0.674713135 batch PCKh 0.1875\n",
      "Trained batch 515 batch loss 0.517180383 batch mAP 0.708404541 batch PCKh 0.6875\n",
      "Trained batch 516 batch loss 0.527274847 batch mAP 0.650177 batch PCKh 0.75\n",
      "Trained batch 517 batch loss 0.50838089 batch mAP 0.632537842 batch PCKh 0.625\n",
      "Trained batch 518 batch loss 0.439585328 batch mAP 0.650299072 batch PCKh 0.625\n",
      "Trained batch 519 batch loss 0.52007544 batch mAP 0.643920898 batch PCKh 0.75\n",
      "Trained batch 520 batch loss 0.514914393 batch mAP 0.616333 batch PCKh 0.75\n",
      "Trained batch 521 batch loss 0.481862366 batch mAP 0.53414917 batch PCKh 0.5\n",
      "Trained batch 522 batch loss 0.474060386 batch mAP 0.532928467 batch PCKh 0.75\n",
      "Trained batch 523 batch loss 0.478819668 batch mAP 0.536376953 batch PCKh 0.8125\n",
      "Trained batch 524 batch loss 0.474044859 batch mAP 0.532470703 batch PCKh 0.6875\n",
      "Trained batch 525 batch loss 0.465416372 batch mAP 0.597686768 batch PCKh 0.625\n",
      "Trained batch 526 batch loss 0.497933179 batch mAP 0.624755859 batch PCKh 0.75\n",
      "Trained batch 527 batch loss 0.576784074 batch mAP 0.565948486 batch PCKh 0.1875\n",
      "Trained batch 528 batch loss 0.510200143 batch mAP 0.623748779 batch PCKh 0.375\n",
      "Trained batch 529 batch loss 0.560012221 batch mAP 0.530914307 batch PCKh 0.375\n",
      "Trained batch 530 batch loss 0.550527334 batch mAP 0.595123291 batch PCKh 0.375\n",
      "Trained batch 531 batch loss 0.508241832 batch mAP 0.603179932 batch PCKh 0.625\n",
      "Trained batch 532 batch loss 0.554268301 batch mAP 0.577697754 batch PCKh 0.125\n",
      "Trained batch 533 batch loss 0.531787515 batch mAP 0.632598877 batch PCKh 0.375\n",
      "Trained batch 534 batch loss 0.613325834 batch mAP 0.56439209 batch PCKh 0.0625\n",
      "Trained batch 535 batch loss 0.522104383 batch mAP 0.633117676 batch PCKh 0.8125\n",
      "Trained batch 536 batch loss 0.640041947 batch mAP 0.518859863 batch PCKh 0\n",
      "Trained batch 537 batch loss 0.575273275 batch mAP 0.482574463 batch PCKh 0.0625\n",
      "Trained batch 538 batch loss 0.630712 batch mAP 0.513549805 batch PCKh 0.0625\n",
      "Trained batch 539 batch loss 0.616454184 batch mAP 0.549469 batch PCKh 0.1875\n",
      "Trained batch 540 batch loss 0.599494934 batch mAP 0.462402344 batch PCKh 0.125\n",
      "Trained batch 541 batch loss 0.630343676 batch mAP 0.501617432 batch PCKh 0.3125\n",
      "Trained batch 542 batch loss 0.516289413 batch mAP 0.610229492 batch PCKh 0.25\n",
      "Trained batch 543 batch loss 0.528318584 batch mAP 0.581756592 batch PCKh 0.5\n",
      "Trained batch 544 batch loss 0.442091405 batch mAP 0.662017822 batch PCKh 0.3125\n",
      "Trained batch 545 batch loss 0.492598921 batch mAP 0.667266846 batch PCKh 0.6875\n",
      "Trained batch 546 batch loss 0.506888151 batch mAP 0.591491699 batch PCKh 0.875\n",
      "Trained batch 547 batch loss 0.608232617 batch mAP 0.555511475 batch PCKh 0.3125\n",
      "Trained batch 548 batch loss 0.628613234 batch mAP 0.491485596 batch PCKh 0.3125\n",
      "Trained batch 549 batch loss 0.612412751 batch mAP 0.600524902 batch PCKh 0.25\n",
      "Trained batch 550 batch loss 0.473047942 batch mAP 0.664276123 batch PCKh 0.5\n",
      "Trained batch 551 batch loss 0.46777153 batch mAP 0.712921143 batch PCKh 0.625\n",
      "Trained batch 552 batch loss 0.520132899 batch mAP 0.694885254 batch PCKh 0.5625\n",
      "Trained batch 553 batch loss 0.500364542 batch mAP 0.664520264 batch PCKh 0.625\n",
      "Trained batch 554 batch loss 0.472130924 batch mAP 0.669189453 batch PCKh 0.8125\n",
      "Trained batch 555 batch loss 0.533123136 batch mAP 0.638366699 batch PCKh 0.625\n",
      "Trained batch 556 batch loss 0.487272292 batch mAP 0.691955566 batch PCKh 0.6875\n",
      "Trained batch 557 batch loss 0.470260084 batch mAP 0.667480469 batch PCKh 0.5\n",
      "Trained batch 558 batch loss 0.445642292 batch mAP 0.67086792 batch PCKh 0.375\n",
      "Trained batch 559 batch loss 0.452266127 batch mAP 0.700927734 batch PCKh 0.4375\n",
      "Trained batch 560 batch loss 0.447779655 batch mAP 0.707397461 batch PCKh 0.375\n",
      "Trained batch 561 batch loss 0.527580619 batch mAP 0.677093506 batch PCKh 0.625\n",
      "Trained batch 562 batch loss 0.556645393 batch mAP 0.638427734 batch PCKh 0.375\n",
      "Trained batch 563 batch loss 0.541717529 batch mAP 0.664917 batch PCKh 0.4375\n",
      "Trained batch 564 batch loss 0.485029697 batch mAP 0.652771 batch PCKh 0.3125\n",
      "Trained batch 565 batch loss 0.449063122 batch mAP 0.649749756 batch PCKh 0.4375\n",
      "Trained batch 566 batch loss 0.519776344 batch mAP 0.60244751 batch PCKh 0.3125\n",
      "Trained batch 567 batch loss 0.5163697 batch mAP 0.602386475 batch PCKh 0.375\n",
      "Trained batch 568 batch loss 0.460816354 batch mAP 0.634429932 batch PCKh 0.75\n",
      "Trained batch 569 batch loss 0.423675299 batch mAP 0.679870605 batch PCKh 0.3125\n",
      "Trained batch 570 batch loss 0.472894192 batch mAP 0.673584 batch PCKh 0.6875\n",
      "Trained batch 571 batch loss 0.468422055 batch mAP 0.614563 batch PCKh 0.25\n",
      "Trained batch 572 batch loss 0.453312576 batch mAP 0.677032471 batch PCKh 0.375\n",
      "Trained batch 573 batch loss 0.444617867 batch mAP 0.695953369 batch PCKh 0.5\n",
      "Trained batch 574 batch loss 0.526569366 batch mAP 0.704559326 batch PCKh 0.4375\n",
      "Trained batch 575 batch loss 0.488971204 batch mAP 0.72253418 batch PCKh 0.5\n",
      "Trained batch 576 batch loss 0.373186499 batch mAP 0.741210938 batch PCKh 0.4375\n",
      "Trained batch 577 batch loss 0.398313046 batch mAP 0.749481201 batch PCKh 0.4375\n",
      "Trained batch 578 batch loss 0.484341025 batch mAP 0.630340576 batch PCKh 0.4375\n",
      "Trained batch 579 batch loss 0.503832221 batch mAP 0.612579346 batch PCKh 0.4375\n",
      "Trained batch 580 batch loss 0.474532574 batch mAP 0.638946533 batch PCKh 0.875\n",
      "Trained batch 581 batch loss 0.437733114 batch mAP 0.658844 batch PCKh 0.8125\n",
      "Trained batch 582 batch loss 0.478586555 batch mAP 0.64328 batch PCKh 0.8125\n",
      "Trained batch 583 batch loss 0.475431442 batch mAP 0.654663086 batch PCKh 0.3125\n",
      "Trained batch 584 batch loss 0.427644134 batch mAP 0.660400391 batch PCKh 0.3125\n",
      "Trained batch 585 batch loss 0.436459213 batch mAP 0.591247559 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 586 batch loss 0.424021304 batch mAP 0.674407959 batch PCKh 0.5\n",
      "Trained batch 587 batch loss 0.523210883 batch mAP 0.666259766 batch PCKh 0.625\n",
      "Trained batch 588 batch loss 0.46702835 batch mAP 0.684112549 batch PCKh 0.25\n",
      "Trained batch 589 batch loss 0.396416694 batch mAP 0.666503906 batch PCKh 0.25\n",
      "Trained batch 590 batch loss 0.539420485 batch mAP 0.520080566 batch PCKh 0.25\n",
      "Trained batch 591 batch loss 0.532907963 batch mAP 0.537658691 batch PCKh 0.5625\n",
      "Trained batch 592 batch loss 0.688180327 batch mAP 0.473419189 batch PCKh 0.0625\n",
      "Trained batch 593 batch loss 0.565231383 batch mAP 0.531280518 batch PCKh 0.6875\n",
      "Trained batch 594 batch loss 0.5884642 batch mAP 0.586456299 batch PCKh 0.5625\n",
      "Trained batch 595 batch loss 0.597862542 batch mAP 0.617614746 batch PCKh 0.6875\n",
      "Trained batch 596 batch loss 0.458224416 batch mAP 0.714477539 batch PCKh 0.5\n",
      "Trained batch 597 batch loss 0.471713901 batch mAP 0.65032959 batch PCKh 0.5\n",
      "Trained batch 598 batch loss 0.593438 batch mAP 0.594268799 batch PCKh 0.1875\n",
      "Trained batch 599 batch loss 0.619304836 batch mAP 0.573791504 batch PCKh 0.0625\n",
      "Trained batch 600 batch loss 0.526673734 batch mAP 0.598449707 batch PCKh 0.4375\n",
      "Trained batch 601 batch loss 0.506182909 batch mAP 0.649658203 batch PCKh 0.1875\n",
      "Trained batch 602 batch loss 0.521238267 batch mAP 0.652862549 batch PCKh 0.4375\n",
      "Trained batch 603 batch loss 0.580104828 batch mAP 0.585357666 batch PCKh 0.125\n",
      "Trained batch 604 batch loss 0.547705293 batch mAP 0.581115723 batch PCKh 0.5\n",
      "Trained batch 605 batch loss 0.534852743 batch mAP 0.62902832 batch PCKh 0.875\n",
      "Trained batch 606 batch loss 0.391322374 batch mAP 0.624786377 batch PCKh 0.75\n",
      "Trained batch 607 batch loss 0.443280697 batch mAP 0.567779541 batch PCKh 0.75\n",
      "Trained batch 608 batch loss 0.50587517 batch mAP 0.619842529 batch PCKh 0.375\n",
      "Trained batch 609 batch loss 0.594215751 batch mAP 0.54498291 batch PCKh 0.6875\n",
      "Trained batch 610 batch loss 0.540278912 batch mAP 0.602478 batch PCKh 0.75\n",
      "Trained batch 611 batch loss 0.483814895 batch mAP 0.594329834 batch PCKh 0.4375\n",
      "Trained batch 612 batch loss 0.494989067 batch mAP 0.608581543 batch PCKh 0.1875\n",
      "Trained batch 613 batch loss 0.505515814 batch mAP 0.635772705 batch PCKh 0.5625\n",
      "Trained batch 614 batch loss 0.53045404 batch mAP 0.610931396 batch PCKh 0.375\n",
      "Trained batch 615 batch loss 0.581425309 batch mAP 0.519287109 batch PCKh 0.75\n",
      "Trained batch 616 batch loss 0.590258241 batch mAP 0.531341553 batch PCKh 0.625\n",
      "Trained batch 617 batch loss 0.4771595 batch mAP 0.571899414 batch PCKh 0.75\n",
      "Trained batch 618 batch loss 0.438454419 batch mAP 0.66116333 batch PCKh 0.375\n",
      "Trained batch 619 batch loss 0.492976487 batch mAP 0.572967529 batch PCKh 0.5625\n",
      "Trained batch 620 batch loss 0.465471923 batch mAP 0.583953857 batch PCKh 0.125\n",
      "Trained batch 621 batch loss 0.535651922 batch mAP 0.637481689 batch PCKh 0.25\n",
      "Trained batch 622 batch loss 0.471058846 batch mAP 0.630645752 batch PCKh 0.375\n",
      "Trained batch 623 batch loss 0.591692328 batch mAP 0.599578857 batch PCKh 0.625\n",
      "Trained batch 624 batch loss 0.557818174 batch mAP 0.551452637 batch PCKh 0.25\n",
      "Trained batch 625 batch loss 0.514918089 batch mAP 0.587982178 batch PCKh 0.5\n",
      "Trained batch 626 batch loss 0.471750855 batch mAP 0.567840576 batch PCKh 0.5625\n",
      "Trained batch 627 batch loss 0.494888484 batch mAP 0.567504883 batch PCKh 0.8125\n",
      "Trained batch 628 batch loss 0.431291908 batch mAP 0.624725342 batch PCKh 0.75\n",
      "Trained batch 629 batch loss 0.60260725 batch mAP 0.625976562 batch PCKh 0.5\n",
      "Trained batch 630 batch loss 0.515779078 batch mAP 0.635345459 batch PCKh 0.5625\n",
      "Trained batch 631 batch loss 0.600982666 batch mAP 0.612915039 batch PCKh 0.5\n",
      "Trained batch 632 batch loss 0.547097862 batch mAP 0.569641113 batch PCKh 0.5625\n",
      "Trained batch 633 batch loss 0.473408878 batch mAP 0.487609863 batch PCKh 0.75\n",
      "Trained batch 634 batch loss 0.477215916 batch mAP 0.586395264 batch PCKh 0.875\n",
      "Trained batch 635 batch loss 0.471709937 batch mAP 0.588806152 batch PCKh 0.5625\n",
      "Trained batch 636 batch loss 0.493425459 batch mAP 0.549957275 batch PCKh 0.375\n",
      "Trained batch 637 batch loss 0.554063439 batch mAP 0.476898193 batch PCKh 0.375\n",
      "Trained batch 638 batch loss 0.513831496 batch mAP 0.536865234 batch PCKh 0.5625\n",
      "Trained batch 639 batch loss 0.509376764 batch mAP 0.557495117 batch PCKh 0.5625\n",
      "Trained batch 640 batch loss 0.53962338 batch mAP 0.586212158 batch PCKh 0.6875\n",
      "Trained batch 641 batch loss 0.579190254 batch mAP 0.625946045 batch PCKh 0.25\n",
      "Trained batch 642 batch loss 0.513053417 batch mAP 0.597076416 batch PCKh 0.4375\n",
      "Trained batch 643 batch loss 0.549157143 batch mAP 0.587493896 batch PCKh 0.375\n",
      "Trained batch 644 batch loss 0.487214535 batch mAP 0.602874756 batch PCKh 0.5625\n",
      "Trained batch 645 batch loss 0.483947 batch mAP 0.625152588 batch PCKh 0.6875\n",
      "Trained batch 646 batch loss 0.433609247 batch mAP 0.629577637 batch PCKh 0.5625\n",
      "Trained batch 647 batch loss 0.400547147 batch mAP 0.628570557 batch PCKh 0.5\n",
      "Trained batch 648 batch loss 0.589425802 batch mAP 0.490783691 batch PCKh 0.125\n",
      "Trained batch 649 batch loss 0.5071854 batch mAP 0.51953125 batch PCKh 0.125\n",
      "Trained batch 650 batch loss 0.581739068 batch mAP 0.531066895 batch PCKh 0.5625\n",
      "Trained batch 651 batch loss 0.555931211 batch mAP 0.501525879 batch PCKh 0.125\n",
      "Trained batch 652 batch loss 0.46338436 batch mAP 0.553619385 batch PCKh 0\n",
      "Trained batch 653 batch loss 0.4920955 batch mAP 0.542572 batch PCKh 0.1875\n",
      "Trained batch 654 batch loss 0.532602608 batch mAP 0.518127441 batch PCKh 0.125\n",
      "Trained batch 655 batch loss 0.657746911 batch mAP 0.389709473 batch PCKh 0.4375\n",
      "Trained batch 656 batch loss 0.583341241 batch mAP 0.524017334 batch PCKh 0.1875\n",
      "Trained batch 657 batch loss 0.646605372 batch mAP 0.539215088 batch PCKh 0.4375\n",
      "Trained batch 658 batch loss 0.43759644 batch mAP 0.600189209 batch PCKh 0.25\n",
      "Trained batch 659 batch loss 0.473400176 batch mAP 0.653656 batch PCKh 0.3125\n",
      "Trained batch 660 batch loss 0.414727509 batch mAP 0.667114258 batch PCKh 0.125\n",
      "Trained batch 661 batch loss 0.426784635 batch mAP 0.697723389 batch PCKh 0.3125\n",
      "Trained batch 662 batch loss 0.386221796 batch mAP 0.686218262 batch PCKh 0.5\n",
      "Trained batch 663 batch loss 0.504616618 batch mAP 0.591461182 batch PCKh 0.0625\n",
      "Trained batch 664 batch loss 0.388741374 batch mAP 0.68359375 batch PCKh 0.4375\n",
      "Trained batch 665 batch loss 0.523002148 batch mAP 0.623260498 batch PCKh 0.1875\n",
      "Trained batch 666 batch loss 0.519496441 batch mAP 0.646118164 batch PCKh 0.3125\n",
      "Trained batch 667 batch loss 0.475898147 batch mAP 0.621307373 batch PCKh 0.8125\n",
      "Trained batch 668 batch loss 0.575714707 batch mAP 0.574066162 batch PCKh 0.125\n",
      "Trained batch 669 batch loss 0.552342713 batch mAP 0.585235596 batch PCKh 0.75\n",
      "Trained batch 670 batch loss 0.545100391 batch mAP 0.583557129 batch PCKh 0.5\n",
      "Trained batch 671 batch loss 0.547808945 batch mAP 0.593414307 batch PCKh 0.4375\n",
      "Trained batch 672 batch loss 0.476866126 batch mAP 0.633087158 batch PCKh 0.1875\n",
      "Trained batch 673 batch loss 0.550368786 batch mAP 0.631561279 batch PCKh 0.375\n",
      "Trained batch 674 batch loss 0.483833313 batch mAP 0.674499512 batch PCKh 0.625\n",
      "Trained batch 675 batch loss 0.486623526 batch mAP 0.654510498 batch PCKh 0.1875\n",
      "Trained batch 676 batch loss 0.532131 batch mAP 0.665313721 batch PCKh 0.3125\n",
      "Trained batch 677 batch loss 0.489610374 batch mAP 0.657409668 batch PCKh 0.1875\n",
      "Trained batch 678 batch loss 0.537144482 batch mAP 0.516601562 batch PCKh 0.125\n",
      "Trained batch 679 batch loss 0.452490807 batch mAP 0.625213623 batch PCKh 0.5\n",
      "Trained batch 680 batch loss 0.525354564 batch mAP 0.627288818 batch PCKh 0.8125\n",
      "Trained batch 681 batch loss 0.452931613 batch mAP 0.559387207 batch PCKh 0.3125\n",
      "Trained batch 682 batch loss 0.666384935 batch mAP 0.555511475 batch PCKh 0.3125\n",
      "Trained batch 683 batch loss 0.587648153 batch mAP 0.613525391 batch PCKh 0.5625\n",
      "Trained batch 684 batch loss 0.537889242 batch mAP 0.573425293 batch PCKh 0.125\n",
      "Trained batch 685 batch loss 0.542180836 batch mAP 0.564605713 batch PCKh 0.6875\n",
      "Trained batch 686 batch loss 0.50637722 batch mAP 0.641418457 batch PCKh 0.875\n",
      "Trained batch 687 batch loss 0.377435148 batch mAP 0.55670166 batch PCKh 0.5625\n",
      "Trained batch 688 batch loss 0.411565542 batch mAP 0.563537598 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 689 batch loss 0.45845741 batch mAP 0.588562 batch PCKh 0.4375\n",
      "Trained batch 690 batch loss 0.409318626 batch mAP 0.601196289 batch PCKh 0.375\n",
      "Trained batch 691 batch loss 0.401990473 batch mAP 0.559417725 batch PCKh 0.4375\n",
      "Trained batch 692 batch loss 0.460749775 batch mAP 0.540252686 batch PCKh 0.4375\n",
      "Trained batch 693 batch loss 0.486820102 batch mAP 0.56640625 batch PCKh 0.5625\n",
      "Trained batch 694 batch loss 0.559911907 batch mAP 0.593658447 batch PCKh 0.25\n",
      "Trained batch 695 batch loss 0.649743199 batch mAP 0.572662354 batch PCKh 0.875\n",
      "Trained batch 696 batch loss 0.654027522 batch mAP 0.607452393 batch PCKh 0.375\n",
      "Trained batch 697 batch loss 0.593257189 batch mAP 0.634460449 batch PCKh 0.3125\n",
      "Trained batch 698 batch loss 0.445985258 batch mAP 0.672363281 batch PCKh 0.1875\n",
      "Trained batch 699 batch loss 0.523268223 batch mAP 0.662994385 batch PCKh 0.75\n",
      "Trained batch 700 batch loss 0.524916828 batch mAP 0.643127441 batch PCKh 0.3125\n",
      "Trained batch 701 batch loss 0.551353812 batch mAP 0.649963379 batch PCKh 0.875\n",
      "Trained batch 702 batch loss 0.571010351 batch mAP 0.697479248 batch PCKh 0.6875\n",
      "Trained batch 703 batch loss 0.580734372 batch mAP 0.62020874 batch PCKh 0.3125\n",
      "Trained batch 704 batch loss 0.538535416 batch mAP 0.595367432 batch PCKh 0.1875\n",
      "Trained batch 705 batch loss 0.522427857 batch mAP 0.653991699 batch PCKh 0.375\n",
      "Trained batch 706 batch loss 0.5603953 batch mAP 0.627868652 batch PCKh 0.3125\n",
      "Trained batch 707 batch loss 0.546060801 batch mAP 0.575592041 batch PCKh 0.3125\n",
      "Trained batch 708 batch loss 0.540742636 batch mAP 0.578460693 batch PCKh 0.5625\n",
      "Trained batch 709 batch loss 0.591284454 batch mAP 0.618896484 batch PCKh 0.875\n",
      "Trained batch 710 batch loss 0.486436069 batch mAP 0.633758545 batch PCKh 0.75\n",
      "Trained batch 711 batch loss 0.58489275 batch mAP 0.551544189 batch PCKh 0.4375\n",
      "Trained batch 712 batch loss 0.642069697 batch mAP 0.519989 batch PCKh 0.3125\n",
      "Trained batch 713 batch loss 0.60051918 batch mAP 0.516693115 batch PCKh 0.5\n",
      "Trained batch 714 batch loss 0.566789627 batch mAP 0.580810547 batch PCKh 0.625\n",
      "Trained batch 715 batch loss 0.572611868 batch mAP 0.555511475 batch PCKh 0.5\n",
      "Trained batch 716 batch loss 0.593864143 batch mAP 0.543548584 batch PCKh 0.8125\n",
      "Trained batch 717 batch loss 0.562706709 batch mAP 0.528564453 batch PCKh 0.3125\n",
      "Trained batch 718 batch loss 0.642569363 batch mAP 0.463470459 batch PCKh 0.75\n",
      "Trained batch 719 batch loss 0.55095613 batch mAP 0.484985352 batch PCKh 0.6875\n",
      "Trained batch 720 batch loss 0.500934124 batch mAP 0.511383057 batch PCKh 0.5625\n",
      "Trained batch 721 batch loss 0.523576736 batch mAP 0.550537109 batch PCKh 0.75\n",
      "Trained batch 722 batch loss 0.477122664 batch mAP 0.615386963 batch PCKh 0.4375\n",
      "Trained batch 723 batch loss 0.594348252 batch mAP 0.556152344 batch PCKh 0.5\n",
      "Trained batch 724 batch loss 0.518021703 batch mAP 0.600647 batch PCKh 0.4375\n",
      "Trained batch 725 batch loss 0.455363929 batch mAP 0.613952637 batch PCKh 0.75\n",
      "Trained batch 726 batch loss 0.5356552 batch mAP 0.578460693 batch PCKh 0.25\n",
      "Trained batch 727 batch loss 0.558125377 batch mAP 0.584747314 batch PCKh 0.6875\n",
      "Trained batch 728 batch loss 0.560464561 batch mAP 0.600128174 batch PCKh 0.75\n",
      "Trained batch 729 batch loss 0.533645809 batch mAP 0.606048584 batch PCKh 0.5\n",
      "Trained batch 730 batch loss 0.504749 batch mAP 0.620300293 batch PCKh 0.6875\n",
      "Trained batch 731 batch loss 0.520025194 batch mAP 0.594543457 batch PCKh 0.375\n",
      "Trained batch 732 batch loss 0.522173822 batch mAP 0.606536865 batch PCKh 0.3125\n",
      "Trained batch 733 batch loss 0.551322818 batch mAP 0.626922607 batch PCKh 0.5625\n",
      "Trained batch 734 batch loss 0.637504816 batch mAP 0.642456055 batch PCKh 0.375\n",
      "Trained batch 735 batch loss 0.52823925 batch mAP 0.670318604 batch PCKh 0.375\n",
      "Trained batch 736 batch loss 0.479893595 batch mAP 0.647979736 batch PCKh 0\n",
      "Trained batch 737 batch loss 0.59581852 batch mAP 0.617584229 batch PCKh 0.625\n",
      "Trained batch 738 batch loss 0.512733161 batch mAP 0.596923828 batch PCKh 0.25\n",
      "Trained batch 739 batch loss 0.530321717 batch mAP 0.605560303 batch PCKh 0.5\n",
      "Trained batch 740 batch loss 0.614058614 batch mAP 0.58682251 batch PCKh 0.25\n",
      "Trained batch 741 batch loss 0.570576072 batch mAP 0.537109375 batch PCKh 0.1875\n",
      "Trained batch 742 batch loss 0.439843088 batch mAP 0.574035645 batch PCKh 0.3125\n",
      "Trained batch 743 batch loss 0.494028211 batch mAP 0.619293213 batch PCKh 0.1875\n",
      "Trained batch 744 batch loss 0.515805 batch mAP 0.630371094 batch PCKh 0.5\n",
      "Trained batch 745 batch loss 0.572759509 batch mAP 0.578674316 batch PCKh 0.3125\n",
      "Trained batch 746 batch loss 0.579948425 batch mAP 0.565948486 batch PCKh 0.1875\n",
      "Trained batch 747 batch loss 0.559945345 batch mAP 0.505828857 batch PCKh 0.1875\n",
      "Trained batch 748 batch loss 0.418055564 batch mAP 0.404449463 batch PCKh 0.1875\n",
      "Trained batch 749 batch loss 0.386177599 batch mAP 0.322692871 batch PCKh 0\n",
      "Trained batch 750 batch loss 0.553288937 batch mAP 0.447692871 batch PCKh 0.25\n",
      "Trained batch 751 batch loss 0.659453094 batch mAP 0.415252686 batch PCKh 0\n",
      "Trained batch 752 batch loss 0.577373624 batch mAP 0.455047607 batch PCKh 0.625\n",
      "Trained batch 753 batch loss 0.611782789 batch mAP 0.505004883 batch PCKh 0.5625\n",
      "Trained batch 754 batch loss 0.625587225 batch mAP 0.490875244 batch PCKh 0.375\n",
      "Trained batch 755 batch loss 0.551438034 batch mAP 0.626617432 batch PCKh 0.4375\n",
      "Trained batch 756 batch loss 0.519844413 batch mAP 0.668029785 batch PCKh 0.75\n",
      "Trained batch 757 batch loss 0.460863888 batch mAP 0.693664551 batch PCKh 0.5625\n",
      "Trained batch 758 batch loss 0.401998341 batch mAP 0.719177246 batch PCKh 0.5\n",
      "Trained batch 759 batch loss 0.441283941 batch mAP 0.721344 batch PCKh 0.5\n",
      "Trained batch 760 batch loss 0.425844789 batch mAP 0.67453 batch PCKh 0.75\n",
      "Trained batch 761 batch loss 0.46167478 batch mAP 0.681915283 batch PCKh 0.5625\n",
      "Trained batch 762 batch loss 0.450063735 batch mAP 0.667511 batch PCKh 0.375\n",
      "Trained batch 763 batch loss 0.43078059 batch mAP 0.656707764 batch PCKh 0.5\n",
      "Trained batch 764 batch loss 0.469895869 batch mAP 0.686889648 batch PCKh 0.4375\n",
      "Trained batch 765 batch loss 0.393087536 batch mAP 0.722991943 batch PCKh 0.5\n",
      "Trained batch 766 batch loss 0.489552855 batch mAP 0.754394531 batch PCKh 0.4375\n",
      "Trained batch 767 batch loss 0.459079146 batch mAP 0.661895752 batch PCKh 0.375\n",
      "Trained batch 768 batch loss 0.47593841 batch mAP 0.604278564 batch PCKh 0.4375\n",
      "Trained batch 769 batch loss 0.492544979 batch mAP 0.564483643 batch PCKh 0.4375\n",
      "Trained batch 770 batch loss 0.448586702 batch mAP 0.648681641 batch PCKh 0.6875\n",
      "Trained batch 771 batch loss 0.494032651 batch mAP 0.607299805 batch PCKh 0.875\n",
      "Trained batch 772 batch loss 0.459245503 batch mAP 0.630401611 batch PCKh 0.875\n",
      "Trained batch 773 batch loss 0.461313456 batch mAP 0.648803711 batch PCKh 0.8125\n",
      "Trained batch 774 batch loss 0.487778872 batch mAP 0.624572754 batch PCKh 0.75\n",
      "Trained batch 775 batch loss 0.578213096 batch mAP 0.555755615 batch PCKh 0.75\n",
      "Trained batch 776 batch loss 0.526492715 batch mAP 0.591156 batch PCKh 0.875\n",
      "Trained batch 777 batch loss 0.534248531 batch mAP 0.579406738 batch PCKh 0.8125\n",
      "Trained batch 778 batch loss 0.57996428 batch mAP 0.534240723 batch PCKh 0.875\n",
      "Trained batch 779 batch loss 0.50690347 batch mAP 0.555084229 batch PCKh 0.375\n",
      "Trained batch 780 batch loss 0.467736095 batch mAP 0.520385742 batch PCKh 0.4375\n",
      "Trained batch 781 batch loss 0.528045535 batch mAP 0.58215332 batch PCKh 0.1875\n",
      "Trained batch 782 batch loss 0.514584064 batch mAP 0.651947 batch PCKh 0.375\n",
      "Trained batch 783 batch loss 0.466118157 batch mAP 0.708190918 batch PCKh 0.6875\n",
      "Trained batch 784 batch loss 0.491467416 batch mAP 0.641113281 batch PCKh 0.4375\n",
      "Trained batch 785 batch loss 0.598513722 batch mAP 0.617767334 batch PCKh 0.375\n",
      "Trained batch 786 batch loss 0.575673342 batch mAP 0.596405 batch PCKh 0.125\n",
      "Trained batch 787 batch loss 0.640309334 batch mAP 0.576141357 batch PCKh 0.625\n",
      "Trained batch 788 batch loss 0.572321475 batch mAP 0.620605469 batch PCKh 0.6875\n",
      "Trained batch 789 batch loss 0.551550865 batch mAP 0.563751221 batch PCKh 0.6875\n",
      "Trained batch 790 batch loss 0.538301408 batch mAP 0.575927734 batch PCKh 0.6875\n",
      "Trained batch 791 batch loss 0.593406081 batch mAP 0.589996338 batch PCKh 0.5\n",
      "Trained batch 792 batch loss 0.575284541 batch mAP 0.577758789 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 793 batch loss 0.491676271 batch mAP 0.603912354 batch PCKh 0.6875\n",
      "Trained batch 794 batch loss 0.551856637 batch mAP 0.618438721 batch PCKh 0.6875\n",
      "Trained batch 795 batch loss 0.523556828 batch mAP 0.610199 batch PCKh 0.875\n",
      "Trained batch 796 batch loss 0.60829556 batch mAP 0.530426 batch PCKh 0.4375\n",
      "Trained batch 797 batch loss 0.593085945 batch mAP 0.592224121 batch PCKh 0.875\n",
      "Trained batch 798 batch loss 0.529895246 batch mAP 0.604888916 batch PCKh 0.625\n",
      "Trained batch 799 batch loss 0.498286366 batch mAP 0.658477783 batch PCKh 0.5\n",
      "Trained batch 800 batch loss 0.549692154 batch mAP 0.566894531 batch PCKh 0.5\n",
      "Trained batch 801 batch loss 0.562180281 batch mAP 0.566436768 batch PCKh 0.5625\n",
      "Trained batch 802 batch loss 0.52680105 batch mAP 0.638000488 batch PCKh 0.4375\n",
      "Trained batch 803 batch loss 0.585967839 batch mAP 0.554901123 batch PCKh 0.625\n",
      "Trained batch 804 batch loss 0.521727324 batch mAP 0.589447 batch PCKh 0.25\n",
      "Trained batch 805 batch loss 0.51040864 batch mAP 0.645874 batch PCKh 0.5\n",
      "Trained batch 806 batch loss 0.504770696 batch mAP 0.678649902 batch PCKh 0.8125\n",
      "Trained batch 807 batch loss 0.560010135 batch mAP 0.613739 batch PCKh 0.625\n",
      "Trained batch 808 batch loss 0.608811855 batch mAP 0.530303955 batch PCKh 0.5625\n",
      "Trained batch 809 batch loss 0.566646576 batch mAP 0.570282 batch PCKh 0.75\n",
      "Trained batch 810 batch loss 0.416179299 batch mAP 0.664855957 batch PCKh 0.75\n",
      "Trained batch 811 batch loss 0.48368 batch mAP 0.58581543 batch PCKh 0.5\n",
      "Trained batch 812 batch loss 0.552306652 batch mAP 0.587677 batch PCKh 0.75\n",
      "Trained batch 813 batch loss 0.524038732 batch mAP 0.595336914 batch PCKh 0.5625\n",
      "Trained batch 814 batch loss 0.511812329 batch mAP 0.506011963 batch PCKh 0.1875\n",
      "Trained batch 815 batch loss 0.512987554 batch mAP 0.513946533 batch PCKh 0.1875\n",
      "Trained batch 816 batch loss 0.501061857 batch mAP 0.544311523 batch PCKh 0.5\n",
      "Trained batch 817 batch loss 0.556834459 batch mAP 0.517150879 batch PCKh 0.75\n",
      "Trained batch 818 batch loss 0.584666133 batch mAP 0.521484375 batch PCKh 0.5625\n",
      "Trained batch 819 batch loss 0.503358483 batch mAP 0.592315674 batch PCKh 0.6875\n",
      "Trained batch 820 batch loss 0.506119549 batch mAP 0.569702148 batch PCKh 0.4375\n",
      "Trained batch 821 batch loss 0.423594356 batch mAP 0.598693848 batch PCKh 0.4375\n",
      "Trained batch 822 batch loss 0.434271544 batch mAP 0.661407471 batch PCKh 0.75\n",
      "Trained batch 823 batch loss 0.531190097 batch mAP 0.560089111 batch PCKh 0.5625\n",
      "Trained batch 824 batch loss 0.489142686 batch mAP 0.617767334 batch PCKh 0\n",
      "Trained batch 825 batch loss 0.476278603 batch mAP 0.632537842 batch PCKh 0.5\n",
      "Trained batch 826 batch loss 0.525669634 batch mAP 0.657165527 batch PCKh 0.5625\n",
      "Trained batch 827 batch loss 0.518660426 batch mAP 0.668945312 batch PCKh 0.5\n",
      "Trained batch 828 batch loss 0.464601696 batch mAP 0.6222229 batch PCKh 0.625\n",
      "Trained batch 829 batch loss 0.507222772 batch mAP 0.641571045 batch PCKh 0.6875\n",
      "Trained batch 830 batch loss 0.583561957 batch mAP 0.602050781 batch PCKh 0.5625\n",
      "Trained batch 831 batch loss 0.387388945 batch mAP 0.724090576 batch PCKh 0.4375\n",
      "Trained batch 832 batch loss 0.452663898 batch mAP 0.687164307 batch PCKh 0.5625\n",
      "Trained batch 833 batch loss 0.411076099 batch mAP 0.680267334 batch PCKh 0.5\n",
      "Trained batch 834 batch loss 0.408396542 batch mAP 0.711120605 batch PCKh 0.75\n",
      "Trained batch 835 batch loss 0.374073923 batch mAP 0.774871826 batch PCKh 0.5625\n",
      "Trained batch 836 batch loss 0.377414912 batch mAP 0.726532 batch PCKh 0.1875\n",
      "Trained batch 837 batch loss 0.486909777 batch mAP 0.680786133 batch PCKh 0.3125\n",
      "Trained batch 838 batch loss 0.612539 batch mAP 0.595245361 batch PCKh 0.6875\n",
      "Trained batch 839 batch loss 0.604984045 batch mAP 0.592193604 batch PCKh 0.25\n",
      "Trained batch 840 batch loss 0.589222074 batch mAP 0.616638184 batch PCKh 0.375\n",
      "Trained batch 841 batch loss 0.581065297 batch mAP 0.570495605 batch PCKh 0.8125\n",
      "Trained batch 842 batch loss 0.631733298 batch mAP 0.52746582 batch PCKh 0.4375\n",
      "Trained batch 843 batch loss 0.610414147 batch mAP 0.576019287 batch PCKh 0.4375\n",
      "Trained batch 844 batch loss 0.608161092 batch mAP 0.556610107 batch PCKh 0.3125\n",
      "Trained batch 845 batch loss 0.599168122 batch mAP 0.560760498 batch PCKh 0.3125\n",
      "Trained batch 846 batch loss 0.580912948 batch mAP 0.584747314 batch PCKh 0.625\n",
      "Trained batch 847 batch loss 0.562452197 batch mAP 0.631317139 batch PCKh 0.25\n",
      "Trained batch 848 batch loss 0.545286119 batch mAP 0.585540771 batch PCKh 0.5\n",
      "Trained batch 849 batch loss 0.546025872 batch mAP 0.582550049 batch PCKh 0.375\n",
      "Trained batch 850 batch loss 0.584089577 batch mAP 0.607269287 batch PCKh 0.625\n",
      "Trained batch 851 batch loss 0.536168516 batch mAP 0.657135 batch PCKh 0.4375\n",
      "Trained batch 852 batch loss 0.446357131 batch mAP 0.685546875 batch PCKh 0.75\n",
      "Trained batch 853 batch loss 0.432502 batch mAP 0.692810059 batch PCKh 0.625\n",
      "Trained batch 854 batch loss 0.497944146 batch mAP 0.608215332 batch PCKh 0.3125\n",
      "Trained batch 855 batch loss 0.543088555 batch mAP 0.551757812 batch PCKh 0.3125\n",
      "Trained batch 856 batch loss 0.527206779 batch mAP 0.620330811 batch PCKh 0.375\n",
      "Trained batch 857 batch loss 0.486082613 batch mAP 0.686859131 batch PCKh 0.3125\n",
      "Trained batch 858 batch loss 0.500876129 batch mAP 0.69708252 batch PCKh 0.375\n",
      "Trained batch 859 batch loss 0.487365961 batch mAP 0.640625 batch PCKh 0.5625\n",
      "Trained batch 860 batch loss 0.532208741 batch mAP 0.673797607 batch PCKh 0.875\n",
      "Trained batch 861 batch loss 0.580213368 batch mAP 0.683105469 batch PCKh 0.6875\n",
      "Trained batch 862 batch loss 0.552277923 batch mAP 0.653900146 batch PCKh 0.8125\n",
      "Trained batch 863 batch loss 0.578511119 batch mAP 0.680145264 batch PCKh 0.75\n",
      "Trained batch 864 batch loss 0.589424133 batch mAP 0.676239 batch PCKh 0.4375\n",
      "Trained batch 865 batch loss 0.477621645 batch mAP 0.648071289 batch PCKh 0.625\n",
      "Trained batch 866 batch loss 0.50850147 batch mAP 0.702362061 batch PCKh 0.75\n",
      "Trained batch 867 batch loss 0.562134624 batch mAP 0.590789795 batch PCKh 0.4375\n",
      "Trained batch 868 batch loss 0.552480042 batch mAP 0.579864502 batch PCKh 0.5625\n",
      "Trained batch 869 batch loss 0.532740653 batch mAP 0.558136 batch PCKh 0.625\n",
      "Trained batch 870 batch loss 0.582606316 batch mAP 0.561401367 batch PCKh 0.75\n",
      "Trained batch 871 batch loss 0.449370325 batch mAP 0.624023438 batch PCKh 0.625\n",
      "Trained batch 872 batch loss 0.460567 batch mAP 0.615936279 batch PCKh 0.6875\n",
      "Trained batch 873 batch loss 0.426633716 batch mAP 0.632598877 batch PCKh 0.6875\n",
      "Trained batch 874 batch loss 0.457333237 batch mAP 0.634155273 batch PCKh 0.75\n",
      "Trained batch 875 batch loss 0.412439227 batch mAP 0.626434326 batch PCKh 0.6875\n",
      "Trained batch 876 batch loss 0.377803922 batch mAP 0.620513916 batch PCKh 0.6875\n",
      "Trained batch 877 batch loss 0.391810656 batch mAP 0.626037598 batch PCKh 0.625\n",
      "Trained batch 878 batch loss 0.351971358 batch mAP 0.638427734 batch PCKh 0.625\n",
      "Trained batch 879 batch loss 0.593407154 batch mAP 0.542053223 batch PCKh 0.875\n",
      "Trained batch 880 batch loss 0.582766891 batch mAP 0.576690674 batch PCKh 0.6875\n",
      "Trained batch 881 batch loss 0.563230276 batch mAP 0.524017334 batch PCKh 0.6875\n",
      "Trained batch 882 batch loss 0.505724907 batch mAP 0.599700928 batch PCKh 0.6875\n",
      "Trained batch 883 batch loss 0.629425287 batch mAP 0.542053223 batch PCKh 0.1875\n",
      "Trained batch 884 batch loss 0.528753638 batch mAP 0.629333496 batch PCKh 0.375\n",
      "Trained batch 885 batch loss 0.559874058 batch mAP 0.628967285 batch PCKh 0.375\n",
      "Trained batch 886 batch loss 0.480956674 batch mAP 0.646881104 batch PCKh 0.375\n",
      "Trained batch 887 batch loss 0.475654095 batch mAP 0.686737061 batch PCKh 0.5\n",
      "Trained batch 888 batch loss 0.453981 batch mAP 0.673675537 batch PCKh 0.4375\n",
      "Trained batch 889 batch loss 0.446067691 batch mAP 0.683502197 batch PCKh 0.5625\n",
      "Trained batch 890 batch loss 0.421808064 batch mAP 0.690429688 batch PCKh 0.625\n",
      "Trained batch 891 batch loss 0.47950536 batch mAP 0.65448 batch PCKh 0.625\n",
      "Trained batch 892 batch loss 0.589460611 batch mAP 0.661804199 batch PCKh 0.375\n",
      "Trained batch 893 batch loss 0.567210793 batch mAP 0.636627197 batch PCKh 0.875\n",
      "Trained batch 894 batch loss 0.49142617 batch mAP 0.671020508 batch PCKh 0.4375\n",
      "Trained batch 895 batch loss 0.483495176 batch mAP 0.657684326 batch PCKh 0.875\n",
      "Trained batch 896 batch loss 0.538666368 batch mAP 0.65512085 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 897 batch loss 0.451623857 batch mAP 0.61203 batch PCKh 0.3125\n",
      "Trained batch 898 batch loss 0.519492924 batch mAP 0.649169922 batch PCKh 0.75\n",
      "Trained batch 899 batch loss 0.585216165 batch mAP 0.605865479 batch PCKh 0.5625\n",
      "Trained batch 900 batch loss 0.532495797 batch mAP 0.60647583 batch PCKh 0.6875\n",
      "Trained batch 901 batch loss 0.646942 batch mAP 0.541229248 batch PCKh 0.6875\n",
      "Trained batch 902 batch loss 0.634075522 batch mAP 0.520050049 batch PCKh 0.8125\n",
      "Trained batch 903 batch loss 0.481297642 batch mAP 0.549804688 batch PCKh 0.75\n",
      "Trained batch 904 batch loss 0.50411737 batch mAP 0.635406494 batch PCKh 0.875\n",
      "Trained batch 905 batch loss 0.51154232 batch mAP 0.618530273 batch PCKh 0.75\n",
      "Trained batch 906 batch loss 0.503200948 batch mAP 0.605224609 batch PCKh 0.875\n",
      "Trained batch 907 batch loss 0.53783 batch mAP 0.556640625 batch PCKh 0.75\n",
      "Trained batch 908 batch loss 0.419956863 batch mAP 0.612976074 batch PCKh 0\n",
      "Trained batch 909 batch loss 0.454354346 batch mAP 0.619384766 batch PCKh 0.6875\n",
      "Trained batch 910 batch loss 0.45592165 batch mAP 0.63848877 batch PCKh 0.6875\n",
      "Trained batch 911 batch loss 0.440726936 batch mAP 0.629089355 batch PCKh 0.5\n",
      "Trained batch 912 batch loss 0.513967752 batch mAP 0.601165771 batch PCKh 0.4375\n",
      "Trained batch 913 batch loss 0.514013886 batch mAP 0.616485596 batch PCKh 0.4375\n",
      "Trained batch 914 batch loss 0.544858038 batch mAP 0.594390869 batch PCKh 0.625\n",
      "Trained batch 915 batch loss 0.517879 batch mAP 0.595397949 batch PCKh 0.75\n",
      "Trained batch 916 batch loss 0.505269527 batch mAP 0.56149292 batch PCKh 0.8125\n",
      "Trained batch 917 batch loss 0.524127364 batch mAP 0.573699951 batch PCKh 0.5\n",
      "Trained batch 918 batch loss 0.566152453 batch mAP 0.489715576 batch PCKh 0.1875\n",
      "Trained batch 919 batch loss 0.508288383 batch mAP 0.537323 batch PCKh 0.625\n",
      "Trained batch 920 batch loss 0.576114058 batch mAP 0.507354736 batch PCKh 0.8125\n",
      "Trained batch 921 batch loss 0.512473166 batch mAP 0.497192383 batch PCKh 0\n",
      "Trained batch 922 batch loss 0.43775019 batch mAP 0.534423828 batch PCKh 0\n",
      "Trained batch 923 batch loss 0.394736767 batch mAP 0.5831604 batch PCKh 0.4375\n",
      "Trained batch 924 batch loss 0.370901793 batch mAP 0.683319092 batch PCKh 0.625\n",
      "Trained batch 925 batch loss 0.449195743 batch mAP 0.687103271 batch PCKh 0.75\n",
      "Trained batch 926 batch loss 0.505103946 batch mAP 0.682067871 batch PCKh 0.6875\n",
      "Trained batch 927 batch loss 0.57944572 batch mAP 0.608764648 batch PCKh 0.5625\n",
      "Trained batch 928 batch loss 0.538209558 batch mAP 0.664520264 batch PCKh 0.75\n",
      "Trained batch 929 batch loss 0.533660829 batch mAP 0.624389648 batch PCKh 0.5\n",
      "Trained batch 930 batch loss 0.66943121 batch mAP 0.478485107 batch PCKh 0.5625\n",
      "Trained batch 931 batch loss 0.569881201 batch mAP 0.511413574 batch PCKh 0.5\n",
      "Trained batch 932 batch loss 0.49935019 batch mAP 0.659606934 batch PCKh 0.625\n",
      "Trained batch 933 batch loss 0.586464107 batch mAP 0.585449219 batch PCKh 0.375\n",
      "Trained batch 934 batch loss 0.616795301 batch mAP 0.578430176 batch PCKh 0.5\n",
      "Trained batch 935 batch loss 0.600861251 batch mAP 0.59854126 batch PCKh 0.3125\n",
      "Trained batch 936 batch loss 0.614295185 batch mAP 0.614685059 batch PCKh 0.0625\n",
      "Trained batch 937 batch loss 0.491934717 batch mAP 0.675689697 batch PCKh 0.375\n",
      "Trained batch 938 batch loss 0.525565624 batch mAP 0.611907959 batch PCKh 0.0625\n",
      "Trained batch 939 batch loss 0.548974276 batch mAP 0.61328125 batch PCKh 0.3125\n",
      "Trained batch 940 batch loss 0.566278696 batch mAP 0.553527832 batch PCKh 0.125\n",
      "Trained batch 941 batch loss 0.495069593 batch mAP 0.599365234 batch PCKh 0.8125\n",
      "Trained batch 942 batch loss 0.536131382 batch mAP 0.613250732 batch PCKh 0.8125\n",
      "Trained batch 943 batch loss 0.500893056 batch mAP 0.587738037 batch PCKh 0.375\n",
      "Trained batch 944 batch loss 0.569872439 batch mAP 0.598815918 batch PCKh 0.6875\n",
      "Trained batch 945 batch loss 0.535073578 batch mAP 0.569397 batch PCKh 0.5625\n",
      "Trained batch 946 batch loss 0.558127642 batch mAP 0.592376709 batch PCKh 0.4375\n",
      "Trained batch 947 batch loss 0.601540625 batch mAP 0.530517578 batch PCKh 0.625\n",
      "Trained batch 948 batch loss 0.585830331 batch mAP 0.499298096 batch PCKh 0.5\n",
      "Trained batch 949 batch loss 0.579635739 batch mAP 0.5809021 batch PCKh 0.75\n",
      "Trained batch 950 batch loss 0.598885179 batch mAP 0.557891846 batch PCKh 0.4375\n",
      "Trained batch 951 batch loss 0.585936189 batch mAP 0.55456543 batch PCKh 0.375\n",
      "Trained batch 952 batch loss 0.532926083 batch mAP 0.525482178 batch PCKh 0.25\n",
      "Trained batch 953 batch loss 0.51544 batch mAP 0.575408936 batch PCKh 0.4375\n",
      "Trained batch 954 batch loss 0.533162355 batch mAP 0.520233154 batch PCKh 0.5\n",
      "Trained batch 955 batch loss 0.503434658 batch mAP 0.527099609 batch PCKh 0.0625\n",
      "Trained batch 956 batch loss 0.561227679 batch mAP 0.450317383 batch PCKh 0.0625\n",
      "Trained batch 957 batch loss 0.552297175 batch mAP 0.477844238 batch PCKh 0.0625\n",
      "Trained batch 958 batch loss 0.689641476 batch mAP 0.535919189 batch PCKh 0.1875\n",
      "Trained batch 959 batch loss 0.562369049 batch mAP 0.581756592 batch PCKh 0.5625\n",
      "Trained batch 960 batch loss 0.616532564 batch mAP 0.649169922 batch PCKh 0.1875\n",
      "Trained batch 961 batch loss 0.617801785 batch mAP 0.636474609 batch PCKh 0.375\n",
      "Trained batch 962 batch loss 0.613023818 batch mAP 0.621490479 batch PCKh 0.375\n",
      "Trained batch 963 batch loss 0.63006866 batch mAP 0.516418457 batch PCKh 0.6875\n",
      "Trained batch 964 batch loss 0.660575628 batch mAP 0.397705078 batch PCKh 0.6875\n",
      "Trained batch 965 batch loss 0.577744484 batch mAP 0.373321533 batch PCKh 0.4375\n",
      "Trained batch 966 batch loss 0.562950253 batch mAP 0.398010254 batch PCKh 0.875\n",
      "Trained batch 967 batch loss 0.565453887 batch mAP 0.452697754 batch PCKh 0.5\n",
      "Trained batch 968 batch loss 0.523961842 batch mAP 0.528381348 batch PCKh 0.5\n",
      "Trained batch 969 batch loss 0.574465513 batch mAP 0.455841064 batch PCKh 0.5\n",
      "Trained batch 970 batch loss 0.525411427 batch mAP 0.495636 batch PCKh 0.25\n",
      "Trained batch 971 batch loss 0.568242192 batch mAP 0.374969482 batch PCKh 0.4375\n",
      "Trained batch 972 batch loss 0.536060214 batch mAP 0.512207031 batch PCKh 0.625\n",
      "Trained batch 973 batch loss 0.569104195 batch mAP 0.493988037 batch PCKh 0.25\n",
      "Trained batch 974 batch loss 0.653416872 batch mAP 0.530395508 batch PCKh 0.6875\n",
      "Trained batch 975 batch loss 0.560100913 batch mAP 0.646972656 batch PCKh 0.5\n",
      "Trained batch 976 batch loss 0.461037874 batch mAP 0.550048828 batch PCKh 0.3125\n",
      "Trained batch 977 batch loss 0.572306812 batch mAP 0.548339844 batch PCKh 0.3125\n",
      "Trained batch 978 batch loss 0.470442504 batch mAP 0.606384277 batch PCKh 0.5625\n",
      "Trained batch 979 batch loss 0.605110407 batch mAP 0.618530273 batch PCKh 0.6875\n",
      "Trained batch 980 batch loss 0.55145371 batch mAP 0.551940918 batch PCKh 0.75\n",
      "Trained batch 981 batch loss 0.449148 batch mAP 0.498565674 batch PCKh 0.5625\n",
      "Trained batch 982 batch loss 0.507892728 batch mAP 0.5496521 batch PCKh 0.5625\n",
      "Trained batch 983 batch loss 0.474990398 batch mAP 0.528106689 batch PCKh 0.25\n",
      "Trained batch 984 batch loss 0.490126222 batch mAP 0.608825684 batch PCKh 0.375\n",
      "Trained batch 985 batch loss 0.416349113 batch mAP 0.582122803 batch PCKh 0.625\n",
      "Trained batch 986 batch loss 0.495995969 batch mAP 0.535949707 batch PCKh 0.75\n",
      "Trained batch 987 batch loss 0.380232334 batch mAP 0.579895 batch PCKh 0.75\n",
      "Trained batch 988 batch loss 0.393365234 batch mAP 0.615631104 batch PCKh 0.75\n",
      "Trained batch 989 batch loss 0.41694358 batch mAP 0.589111328 batch PCKh 0.75\n",
      "Trained batch 990 batch loss 0.568536341 batch mAP 0.519744873 batch PCKh 0.625\n",
      "Trained batch 991 batch loss 0.596996188 batch mAP 0.481140137 batch PCKh 0.5\n",
      "Trained batch 992 batch loss 0.523223 batch mAP 0.558288574 batch PCKh 0.75\n",
      "Trained batch 993 batch loss 0.495423734 batch mAP 0.502380371 batch PCKh 0.875\n",
      "Trained batch 994 batch loss 0.546421 batch mAP 0.501678467 batch PCKh 0.6875\n",
      "Trained batch 995 batch loss 0.630969465 batch mAP 0.50201416 batch PCKh 0.4375\n",
      "Trained batch 996 batch loss 0.55196 batch mAP 0.575164795 batch PCKh 0.4375\n",
      "Trained batch 997 batch loss 0.553466082 batch mAP 0.532592773 batch PCKh 0.75\n",
      "Trained batch 998 batch loss 0.589210629 batch mAP 0.521575928 batch PCKh 0.0625\n",
      "Trained batch 999 batch loss 0.527602613 batch mAP 0.513214111 batch PCKh 0.625\n",
      "Trained batch 1000 batch loss 0.560619354 batch mAP 0.588897705 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1001 batch loss 0.650261819 batch mAP 0.524780273 batch PCKh 0.375\n",
      "Trained batch 1002 batch loss 0.562474608 batch mAP 0.582702637 batch PCKh 0.625\n",
      "Trained batch 1003 batch loss 0.496214122 batch mAP 0.567565918 batch PCKh 0.25\n",
      "Trained batch 1004 batch loss 0.498214394 batch mAP 0.542511 batch PCKh 0.4375\n",
      "Trained batch 1005 batch loss 0.530170143 batch mAP 0.594482422 batch PCKh 0.75\n",
      "Trained batch 1006 batch loss 0.563920259 batch mAP 0.561340332 batch PCKh 0.625\n",
      "Trained batch 1007 batch loss 0.636728823 batch mAP 0.559356689 batch PCKh 0.625\n",
      "Trained batch 1008 batch loss 0.493824244 batch mAP 0.630737305 batch PCKh 0.6875\n",
      "Trained batch 1009 batch loss 0.500112593 batch mAP 0.613525391 batch PCKh 0.625\n",
      "Trained batch 1010 batch loss 0.501853585 batch mAP 0.602081299 batch PCKh 0.5\n",
      "Trained batch 1011 batch loss 0.55853039 batch mAP 0.574737549 batch PCKh 0.6875\n",
      "Trained batch 1012 batch loss 0.535118222 batch mAP 0.602905273 batch PCKh 0.5625\n",
      "Trained batch 1013 batch loss 0.477407902 batch mAP 0.61038208 batch PCKh 0.375\n",
      "Trained batch 1014 batch loss 0.491337627 batch mAP 0.579711914 batch PCKh 0.75\n",
      "Trained batch 1015 batch loss 0.482664973 batch mAP 0.578643799 batch PCKh 0.5\n",
      "Trained batch 1016 batch loss 0.553202927 batch mAP 0.555145264 batch PCKh 0.6875\n",
      "Trained batch 1017 batch loss 0.48896113 batch mAP 0.55078125 batch PCKh 0\n",
      "Trained batch 1018 batch loss 0.455020547 batch mAP 0.639038086 batch PCKh 0.5625\n",
      "Trained batch 1019 batch loss 0.532418609 batch mAP 0.686035156 batch PCKh 0.375\n",
      "Trained batch 1020 batch loss 0.510085762 batch mAP 0.654418945 batch PCKh 0.5\n",
      "Trained batch 1021 batch loss 0.502551496 batch mAP 0.703460693 batch PCKh 0.6875\n",
      "Trained batch 1022 batch loss 0.477949739 batch mAP 0.652160645 batch PCKh 0.6875\n",
      "Trained batch 1023 batch loss 0.54824388 batch mAP 0.656402588 batch PCKh 0.375\n",
      "Trained batch 1024 batch loss 0.598049045 batch mAP 0.573242188 batch PCKh 0.875\n",
      "Trained batch 1025 batch loss 0.56597662 batch mAP 0.566680908 batch PCKh 0.6875\n",
      "Trained batch 1026 batch loss 0.607216597 batch mAP 0.566497803 batch PCKh 0.5\n",
      "Trained batch 1027 batch loss 0.624418 batch mAP 0.583984375 batch PCKh 0.1875\n",
      "Trained batch 1028 batch loss 0.596427083 batch mAP 0.537902832 batch PCKh 0.5\n",
      "Trained batch 1029 batch loss 0.55898124 batch mAP 0.578155518 batch PCKh 0.1875\n",
      "Trained batch 1030 batch loss 0.558753908 batch mAP 0.556396484 batch PCKh 0.5625\n",
      "Trained batch 1031 batch loss 0.605316758 batch mAP 0.548797607 batch PCKh 0.4375\n",
      "Trained batch 1032 batch loss 0.535523593 batch mAP 0.548431396 batch PCKh 0.5625\n",
      "Trained batch 1033 batch loss 0.519650102 batch mAP 0.514526367 batch PCKh 0.625\n",
      "Trained batch 1034 batch loss 0.469919533 batch mAP 0.48034668 batch PCKh 0.3125\n",
      "Trained batch 1035 batch loss 0.451675475 batch mAP 0.554290771 batch PCKh 0.75\n",
      "Trained batch 1036 batch loss 0.430844128 batch mAP 0.562835693 batch PCKh 0.5625\n",
      "Trained batch 1037 batch loss 0.39204663 batch mAP 0.514984131 batch PCKh 0.5\n",
      "Trained batch 1038 batch loss 0.360700727 batch mAP 0.576873779 batch PCKh 0.75\n",
      "Trained batch 1039 batch loss 0.38881278 batch mAP 0.587188721 batch PCKh 0.6875\n",
      "Trained batch 1040 batch loss 0.401068866 batch mAP 0.615478516 batch PCKh 0.75\n",
      "Trained batch 1041 batch loss 0.442037135 batch mAP 0.599121094 batch PCKh 0.6875\n",
      "Trained batch 1042 batch loss 0.390337616 batch mAP 0.625152588 batch PCKh 0.75\n",
      "Trained batch 1043 batch loss 0.38717097 batch mAP 0.634552 batch PCKh 0.5625\n",
      "Trained batch 1044 batch loss 0.347801507 batch mAP 0.685028076 batch PCKh 0\n",
      "Trained batch 1045 batch loss 0.531923234 batch mAP 0.602630615 batch PCKh 0.5\n",
      "Trained batch 1046 batch loss 0.558114588 batch mAP 0.634643555 batch PCKh 0.625\n",
      "Trained batch 1047 batch loss 0.63878727 batch mAP 0.592468262 batch PCKh 0.4375\n",
      "Trained batch 1048 batch loss 0.606551528 batch mAP 0.578308105 batch PCKh 0.5\n",
      "Trained batch 1049 batch loss 0.533054471 batch mAP 0.607605 batch PCKh 0.75\n",
      "Trained batch 1050 batch loss 0.550773263 batch mAP 0.564849854 batch PCKh 0.375\n",
      "Trained batch 1051 batch loss 0.555946529 batch mAP 0.547393799 batch PCKh 0.5\n",
      "Trained batch 1052 batch loss 0.542330265 batch mAP 0.610931396 batch PCKh 0.875\n",
      "Trained batch 1053 batch loss 0.578404486 batch mAP 0.574035645 batch PCKh 0.75\n",
      "Trained batch 1054 batch loss 0.546282411 batch mAP 0.614959717 batch PCKh 0.375\n",
      "Trained batch 1055 batch loss 0.715807378 batch mAP 0.58203125 batch PCKh 0.3125\n",
      "Trained batch 1056 batch loss 0.603023887 batch mAP 0.626251221 batch PCKh 0.5625\n",
      "Trained batch 1057 batch loss 0.59310019 batch mAP 0.650482178 batch PCKh 0.6875\n",
      "Trained batch 1058 batch loss 0.498661816 batch mAP 0.65234375 batch PCKh 0.625\n",
      "Trained batch 1059 batch loss 0.507493258 batch mAP 0.615753174 batch PCKh 0.5625\n",
      "Trained batch 1060 batch loss 0.604211807 batch mAP 0.622070312 batch PCKh 0.6875\n",
      "Trained batch 1061 batch loss 0.57517463 batch mAP 0.614624 batch PCKh 0.6875\n",
      "Trained batch 1062 batch loss 0.576432288 batch mAP 0.592926 batch PCKh 0.4375\n",
      "Trained batch 1063 batch loss 0.563572288 batch mAP 0.54888916 batch PCKh 0.375\n",
      "Trained batch 1064 batch loss 0.554801643 batch mAP 0.653656 batch PCKh 0.4375\n",
      "Trained batch 1065 batch loss 0.549882889 batch mAP 0.555450439 batch PCKh 0.375\n",
      "Trained batch 1066 batch loss 0.424234241 batch mAP 0.629089355 batch PCKh 0.3125\n",
      "Trained batch 1067 batch loss 0.472204983 batch mAP 0.621154785 batch PCKh 0.4375\n",
      "Trained batch 1068 batch loss 0.461135983 batch mAP 0.642425537 batch PCKh 0.4375\n",
      "Trained batch 1069 batch loss 0.510531425 batch mAP 0.604827881 batch PCKh 0.6875\n",
      "Trained batch 1070 batch loss 0.394037575 batch mAP 0.622650146 batch PCKh 0.3125\n",
      "Trained batch 1071 batch loss 0.360308766 batch mAP 0.605285645 batch PCKh 0\n",
      "Trained batch 1072 batch loss 0.459721744 batch mAP 0.595733643 batch PCKh 0.4375\n",
      "Trained batch 1073 batch loss 0.430553436 batch mAP 0.642486572 batch PCKh 0.25\n",
      "Trained batch 1074 batch loss 0.501511 batch mAP 0.631256104 batch PCKh 0.625\n",
      "Trained batch 1075 batch loss 0.546826422 batch mAP 0.593994141 batch PCKh 0.1875\n",
      "Trained batch 1076 batch loss 0.560224295 batch mAP 0.641418457 batch PCKh 0.3125\n",
      "Trained batch 1077 batch loss 0.53833425 batch mAP 0.596496582 batch PCKh 0.125\n",
      "Trained batch 1078 batch loss 0.518597126 batch mAP 0.584259033 batch PCKh 0.125\n",
      "Trained batch 1079 batch loss 0.551867068 batch mAP 0.565002441 batch PCKh 0.75\n",
      "Trained batch 1080 batch loss 0.476529151 batch mAP 0.630828857 batch PCKh 0.75\n",
      "Trained batch 1081 batch loss 0.575793445 batch mAP 0.598327637 batch PCKh 0.75\n",
      "Trained batch 1082 batch loss 0.587655723 batch mAP 0.577362061 batch PCKh 0.75\n",
      "Trained batch 1083 batch loss 0.557509661 batch mAP 0.546020508 batch PCKh 0.6875\n",
      "Trained batch 1084 batch loss 0.509863257 batch mAP 0.613616943 batch PCKh 0.5\n",
      "Trained batch 1085 batch loss 0.45471102 batch mAP 0.647766113 batch PCKh 0.6875\n",
      "Trained batch 1086 batch loss 0.49675566 batch mAP 0.710754395 batch PCKh 0.75\n",
      "Trained batch 1087 batch loss 0.543412864 batch mAP 0.687072754 batch PCKh 0.6875\n",
      "Trained batch 1088 batch loss 0.442377269 batch mAP 0.667816162 batch PCKh 0.375\n",
      "Trained batch 1089 batch loss 0.459661633 batch mAP 0.650054932 batch PCKh 0.6875\n",
      "Trained batch 1090 batch loss 0.536439538 batch mAP 0.695983887 batch PCKh 0.0625\n",
      "Trained batch 1091 batch loss 0.475862026 batch mAP 0.713684082 batch PCKh 0.5625\n",
      "Trained batch 1092 batch loss 0.499499828 batch mAP 0.700256348 batch PCKh 0.5625\n",
      "Trained batch 1093 batch loss 0.529606342 batch mAP 0.580444336 batch PCKh 0.75\n",
      "Trained batch 1094 batch loss 0.531996727 batch mAP 0.629852295 batch PCKh 0.1875\n",
      "Trained batch 1095 batch loss 0.578651726 batch mAP 0.556274414 batch PCKh 0.625\n",
      "Trained batch 1096 batch loss 0.59394908 batch mAP 0.555175781 batch PCKh 0.25\n",
      "Trained batch 1097 batch loss 0.499804676 batch mAP 0.498291016 batch PCKh 0.5\n",
      "Trained batch 1098 batch loss 0.498888433 batch mAP 0.490478516 batch PCKh 0.5625\n",
      "Trained batch 1099 batch loss 0.467249215 batch mAP 0.561462402 batch PCKh 0.4375\n",
      "Trained batch 1100 batch loss 0.535312951 batch mAP 0.482910156 batch PCKh 0.375\n",
      "Trained batch 1101 batch loss 0.569418967 batch mAP 0.537567139 batch PCKh 0.3125\n",
      "Trained batch 1102 batch loss 0.607692659 batch mAP 0.543212891 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1103 batch loss 0.551838279 batch mAP 0.575653076 batch PCKh 0.75\n",
      "Trained batch 1104 batch loss 0.532094896 batch mAP 0.572692871 batch PCKh 0.75\n",
      "Trained batch 1105 batch loss 0.553084254 batch mAP 0.555511475 batch PCKh 0.625\n",
      "Trained batch 1106 batch loss 0.579400063 batch mAP 0.517944336 batch PCKh 0.375\n",
      "Trained batch 1107 batch loss 0.617818356 batch mAP 0.494659424 batch PCKh 0.875\n",
      "Trained batch 1108 batch loss 0.525824904 batch mAP 0.561676 batch PCKh 0.625\n",
      "Trained batch 1109 batch loss 0.520151556 batch mAP 0.547149658 batch PCKh 0.5625\n",
      "Trained batch 1110 batch loss 0.456444442 batch mAP 0.582183838 batch PCKh 0.625\n",
      "Trained batch 1111 batch loss 0.441529542 batch mAP 0.623748779 batch PCKh 0.5625\n",
      "Trained batch 1112 batch loss 0.492324084 batch mAP 0.581878662 batch PCKh 0.1875\n",
      "Trained batch 1113 batch loss 0.487136632 batch mAP 0.628723145 batch PCKh 0.3125\n",
      "Trained batch 1114 batch loss 0.528412163 batch mAP 0.623962402 batch PCKh 0.125\n",
      "Trained batch 1115 batch loss 0.564519465 batch mAP 0.610107422 batch PCKh 0.5625\n",
      "Trained batch 1116 batch loss 0.513998866 batch mAP 0.593109131 batch PCKh 0.0625\n",
      "Trained batch 1117 batch loss 0.571896315 batch mAP 0.601715088 batch PCKh 0.3125\n",
      "Trained batch 1118 batch loss 0.585746109 batch mAP 0.656005859 batch PCKh 0.375\n",
      "Trained batch 1119 batch loss 0.646134 batch mAP 0.590606689 batch PCKh 0.25\n",
      "Trained batch 1120 batch loss 0.485286534 batch mAP 0.688049316 batch PCKh 0.5625\n",
      "Trained batch 1121 batch loss 0.637017965 batch mAP 0.642425537 batch PCKh 0.625\n",
      "Trained batch 1122 batch loss 0.578207791 batch mAP 0.574920654 batch PCKh 0.6875\n",
      "Trained batch 1123 batch loss 0.503234625 batch mAP 0.567169189 batch PCKh 0.1875\n",
      "Trained batch 1124 batch loss 0.428753972 batch mAP 0.618621826 batch PCKh 0.1875\n",
      "Trained batch 1125 batch loss 0.462916255 batch mAP 0.648590088 batch PCKh 0.3125\n",
      "Trained batch 1126 batch loss 0.510937214 batch mAP 0.638702393 batch PCKh 0.4375\n",
      "Trained batch 1127 batch loss 0.506107688 batch mAP 0.649505615 batch PCKh 0.75\n",
      "Trained batch 1128 batch loss 0.434492022 batch mAP 0.625762939 batch PCKh 0.3125\n",
      "Trained batch 1129 batch loss 0.45289433 batch mAP 0.609008789 batch PCKh 0.25\n",
      "Trained batch 1130 batch loss 0.474618584 batch mAP 0.568084717 batch PCKh 0.4375\n",
      "Trained batch 1131 batch loss 0.410563111 batch mAP 0.611541748 batch PCKh 0.125\n",
      "Trained batch 1132 batch loss 0.465244889 batch mAP 0.638916 batch PCKh 0.375\n",
      "Trained batch 1133 batch loss 0.438532054 batch mAP 0.662384033 batch PCKh 0.375\n",
      "Trained batch 1134 batch loss 0.539091527 batch mAP 0.657653809 batch PCKh 0.625\n",
      "Trained batch 1135 batch loss 0.486919403 batch mAP 0.66583252 batch PCKh 0.75\n",
      "Trained batch 1136 batch loss 0.44684732 batch mAP 0.650878906 batch PCKh 0.25\n",
      "Trained batch 1137 batch loss 0.470577538 batch mAP 0.671569824 batch PCKh 0.75\n",
      "Trained batch 1138 batch loss 0.498479784 batch mAP 0.662963867 batch PCKh 0.5625\n",
      "Trained batch 1139 batch loss 0.465676874 batch mAP 0.722290039 batch PCKh 0.5\n",
      "Trained batch 1140 batch loss 0.44045046 batch mAP 0.726226807 batch PCKh 0.5625\n",
      "Trained batch 1141 batch loss 0.370481312 batch mAP 0.757019043 batch PCKh 0.625\n",
      "Trained batch 1142 batch loss 0.450978041 batch mAP 0.672576904 batch PCKh 0.75\n",
      "Trained batch 1143 batch loss 0.540975034 batch mAP 0.4972229 batch PCKh 0.25\n",
      "Trained batch 1144 batch loss 0.502031446 batch mAP 0.548034668 batch PCKh 0.4375\n",
      "Trained batch 1145 batch loss 0.568051 batch mAP 0.550476074 batch PCKh 0.3125\n",
      "Trained batch 1146 batch loss 0.462072045 batch mAP 0.593231201 batch PCKh 0.6875\n",
      "Trained batch 1147 batch loss 0.440491974 batch mAP 0.671813965 batch PCKh 0.4375\n",
      "Trained batch 1148 batch loss 0.508652449 batch mAP 0.589630127 batch PCKh 0.1875\n",
      "Trained batch 1149 batch loss 0.546398 batch mAP 0.510467529 batch PCKh 0.4375\n",
      "Trained batch 1150 batch loss 0.413266718 batch mAP 0.57131958 batch PCKh 0.1875\n",
      "Trained batch 1151 batch loss 0.510049045 batch mAP 0.630188 batch PCKh 0.5\n",
      "Trained batch 1152 batch loss 0.385936081 batch mAP 0.646911621 batch PCKh 0.6875\n",
      "Trained batch 1153 batch loss 0.52483809 batch mAP 0.572845459 batch PCKh 0.125\n",
      "Trained batch 1154 batch loss 0.443189919 batch mAP 0.649139404 batch PCKh 0.875\n",
      "Trained batch 1155 batch loss 0.478350699 batch mAP 0.584259033 batch PCKh 0.8125\n",
      "Trained batch 1156 batch loss 0.496153295 batch mAP 0.615234375 batch PCKh 0.875\n",
      "Trained batch 1157 batch loss 0.488779038 batch mAP 0.623840332 batch PCKh 0.1875\n",
      "Trained batch 1158 batch loss 0.525372 batch mAP 0.64755249 batch PCKh 0.5\n",
      "Trained batch 1159 batch loss 0.49687022 batch mAP 0.613250732 batch PCKh 0.625\n",
      "Trained batch 1160 batch loss 0.522733748 batch mAP 0.589111328 batch PCKh 0.4375\n",
      "Trained batch 1161 batch loss 0.618405759 batch mAP 0.564758301 batch PCKh 0.3125\n",
      "Trained batch 1162 batch loss 0.624404192 batch mAP 0.581359863 batch PCKh 0.5\n",
      "Trained batch 1163 batch loss 0.678061485 batch mAP 0.562469482 batch PCKh 0.1875\n",
      "Trained batch 1164 batch loss 0.494895339 batch mAP 0.547180176 batch PCKh 0.125\n",
      "Trained batch 1165 batch loss 0.565449834 batch mAP 0.505371094 batch PCKh 0.75\n",
      "Trained batch 1166 batch loss 0.503196657 batch mAP 0.497253418 batch PCKh 0.5625\n",
      "Trained batch 1167 batch loss 0.430162847 batch mAP 0.623199463 batch PCKh 0.6875\n",
      "Trained batch 1168 batch loss 0.46814543 batch mAP 0.545166 batch PCKh 0.5625\n",
      "Trained batch 1169 batch loss 0.589249849 batch mAP 0.514862061 batch PCKh 0.6875\n",
      "Trained batch 1170 batch loss 0.457973301 batch mAP 0.47958374 batch PCKh 0.375\n",
      "Trained batch 1171 batch loss 0.493616581 batch mAP 0.552185059 batch PCKh 0.375\n",
      "Trained batch 1172 batch loss 0.51401943 batch mAP 0.622314453 batch PCKh 0.75\n",
      "Trained batch 1173 batch loss 0.55749917 batch mAP 0.711090088 batch PCKh 0.6875\n",
      "Trained batch 1174 batch loss 0.526847661 batch mAP 0.611999512 batch PCKh 0.375\n",
      "Trained batch 1175 batch loss 0.490491927 batch mAP 0.655334473 batch PCKh 0.5\n",
      "Trained batch 1176 batch loss 0.492905319 batch mAP 0.649658203 batch PCKh 0.6875\n",
      "Trained batch 1177 batch loss 0.415330738 batch mAP 0.573028564 batch PCKh 0.375\n",
      "Trained batch 1178 batch loss 0.39916876 batch mAP 0.579406738 batch PCKh 0\n",
      "Trained batch 1179 batch loss 0.432198584 batch mAP 0.614227295 batch PCKh 0.75\n",
      "Trained batch 1180 batch loss 0.518458247 batch mAP 0.657043457 batch PCKh 0.375\n",
      "Trained batch 1181 batch loss 0.500607193 batch mAP 0.638946533 batch PCKh 0.25\n",
      "Trained batch 1182 batch loss 0.480111 batch mAP 0.603424072 batch PCKh 0.3125\n",
      "Trained batch 1183 batch loss 0.513623297 batch mAP 0.597717285 batch PCKh 0.5625\n",
      "Trained batch 1184 batch loss 0.557628274 batch mAP 0.608917236 batch PCKh 0.1875\n",
      "Trained batch 1185 batch loss 0.548645 batch mAP 0.622528076 batch PCKh 0.4375\n",
      "Trained batch 1186 batch loss 0.55539155 batch mAP 0.671203613 batch PCKh 0.4375\n",
      "Trained batch 1187 batch loss 0.571984768 batch mAP 0.592681885 batch PCKh 0.0625\n",
      "Trained batch 1188 batch loss 0.489128828 batch mAP 0.592926 batch PCKh 0.5\n",
      "Trained batch 1189 batch loss 0.639767647 batch mAP 0.488922119 batch PCKh 0.0625\n",
      "Trained batch 1190 batch loss 0.540231049 batch mAP 0.49230957 batch PCKh 0.3125\n",
      "Trained batch 1191 batch loss 0.575053394 batch mAP 0.535888672 batch PCKh 0.375\n",
      "Trained batch 1192 batch loss 0.541276813 batch mAP 0.621459961 batch PCKh 0.5\n",
      "Trained batch 1193 batch loss 0.517386436 batch mAP 0.633514404 batch PCKh 0.375\n",
      "Trained batch 1194 batch loss 0.542277575 batch mAP 0.614837646 batch PCKh 0.375\n",
      "Trained batch 1195 batch loss 0.505569577 batch mAP 0.599243164 batch PCKh 0.0625\n",
      "Trained batch 1196 batch loss 0.521750212 batch mAP 0.620422363 batch PCKh 0.4375\n",
      "Trained batch 1197 batch loss 0.49289763 batch mAP 0.653900146 batch PCKh 0.6875\n",
      "Trained batch 1198 batch loss 0.56173563 batch mAP 0.589752197 batch PCKh 0.5625\n",
      "Trained batch 1199 batch loss 0.547318399 batch mAP 0.571411133 batch PCKh 0.0625\n",
      "Trained batch 1200 batch loss 0.505904436 batch mAP 0.647338867 batch PCKh 0.125\n",
      "Trained batch 1201 batch loss 0.41029796 batch mAP 0.573883057 batch PCKh 0.1875\n",
      "Trained batch 1202 batch loss 0.424691617 batch mAP 0.641143799 batch PCKh 0.1875\n",
      "Trained batch 1203 batch loss 0.396214873 batch mAP 0.588653564 batch PCKh 0.1875\n",
      "Trained batch 1204 batch loss 0.337200016 batch mAP 0.576355 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1205 batch loss 0.395599127 batch mAP 0.583007812 batch PCKh 0.25\n",
      "Trained batch 1206 batch loss 0.472412258 batch mAP 0.738830566 batch PCKh 0.875\n",
      "Trained batch 1207 batch loss 0.523319185 batch mAP 0.663696289 batch PCKh 0.25\n",
      "Trained batch 1208 batch loss 0.535615146 batch mAP 0.699401855 batch PCKh 0.5\n",
      "Trained batch 1209 batch loss 0.565790892 batch mAP 0.7315979 batch PCKh 0.6875\n",
      "Trained batch 1210 batch loss 0.592244267 batch mAP 0.687591553 batch PCKh 0.3125\n",
      "Trained batch 1211 batch loss 0.603003085 batch mAP 0.624267578 batch PCKh 0\n",
      "Trained batch 1212 batch loss 0.618126273 batch mAP 0.592895508 batch PCKh 0.4375\n",
      "Trained batch 1213 batch loss 0.590392292 batch mAP 0.561340332 batch PCKh 0.5\n",
      "Trained batch 1214 batch loss 0.554834843 batch mAP 0.614624 batch PCKh 0.625\n",
      "Trained batch 1215 batch loss 0.588462293 batch mAP 0.639770508 batch PCKh 0.625\n",
      "Trained batch 1216 batch loss 0.515899539 batch mAP 0.56237793 batch PCKh 0.4375\n",
      "Trained batch 1217 batch loss 0.508733511 batch mAP 0.583221436 batch PCKh 0.3125\n",
      "Trained batch 1218 batch loss 0.538714468 batch mAP 0.650115967 batch PCKh 0.6875\n",
      "Trained batch 1219 batch loss 0.471566021 batch mAP 0.656921387 batch PCKh 0.5625\n",
      "Trained batch 1220 batch loss 0.469958603 batch mAP 0.626068115 batch PCKh 0.6875\n",
      "Trained batch 1221 batch loss 0.480363786 batch mAP 0.637237549 batch PCKh 0.25\n",
      "Trained batch 1222 batch loss 0.568092763 batch mAP 0.588439941 batch PCKh 0.75\n",
      "Trained batch 1223 batch loss 0.543313622 batch mAP 0.584838867 batch PCKh 0.5\n",
      "Trained batch 1224 batch loss 0.518011868 batch mAP 0.564239502 batch PCKh 0.5\n",
      "Trained batch 1225 batch loss 0.469043314 batch mAP 0.589752197 batch PCKh 0.5625\n",
      "Trained batch 1226 batch loss 0.563015 batch mAP 0.560455322 batch PCKh 0.5\n",
      "Trained batch 1227 batch loss 0.491416484 batch mAP 0.595703125 batch PCKh 0.5625\n",
      "Trained batch 1228 batch loss 0.490890384 batch mAP 0.63583374 batch PCKh 0.75\n",
      "Trained batch 1229 batch loss 0.481354088 batch mAP 0.58694458 batch PCKh 0.5625\n",
      "Trained batch 1230 batch loss 0.473311 batch mAP 0.665344238 batch PCKh 0.4375\n",
      "Trained batch 1231 batch loss 0.546768308 batch mAP 0.60244751 batch PCKh 0.875\n",
      "Trained batch 1232 batch loss 0.498593032 batch mAP 0.663909912 batch PCKh 0.6875\n",
      "Trained batch 1233 batch loss 0.481679499 batch mAP 0.606781 batch PCKh 0.8125\n",
      "Trained batch 1234 batch loss 0.548934579 batch mAP 0.532623291 batch PCKh 0.625\n",
      "Trained batch 1235 batch loss 0.535942495 batch mAP 0.595825195 batch PCKh 0.5\n",
      "Trained batch 1236 batch loss 0.578307152 batch mAP 0.594848633 batch PCKh 0.5625\n",
      "Trained batch 1237 batch loss 0.493050158 batch mAP 0.638275146 batch PCKh 0.4375\n",
      "Trained batch 1238 batch loss 0.408642262 batch mAP 0.702301 batch PCKh 0.5\n",
      "Trained batch 1239 batch loss 0.419470787 batch mAP 0.62689209 batch PCKh 0.3125\n",
      "Trained batch 1240 batch loss 0.455757082 batch mAP 0.610565186 batch PCKh 0.6875\n",
      "Trained batch 1241 batch loss 0.489713341 batch mAP 0.677124 batch PCKh 0.6875\n",
      "Trained batch 1242 batch loss 0.443716347 batch mAP 0.679901123 batch PCKh 0.25\n",
      "Trained batch 1243 batch loss 0.53756249 batch mAP 0.63381958 batch PCKh 0.5\n",
      "Trained batch 1244 batch loss 0.520932317 batch mAP 0.652099609 batch PCKh 0.6875\n",
      "Trained batch 1245 batch loss 0.491215885 batch mAP 0.657684326 batch PCKh 0.6875\n",
      "Trained batch 1246 batch loss 0.480920613 batch mAP 0.657714844 batch PCKh 0.625\n",
      "Trained batch 1247 batch loss 0.478397787 batch mAP 0.651092529 batch PCKh 0.6875\n",
      "Trained batch 1248 batch loss 0.517001808 batch mAP 0.629303 batch PCKh 0.5\n",
      "Trained batch 1249 batch loss 0.561292291 batch mAP 0.624664307 batch PCKh 0.3125\n",
      "Trained batch 1250 batch loss 0.477413714 batch mAP 0.643188477 batch PCKh 0.3125\n",
      "Trained batch 1251 batch loss 0.499717146 batch mAP 0.607208252 batch PCKh 0.4375\n",
      "Trained batch 1252 batch loss 0.449940979 batch mAP 0.667816162 batch PCKh 0.75\n",
      "Trained batch 1253 batch loss 0.453284025 batch mAP 0.64074707 batch PCKh 0.25\n",
      "Trained batch 1254 batch loss 0.425789326 batch mAP 0.671081543 batch PCKh 0.875\n",
      "Trained batch 1255 batch loss 0.513947904 batch mAP 0.617919922 batch PCKh 0.5\n",
      "Trained batch 1256 batch loss 0.520673037 batch mAP 0.631835938 batch PCKh 0.25\n",
      "Trained batch 1257 batch loss 0.406664 batch mAP 0.568695068 batch PCKh 0.3125\n",
      "Trained batch 1258 batch loss 0.58273685 batch mAP 0.604003906 batch PCKh 0.125\n",
      "Trained batch 1259 batch loss 0.577896 batch mAP 0.604003906 batch PCKh 0.625\n",
      "Trained batch 1260 batch loss 0.510796905 batch mAP 0.598388672 batch PCKh 0.4375\n",
      "Trained batch 1261 batch loss 0.605041623 batch mAP 0.554962158 batch PCKh 0.5625\n",
      "Trained batch 1262 batch loss 0.547647953 batch mAP 0.527069092 batch PCKh 0.375\n",
      "Trained batch 1263 batch loss 0.438203633 batch mAP 0.456787109 batch PCKh 0.4375\n",
      "Trained batch 1264 batch loss 0.481482923 batch mAP 0.520202637 batch PCKh 0.5625\n",
      "Trained batch 1265 batch loss 0.45113185 batch mAP 0.563964844 batch PCKh 0.5625\n",
      "Trained batch 1266 batch loss 0.384930313 batch mAP 0.678253174 batch PCKh 0.5\n",
      "Trained batch 1267 batch loss 0.418133408 batch mAP 0.522399902 batch PCKh 0.3125\n",
      "Trained batch 1268 batch loss 0.447116017 batch mAP 0.639343262 batch PCKh 0.75\n",
      "Trained batch 1269 batch loss 0.435575545 batch mAP 0.659759521 batch PCKh 0.25\n",
      "Trained batch 1270 batch loss 0.60902679 batch mAP 0.624084473 batch PCKh 0.5625\n",
      "Trained batch 1271 batch loss 0.634674788 batch mAP 0.550292969 batch PCKh 0.5625\n",
      "Trained batch 1272 batch loss 0.70615983 batch mAP 0.525268555 batch PCKh 0.1875\n",
      "Trained batch 1273 batch loss 0.512257695 batch mAP 0.716430664 batch PCKh 0.1875\n",
      "Trained batch 1274 batch loss 0.482333124 batch mAP 0.672912598 batch PCKh 0.8125\n",
      "Trained batch 1275 batch loss 0.508175 batch mAP 0.629425049 batch PCKh 0.5\n",
      "Trained batch 1276 batch loss 0.475675464 batch mAP 0.62399292 batch PCKh 0.125\n",
      "Trained batch 1277 batch loss 0.446499407 batch mAP 0.68951416 batch PCKh 0.6875\n",
      "Trained batch 1278 batch loss 0.453665346 batch mAP 0.669494629 batch PCKh 0.25\n",
      "Trained batch 1279 batch loss 0.506884456 batch mAP 0.640014648 batch PCKh 0.3125\n",
      "Trained batch 1280 batch loss 0.538842797 batch mAP 0.63067627 batch PCKh 0.4375\n",
      "Trained batch 1281 batch loss 0.483407825 batch mAP 0.64050293 batch PCKh 0.4375\n",
      "Trained batch 1282 batch loss 0.448862374 batch mAP 0.655670166 batch PCKh 0.6875\n",
      "Trained batch 1283 batch loss 0.440845 batch mAP 0.64944458 batch PCKh 0.375\n",
      "Trained batch 1284 batch loss 0.451241374 batch mAP 0.64642334 batch PCKh 0.875\n",
      "Trained batch 1285 batch loss 0.550317883 batch mAP 0.635742188 batch PCKh 0.75\n",
      "Trained batch 1286 batch loss 0.489914924 batch mAP 0.684906 batch PCKh 0.5\n",
      "Trained batch 1287 batch loss 0.536180615 batch mAP 0.637390137 batch PCKh 0.375\n",
      "Trained batch 1288 batch loss 0.467210382 batch mAP 0.63583374 batch PCKh 0.5625\n",
      "Trained batch 1289 batch loss 0.558094263 batch mAP 0.666595459 batch PCKh 0.6875\n",
      "Trained batch 1290 batch loss 0.535004377 batch mAP 0.661346436 batch PCKh 0.625\n",
      "Trained batch 1291 batch loss 0.508958876 batch mAP 0.626495361 batch PCKh 0.8125\n",
      "Trained batch 1292 batch loss 0.498347044 batch mAP 0.656921387 batch PCKh 0.25\n",
      "Trained batch 1293 batch loss 0.571770549 batch mAP 0.583068848 batch PCKh 0.75\n",
      "Trained batch 1294 batch loss 0.585042536 batch mAP 0.64956665 batch PCKh 0.125\n",
      "Trained batch 1295 batch loss 0.432637 batch mAP 0.673278809 batch PCKh 0.1875\n",
      "Trained batch 1296 batch loss 0.476396024 batch mAP 0.70224 batch PCKh 0.375\n",
      "Trained batch 1297 batch loss 0.460244536 batch mAP 0.712127686 batch PCKh 0.4375\n",
      "Trained batch 1298 batch loss 0.496872097 batch mAP 0.684875488 batch PCKh 0.625\n",
      "Trained batch 1299 batch loss 0.57338953 batch mAP 0.717376709 batch PCKh 0.5625\n",
      "Trained batch 1300 batch loss 0.516523659 batch mAP 0.651916504 batch PCKh 0.5625\n",
      "Trained batch 1301 batch loss 0.513788462 batch mAP 0.677703857 batch PCKh 0.25\n",
      "Trained batch 1302 batch loss 0.388079315 batch mAP 0.64932251 batch PCKh 0.1875\n",
      "Trained batch 1303 batch loss 0.40145424 batch mAP 0.63861084 batch PCKh 0\n",
      "Trained batch 1304 batch loss 0.358573556 batch mAP 0.565185547 batch PCKh 0\n",
      "Trained batch 1305 batch loss 0.399734944 batch mAP 0.635070801 batch PCKh 0\n",
      "Trained batch 1306 batch loss 0.408187062 batch mAP 0.631164551 batch PCKh 0.0625\n",
      "Trained batch 1307 batch loss 0.530438781 batch mAP 0.624633789 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1308 batch loss 0.606599808 batch mAP 0.595733643 batch PCKh 0.5625\n",
      "Trained batch 1309 batch loss 0.577458739 batch mAP 0.581878662 batch PCKh 0.1875\n",
      "Trained batch 1310 batch loss 0.570606887 batch mAP 0.593688965 batch PCKh 0.4375\n",
      "Trained batch 1311 batch loss 0.679453135 batch mAP 0.573944092 batch PCKh 0.3125\n",
      "Trained batch 1312 batch loss 0.678338528 batch mAP 0.555389404 batch PCKh 0.0625\n",
      "Trained batch 1313 batch loss 0.722853422 batch mAP 0.603363037 batch PCKh 0.1875\n",
      "Trained batch 1314 batch loss 0.450934 batch mAP 0.634796143 batch PCKh 0.0625\n",
      "Trained batch 1315 batch loss 0.560371935 batch mAP 0.540527344 batch PCKh 0.6875\n",
      "Trained batch 1316 batch loss 0.438282162 batch mAP 0.580749512 batch PCKh 0.5625\n",
      "Trained batch 1317 batch loss 0.507227778 batch mAP 0.58203125 batch PCKh 0.5625\n",
      "Trained batch 1318 batch loss 0.522866607 batch mAP 0.589752197 batch PCKh 0.5\n",
      "Trained batch 1319 batch loss 0.552810133 batch mAP 0.620666504 batch PCKh 0.75\n",
      "Trained batch 1320 batch loss 0.562137425 batch mAP 0.619384766 batch PCKh 0.3125\n",
      "Trained batch 1321 batch loss 0.523526073 batch mAP 0.68145752 batch PCKh 0.4375\n",
      "Trained batch 1322 batch loss 0.571084261 batch mAP 0.659240723 batch PCKh 0.5625\n",
      "Trained batch 1323 batch loss 0.58197093 batch mAP 0.640960693 batch PCKh 0.4375\n",
      "Trained batch 1324 batch loss 0.50938 batch mAP 0.659423828 batch PCKh 0.4375\n",
      "Trained batch 1325 batch loss 0.519889414 batch mAP 0.676208496 batch PCKh 0.75\n",
      "Trained batch 1326 batch loss 0.510720849 batch mAP 0.669799805 batch PCKh 0.3125\n",
      "Trained batch 1327 batch loss 0.520489 batch mAP 0.671783447 batch PCKh 0.75\n",
      "Trained batch 1328 batch loss 0.559544563 batch mAP 0.617919922 batch PCKh 0.5625\n",
      "Trained batch 1329 batch loss 0.581899166 batch mAP 0.5909729 batch PCKh 0.5\n",
      "Trained batch 1330 batch loss 0.534362793 batch mAP 0.590606689 batch PCKh 0.625\n",
      "Trained batch 1331 batch loss 0.514933586 batch mAP 0.56073 batch PCKh 0.5\n",
      "Trained batch 1332 batch loss 0.53122443 batch mAP 0.594207764 batch PCKh 0.5\n",
      "Trained batch 1333 batch loss 0.496091843 batch mAP 0.617034912 batch PCKh 0.3125\n",
      "Trained batch 1334 batch loss 0.500012398 batch mAP 0.629211426 batch PCKh 0.3125\n",
      "Trained batch 1335 batch loss 0.42259714 batch mAP 0.644165039 batch PCKh 0.1875\n",
      "Trained batch 1336 batch loss 0.498556048 batch mAP 0.592834473 batch PCKh 0.3125\n",
      "Trained batch 1337 batch loss 0.482596278 batch mAP 0.619842529 batch PCKh 0.1875\n",
      "Trained batch 1338 batch loss 0.535142899 batch mAP 0.657196045 batch PCKh 0.75\n",
      "Trained batch 1339 batch loss 0.542986 batch mAP 0.694793701 batch PCKh 0.75\n",
      "Trained batch 1340 batch loss 0.543266177 batch mAP 0.665130615 batch PCKh 0.4375\n",
      "Trained batch 1341 batch loss 0.547687292 batch mAP 0.637207031 batch PCKh 0.8125\n",
      "Trained batch 1342 batch loss 0.455760866 batch mAP 0.622924805 batch PCKh 0.6875\n",
      "Trained batch 1343 batch loss 0.435724765 batch mAP 0.637420654 batch PCKh 0.4375\n",
      "Trained batch 1344 batch loss 0.357086629 batch mAP 0.614624 batch PCKh 0.4375\n",
      "Trained batch 1345 batch loss 0.441859603 batch mAP 0.572570801 batch PCKh 0\n",
      "Trained batch 1346 batch loss 0.570002258 batch mAP 0.586273193 batch PCKh 0.625\n",
      "Trained batch 1347 batch loss 0.489266783 batch mAP 0.586730957 batch PCKh 0.4375\n",
      "Trained batch 1348 batch loss 0.54910177 batch mAP 0.586547852 batch PCKh 0.625\n",
      "Trained batch 1349 batch loss 0.495970696 batch mAP 0.550506592 batch PCKh 0.5625\n",
      "Trained batch 1350 batch loss 0.517829597 batch mAP 0.561767578 batch PCKh 0.5\n",
      "Trained batch 1351 batch loss 0.645111442 batch mAP 0.530029297 batch PCKh 0.75\n",
      "Trained batch 1352 batch loss 0.548742831 batch mAP 0.635192871 batch PCKh 0.625\n",
      "Trained batch 1353 batch loss 0.461166382 batch mAP 0.631774902 batch PCKh 0.875\n",
      "Trained batch 1354 batch loss 0.556325078 batch mAP 0.541503906 batch PCKh 0.1875\n",
      "Trained batch 1355 batch loss 0.546168089 batch mAP 0.573883057 batch PCKh 0.4375\n",
      "Trained batch 1356 batch loss 0.549085617 batch mAP 0.53414917 batch PCKh 0.6875\n",
      "Trained batch 1357 batch loss 0.524391413 batch mAP 0.543731689 batch PCKh 0.6875\n",
      "Trained batch 1358 batch loss 0.531669796 batch mAP 0.58694458 batch PCKh 0.3125\n",
      "Trained batch 1359 batch loss 0.453839511 batch mAP 0.617218 batch PCKh 0.625\n",
      "Trained batch 1360 batch loss 0.578190088 batch mAP 0.602325439 batch PCKh 0.5\n",
      "Trained batch 1361 batch loss 0.634433925 batch mAP 0.623260498 batch PCKh 0.6875\n",
      "Trained batch 1362 batch loss 0.636943638 batch mAP 0.590301514 batch PCKh 0.625\n",
      "Trained batch 1363 batch loss 0.510810912 batch mAP 0.609466553 batch PCKh 0.1875\n",
      "Trained batch 1364 batch loss 0.405356467 batch mAP 0.634887695 batch PCKh 0.5\n",
      "Trained batch 1365 batch loss 0.39222607 batch mAP 0.65423584 batch PCKh 0.5625\n",
      "Trained batch 1366 batch loss 0.462224543 batch mAP 0.56942749 batch PCKh 0.375\n",
      "Trained batch 1367 batch loss 0.502631545 batch mAP 0.615478516 batch PCKh 0.625\n",
      "Trained batch 1368 batch loss 0.500256658 batch mAP 0.615081787 batch PCKh 0.375\n",
      "Trained batch 1369 batch loss 0.516910732 batch mAP 0.637237549 batch PCKh 0.625\n",
      "Trained batch 1370 batch loss 0.558673859 batch mAP 0.633270264 batch PCKh 0.5625\n",
      "Trained batch 1371 batch loss 0.545257032 batch mAP 0.634613037 batch PCKh 0.6875\n",
      "Trained batch 1372 batch loss 0.513487458 batch mAP 0.628936768 batch PCKh 0\n",
      "Trained batch 1373 batch loss 0.496372104 batch mAP 0.651031494 batch PCKh 0.3125\n",
      "Trained batch 1374 batch loss 0.530841827 batch mAP 0.65423584 batch PCKh 0.5\n",
      "Trained batch 1375 batch loss 0.552089 batch mAP 0.606414795 batch PCKh 0.8125\n",
      "Trained batch 1376 batch loss 0.533246577 batch mAP 0.607086182 batch PCKh 0.3125\n",
      "Trained batch 1377 batch loss 0.563691 batch mAP 0.653533936 batch PCKh 0.8125\n",
      "Trained batch 1378 batch loss 0.49917683 batch mAP 0.59954834 batch PCKh 0.5625\n",
      "Trained batch 1379 batch loss 0.589269042 batch mAP 0.592102051 batch PCKh 0.875\n",
      "Trained batch 1380 batch loss 0.651259124 batch mAP 0.534423828 batch PCKh 0.1875\n",
      "Trained batch 1381 batch loss 0.581550241 batch mAP 0.521728516 batch PCKh 0.375\n",
      "Trained batch 1382 batch loss 0.503566444 batch mAP 0.55871582 batch PCKh 0.5\n",
      "Trained batch 1383 batch loss 0.50983876 batch mAP 0.597137451 batch PCKh 0.875\n",
      "Trained batch 1384 batch loss 0.591255367 batch mAP 0.487426758 batch PCKh 0.8125\n",
      "Trained batch 1385 batch loss 0.614738703 batch mAP 0.558929443 batch PCKh 0.3125\n",
      "Trained batch 1386 batch loss 0.610447586 batch mAP 0.616851807 batch PCKh 0.125\n",
      "Trained batch 1387 batch loss 0.612837315 batch mAP 0.6144104 batch PCKh 0.25\n",
      "Trained batch 1388 batch loss 0.710129499 batch mAP 0.548584 batch PCKh 0.8125\n",
      "Trained batch 1389 batch loss 0.647581875 batch mAP 0.5496521 batch PCKh 0.625\n",
      "Trained batch 1390 batch loss 0.623982906 batch mAP 0.498809814 batch PCKh 0.1875\n",
      "Trained batch 1391 batch loss 0.63286835 batch mAP 0.513244629 batch PCKh 0\n",
      "Trained batch 1392 batch loss 0.574476421 batch mAP 0.601348877 batch PCKh 0.1875\n",
      "Trained batch 1393 batch loss 0.607016325 batch mAP 0.572723389 batch PCKh 0.1875\n",
      "Trained batch 1394 batch loss 0.586583555 batch mAP 0.561431885 batch PCKh 0.25\n",
      "Trained batch 1395 batch loss 0.479828238 batch mAP 0.636108398 batch PCKh 0.75\n",
      "Trained batch 1396 batch loss 0.479744703 batch mAP 0.592559814 batch PCKh 0.375\n",
      "Trained batch 1397 batch loss 0.481643826 batch mAP 0.545928955 batch PCKh 0.125\n",
      "Trained batch 1398 batch loss 0.569259644 batch mAP 0.577545166 batch PCKh 0.5\n",
      "Trained batch 1399 batch loss 0.56261003 batch mAP 0.59942627 batch PCKh 0.3125\n",
      "Trained batch 1400 batch loss 0.522285163 batch mAP 0.66494751 batch PCKh 0.8125\n",
      "Trained batch 1401 batch loss 0.560451 batch mAP 0.621765137 batch PCKh 0.5\n",
      "Trained batch 1402 batch loss 0.473555326 batch mAP 0.630188 batch PCKh 0.25\n",
      "Trained batch 1403 batch loss 0.492779344 batch mAP 0.667358398 batch PCKh 0.75\n",
      "Trained batch 1404 batch loss 0.475184977 batch mAP 0.565429688 batch PCKh 0.25\n",
      "Trained batch 1405 batch loss 0.448179603 batch mAP 0.58581543 batch PCKh 0.625\n",
      "Trained batch 1406 batch loss 0.417918205 batch mAP 0.576934814 batch PCKh 0.25\n",
      "Trained batch 1407 batch loss 0.435865521 batch mAP 0.603393555 batch PCKh 0.75\n",
      "Trained batch 1408 batch loss 0.508231521 batch mAP 0.567718506 batch PCKh 0.5625\n",
      "Trained batch 1409 batch loss 0.478840947 batch mAP 0.591430664 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1410 batch loss 0.496112376 batch mAP 0.620636 batch PCKh 0.75\n",
      "Trained batch 1411 batch loss 0.535372198 batch mAP 0.618530273 batch PCKh 0.375\n",
      "Trained batch 1412 batch loss 0.422631621 batch mAP 0.662994385 batch PCKh 0.5\n",
      "Trained batch 1413 batch loss 0.442453921 batch mAP 0.642395 batch PCKh 0.625\n",
      "Trained batch 1414 batch loss 0.473502189 batch mAP 0.632751465 batch PCKh 0.6875\n",
      "Trained batch 1415 batch loss 0.478086054 batch mAP 0.662750244 batch PCKh 0.3125\n",
      "Trained batch 1416 batch loss 0.513079345 batch mAP 0.654785156 batch PCKh 0.5\n",
      "Trained batch 1417 batch loss 0.551868677 batch mAP 0.587493896 batch PCKh 0.25\n",
      "Trained batch 1418 batch loss 0.518718 batch mAP 0.5284729 batch PCKh 0.75\n",
      "Trained batch 1419 batch loss 0.547549129 batch mAP 0.515197754 batch PCKh 0.75\n",
      "Trained batch 1420 batch loss 0.549287498 batch mAP 0.483764648 batch PCKh 0.5\n",
      "Trained batch 1421 batch loss 0.472726 batch mAP 0.574890137 batch PCKh 0.6875\n",
      "Trained batch 1422 batch loss 0.520388544 batch mAP 0.613769531 batch PCKh 0.6875\n",
      "Trained batch 1423 batch loss 0.512569726 batch mAP 0.590240479 batch PCKh 0.625\n",
      "Trained batch 1424 batch loss 0.492681235 batch mAP 0.645172119 batch PCKh 0.75\n",
      "Trained batch 1425 batch loss 0.550249577 batch mAP 0.653259277 batch PCKh 0.6875\n",
      "Trained batch 1426 batch loss 0.583359182 batch mAP 0.6015625 batch PCKh 0.6875\n",
      "Trained batch 1427 batch loss 0.583095 batch mAP 0.617462158 batch PCKh 0.125\n",
      "Trained batch 1428 batch loss 0.60683918 batch mAP 0.642608643 batch PCKh 0.25\n",
      "Trained batch 1429 batch loss 0.525635958 batch mAP 0.662323 batch PCKh 0.25\n",
      "Trained batch 1430 batch loss 0.532799065 batch mAP 0.607299805 batch PCKh 0.3125\n",
      "Trained batch 1431 batch loss 0.556341588 batch mAP 0.58605957 batch PCKh 0.4375\n",
      "Trained batch 1432 batch loss 0.553529322 batch mAP 0.61340332 batch PCKh 0.5\n",
      "Trained batch 1433 batch loss 0.540722847 batch mAP 0.576782227 batch PCKh 0.75\n",
      "Trained batch 1434 batch loss 0.564604044 batch mAP 0.56350708 batch PCKh 0.3125\n",
      "Trained batch 1435 batch loss 0.543215215 batch mAP 0.599395752 batch PCKh 0.4375\n",
      "Trained batch 1436 batch loss 0.63141 batch mAP 0.519958496 batch PCKh 0.75\n",
      "Trained batch 1437 batch loss 0.511146605 batch mAP 0.561065674 batch PCKh 0.4375\n",
      "Trained batch 1438 batch loss 0.54024744 batch mAP 0.576629639 batch PCKh 0.875\n",
      "Trained batch 1439 batch loss 0.528322637 batch mAP 0.574401855 batch PCKh 0.3125\n",
      "Trained batch 1440 batch loss 0.527743101 batch mAP 0.578155518 batch PCKh 0.6875\n",
      "Trained batch 1441 batch loss 0.585861564 batch mAP 0.591766357 batch PCKh 0.75\n",
      "Trained batch 1442 batch loss 0.565119 batch mAP 0.617736816 batch PCKh 0.5\n",
      "Trained batch 1443 batch loss 0.552773058 batch mAP 0.630889893 batch PCKh 0.75\n",
      "Trained batch 1444 batch loss 0.503775239 batch mAP 0.663879395 batch PCKh 0.625\n",
      "Trained batch 1445 batch loss 0.523887038 batch mAP 0.632446289 batch PCKh 0.5\n",
      "Trained batch 1446 batch loss 0.50685817 batch mAP 0.668884277 batch PCKh 0.625\n",
      "Trained batch 1447 batch loss 0.51205194 batch mAP 0.621948242 batch PCKh 0.4375\n",
      "Trained batch 1448 batch loss 0.418359697 batch mAP 0.615112305 batch PCKh 0.25\n",
      "Trained batch 1449 batch loss 0.487469733 batch mAP 0.640350342 batch PCKh 0.625\n",
      "Trained batch 1450 batch loss 0.421996087 batch mAP 0.621673584 batch PCKh 0.5\n",
      "Trained batch 1451 batch loss 0.386506379 batch mAP 0.621429443 batch PCKh 0.5625\n",
      "Trained batch 1452 batch loss 0.490447879 batch mAP 0.587524414 batch PCKh 0.6875\n",
      "Trained batch 1453 batch loss 0.399918467 batch mAP 0.629180908 batch PCKh 0.75\n",
      "Trained batch 1454 batch loss 0.477406293 batch mAP 0.574401855 batch PCKh 0.5\n",
      "Trained batch 1455 batch loss 0.518066645 batch mAP 0.575622559 batch PCKh 0.875\n",
      "Trained batch 1456 batch loss 0.480275631 batch mAP 0.585235596 batch PCKh 0.75\n",
      "Trained batch 1457 batch loss 0.42240867 batch mAP 0.581512451 batch PCKh 0.75\n",
      "Trained batch 1458 batch loss 0.409824044 batch mAP 0.653198242 batch PCKh 0.5625\n",
      "Trained batch 1459 batch loss 0.328688 batch mAP 0.666320801 batch PCKh 0.6875\n",
      "Trained batch 1460 batch loss 0.344039351 batch mAP 0.686523438 batch PCKh 0\n",
      "Trained batch 1461 batch loss 0.393335521 batch mAP 0.617706299 batch PCKh 0\n",
      "Trained batch 1462 batch loss 0.413256586 batch mAP 0.614074707 batch PCKh 0\n",
      "Trained batch 1463 batch loss 0.325791031 batch mAP 0.686187744 batch PCKh 0.5\n",
      "Trained batch 1464 batch loss 0.361556113 batch mAP 0.670043945 batch PCKh 0.5\n",
      "Trained batch 1465 batch loss 0.44326216 batch mAP 0.65335083 batch PCKh 0.5\n",
      "Trained batch 1466 batch loss 0.450869471 batch mAP 0.680786133 batch PCKh 0.5625\n",
      "Trained batch 1467 batch loss 0.581755877 batch mAP 0.656311035 batch PCKh 0.375\n",
      "Trained batch 1468 batch loss 0.45817858 batch mAP 0.704406738 batch PCKh 0.375\n",
      "Trained batch 1469 batch loss 0.535824597 batch mAP 0.652984619 batch PCKh 0.4375\n",
      "Trained batch 1470 batch loss 0.434533119 batch mAP 0.680297852 batch PCKh 0.375\n",
      "Trained batch 1471 batch loss 0.52070564 batch mAP 0.675109863 batch PCKh 0.375\n",
      "Trained batch 1472 batch loss 0.503892541 batch mAP 0.678710938 batch PCKh 0.3125\n",
      "Trained batch 1473 batch loss 0.442048609 batch mAP 0.678375244 batch PCKh 0.625\n",
      "Trained batch 1474 batch loss 0.510926723 batch mAP 0.66027832 batch PCKh 0.375\n",
      "Trained batch 1475 batch loss 0.553217769 batch mAP 0.628997803 batch PCKh 0.5625\n",
      "Trained batch 1476 batch loss 0.548625827 batch mAP 0.632354736 batch PCKh 0.625\n",
      "Trained batch 1477 batch loss 0.566631556 batch mAP 0.613983154 batch PCKh 0.8125\n",
      "Trained batch 1478 batch loss 0.555788875 batch mAP 0.701843262 batch PCKh 0.875\n",
      "Trained batch 1479 batch loss 0.595318735 batch mAP 0.615814209 batch PCKh 0.25\n",
      "Trained batch 1480 batch loss 0.558362901 batch mAP 0.724456787 batch PCKh 0.5\n",
      "Trained batch 1481 batch loss 0.520388901 batch mAP 0.671447754 batch PCKh 0.3125\n",
      "Trained batch 1482 batch loss 0.455803365 batch mAP 0.701324463 batch PCKh 0.3125\n",
      "Trained batch 1483 batch loss 0.547613442 batch mAP 0.641174316 batch PCKh 0.375\n",
      "Trained batch 1484 batch loss 0.586597562 batch mAP 0.560577393 batch PCKh 0.5\n",
      "Trained batch 1485 batch loss 0.682831645 batch mAP 0.550628662 batch PCKh 0.5\n",
      "Trained batch 1486 batch loss 0.609369099 batch mAP 0.548461914 batch PCKh 0.5\n",
      "Trained batch 1487 batch loss 0.651726782 batch mAP 0.518676758 batch PCKh 0.8125\n",
      "Trained batch 1488 batch loss 0.533840537 batch mAP 0.641571045 batch PCKh 0.375\n",
      "Trained batch 1489 batch loss 0.569792151 batch mAP 0.556854248 batch PCKh 0.8125\n",
      "Trained batch 1490 batch loss 0.561562598 batch mAP 0.604064941 batch PCKh 0.875\n",
      "Trained batch 1491 batch loss 0.486448467 batch mAP 0.580291748 batch PCKh 0.875\n",
      "Trained batch 1492 batch loss 0.501367152 batch mAP 0.60168457 batch PCKh 0.75\n",
      "Trained batch 1493 batch loss 0.490457624 batch mAP 0.615936279 batch PCKh 0.6875\n",
      "Trained batch 1494 batch loss 0.52746141 batch mAP 0.664581299 batch PCKh 0.875\n",
      "Trained batch 1495 batch loss 0.481360316 batch mAP 0.685913086 batch PCKh 0.375\n",
      "Trained batch 1496 batch loss 0.542493224 batch mAP 0.571350098 batch PCKh 0.5625\n",
      "Trained batch 1497 batch loss 0.635165453 batch mAP 0.563232422 batch PCKh 0.5625\n",
      "Trained batch 1498 batch loss 0.61664784 batch mAP 0.616088867 batch PCKh 0.0625\n",
      "Trained batch 1499 batch loss 0.547642589 batch mAP 0.62020874 batch PCKh 0.6875\n",
      "Trained batch 1500 batch loss 0.550079644 batch mAP 0.609710693 batch PCKh 0.1875\n",
      "Trained batch 1501 batch loss 0.654241323 batch mAP 0.539794922 batch PCKh 0.4375\n",
      "Trained batch 1502 batch loss 0.714045644 batch mAP 0.516998291 batch PCKh 0.3125\n",
      "Trained batch 1503 batch loss 0.625070035 batch mAP 0.56060791 batch PCKh 0.375\n",
      "Trained batch 1504 batch loss 0.459399283 batch mAP 0.611419678 batch PCKh 0.75\n",
      "Trained batch 1505 batch loss 0.53432548 batch mAP 0.628814697 batch PCKh 0.3125\n",
      "Trained batch 1506 batch loss 0.483379602 batch mAP 0.640472412 batch PCKh 0.5\n",
      "Trained batch 1507 batch loss 0.534440756 batch mAP 0.587463379 batch PCKh 0.375\n",
      "Trained batch 1508 batch loss 0.533922553 batch mAP 0.615814209 batch PCKh 0.25\n",
      "Trained batch 1509 batch loss 0.626171947 batch mAP 0.586181641 batch PCKh 0.25\n",
      "Trained batch 1510 batch loss 0.560574055 batch mAP 0.570861816 batch PCKh 0.4375\n",
      "Trained batch 1511 batch loss 0.542576194 batch mAP 0.599334717 batch PCKh 0.4375\n",
      "Trained batch 1512 batch loss 0.595360875 batch mAP 0.52355957 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1513 batch loss 0.612339139 batch mAP 0.564483643 batch PCKh 0.0625\n",
      "Trained batch 1514 batch loss 0.533309579 batch mAP 0.593292236 batch PCKh 0.4375\n",
      "Trained batch 1515 batch loss 0.609542906 batch mAP 0.602752686 batch PCKh 0.5625\n",
      "Trained batch 1516 batch loss 0.432003468 batch mAP 0.668884277 batch PCKh 0.625\n",
      "Trained batch 1517 batch loss 0.426421583 batch mAP 0.615112305 batch PCKh 0.25\n",
      "Trained batch 1518 batch loss 0.484659255 batch mAP 0.639953613 batch PCKh 0.625\n",
      "Trained batch 1519 batch loss 0.47400403 batch mAP 0.61932373 batch PCKh 0.375\n",
      "Trained batch 1520 batch loss 0.51543349 batch mAP 0.645233154 batch PCKh 0.875\n",
      "Trained batch 1521 batch loss 0.50152421 batch mAP 0.582855225 batch PCKh 0.6875\n",
      "Trained batch 1522 batch loss 0.525758862 batch mAP 0.578399658 batch PCKh 0.75\n",
      "Trained batch 1523 batch loss 0.617503881 batch mAP 0.501647949 batch PCKh 0.625\n",
      "Trained batch 1524 batch loss 0.547203422 batch mAP 0.543029785 batch PCKh 0.625\n",
      "Trained batch 1525 batch loss 0.507011056 batch mAP 0.637695312 batch PCKh 0.625\n",
      "Trained batch 1526 batch loss 0.554206252 batch mAP 0.605804443 batch PCKh 0.875\n",
      "Trained batch 1527 batch loss 0.647898912 batch mAP 0.471160889 batch PCKh 0.3125\n",
      "Trained batch 1528 batch loss 0.581091404 batch mAP 0.584625244 batch PCKh 0.375\n",
      "Trained batch 1529 batch loss 0.624533772 batch mAP 0.488189697 batch PCKh 0.5625\n",
      "Trained batch 1530 batch loss 0.627927184 batch mAP 0.543914795 batch PCKh 0.0625\n",
      "Trained batch 1531 batch loss 0.718590498 batch mAP 0.51184082 batch PCKh 0.4375\n",
      "Trained batch 1532 batch loss 0.626340747 batch mAP 0.589141846 batch PCKh 0\n",
      "Trained batch 1533 batch loss 0.643209875 batch mAP 0.513946533 batch PCKh 0.4375\n",
      "Trained batch 1534 batch loss 0.51134187 batch mAP 0.501953125 batch PCKh 0.3125\n",
      "Trained batch 1535 batch loss 0.410407484 batch mAP 0.514038086 batch PCKh 0\n",
      "Trained batch 1536 batch loss 0.398743629 batch mAP 0.569885254 batch PCKh 0.125\n",
      "Trained batch 1537 batch loss 0.517561436 batch mAP 0.60333252 batch PCKh 0.5625\n",
      "Trained batch 1538 batch loss 0.576402187 batch mAP 0.559906 batch PCKh 0.3125\n",
      "Trained batch 1539 batch loss 0.484652102 batch mAP 0.627410889 batch PCKh 0.6875\n",
      "Trained batch 1540 batch loss 0.489338934 batch mAP 0.653259277 batch PCKh 0.4375\n",
      "Trained batch 1541 batch loss 0.560001135 batch mAP 0.580993652 batch PCKh 0.1875\n",
      "Trained batch 1542 batch loss 0.450351894 batch mAP 0.613250732 batch PCKh 0\n",
      "Trained batch 1543 batch loss 0.43574661 batch mAP 0.655761719 batch PCKh 0.5\n",
      "Trained batch 1544 batch loss 0.449517071 batch mAP 0.673522949 batch PCKh 0.375\n",
      "Trained batch 1545 batch loss 0.460846275 batch mAP 0.668884277 batch PCKh 0.6875\n",
      "Trained batch 1546 batch loss 0.420562714 batch mAP 0.680236816 batch PCKh 0.625\n",
      "Trained batch 1547 batch loss 0.453288347 batch mAP 0.698791504 batch PCKh 0.4375\n",
      "Trained batch 1548 batch loss 0.492482632 batch mAP 0.627441406 batch PCKh 0.1875\n",
      "Trained batch 1549 batch loss 0.376124501 batch mAP 0.73034668 batch PCKh 0.3125\n",
      "Trained batch 1550 batch loss 0.397339225 batch mAP 0.71975708 batch PCKh 0.3125\n",
      "Trained batch 1551 batch loss 0.382150561 batch mAP 0.719787598 batch PCKh 0.75\n",
      "Trained batch 1552 batch loss 0.364503384 batch mAP 0.71698 batch PCKh 0.3125\n",
      "Trained batch 1553 batch loss 0.421320528 batch mAP 0.703125 batch PCKh 0.375\n",
      "Trained batch 1554 batch loss 0.529071569 batch mAP 0.615966797 batch PCKh 0.3125\n",
      "Trained batch 1555 batch loss 0.524192691 batch mAP 0.618377686 batch PCKh 0.375\n",
      "Trained batch 1556 batch loss 0.544457257 batch mAP 0.5703125 batch PCKh 0.25\n",
      "Trained batch 1557 batch loss 0.451704204 batch mAP 0.690124512 batch PCKh 0.5625\n",
      "Trained batch 1558 batch loss 0.471166432 batch mAP 0.670379639 batch PCKh 0.25\n",
      "Trained batch 1559 batch loss 0.372174621 batch mAP 0.689819336 batch PCKh 0.4375\n",
      "Trained batch 1560 batch loss 0.436705053 batch mAP 0.64831543 batch PCKh 0.4375\n",
      "Trained batch 1561 batch loss 0.428103149 batch mAP 0.668304443 batch PCKh 0.625\n",
      "Trained batch 1562 batch loss 0.566029489 batch mAP 0.542205811 batch PCKh 0.1875\n",
      "Trained batch 1563 batch loss 0.543641865 batch mAP 0.582794189 batch PCKh 0.3125\n",
      "Trained batch 1564 batch loss 0.519537568 batch mAP 0.540679932 batch PCKh 0.75\n",
      "Trained batch 1565 batch loss 0.474568367 batch mAP 0.540924072 batch PCKh 0.625\n",
      "Trained batch 1566 batch loss 0.460183829 batch mAP 0.58404541 batch PCKh 0.5\n",
      "Trained batch 1567 batch loss 0.551545 batch mAP 0.538543701 batch PCKh 0.4375\n",
      "Trained batch 1568 batch loss 0.526485503 batch mAP 0.581176758 batch PCKh 0.875\n",
      "Trained batch 1569 batch loss 0.540631294 batch mAP 0.61517334 batch PCKh 0\n",
      "Trained batch 1570 batch loss 0.554292917 batch mAP 0.604736328 batch PCKh 0.375\n",
      "Trained batch 1571 batch loss 0.607769072 batch mAP 0.548095703 batch PCKh 0.375\n",
      "Trained batch 1572 batch loss 0.514063478 batch mAP 0.545471191 batch PCKh 0.25\n",
      "Trained batch 1573 batch loss 0.603286505 batch mAP 0.491882324 batch PCKh 0.875\n",
      "Trained batch 1574 batch loss 0.540886879 batch mAP 0.552307129 batch PCKh 0.625\n",
      "Trained batch 1575 batch loss 0.506677389 batch mAP 0.552948 batch PCKh 0.625\n",
      "Trained batch 1576 batch loss 0.470746487 batch mAP 0.550598145 batch PCKh 0.375\n",
      "Trained batch 1577 batch loss 0.429206848 batch mAP 0.667907715 batch PCKh 0.625\n",
      "Trained batch 1578 batch loss 0.513193846 batch mAP 0.582305908 batch PCKh 0.5\n",
      "Trained batch 1579 batch loss 0.397998899 batch mAP 0.606109619 batch PCKh 0.375\n",
      "Trained batch 1580 batch loss 0.374352843 batch mAP 0.59274292 batch PCKh 0.5625\n",
      "Trained batch 1581 batch loss 0.424418092 batch mAP 0.631500244 batch PCKh 0.875\n",
      "Trained batch 1582 batch loss 0.388125598 batch mAP 0.632843 batch PCKh 0.8125\n",
      "Trained batch 1583 batch loss 0.411515385 batch mAP 0.57019043 batch PCKh 0.8125\n",
      "Trained batch 1584 batch loss 0.586827934 batch mAP 0.567169189 batch PCKh 0.875\n",
      "Trained batch 1585 batch loss 0.484064698 batch mAP 0.575775146 batch PCKh 0.75\n",
      "Trained batch 1586 batch loss 0.421183169 batch mAP 0.602844238 batch PCKh 0.625\n",
      "Trained batch 1587 batch loss 0.504108727 batch mAP 0.624816895 batch PCKh 0.75\n",
      "Trained batch 1588 batch loss 0.588584185 batch mAP 0.593048096 batch PCKh 0.75\n",
      "Trained batch 1589 batch loss 0.460147738 batch mAP 0.619232178 batch PCKh 0.5\n",
      "Trained batch 1590 batch loss 0.492222458 batch mAP 0.616149902 batch PCKh 0.625\n",
      "Trained batch 1591 batch loss 0.467351019 batch mAP 0.634918213 batch PCKh 0.625\n",
      "Trained batch 1592 batch loss 0.518100858 batch mAP 0.642425537 batch PCKh 0.75\n",
      "Trained batch 1593 batch loss 0.546110451 batch mAP 0.637786865 batch PCKh 0.8125\n",
      "Trained batch 1594 batch loss 0.454604417 batch mAP 0.687164307 batch PCKh 0.75\n",
      "Trained batch 1595 batch loss 0.438948572 batch mAP 0.693786621 batch PCKh 0.75\n",
      "Trained batch 1596 batch loss 0.36726892 batch mAP 0.700164795 batch PCKh 0.4375\n",
      "Trained batch 1597 batch loss 0.49885568 batch mAP 0.638855 batch PCKh 0.375\n",
      "Trained batch 1598 batch loss 0.615022361 batch mAP 0.678863525 batch PCKh 0.5\n",
      "Trained batch 1599 batch loss 0.53562361 batch mAP 0.591583252 batch PCKh 0.5\n",
      "Trained batch 1600 batch loss 0.559103668 batch mAP 0.65435791 batch PCKh 0.5625\n",
      "Trained batch 1601 batch loss 0.509501934 batch mAP 0.603515625 batch PCKh 0.5\n",
      "Trained batch 1602 batch loss 0.572482109 batch mAP 0.668640137 batch PCKh 0.5\n",
      "Trained batch 1603 batch loss 0.494916409 batch mAP 0.609649658 batch PCKh 0.625\n",
      "Trained batch 1604 batch loss 0.580203295 batch mAP 0.593963623 batch PCKh 0.3125\n",
      "Trained batch 1605 batch loss 0.610046387 batch mAP 0.550567627 batch PCKh 0.4375\n",
      "Trained batch 1606 batch loss 0.537811935 batch mAP 0.618133545 batch PCKh 0.4375\n",
      "Trained batch 1607 batch loss 0.520843327 batch mAP 0.645080566 batch PCKh 0.5625\n",
      "Trained batch 1608 batch loss 0.552562773 batch mAP 0.616790771 batch PCKh 0.1875\n",
      "Trained batch 1609 batch loss 0.503880501 batch mAP 0.674469 batch PCKh 0.625\n",
      "Trained batch 1610 batch loss 0.476499617 batch mAP 0.682159424 batch PCKh 0.625\n",
      "Trained batch 1611 batch loss 0.580741882 batch mAP 0.61630249 batch PCKh 0.625\n",
      "Trained batch 1612 batch loss 0.572173178 batch mAP 0.595031738 batch PCKh 0.3125\n",
      "Trained batch 1613 batch loss 0.515987694 batch mAP 0.645904541 batch PCKh 0.4375\n",
      "Trained batch 1614 batch loss 0.545432091 batch mAP 0.667114258 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1615 batch loss 0.554614246 batch mAP 0.617645264 batch PCKh 0.625\n",
      "Trained batch 1616 batch loss 0.507490396 batch mAP 0.675079346 batch PCKh 0.375\n",
      "Trained batch 1617 batch loss 0.539821208 batch mAP 0.674224854 batch PCKh 0.6875\n",
      "Trained batch 1618 batch loss 0.524951041 batch mAP 0.631073 batch PCKh 0.4375\n",
      "Trained batch 1619 batch loss 0.510260463 batch mAP 0.629516602 batch PCKh 0.1875\n",
      "Trained batch 1620 batch loss 0.544452071 batch mAP 0.648498535 batch PCKh 0.3125\n",
      "Trained batch 1621 batch loss 0.488257378 batch mAP 0.639282227 batch PCKh 0.4375\n",
      "Trained batch 1622 batch loss 0.448738039 batch mAP 0.679595947 batch PCKh 0.75\n",
      "Trained batch 1623 batch loss 0.47521916 batch mAP 0.646209717 batch PCKh 0.8125\n",
      "Trained batch 1624 batch loss 0.493290633 batch mAP 0.654022217 batch PCKh 0.5625\n",
      "Trained batch 1625 batch loss 0.48191309 batch mAP 0.666168213 batch PCKh 0.625\n",
      "Trained batch 1626 batch loss 0.462536871 batch mAP 0.722229 batch PCKh 0.5625\n",
      "Trained batch 1627 batch loss 0.406170368 batch mAP 0.711364746 batch PCKh 0.5625\n",
      "Trained batch 1628 batch loss 0.534738421 batch mAP 0.659759521 batch PCKh 0.625\n",
      "Trained batch 1629 batch loss 0.490609497 batch mAP 0.712890625 batch PCKh 0.4375\n",
      "Trained batch 1630 batch loss 0.567519844 batch mAP 0.657226562 batch PCKh 0.1875\n",
      "Trained batch 1631 batch loss 0.527404547 batch mAP 0.611999512 batch PCKh 0.6875\n",
      "Trained batch 1632 batch loss 0.51544553 batch mAP 0.625885 batch PCKh 0.125\n",
      "Trained batch 1633 batch loss 0.44383496 batch mAP 0.68862915 batch PCKh 0.625\n",
      "Trained batch 1634 batch loss 0.487070024 batch mAP 0.686981201 batch PCKh 0.375\n",
      "Trained batch 1635 batch loss 0.40374887 batch mAP 0.742706299 batch PCKh 0.5\n",
      "Trained batch 1636 batch loss 0.477627516 batch mAP 0.694793701 batch PCKh 0.375\n",
      "Trained batch 1637 batch loss 0.367158949 batch mAP 0.651519775 batch PCKh 0.375\n",
      "Trained batch 1638 batch loss 0.470635951 batch mAP 0.622467041 batch PCKh 0.875\n",
      "Trained batch 1639 batch loss 0.515333533 batch mAP 0.625091553 batch PCKh 0.3125\n",
      "Trained batch 1640 batch loss 0.516980052 batch mAP 0.596008301 batch PCKh 0.5625\n",
      "Trained batch 1641 batch loss 0.574037075 batch mAP 0.512023926 batch PCKh 0.4375\n",
      "Trained batch 1642 batch loss 0.571565747 batch mAP 0.534423828 batch PCKh 0.75\n",
      "Trained batch 1643 batch loss 0.538942 batch mAP 0.570526123 batch PCKh 0.4375\n",
      "Trained batch 1644 batch loss 0.442343 batch mAP 0.604614258 batch PCKh 0.1875\n",
      "Trained batch 1645 batch loss 0.587486446 batch mAP 0.500549316 batch PCKh 0.75\n",
      "Trained batch 1646 batch loss 0.571230769 batch mAP 0.571228 batch PCKh 0.625\n",
      "Trained batch 1647 batch loss 0.52030021 batch mAP 0.536895752 batch PCKh 0.5625\n",
      "Trained batch 1648 batch loss 0.591484189 batch mAP 0.550872803 batch PCKh 0.75\n",
      "Trained batch 1649 batch loss 0.544098258 batch mAP 0.561981201 batch PCKh 0.4375\n",
      "Trained batch 1650 batch loss 0.542699 batch mAP 0.585388184 batch PCKh 0.625\n",
      "Trained batch 1651 batch loss 0.52532655 batch mAP 0.658111572 batch PCKh 0.6875\n",
      "Trained batch 1652 batch loss 0.486567736 batch mAP 0.70401 batch PCKh 0.5625\n",
      "Trained batch 1653 batch loss 0.432631969 batch mAP 0.676086426 batch PCKh 0.625\n",
      "Trained batch 1654 batch loss 0.462140113 batch mAP 0.683074951 batch PCKh 0.5\n",
      "Trained batch 1655 batch loss 0.509676456 batch mAP 0.654052734 batch PCKh 0.5\n",
      "Trained batch 1656 batch loss 0.576819837 batch mAP 0.57699585 batch PCKh 0.4375\n",
      "Trained batch 1657 batch loss 0.566578448 batch mAP 0.592681885 batch PCKh 0.4375\n",
      "Trained batch 1658 batch loss 0.564159691 batch mAP 0.589782715 batch PCKh 0.625\n",
      "Trained batch 1659 batch loss 0.557168663 batch mAP 0.569824219 batch PCKh 0.375\n",
      "Trained batch 1660 batch loss 0.565073192 batch mAP 0.583587646 batch PCKh 0.4375\n",
      "Trained batch 1661 batch loss 0.549823642 batch mAP 0.595947266 batch PCKh 0.8125\n",
      "Trained batch 1662 batch loss 0.542883754 batch mAP 0.564697266 batch PCKh 0.625\n",
      "Trained batch 1663 batch loss 0.507631838 batch mAP 0.58078 batch PCKh 0.875\n",
      "Trained batch 1664 batch loss 0.520933568 batch mAP 0.593170166 batch PCKh 0.6875\n",
      "Trained batch 1665 batch loss 0.564919 batch mAP 0.566040039 batch PCKh 0.6875\n",
      "Trained batch 1666 batch loss 0.523090482 batch mAP 0.60723877 batch PCKh 0.5625\n",
      "Trained batch 1667 batch loss 0.57804358 batch mAP 0.511657715 batch PCKh 0.125\n",
      "Trained batch 1668 batch loss 0.568004072 batch mAP 0.559539795 batch PCKh 0.3125\n",
      "Trained batch 1669 batch loss 0.565976322 batch mAP 0.517364502 batch PCKh 0.1875\n",
      "Trained batch 1670 batch loss 0.533622682 batch mAP 0.54296875 batch PCKh 0.3125\n",
      "Trained batch 1671 batch loss 0.484124541 batch mAP 0.653198242 batch PCKh 0.6875\n",
      "Trained batch 1672 batch loss 0.469746947 batch mAP 0.602874756 batch PCKh 0.3125\n",
      "Trained batch 1673 batch loss 0.559542656 batch mAP 0.526092529 batch PCKh 0.6875\n",
      "Trained batch 1674 batch loss 0.468440324 batch mAP 0.595367432 batch PCKh 0.5\n",
      "Trained batch 1675 batch loss 0.510051906 batch mAP 0.650665283 batch PCKh 0.375\n",
      "Trained batch 1676 batch loss 0.487410963 batch mAP 0.607666 batch PCKh 0.6875\n",
      "Trained batch 1677 batch loss 0.541671634 batch mAP 0.495178223 batch PCKh 0.75\n",
      "Trained batch 1678 batch loss 0.475806355 batch mAP 0.516876221 batch PCKh 0.375\n",
      "Trained batch 1679 batch loss 0.395205885 batch mAP 0.594177246 batch PCKh 0.25\n",
      "Trained batch 1680 batch loss 0.43826735 batch mAP 0.627410889 batch PCKh 0.625\n",
      "Trained batch 1681 batch loss 0.467626035 batch mAP 0.55859375 batch PCKh 0.1875\n",
      "Trained batch 1682 batch loss 0.516749084 batch mAP 0.522033691 batch PCKh 0.375\n",
      "Trained batch 1683 batch loss 0.543556035 batch mAP 0.55947876 batch PCKh 0.6875\n",
      "Trained batch 1684 batch loss 0.452425 batch mAP 0.610565186 batch PCKh 0.5625\n",
      "Trained batch 1685 batch loss 0.458347142 batch mAP 0.644042969 batch PCKh 0.5625\n",
      "Trained batch 1686 batch loss 0.543412566 batch mAP 0.643249512 batch PCKh 0.5625\n",
      "Trained batch 1687 batch loss 0.456747651 batch mAP 0.655944824 batch PCKh 0.5625\n",
      "Trained batch 1688 batch loss 0.458565295 batch mAP 0.6640625 batch PCKh 0.75\n",
      "Trained batch 1689 batch loss 0.395085245 batch mAP 0.646759033 batch PCKh 0.75\n",
      "Trained batch 1690 batch loss 0.462231934 batch mAP 0.595916748 batch PCKh 0.625\n",
      "Trained batch 1691 batch loss 0.475642592 batch mAP 0.581298828 batch PCKh 0.25\n",
      "Trained batch 1692 batch loss 0.584161282 batch mAP 0.608581543 batch PCKh 0.875\n",
      "Trained batch 1693 batch loss 0.556022525 batch mAP 0.519287109 batch PCKh 0.5625\n",
      "Trained batch 1694 batch loss 0.438191652 batch mAP 0.646392822 batch PCKh 0.25\n",
      "Trained batch 1695 batch loss 0.485699624 batch mAP 0.613830566 batch PCKh 0.1875\n",
      "Trained batch 1696 batch loss 0.559139967 batch mAP 0.493225098 batch PCKh 0.4375\n",
      "Trained batch 1697 batch loss 0.588026345 batch mAP 0.515167236 batch PCKh 0.25\n",
      "Trained batch 1698 batch loss 0.554409921 batch mAP 0.507598877 batch PCKh 0.375\n",
      "Trained batch 1699 batch loss 0.502828717 batch mAP 0.583343506 batch PCKh 0.3125\n",
      "Trained batch 1700 batch loss 0.587493062 batch mAP 0.572143555 batch PCKh 0.75\n",
      "Trained batch 1701 batch loss 0.630833626 batch mAP 0.569488525 batch PCKh 0.75\n",
      "Trained batch 1702 batch loss 0.591921568 batch mAP 0.597503662 batch PCKh 0.75\n",
      "Trained batch 1703 batch loss 0.61578691 batch mAP 0.553009033 batch PCKh 0.75\n",
      "Trained batch 1704 batch loss 0.690871239 batch mAP 0.48626709 batch PCKh 0\n",
      "Trained batch 1705 batch loss 0.611828268 batch mAP 0.537323 batch PCKh 0.125\n",
      "Trained batch 1706 batch loss 0.467798054 batch mAP 0.582580566 batch PCKh 0.75\n",
      "Trained batch 1707 batch loss 0.50741154 batch mAP 0.502594 batch PCKh 0.8125\n",
      "Trained batch 1708 batch loss 0.569212794 batch mAP 0.36895752 batch PCKh 0.5\n",
      "Trained batch 1709 batch loss 0.504620135 batch mAP 0.488922119 batch PCKh 0.75\n",
      "Trained batch 1710 batch loss 0.522188902 batch mAP 0.481140137 batch PCKh 0.75\n",
      "Trained batch 1711 batch loss 0.477101505 batch mAP 0.429870605 batch PCKh 0.5\n",
      "Trained batch 1712 batch loss 0.59970504 batch mAP 0.419647217 batch PCKh 0.5625\n",
      "Trained batch 1713 batch loss 0.515410364 batch mAP 0.530548096 batch PCKh 0.5625\n",
      "Trained batch 1714 batch loss 0.600059628 batch mAP 0.613494873 batch PCKh 0.3125\n",
      "Trained batch 1715 batch loss 0.552260935 batch mAP 0.664123535 batch PCKh 0.3125\n",
      "Trained batch 1716 batch loss 0.579527497 batch mAP 0.54989624 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1717 batch loss 0.571834505 batch mAP 0.556365967 batch PCKh 0.1875\n",
      "Trained batch 1718 batch loss 0.53580308 batch mAP 0.612121582 batch PCKh 0.4375\n",
      "Trained batch 1719 batch loss 0.587614 batch mAP 0.582641602 batch PCKh 0.1875\n",
      "Trained batch 1720 batch loss 0.626681805 batch mAP 0.650390625 batch PCKh 0.375\n",
      "Trained batch 1721 batch loss 0.608163178 batch mAP 0.633453369 batch PCKh 0.375\n",
      "Trained batch 1722 batch loss 0.571535766 batch mAP 0.659454346 batch PCKh 0.4375\n",
      "Trained batch 1723 batch loss 0.650300682 batch mAP 0.569000244 batch PCKh 0.625\n",
      "Trained batch 1724 batch loss 0.609064102 batch mAP 0.574523926 batch PCKh 0.1875\n",
      "Trained batch 1725 batch loss 0.603667855 batch mAP 0.613830566 batch PCKh 0.4375\n",
      "Trained batch 1726 batch loss 0.65080291 batch mAP 0.567993164 batch PCKh 0.5\n",
      "Trained batch 1727 batch loss 0.602496386 batch mAP 0.596099854 batch PCKh 0.625\n",
      "Trained batch 1728 batch loss 0.613655329 batch mAP 0.599609375 batch PCKh 0.875\n",
      "Trained batch 1729 batch loss 0.548115373 batch mAP 0.544036865 batch PCKh 0.4375\n",
      "Trained batch 1730 batch loss 0.637222946 batch mAP 0.564666748 batch PCKh 0.3125\n",
      "Trained batch 1731 batch loss 0.536573052 batch mAP 0.598846436 batch PCKh 0.6875\n",
      "Trained batch 1732 batch loss 0.529913723 batch mAP 0.630126953 batch PCKh 0.8125\n",
      "Trained batch 1733 batch loss 0.601711 batch mAP 0.586883545 batch PCKh 0.5\n",
      "Trained batch 1734 batch loss 0.544156194 batch mAP 0.551391602 batch PCKh 0.8125\n",
      "Trained batch 1735 batch loss 0.573148549 batch mAP 0.537200928 batch PCKh 0.625\n",
      "Trained batch 1736 batch loss 0.516958117 batch mAP 0.586761475 batch PCKh 0.8125\n",
      "Trained batch 1737 batch loss 0.536662 batch mAP 0.536621094 batch PCKh 0.875\n",
      "Trained batch 1738 batch loss 0.513943672 batch mAP 0.551208496 batch PCKh 0.8125\n",
      "Trained batch 1739 batch loss 0.558306 batch mAP 0.495178223 batch PCKh 0.75\n",
      "Trained batch 1740 batch loss 0.49836871 batch mAP 0.626709 batch PCKh 0.6875\n",
      "Trained batch 1741 batch loss 0.417699933 batch mAP 0.63583374 batch PCKh 0.375\n",
      "Trained batch 1742 batch loss 0.372726 batch mAP 0.727844238 batch PCKh 0.5\n",
      "Trained batch 1743 batch loss 0.408503473 batch mAP 0.707336426 batch PCKh 0.5\n",
      "Trained batch 1744 batch loss 0.471659929 batch mAP 0.685943604 batch PCKh 0.5\n",
      "Trained batch 1745 batch loss 0.470375866 batch mAP 0.673461914 batch PCKh 0.3125\n",
      "Trained batch 1746 batch loss 0.476312935 batch mAP 0.695709229 batch PCKh 0.375\n",
      "Trained batch 1747 batch loss 0.467783928 batch mAP 0.68850708 batch PCKh 0.375\n",
      "Trained batch 1748 batch loss 0.468905389 batch mAP 0.654815674 batch PCKh 0.3125\n",
      "Trained batch 1749 batch loss 0.461415082 batch mAP 0.726593 batch PCKh 0.5\n",
      "Trained batch 1750 batch loss 0.557123065 batch mAP 0.631744385 batch PCKh 0.5\n",
      "Trained batch 1751 batch loss 0.515607059 batch mAP 0.631378174 batch PCKh 0.4375\n",
      "Trained batch 1752 batch loss 0.47772 batch mAP 0.580596924 batch PCKh 0.625\n",
      "Trained batch 1753 batch loss 0.460735321 batch mAP 0.625488281 batch PCKh 0.1875\n",
      "Trained batch 1754 batch loss 0.572818756 batch mAP 0.660675049 batch PCKh 0.5625\n",
      "Trained batch 1755 batch loss 0.548837543 batch mAP 0.697418213 batch PCKh 0.875\n",
      "Trained batch 1756 batch loss 0.582535207 batch mAP 0.670288086 batch PCKh 0.5625\n",
      "Trained batch 1757 batch loss 0.632322907 batch mAP 0.621948242 batch PCKh 0.3125\n",
      "Trained batch 1758 batch loss 0.526236534 batch mAP 0.574401855 batch PCKh 0.3125\n",
      "Trained batch 1759 batch loss 0.62433362 batch mAP 0.526855469 batch PCKh 0.625\n",
      "Trained batch 1760 batch loss 0.563329637 batch mAP 0.517547607 batch PCKh 0.5625\n",
      "Trained batch 1761 batch loss 0.6136204 batch mAP 0.533355713 batch PCKh 0.625\n",
      "Trained batch 1762 batch loss 0.487919241 batch mAP 0.533172607 batch PCKh 0.375\n",
      "Trained batch 1763 batch loss 0.474117 batch mAP 0.569458 batch PCKh 0.25\n",
      "Trained batch 1764 batch loss 0.412832022 batch mAP 0.624908447 batch PCKh 0.3125\n",
      "Trained batch 1765 batch loss 0.503548324 batch mAP 0.684936523 batch PCKh 0.5\n",
      "Trained batch 1766 batch loss 0.46833694 batch mAP 0.583770752 batch PCKh 0.75\n",
      "Trained batch 1767 batch loss 0.496731073 batch mAP 0.576660156 batch PCKh 0.75\n",
      "Trained batch 1768 batch loss 0.55176568 batch mAP 0.609710693 batch PCKh 0.8125\n",
      "Trained batch 1769 batch loss 0.573036075 batch mAP 0.620300293 batch PCKh 0.75\n",
      "Trained batch 1770 batch loss 0.594718695 batch mAP 0.588470459 batch PCKh 0.4375\n",
      "Trained batch 1771 batch loss 0.559815526 batch mAP 0.603790283 batch PCKh 0.5\n",
      "Trained batch 1772 batch loss 0.561141849 batch mAP 0.672973633 batch PCKh 0.3125\n",
      "Trained batch 1773 batch loss 0.53676331 batch mAP 0.675994873 batch PCKh 0.5\n",
      "Trained batch 1774 batch loss 0.572306037 batch mAP 0.721130371 batch PCKh 0.5\n",
      "Trained batch 1775 batch loss 0.555709839 batch mAP 0.610870361 batch PCKh 0.375\n",
      "Trained batch 1776 batch loss 0.48789525 batch mAP 0.677063 batch PCKh 0.25\n",
      "Trained batch 1777 batch loss 0.49644 batch mAP 0.593414307 batch PCKh 0.75\n",
      "Trained batch 1778 batch loss 0.485620022 batch mAP 0.564056396 batch PCKh 0.1875\n",
      "Trained batch 1779 batch loss 0.462239921 batch mAP 0.667755127 batch PCKh 0.3125\n",
      "Trained batch 1780 batch loss 0.414802164 batch mAP 0.711639404 batch PCKh 0.1875\n",
      "Trained batch 1781 batch loss 0.383264363 batch mAP 0.709655762 batch PCKh 0.625\n",
      "Trained batch 1782 batch loss 0.506455123 batch mAP 0.691040039 batch PCKh 0.3125\n",
      "Trained batch 1783 batch loss 0.556233048 batch mAP 0.657318115 batch PCKh 0.375\n",
      "Trained batch 1784 batch loss 0.624853373 batch mAP 0.615020752 batch PCKh 0.625\n",
      "Trained batch 1785 batch loss 0.497849 batch mAP 0.590393066 batch PCKh 0.5625\n",
      "Trained batch 1786 batch loss 0.568466663 batch mAP 0.584442139 batch PCKh 0.625\n",
      "Trained batch 1787 batch loss 0.552270293 batch mAP 0.61831665 batch PCKh 0.6875\n",
      "Trained batch 1788 batch loss 0.512980103 batch mAP 0.598693848 batch PCKh 0.625\n",
      "Trained batch 1789 batch loss 0.631022811 batch mAP 0.635406494 batch PCKh 0.5\n",
      "Trained batch 1790 batch loss 0.547369301 batch mAP 0.639434814 batch PCKh 0.375\n",
      "Trained batch 1791 batch loss 0.586218774 batch mAP 0.58416748 batch PCKh 0.375\n",
      "Trained batch 1792 batch loss 0.521438837 batch mAP 0.630828857 batch PCKh 0.625\n",
      "Trained batch 1793 batch loss 0.444592893 batch mAP 0.603240967 batch PCKh 0.8125\n",
      "Trained batch 1794 batch loss 0.449503094 batch mAP 0.662231445 batch PCKh 0.75\n",
      "Trained batch 1795 batch loss 0.460583925 batch mAP 0.606994629 batch PCKh 0.875\n",
      "Trained batch 1796 batch loss 0.446538627 batch mAP 0.640625 batch PCKh 0.75\n",
      "Trained batch 1797 batch loss 0.422805548 batch mAP 0.65826416 batch PCKh 0.8125\n",
      "Trained batch 1798 batch loss 0.494655401 batch mAP 0.618286133 batch PCKh 0.75\n",
      "Trained batch 1799 batch loss 0.560295343 batch mAP 0.599334717 batch PCKh 0.6875\n",
      "Trained batch 1800 batch loss 0.508469224 batch mAP 0.616455078 batch PCKh 0.75\n",
      "Trained batch 1801 batch loss 0.51104 batch mAP 0.602630615 batch PCKh 0.5625\n",
      "Trained batch 1802 batch loss 0.529499054 batch mAP 0.592346191 batch PCKh 0.375\n",
      "Trained batch 1803 batch loss 0.496628881 batch mAP 0.638305664 batch PCKh 0.6875\n",
      "Trained batch 1804 batch loss 0.599706829 batch mAP 0.545440674 batch PCKh 0.625\n",
      "Trained batch 1805 batch loss 0.612350106 batch mAP 0.477417 batch PCKh 0.6875\n",
      "Trained batch 1806 batch loss 0.61075139 batch mAP 0.62689209 batch PCKh 0.5\n",
      "Trained batch 1807 batch loss 0.543546 batch mAP 0.528778076 batch PCKh 0.75\n",
      "Trained batch 1808 batch loss 0.539255202 batch mAP 0.586242676 batch PCKh 0.375\n",
      "Trained batch 1809 batch loss 0.570116699 batch mAP 0.52130127 batch PCKh 0.5625\n",
      "Trained batch 1810 batch loss 0.463667631 batch mAP 0.574920654 batch PCKh 0.5\n",
      "Trained batch 1811 batch loss 0.636486351 batch mAP 0.490386963 batch PCKh 0.75\n",
      "Trained batch 1812 batch loss 0.602618456 batch mAP 0.490753174 batch PCKh 0.6875\n",
      "Trained batch 1813 batch loss 0.63806653 batch mAP 0.50213623 batch PCKh 0.4375\n",
      "Trained batch 1814 batch loss 0.622549176 batch mAP 0.499969482 batch PCKh 0.625\n",
      "Trained batch 1815 batch loss 0.60359329 batch mAP 0.539032 batch PCKh 0.8125\n",
      "Trained batch 1816 batch loss 0.556390524 batch mAP 0.495513916 batch PCKh 0.8125\n",
      "Trained batch 1817 batch loss 0.609725356 batch mAP 0.549743652 batch PCKh 0.5\n",
      "Trained batch 1818 batch loss 0.602794409 batch mAP 0.566558838 batch PCKh 0.875\n",
      "Trained batch 1819 batch loss 0.632246 batch mAP 0.526733398 batch PCKh 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1820 batch loss 0.495121777 batch mAP 0.597564697 batch PCKh 0.6875\n",
      "Trained batch 1821 batch loss 0.559144855 batch mAP 0.542663574 batch PCKh 0.5\n",
      "Trained batch 1822 batch loss 0.452824533 batch mAP 0.692108154 batch PCKh 0.625\n",
      "Trained batch 1823 batch loss 0.535512447 batch mAP 0.524505615 batch PCKh 0.625\n",
      "Trained batch 1824 batch loss 0.494792163 batch mAP 0.545806885 batch PCKh 0.4375\n",
      "Trained batch 1825 batch loss 0.508677423 batch mAP 0.561920166 batch PCKh 0.625\n",
      "Trained batch 1826 batch loss 0.549422801 batch mAP 0.512542725 batch PCKh 0\n",
      "Trained batch 1827 batch loss 0.638961196 batch mAP 0.519622803 batch PCKh 0.6875\n",
      "Trained batch 1828 batch loss 0.60374105 batch mAP 0.500366211 batch PCKh 0.3125\n",
      "Trained batch 1829 batch loss 0.577670157 batch mAP 0.595581055 batch PCKh 0.8125\n",
      "Trained batch 1830 batch loss 0.655751705 batch mAP 0.559326172 batch PCKh 0.3125\n",
      "Trained batch 1831 batch loss 0.623376369 batch mAP 0.581970215 batch PCKh 0.625\n",
      "Trained batch 1832 batch loss 0.59390831 batch mAP 0.636383057 batch PCKh 0.5625\n",
      "Trained batch 1833 batch loss 0.533051848 batch mAP 0.654663086 batch PCKh 0.625\n",
      "Trained batch 1834 batch loss 0.587442517 batch mAP 0.621185303 batch PCKh 0.375\n",
      "Trained batch 1835 batch loss 0.5299263 batch mAP 0.592346191 batch PCKh 0.3125\n",
      "Trained batch 1836 batch loss 0.540449619 batch mAP 0.532775879 batch PCKh 0.6875\n",
      "Trained batch 1837 batch loss 0.543321192 batch mAP 0.624481201 batch PCKh 0.375\n",
      "Trained batch 1838 batch loss 0.52978462 batch mAP 0.611236572 batch PCKh 0.5\n",
      "Trained batch 1839 batch loss 0.560898721 batch mAP 0.623657227 batch PCKh 0.5625\n",
      "Trained batch 1840 batch loss 0.459355712 batch mAP 0.612854 batch PCKh 0.75\n",
      "Trained batch 1841 batch loss 0.393318206 batch mAP 0.598846436 batch PCKh 0.1875\n",
      "Trained batch 1842 batch loss 0.498548329 batch mAP 0.608825684 batch PCKh 0.5625\n",
      "Trained batch 1843 batch loss 0.490844786 batch mAP 0.662658691 batch PCKh 0.25\n",
      "Trained batch 1844 batch loss 0.53889972 batch mAP 0.646240234 batch PCKh 0.5\n",
      "Trained batch 1845 batch loss 0.563225329 batch mAP 0.549621582 batch PCKh 0.1875\n",
      "Trained batch 1846 batch loss 0.552045882 batch mAP 0.567047119 batch PCKh 0.375\n",
      "Trained batch 1847 batch loss 0.589357674 batch mAP 0.595123291 batch PCKh 0.5625\n",
      "Trained batch 1848 batch loss 0.5044204 batch mAP 0.572601318 batch PCKh 0.75\n",
      "Trained batch 1849 batch loss 0.517262816 batch mAP 0.567199707 batch PCKh 0.5\n",
      "Trained batch 1850 batch loss 0.519420385 batch mAP 0.577575684 batch PCKh 0.5625\n",
      "Trained batch 1851 batch loss 0.495254934 batch mAP 0.625549316 batch PCKh 0.125\n",
      "Trained batch 1852 batch loss 0.467464626 batch mAP 0.586792 batch PCKh 0.6875\n",
      "Trained batch 1853 batch loss 0.521230638 batch mAP 0.599639893 batch PCKh 0.25\n",
      "Trained batch 1854 batch loss 0.446942151 batch mAP 0.617706299 batch PCKh 0.5\n",
      "Trained batch 1855 batch loss 0.411273241 batch mAP 0.713562 batch PCKh 0.5625\n",
      "Trained batch 1856 batch loss 0.389935136 batch mAP 0.680877686 batch PCKh 0.375\n",
      "Trained batch 1857 batch loss 0.435664773 batch mAP 0.676422119 batch PCKh 0.1875\n",
      "Trained batch 1858 batch loss 0.510343909 batch mAP 0.598632812 batch PCKh 0.5625\n",
      "Trained batch 1859 batch loss 0.477529109 batch mAP 0.690979 batch PCKh 0.75\n",
      "Trained batch 1860 batch loss 0.494922042 batch mAP 0.650238037 batch PCKh 0.8125\n",
      "Trained batch 1861 batch loss 0.492870688 batch mAP 0.643920898 batch PCKh 0.5\n",
      "Trained batch 1862 batch loss 0.436672509 batch mAP 0.67074585 batch PCKh 0.625\n",
      "Trained batch 1863 batch loss 0.465609848 batch mAP 0.694549561 batch PCKh 0.25\n",
      "Trained batch 1864 batch loss 0.490302324 batch mAP 0.665466309 batch PCKh 0.4375\n",
      "Trained batch 1865 batch loss 0.551245689 batch mAP 0.617034912 batch PCKh 0.25\n",
      "Trained batch 1866 batch loss 0.552988887 batch mAP 0.621856689 batch PCKh 0.3125\n",
      "Trained batch 1867 batch loss 0.556026578 batch mAP 0.633911133 batch PCKh 0.3125\n",
      "Trained batch 1868 batch loss 0.554305673 batch mAP 0.644104 batch PCKh 0.4375\n",
      "Trained batch 1869 batch loss 0.523803771 batch mAP 0.629119873 batch PCKh 0.5625\n",
      "Trained batch 1870 batch loss 0.508509696 batch mAP 0.625946045 batch PCKh 0.375\n",
      "Trained batch 1871 batch loss 0.489491403 batch mAP 0.591644287 batch PCKh 0.125\n",
      "Trained batch 1872 batch loss 0.414866388 batch mAP 0.65145874 batch PCKh 0.5\n",
      "Trained batch 1873 batch loss 0.410831571 batch mAP 0.722167969 batch PCKh 0.5\n",
      "Trained batch 1874 batch loss 0.485665321 batch mAP 0.650421143 batch PCKh 0.4375\n",
      "Trained batch 1875 batch loss 0.554659724 batch mAP 0.677337646 batch PCKh 0.5\n",
      "Trained batch 1876 batch loss 0.517129421 batch mAP 0.675964355 batch PCKh 0.625\n",
      "Trained batch 1877 batch loss 0.45953691 batch mAP 0.721923828 batch PCKh 0.5625\n",
      "Trained batch 1878 batch loss 0.449562758 batch mAP 0.683624268 batch PCKh 0.625\n",
      "Trained batch 1879 batch loss 0.538297713 batch mAP 0.636810303 batch PCKh 0.75\n",
      "Trained batch 1880 batch loss 0.415143967 batch mAP 0.696014404 batch PCKh 0.875\n",
      "Trained batch 1881 batch loss 0.493765175 batch mAP 0.603973389 batch PCKh 0.5625\n",
      "Trained batch 1882 batch loss 0.481544 batch mAP 0.560089111 batch PCKh 0.4375\n",
      "Trained batch 1883 batch loss 0.603945 batch mAP 0.536621094 batch PCKh 0.5625\n",
      "Trained batch 1884 batch loss 0.579472542 batch mAP 0.572296143 batch PCKh 0.25\n",
      "Trained batch 1885 batch loss 0.478883862 batch mAP 0.607879639 batch PCKh 0\n",
      "Trained batch 1886 batch loss 0.484237045 batch mAP 0.689361572 batch PCKh 0.5625\n",
      "Trained batch 1887 batch loss 0.48083052 batch mAP 0.671875 batch PCKh 0.3125\n",
      "Trained batch 1888 batch loss 0.487664819 batch mAP 0.658233643 batch PCKh 0.6875\n",
      "Trained batch 1889 batch loss 0.535756469 batch mAP 0.584075928 batch PCKh 0.4375\n",
      "Trained batch 1890 batch loss 0.516498148 batch mAP 0.605316162 batch PCKh 0.5\n",
      "Trained batch 1891 batch loss 0.540418267 batch mAP 0.580291748 batch PCKh 0.4375\n",
      "Trained batch 1892 batch loss 0.659832537 batch mAP 0.553497314 batch PCKh 0.25\n",
      "Trained batch 1893 batch loss 0.655093253 batch mAP 0.568267822 batch PCKh 0.3125\n",
      "Trained batch 1894 batch loss 0.684220076 batch mAP 0.555999756 batch PCKh 0.1875\n",
      "Trained batch 1895 batch loss 0.612996459 batch mAP 0.561859131 batch PCKh 0\n",
      "Trained batch 1896 batch loss 0.661823332 batch mAP 0.50604248 batch PCKh 0.5\n",
      "Trained batch 1897 batch loss 0.639916062 batch mAP 0.513549805 batch PCKh 0\n",
      "Trained batch 1898 batch loss 0.521792412 batch mAP 0.518554688 batch PCKh 0.3125\n",
      "Trained batch 1899 batch loss 0.487620175 batch mAP 0.520629883 batch PCKh 0.375\n",
      "Trained batch 1900 batch loss 0.435066879 batch mAP 0.539459229 batch PCKh 0\n",
      "Trained batch 1901 batch loss 0.427121758 batch mAP 0.529907227 batch PCKh 0.125\n",
      "Trained batch 1902 batch loss 0.533164442 batch mAP 0.510955811 batch PCKh 0.125\n",
      "Trained batch 1903 batch loss 0.567883492 batch mAP 0.561004639 batch PCKh 0.6875\n",
      "Trained batch 1904 batch loss 0.617787957 batch mAP 0.530639648 batch PCKh 0.4375\n",
      "Trained batch 1905 batch loss 0.62966162 batch mAP 0.500549316 batch PCKh 0.3125\n",
      "Trained batch 1906 batch loss 0.606107056 batch mAP 0.490844727 batch PCKh 0.0625\n",
      "Trained batch 1907 batch loss 0.608173251 batch mAP 0.479492188 batch PCKh 0.0625\n",
      "Trained batch 1908 batch loss 0.481852651 batch mAP 0.647949219 batch PCKh 0.8125\n",
      "Trained batch 1909 batch loss 0.502500772 batch mAP 0.639190674 batch PCKh 0.75\n",
      "Trained batch 1910 batch loss 0.536533058 batch mAP 0.586517334 batch PCKh 0.6875\n",
      "Trained batch 1911 batch loss 0.523483396 batch mAP 0.657562256 batch PCKh 0.375\n",
      "Trained batch 1912 batch loss 0.601847887 batch mAP 0.650146484 batch PCKh 0.3125\n",
      "Trained batch 1913 batch loss 0.596441031 batch mAP 0.615692139 batch PCKh 0.4375\n",
      "Trained batch 1914 batch loss 0.611775935 batch mAP 0.640991211 batch PCKh 0.1875\n",
      "Trained batch 1915 batch loss 0.521059334 batch mAP 0.691070557 batch PCKh 0.375\n",
      "Trained batch 1916 batch loss 0.498869658 batch mAP 0.700683594 batch PCKh 0.25\n",
      "Trained batch 1917 batch loss 0.524194479 batch mAP 0.703979492 batch PCKh 0.3125\n",
      "Trained batch 1918 batch loss 0.513031065 batch mAP 0.706726074 batch PCKh 0.5625\n",
      "Trained batch 1919 batch loss 0.554661274 batch mAP 0.638702393 batch PCKh 0.5\n",
      "Trained batch 1920 batch loss 0.528251231 batch mAP 0.693756104 batch PCKh 0.4375\n",
      "Trained batch 1921 batch loss 0.583651304 batch mAP 0.640441895 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1922 batch loss 0.443885565 batch mAP 0.650085449 batch PCKh 0.25\n",
      "Trained batch 1923 batch loss 0.491898209 batch mAP 0.706420898 batch PCKh 0.625\n",
      "Trained batch 1924 batch loss 0.522870719 batch mAP 0.570098877 batch PCKh 0.125\n",
      "Trained batch 1925 batch loss 0.428993344 batch mAP 0.557891846 batch PCKh 0.6875\n",
      "Trained batch 1926 batch loss 0.532491148 batch mAP 0.559204102 batch PCKh 0.6875\n",
      "Trained batch 1927 batch loss 0.485399097 batch mAP 0.524719238 batch PCKh 0.75\n",
      "Trained batch 1928 batch loss 0.51639992 batch mAP 0.519165039 batch PCKh 0.875\n",
      "Trained batch 1929 batch loss 0.464483887 batch mAP 0.571105957 batch PCKh 0.4375\n",
      "Trained batch 1930 batch loss 0.462258667 batch mAP 0.637359619 batch PCKh 0.6875\n",
      "Trained batch 1931 batch loss 0.451232582 batch mAP 0.547210693 batch PCKh 0.4375\n",
      "Trained batch 1932 batch loss 0.503133178 batch mAP 0.607543945 batch PCKh 0.625\n",
      "Trained batch 1933 batch loss 0.469538689 batch mAP 0.558227539 batch PCKh 0.875\n",
      "Trained batch 1934 batch loss 0.47700119 batch mAP 0.572418213 batch PCKh 0.5625\n",
      "Trained batch 1935 batch loss 0.452528954 batch mAP 0.607574463 batch PCKh 0.75\n",
      "Trained batch 1936 batch loss 0.435239851 batch mAP 0.640350342 batch PCKh 0.625\n",
      "Trained batch 1937 batch loss 0.535979092 batch mAP 0.616851807 batch PCKh 0.375\n",
      "Trained batch 1938 batch loss 0.485326231 batch mAP 0.573852539 batch PCKh 0.75\n",
      "Trained batch 1939 batch loss 0.496130407 batch mAP 0.552490234 batch PCKh 0.4375\n",
      "Trained batch 1940 batch loss 0.587562859 batch mAP 0.570983887 batch PCKh 0.5625\n",
      "Trained batch 1941 batch loss 0.473733842 batch mAP 0.589172363 batch PCKh 0.4375\n",
      "Trained batch 1942 batch loss 0.462639749 batch mAP 0.610015869 batch PCKh 0.6875\n",
      "Trained batch 1943 batch loss 0.464762568 batch mAP 0.62008667 batch PCKh 0.1875\n",
      "Trained batch 1944 batch loss 0.527231038 batch mAP 0.594299316 batch PCKh 0.4375\n",
      "Trained batch 1945 batch loss 0.523457468 batch mAP 0.679321289 batch PCKh 0.4375\n",
      "Trained batch 1946 batch loss 0.498738468 batch mAP 0.641601562 batch PCKh 0.125\n",
      "Trained batch 1947 batch loss 0.52070272 batch mAP 0.659118652 batch PCKh 0.625\n",
      "Trained batch 1948 batch loss 0.428069204 batch mAP 0.650054932 batch PCKh 0.25\n",
      "Trained batch 1949 batch loss 0.587749362 batch mAP 0.547637939 batch PCKh 0.5\n",
      "Trained batch 1950 batch loss 0.55486083 batch mAP 0.584075928 batch PCKh 0.5\n",
      "Trained batch 1951 batch loss 0.546706855 batch mAP 0.575683594 batch PCKh 0.5\n",
      "Trained batch 1952 batch loss 0.57727313 batch mAP 0.556152344 batch PCKh 0.5625\n",
      "Trained batch 1953 batch loss 0.569803596 batch mAP 0.600494385 batch PCKh 0.375\n",
      "Trained batch 1954 batch loss 0.573380828 batch mAP 0.612640381 batch PCKh 0.5\n",
      "Trained batch 1955 batch loss 0.593217492 batch mAP 0.61807251 batch PCKh 0.75\n",
      "Trained batch 1956 batch loss 0.586942554 batch mAP 0.598876953 batch PCKh 0.5625\n",
      "Trained batch 1957 batch loss 0.583850801 batch mAP 0.585205078 batch PCKh 0.5\n",
      "Trained batch 1958 batch loss 0.553176939 batch mAP 0.576049805 batch PCKh 0.75\n",
      "Trained batch 1959 batch loss 0.573492825 batch mAP 0.582855225 batch PCKh 0.6875\n",
      "Trained batch 1960 batch loss 0.503908277 batch mAP 0.571899414 batch PCKh 0.375\n",
      "Trained batch 1961 batch loss 0.461367637 batch mAP 0.461364746 batch PCKh 0.25\n",
      "Trained batch 1962 batch loss 0.520107746 batch mAP 0.532012939 batch PCKh 0.3125\n",
      "Trained batch 1963 batch loss 0.568787336 batch mAP 0.437011719 batch PCKh 0.75\n",
      "Trained batch 1964 batch loss 0.519167125 batch mAP 0.580505371 batch PCKh 0.5625\n",
      "Trained batch 1965 batch loss 0.469418108 batch mAP 0.603759766 batch PCKh 0.5625\n",
      "Trained batch 1966 batch loss 0.457181931 batch mAP 0.668396 batch PCKh 0.625\n",
      "Trained batch 1967 batch loss 0.476984531 batch mAP 0.679870605 batch PCKh 0.75\n",
      "Trained batch 1968 batch loss 0.602058649 batch mAP 0.600402832 batch PCKh 0.5\n",
      "Trained batch 1969 batch loss 0.467615902 batch mAP 0.637420654 batch PCKh 0.375\n",
      "Trained batch 1970 batch loss 0.554794848 batch mAP 0.613830566 batch PCKh 0.625\n",
      "Trained batch 1971 batch loss 0.591367066 batch mAP 0.597991943 batch PCKh 0.625\n",
      "Trained batch 1972 batch loss 0.618878603 batch mAP 0.577606201 batch PCKh 0.4375\n",
      "Trained batch 1973 batch loss 0.479301572 batch mAP 0.604736328 batch PCKh 0.3125\n",
      "Trained batch 1974 batch loss 0.520076334 batch mAP 0.57611084 batch PCKh 0.625\n",
      "Trained batch 1975 batch loss 0.515273929 batch mAP 0.59387207 batch PCKh 0.625\n",
      "Trained batch 1976 batch loss 0.566586852 batch mAP 0.601318359 batch PCKh 0.6875\n",
      "Trained batch 1977 batch loss 0.57047081 batch mAP 0.621002197 batch PCKh 0.0625\n",
      "Trained batch 1978 batch loss 0.528609574 batch mAP 0.636932373 batch PCKh 0.5\n",
      "Trained batch 1979 batch loss 0.534638762 batch mAP 0.595184326 batch PCKh 0.375\n",
      "Trained batch 1980 batch loss 0.430622041 batch mAP 0.707397461 batch PCKh 0.5\n",
      "Trained batch 1981 batch loss 0.437468231 batch mAP 0.70892334 batch PCKh 0.5\n",
      "Trained batch 1982 batch loss 0.478775799 batch mAP 0.681152344 batch PCKh 0.3125\n",
      "Trained batch 1983 batch loss 0.52509135 batch mAP 0.682281494 batch PCKh 0.5625\n",
      "Trained batch 1984 batch loss 0.524175048 batch mAP 0.663085938 batch PCKh 0.5625\n",
      "Trained batch 1985 batch loss 0.538658738 batch mAP 0.662353516 batch PCKh 0.875\n",
      "Trained batch 1986 batch loss 0.460203737 batch mAP 0.712463379 batch PCKh 0.6875\n",
      "Trained batch 1987 batch loss 0.446754336 batch mAP 0.724578857 batch PCKh 0.5\n",
      "Trained batch 1988 batch loss 0.400468528 batch mAP 0.75100708 batch PCKh 0.5625\n",
      "Trained batch 1989 batch loss 0.431739271 batch mAP 0.751251221 batch PCKh 0.4375\n",
      "Trained batch 1990 batch loss 0.461191654 batch mAP 0.709747314 batch PCKh 0.5\n",
      "Trained batch 1991 batch loss 0.43760556 batch mAP 0.736114502 batch PCKh 0.875\n",
      "Trained batch 1992 batch loss 0.512340844 batch mAP 0.678894043 batch PCKh 0.375\n",
      "Trained batch 1993 batch loss 0.477431238 batch mAP 0.696716309 batch PCKh 0.625\n",
      "Trained batch 1994 batch loss 0.564603686 batch mAP 0.561859131 batch PCKh 0.9375\n",
      "Trained batch 1995 batch loss 0.564045608 batch mAP 0.573120117 batch PCKh 0.8125\n",
      "Trained batch 1996 batch loss 0.53932178 batch mAP 0.67199707 batch PCKh 0.6875\n",
      "Trained batch 1997 batch loss 0.488126755 batch mAP 0.650787354 batch PCKh 0.75\n",
      "Trained batch 1998 batch loss 0.457413763 batch mAP 0.648712158 batch PCKh 0.5\n",
      "Trained batch 1999 batch loss 0.412125766 batch mAP 0.634246826 batch PCKh 0.4375\n",
      "Trained batch 2000 batch loss 0.393755108 batch mAP 0.673339844 batch PCKh 0.4375\n",
      "Trained batch 2001 batch loss 0.467853248 batch mAP 0.644195557 batch PCKh 0.5625\n",
      "Trained batch 2002 batch loss 0.476242572 batch mAP 0.701202393 batch PCKh 0.75\n",
      "Trained batch 2003 batch loss 0.469569862 batch mAP 0.63482666 batch PCKh 0.625\n",
      "Trained batch 2004 batch loss 0.550486326 batch mAP 0.607208252 batch PCKh 0.875\n",
      "Trained batch 2005 batch loss 0.55869776 batch mAP 0.616973877 batch PCKh 0.625\n",
      "Trained batch 2006 batch loss 0.555372357 batch mAP 0.581298828 batch PCKh 0.25\n",
      "Trained batch 2007 batch loss 0.562310219 batch mAP 0.600769043 batch PCKh 0.5625\n",
      "Trained batch 2008 batch loss 0.53164506 batch mAP 0.585388184 batch PCKh 0.6875\n",
      "Trained batch 2009 batch loss 0.478264183 batch mAP 0.592529297 batch PCKh 0.6875\n",
      "Trained batch 2010 batch loss 0.539865732 batch mAP 0.577819824 batch PCKh 0.75\n",
      "Trained batch 2011 batch loss 0.483267874 batch mAP 0.633117676 batch PCKh 0.625\n",
      "Trained batch 2012 batch loss 0.455931336 batch mAP 0.57901 batch PCKh 0.1875\n",
      "Trained batch 2013 batch loss 0.406952024 batch mAP 0.578613281 batch PCKh 0.25\n",
      "Trained batch 2014 batch loss 0.501918077 batch mAP 0.606811523 batch PCKh 0.4375\n",
      "Trained batch 2015 batch loss 0.500821292 batch mAP 0.528259277 batch PCKh 0.5\n",
      "Trained batch 2016 batch loss 0.459078789 batch mAP 0.640289307 batch PCKh 0.5625\n",
      "Trained batch 2017 batch loss 0.527472854 batch mAP 0.619567871 batch PCKh 0.25\n",
      "Trained batch 2018 batch loss 0.52869755 batch mAP 0.684234619 batch PCKh 0.8125\n",
      "Trained batch 2019 batch loss 0.505904377 batch mAP 0.588104248 batch PCKh 0.25\n",
      "Trained batch 2020 batch loss 0.563319802 batch mAP 0.60546875 batch PCKh 0.375\n",
      "Trained batch 2021 batch loss 0.579873443 batch mAP 0.564544678 batch PCKh 0.1875\n",
      "Trained batch 2022 batch loss 0.538587511 batch mAP 0.625915527 batch PCKh 0.6875\n",
      "Trained batch 2023 batch loss 0.555125237 batch mAP 0.605224609 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2024 batch loss 0.591645718 batch mAP 0.677368164 batch PCKh 0.4375\n",
      "Trained batch 2025 batch loss 0.531418085 batch mAP 0.663848877 batch PCKh 0.625\n",
      "Trained batch 2026 batch loss 0.524355471 batch mAP 0.653778076 batch PCKh 0.6875\n",
      "Trained batch 2027 batch loss 0.523312926 batch mAP 0.581176758 batch PCKh 0.4375\n",
      "Trained batch 2028 batch loss 0.488473713 batch mAP 0.634338379 batch PCKh 0.75\n",
      "Trained batch 2029 batch loss 0.526115894 batch mAP 0.602020264 batch PCKh 0.5\n",
      "Trained batch 2030 batch loss 0.52282542 batch mAP 0.607147217 batch PCKh 0.5625\n",
      "Trained batch 2031 batch loss 0.500834644 batch mAP 0.622772217 batch PCKh 0.125\n",
      "Trained batch 2032 batch loss 0.496350467 batch mAP 0.656219482 batch PCKh 0.3125\n",
      "Trained batch 2033 batch loss 0.553025544 batch mAP 0.592498779 batch PCKh 0.5625\n",
      "Trained batch 2034 batch loss 0.513387382 batch mAP 0.572692871 batch PCKh 0.25\n",
      "Trained batch 2035 batch loss 0.570122957 batch mAP 0.591522217 batch PCKh 0.625\n",
      "Trained batch 2036 batch loss 0.505291402 batch mAP 0.654968262 batch PCKh 0.8125\n",
      "Trained batch 2037 batch loss 0.58188 batch mAP 0.605011 batch PCKh 0.3125\n",
      "Trained batch 2038 batch loss 0.513521314 batch mAP 0.568267822 batch PCKh 0.5\n",
      "Trained batch 2039 batch loss 0.467513233 batch mAP 0.661834717 batch PCKh 0.875\n",
      "Trained batch 2040 batch loss 0.570636153 batch mAP 0.539764404 batch PCKh 0.625\n",
      "Trained batch 2041 batch loss 0.538379192 batch mAP 0.493225098 batch PCKh 0.75\n",
      "Trained batch 2042 batch loss 0.531016469 batch mAP 0.509185791 batch PCKh 0.625\n",
      "Trained batch 2043 batch loss 0.500924706 batch mAP 0.660247803 batch PCKh 0.625\n",
      "Trained batch 2044 batch loss 0.566162527 batch mAP 0.619751 batch PCKh 0.625\n",
      "Trained batch 2045 batch loss 0.484351337 batch mAP 0.669525146 batch PCKh 0.3125\n",
      "Trained batch 2046 batch loss 0.581658125 batch mAP 0.700317383 batch PCKh 0.3125\n",
      "Trained batch 2047 batch loss 0.401987821 batch mAP 0.709838867 batch PCKh 0.1875\n",
      "Trained batch 2048 batch loss 0.511746049 batch mAP 0.732666 batch PCKh 0.5\n",
      "Trained batch 2049 batch loss 0.492518902 batch mAP 0.651153564 batch PCKh 0.6875\n",
      "Trained batch 2050 batch loss 0.47485128 batch mAP 0.639190674 batch PCKh 0.375\n",
      "Trained batch 2051 batch loss 0.510640264 batch mAP 0.668212891 batch PCKh 0.75\n",
      "Trained batch 2052 batch loss 0.565866351 batch mAP 0.63092041 batch PCKh 0.6875\n",
      "Trained batch 2053 batch loss 0.54456234 batch mAP 0.60559082 batch PCKh 0.6875\n",
      "Trained batch 2054 batch loss 0.477843255 batch mAP 0.570495605 batch PCKh 0.625\n",
      "Trained batch 2055 batch loss 0.624140441 batch mAP 0.542022705 batch PCKh 0.75\n",
      "Trained batch 2056 batch loss 0.422436595 batch mAP 0.630615234 batch PCKh 0.375\n",
      "Trained batch 2057 batch loss 0.564466 batch mAP 0.573730469 batch PCKh 0.75\n",
      "Trained batch 2058 batch loss 0.636718154 batch mAP 0.551116943 batch PCKh 0.5\n",
      "Trained batch 2059 batch loss 0.601941228 batch mAP 0.563690186 batch PCKh 0.25\n",
      "Trained batch 2060 batch loss 0.627173424 batch mAP 0.529693604 batch PCKh 0.5\n",
      "Trained batch 2061 batch loss 0.574145555 batch mAP 0.630188 batch PCKh 0.5\n",
      "Trained batch 2062 batch loss 0.585022449 batch mAP 0.633636475 batch PCKh 0.3125\n",
      "Trained batch 2063 batch loss 0.537248135 batch mAP 0.594329834 batch PCKh 0.625\n",
      "Trained batch 2064 batch loss 0.581335425 batch mAP 0.563964844 batch PCKh 0.25\n",
      "Trained batch 2065 batch loss 0.594321489 batch mAP 0.545532227 batch PCKh 0.3125\n",
      "Trained batch 2066 batch loss 0.628283203 batch mAP 0.515228271 batch PCKh 0.4375\n",
      "Trained batch 2067 batch loss 0.585026503 batch mAP 0.604705811 batch PCKh 0.625\n",
      "Trained batch 2068 batch loss 0.644421101 batch mAP 0.532562256 batch PCKh 0.25\n",
      "Trained batch 2069 batch loss 0.379069388 batch mAP 0.538757324 batch PCKh 0.1875\n",
      "Trained batch 2070 batch loss 0.531871259 batch mAP 0.487213135 batch PCKh 0.375\n",
      "Trained batch 2071 batch loss 0.460786283 batch mAP 0.659088135 batch PCKh 0.5\n",
      "Trained batch 2072 batch loss 0.466771901 batch mAP 0.576843262 batch PCKh 0.375\n",
      "Trained batch 2073 batch loss 0.548649371 batch mAP 0.553314209 batch PCKh 0.375\n",
      "Trained batch 2074 batch loss 0.514294922 batch mAP 0.597686768 batch PCKh 0.3125\n",
      "Trained batch 2075 batch loss 0.399875283 batch mAP 0.626464844 batch PCKh 0.5\n",
      "Trained batch 2076 batch loss 0.435599864 batch mAP 0.635467529 batch PCKh 0.4375\n",
      "Trained batch 2077 batch loss 0.517308235 batch mAP 0.602050781 batch PCKh 0.25\n",
      "Trained batch 2078 batch loss 0.461296976 batch mAP 0.64956665 batch PCKh 0.3125\n",
      "Trained batch 2079 batch loss 0.449901104 batch mAP 0.621398926 batch PCKh 0.6875\n",
      "Trained batch 2080 batch loss 0.553750634 batch mAP 0.600769043 batch PCKh 0.625\n",
      "Trained batch 2081 batch loss 0.50349623 batch mAP 0.637786865 batch PCKh 0.6875\n",
      "Trained batch 2082 batch loss 0.547497749 batch mAP 0.593688965 batch PCKh 0.375\n",
      "Trained batch 2083 batch loss 0.578362525 batch mAP 0.538299561 batch PCKh 0.625\n",
      "Trained batch 2084 batch loss 0.501205206 batch mAP 0.550109863 batch PCKh 0.3125\n",
      "Trained batch 2085 batch loss 0.46638158 batch mAP 0.609405518 batch PCKh 0.5625\n",
      "Trained batch 2086 batch loss 0.499286264 batch mAP 0.591918945 batch PCKh 0.4375\n",
      "Trained batch 2087 batch loss 0.492897213 batch mAP 0.630157471 batch PCKh 0.5\n",
      "Trained batch 2088 batch loss 0.496349692 batch mAP 0.683410645 batch PCKh 0.6875\n",
      "Trained batch 2089 batch loss 0.4918589 batch mAP 0.661376953 batch PCKh 0.5625\n",
      "Trained batch 2090 batch loss 0.553139687 batch mAP 0.6199646 batch PCKh 0.3125\n",
      "Trained batch 2091 batch loss 0.449251145 batch mAP 0.575866699 batch PCKh 0.1875\n",
      "Trained batch 2092 batch loss 0.496224701 batch mAP 0.563903809 batch PCKh 0.125\n",
      "Trained batch 2093 batch loss 0.47379142 batch mAP 0.605743408 batch PCKh 0.4375\n",
      "Trained batch 2094 batch loss 0.494663239 batch mAP 0.700927734 batch PCKh 0.4375\n",
      "Trained batch 2095 batch loss 0.476572752 batch mAP 0.643035889 batch PCKh 0.625\n",
      "Trained batch 2096 batch loss 0.519713104 batch mAP 0.689300537 batch PCKh 0.4375\n",
      "Trained batch 2097 batch loss 0.469954222 batch mAP 0.697753906 batch PCKh 0.3125\n",
      "Trained batch 2098 batch loss 0.454746246 batch mAP 0.701782227 batch PCKh 0.75\n",
      "Trained batch 2099 batch loss 0.447704792 batch mAP 0.702606201 batch PCKh 0.5625\n",
      "Trained batch 2100 batch loss 0.403649747 batch mAP 0.719360352 batch PCKh 0.4375\n",
      "Trained batch 2101 batch loss 0.407963753 batch mAP 0.6746521 batch PCKh 0.625\n",
      "Trained batch 2102 batch loss 0.419249386 batch mAP 0.699371338 batch PCKh 0.5\n",
      "Trained batch 2103 batch loss 0.387131393 batch mAP 0.7081604 batch PCKh 0.5625\n",
      "Trained batch 2104 batch loss 0.375489414 batch mAP 0.6925354 batch PCKh 0.625\n",
      "Trained batch 2105 batch loss 0.481391072 batch mAP 0.647949219 batch PCKh 0.75\n",
      "Trained batch 2106 batch loss 0.468469232 batch mAP 0.670349121 batch PCKh 0.6875\n",
      "Trained batch 2107 batch loss 0.508069634 batch mAP 0.624572754 batch PCKh 0.625\n",
      "Trained batch 2108 batch loss 0.452726364 batch mAP 0.634368896 batch PCKh 0.5\n",
      "Trained batch 2109 batch loss 0.463140845 batch mAP 0.59362793 batch PCKh 0.625\n",
      "Trained batch 2110 batch loss 0.443139732 batch mAP 0.695251465 batch PCKh 0.375\n",
      "Trained batch 2111 batch loss 0.467750311 batch mAP 0.628570557 batch PCKh 0.4375\n",
      "Trained batch 2112 batch loss 0.433689952 batch mAP 0.641296387 batch PCKh 0.6875\n",
      "Trained batch 2113 batch loss 0.573505938 batch mAP 0.588134766 batch PCKh 0.4375\n",
      "Trained batch 2114 batch loss 0.46153909 batch mAP 0.667541504 batch PCKh 0.4375\n",
      "Trained batch 2115 batch loss 0.419640064 batch mAP 0.673217773 batch PCKh 0.625\n",
      "Trained batch 2116 batch loss 0.514350533 batch mAP 0.685302734 batch PCKh 0.25\n",
      "Trained batch 2117 batch loss 0.510264337 batch mAP 0.621582031 batch PCKh 0.3125\n",
      "Trained batch 2118 batch loss 0.531423688 batch mAP 0.633300781 batch PCKh 0.375\n",
      "Trained batch 2119 batch loss 0.520713449 batch mAP 0.677063 batch PCKh 0.6875\n",
      "Trained batch 2120 batch loss 0.549619496 batch mAP 0.648956299 batch PCKh 0.625\n",
      "Trained batch 2121 batch loss 0.576278 batch mAP 0.676574707 batch PCKh 0.4375\n",
      "Trained batch 2122 batch loss 0.505347 batch mAP 0.710784912 batch PCKh 0.4375\n",
      "Trained batch 2123 batch loss 0.576550841 batch mAP 0.688293457 batch PCKh 0.5\n",
      "Trained batch 2124 batch loss 0.52213037 batch mAP 0.643798828 batch PCKh 0.1875\n",
      "Trained batch 2125 batch loss 0.547019899 batch mAP 0.659576416 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2126 batch loss 0.455100507 batch mAP 0.691070557 batch PCKh 0.375\n",
      "Trained batch 2127 batch loss 0.51250267 batch mAP 0.650360107 batch PCKh 0.375\n",
      "Trained batch 2128 batch loss 0.556808531 batch mAP 0.64151 batch PCKh 0.375\n",
      "Trained batch 2129 batch loss 0.539945662 batch mAP 0.638763428 batch PCKh 0.5\n",
      "Trained batch 2130 batch loss 0.565336466 batch mAP 0.589691162 batch PCKh 0.5625\n",
      "Trained batch 2131 batch loss 0.586280823 batch mAP 0.571960449 batch PCKh 0.3125\n",
      "Trained batch 2132 batch loss 0.492004037 batch mAP 0.560455322 batch PCKh 0.75\n",
      "Trained batch 2133 batch loss 0.4667539 batch mAP 0.652252197 batch PCKh 0.875\n",
      "Trained batch 2134 batch loss 0.546171188 batch mAP 0.652679443 batch PCKh 0.5625\n",
      "Trained batch 2135 batch loss 0.626309574 batch mAP 0.56463623 batch PCKh 0.75\n",
      "Trained batch 2136 batch loss 0.692949295 batch mAP 0.572570801 batch PCKh 0\n",
      "Trained batch 2137 batch loss 0.638073206 batch mAP 0.540496826 batch PCKh 0.5625\n",
      "Trained batch 2138 batch loss 0.648403 batch mAP 0.560943604 batch PCKh 0.6875\n",
      "Trained batch 2139 batch loss 0.517766833 batch mAP 0.517059326 batch PCKh 0.3125\n",
      "Trained batch 2140 batch loss 0.53382051 batch mAP 0.575653076 batch PCKh 0.875\n",
      "Trained batch 2141 batch loss 0.513813078 batch mAP 0.589416504 batch PCKh 0.6875\n",
      "Trained batch 2142 batch loss 0.47598213 batch mAP 0.591827393 batch PCKh 0.6875\n",
      "Trained batch 2143 batch loss 0.485135704 batch mAP 0.642303467 batch PCKh 0.3125\n",
      "Trained batch 2144 batch loss 0.368809521 batch mAP 0.580596924 batch PCKh 0.5\n",
      "Trained batch 2145 batch loss 0.366886795 batch mAP 0.548950195 batch PCKh 0.5625\n",
      "Trained batch 2146 batch loss 0.369014859 batch mAP 0.569580078 batch PCKh 0.0625\n",
      "Trained batch 2147 batch loss 0.492982715 batch mAP 0.619903564 batch PCKh 0.5\n",
      "Trained batch 2148 batch loss 0.487121761 batch mAP 0.558776855 batch PCKh 0.6875\n",
      "Trained batch 2149 batch loss 0.527291119 batch mAP 0.564849854 batch PCKh 0.625\n",
      "Trained batch 2150 batch loss 0.475503892 batch mAP 0.515808105 batch PCKh 0.1875\n",
      "Trained batch 2151 batch loss 0.377743304 batch mAP 0.589263916 batch PCKh 0.3125\n",
      "Trained batch 2152 batch loss 0.34152025 batch mAP 0.636993408 batch PCKh 0\n",
      "Trained batch 2153 batch loss 0.335340381 batch mAP 0.67276 batch PCKh 0\n",
      "Trained batch 2154 batch loss 0.361419439 batch mAP 0.612304688 batch PCKh 0\n",
      "Trained batch 2155 batch loss 0.399519324 batch mAP 0.620727539 batch PCKh 0.5625\n",
      "Trained batch 2156 batch loss 0.393344492 batch mAP 0.592437744 batch PCKh 0.75\n",
      "Trained batch 2157 batch loss 0.499358594 batch mAP 0.568573 batch PCKh 0.625\n",
      "Trained batch 2158 batch loss 0.455240488 batch mAP 0.603240967 batch PCKh 0.625\n",
      "Trained batch 2159 batch loss 0.440651298 batch mAP 0.5859375 batch PCKh 0.75\n",
      "Trained batch 2160 batch loss 0.494988441 batch mAP 0.58392334 batch PCKh 0.4375\n",
      "Trained batch 2161 batch loss 0.379893839 batch mAP 0.580993652 batch PCKh 0.75\n",
      "Trained batch 2162 batch loss 0.576332569 batch mAP 0.613189697 batch PCKh 0.3125\n",
      "Trained batch 2163 batch loss 0.46202 batch mAP 0.599609375 batch PCKh 0.625\n",
      "Trained batch 2164 batch loss 0.51575923 batch mAP 0.640838623 batch PCKh 0.0625\n",
      "Trained batch 2165 batch loss 0.464529872 batch mAP 0.667205811 batch PCKh 0.5625\n",
      "Trained batch 2166 batch loss 0.507726073 batch mAP 0.597351074 batch PCKh 0.5625\n",
      "Trained batch 2167 batch loss 0.500432 batch mAP 0.654418945 batch PCKh 0.5\n",
      "Trained batch 2168 batch loss 0.498109102 batch mAP 0.650848389 batch PCKh 0.75\n",
      "Trained batch 2169 batch loss 0.578810453 batch mAP 0.602905273 batch PCKh 0.5\n",
      "Trained batch 2170 batch loss 0.508185 batch mAP 0.610321045 batch PCKh 0.5625\n",
      "Trained batch 2171 batch loss 0.546722472 batch mAP 0.603973389 batch PCKh 0.5\n",
      "Trained batch 2172 batch loss 0.492355794 batch mAP 0.623443604 batch PCKh 0.75\n",
      "Trained batch 2173 batch loss 0.495676756 batch mAP 0.608428955 batch PCKh 0.625\n",
      "Trained batch 2174 batch loss 0.539380729 batch mAP 0.651245117 batch PCKh 0.6875\n",
      "Trained batch 2175 batch loss 0.532406569 batch mAP 0.595092773 batch PCKh 0.75\n",
      "Trained batch 2176 batch loss 0.51095885 batch mAP 0.668212891 batch PCKh 0.5625\n",
      "Trained batch 2177 batch loss 0.470167637 batch mAP 0.684692383 batch PCKh 0.75\n",
      "Trained batch 2178 batch loss 0.527221501 batch mAP 0.719696045 batch PCKh 0.4375\n",
      "Trained batch 2179 batch loss 0.492172241 batch mAP 0.70980835 batch PCKh 0.3125\n",
      "Trained batch 2180 batch loss 0.555922151 batch mAP 0.617095947 batch PCKh 0.75\n",
      "Trained batch 2181 batch loss 0.534967899 batch mAP 0.625976562 batch PCKh 0.75\n",
      "Trained batch 2182 batch loss 0.644282699 batch mAP 0.57623291 batch PCKh 0.0625\n",
      "Trained batch 2183 batch loss 0.559216 batch mAP 0.571685791 batch PCKh 0\n",
      "Trained batch 2184 batch loss 0.612552285 batch mAP 0.608764648 batch PCKh 0.875\n",
      "Trained batch 2185 batch loss 0.579117894 batch mAP 0.656005859 batch PCKh 0.8125\n",
      "Trained batch 2186 batch loss 0.58252275 batch mAP 0.578552246 batch PCKh 0.25\n",
      "Trained batch 2187 batch loss 0.556953311 batch mAP 0.554382324 batch PCKh 0.5625\n",
      "Trained batch 2188 batch loss 0.558807135 batch mAP 0.606140137 batch PCKh 0.5625\n",
      "Trained batch 2189 batch loss 0.52110523 batch mAP 0.636627197 batch PCKh 0.75\n",
      "Trained batch 2190 batch loss 0.555545688 batch mAP 0.593994141 batch PCKh 0.3125\n",
      "Trained batch 2191 batch loss 0.525853157 batch mAP 0.475891113 batch PCKh 0.375\n",
      "Trained batch 2192 batch loss 0.498197705 batch mAP 0.352722168 batch PCKh 0.75\n",
      "Trained batch 2193 batch loss 0.468230724 batch mAP 0.462554932 batch PCKh 0.6875\n",
      "Trained batch 2194 batch loss 0.392317444 batch mAP 0.415008545 batch PCKh 0.5625\n",
      "Trained batch 2195 batch loss 0.441346407 batch mAP 0.425018311 batch PCKh 0.75\n",
      "Trained batch 2196 batch loss 0.50898248 batch mAP 0.365325928 batch PCKh 0.75\n",
      "Trained batch 2197 batch loss 0.49359715 batch mAP 0.425445557 batch PCKh 0.125\n",
      "Trained batch 2198 batch loss 0.557686388 batch mAP 0.487640381 batch PCKh 0.5625\n",
      "Trained batch 2199 batch loss 0.610525906 batch mAP 0.439331055 batch PCKh 0.75\n",
      "Trained batch 2200 batch loss 0.643743515 batch mAP 0.51071167 batch PCKh 0.8125\n",
      "Trained batch 2201 batch loss 0.660778224 batch mAP 0.549224854 batch PCKh 0.1875\n",
      "Trained batch 2202 batch loss 0.522342145 batch mAP 0.533447266 batch PCKh 0.4375\n",
      "Trained batch 2203 batch loss 0.553532 batch mAP 0.613433838 batch PCKh 0.75\n",
      "Trained batch 2204 batch loss 0.558307528 batch mAP 0.590393066 batch PCKh 0.75\n",
      "Trained batch 2205 batch loss 0.460394084 batch mAP 0.574279785 batch PCKh 0.3125\n",
      "Trained batch 2206 batch loss 0.506419182 batch mAP 0.507019043 batch PCKh 0.1875\n",
      "Trained batch 2207 batch loss 0.459896624 batch mAP 0.595001221 batch PCKh 0.4375\n",
      "Trained batch 2208 batch loss 0.573507726 batch mAP 0.61026 batch PCKh 0.25\n",
      "Trained batch 2209 batch loss 0.529414952 batch mAP 0.635009766 batch PCKh 0.75\n",
      "Trained batch 2210 batch loss 0.463877976 batch mAP 0.621704102 batch PCKh 0.5625\n",
      "Trained batch 2211 batch loss 0.476418257 batch mAP 0.586914062 batch PCKh 0.375\n",
      "Trained batch 2212 batch loss 0.498771667 batch mAP 0.600128174 batch PCKh 0.5625\n",
      "Trained batch 2213 batch loss 0.472810805 batch mAP 0.576477051 batch PCKh 0.375\n",
      "Trained batch 2214 batch loss 0.58233422 batch mAP 0.499694824 batch PCKh 0.5\n",
      "Trained batch 2215 batch loss 0.518661201 batch mAP 0.527130127 batch PCKh 0.5625\n",
      "Trained batch 2216 batch loss 0.502238 batch mAP 0.51864624 batch PCKh 0.625\n",
      "Trained batch 2217 batch loss 0.556587398 batch mAP 0.569702148 batch PCKh 0.3125\n",
      "Trained batch 2218 batch loss 0.514935255 batch mAP 0.591918945 batch PCKh 0.25\n",
      "Trained batch 2219 batch loss 0.487307817 batch mAP 0.563476562 batch PCKh 0.1875\n",
      "Trained batch 2220 batch loss 0.54601866 batch mAP 0.593902588 batch PCKh 0.375\n",
      "Trained batch 2221 batch loss 0.537746072 batch mAP 0.612884521 batch PCKh 0.5625\n",
      "Trained batch 2222 batch loss 0.600676179 batch mAP 0.555145264 batch PCKh 0.75\n",
      "Trained batch 2223 batch loss 0.478287816 batch mAP 0.544799805 batch PCKh 0.8125\n",
      "Trained batch 2224 batch loss 0.479565054 batch mAP 0.600982666 batch PCKh 0.6875\n",
      "Trained batch 2225 batch loss 0.542566776 batch mAP 0.645629883 batch PCKh 0.3125\n",
      "Trained batch 2226 batch loss 0.580078065 batch mAP 0.637054443 batch PCKh 0.3125\n",
      "Trained batch 2227 batch loss 0.484573364 batch mAP 0.637420654 batch PCKh 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2228 batch loss 0.484971881 batch mAP 0.634338379 batch PCKh 0.375\n",
      "Trained batch 2229 batch loss 0.49677372 batch mAP 0.670440674 batch PCKh 0.6875\n",
      "Trained batch 2230 batch loss 0.451295793 batch mAP 0.66619873 batch PCKh 0.625\n",
      "Trained batch 2231 batch loss 0.444010317 batch mAP 0.70413208 batch PCKh 0.5625\n",
      "Trained batch 2232 batch loss 0.452102304 batch mAP 0.701080322 batch PCKh 0.625\n",
      "Trained batch 2233 batch loss 0.53781122 batch mAP 0.611175537 batch PCKh 0.125\n",
      "Trained batch 2234 batch loss 0.564634 batch mAP 0.612823486 batch PCKh 0.625\n",
      "Trained batch 2235 batch loss 0.565359652 batch mAP 0.609741211 batch PCKh 0.1875\n",
      "Trained batch 2236 batch loss 0.527678788 batch mAP 0.622009277 batch PCKh 0.875\n",
      "Trained batch 2237 batch loss 0.608613431 batch mAP 0.587982178 batch PCKh 0.25\n",
      "Trained batch 2238 batch loss 0.54302 batch mAP 0.620941162 batch PCKh 0.375\n",
      "Trained batch 2239 batch loss 0.589260459 batch mAP 0.626251221 batch PCKh 0.5\n",
      "Trained batch 2240 batch loss 0.468765497 batch mAP 0.641265869 batch PCKh 0.5625\n",
      "Trained batch 2241 batch loss 0.521962523 batch mAP 0.633483887 batch PCKh 0.6875\n",
      "Trained batch 2242 batch loss 0.535111964 batch mAP 0.627044678 batch PCKh 0.5625\n",
      "Trained batch 2243 batch loss 0.527343571 batch mAP 0.635223389 batch PCKh 0.5\n",
      "Trained batch 2244 batch loss 0.583720267 batch mAP 0.571411133 batch PCKh 0.6875\n",
      "Trained batch 2245 batch loss 0.523033619 batch mAP 0.578094482 batch PCKh 0.625\n",
      "Trained batch 2246 batch loss 0.473875701 batch mAP 0.634155273 batch PCKh 0.375\n",
      "Trained batch 2247 batch loss 0.525756121 batch mAP 0.621765137 batch PCKh 0.4375\n",
      "Trained batch 2248 batch loss 0.484083116 batch mAP 0.685943604 batch PCKh 0.625\n",
      "Trained batch 2249 batch loss 0.447805107 batch mAP 0.703063965 batch PCKh 0.375\n",
      "Trained batch 2250 batch loss 0.490474671 batch mAP 0.660827637 batch PCKh 0.75\n",
      "Trained batch 2251 batch loss 0.505865097 batch mAP 0.612823486 batch PCKh 0.1875\n",
      "Trained batch 2252 batch loss 0.439697385 batch mAP 0.695282 batch PCKh 0.625\n",
      "Trained batch 2253 batch loss 0.370594859 batch mAP 0.762176514 batch PCKh 0.625\n",
      "Trained batch 2254 batch loss 0.509614587 batch mAP 0.589141846 batch PCKh 0.75\n",
      "Trained batch 2255 batch loss 0.535805166 batch mAP 0.613220215 batch PCKh 0.875\n",
      "Trained batch 2256 batch loss 0.519186735 batch mAP 0.610534668 batch PCKh 0.625\n",
      "Trained batch 2257 batch loss 0.48175627 batch mAP 0.63079834 batch PCKh 0.625\n",
      "Trained batch 2258 batch loss 0.468017489 batch mAP 0.69442749 batch PCKh 0.6875\n",
      "Trained batch 2259 batch loss 0.480112135 batch mAP 0.636322 batch PCKh 0.5625\n",
      "Trained batch 2260 batch loss 0.68214643 batch mAP 0.549163818 batch PCKh 0.25\n",
      "Trained batch 2261 batch loss 0.68426013 batch mAP 0.522186279 batch PCKh 0.5625\n",
      "Trained batch 2262 batch loss 0.626221299 batch mAP 0.56262207 batch PCKh 0.25\n",
      "Trained batch 2263 batch loss 0.567934811 batch mAP 0.610534668 batch PCKh 0.1875\n",
      "Trained batch 2264 batch loss 0.525381684 batch mAP 0.619537354 batch PCKh 0.375\n",
      "Trained batch 2265 batch loss 0.520317376 batch mAP 0.670074463 batch PCKh 0.5\n",
      "Trained batch 2266 batch loss 0.582424641 batch mAP 0.59677124 batch PCKh 0.3125\n",
      "Trained batch 2267 batch loss 0.527843595 batch mAP 0.579772949 batch PCKh 0.5\n",
      "Trained batch 2268 batch loss 0.441728354 batch mAP 0.622345 batch PCKh 0.6875\n",
      "Trained batch 2269 batch loss 0.490213662 batch mAP 0.648986816 batch PCKh 0.6875\n",
      "Trained batch 2270 batch loss 0.574379921 batch mAP 0.557220459 batch PCKh 0.625\n",
      "Trained batch 2271 batch loss 0.486153185 batch mAP 0.547149658 batch PCKh 0.375\n",
      "Trained batch 2272 batch loss 0.44763127 batch mAP 0.679931641 batch PCKh 0.5\n",
      "Trained batch 2273 batch loss 0.539011955 batch mAP 0.6043396 batch PCKh 0.5\n",
      "Trained batch 2274 batch loss 0.491614431 batch mAP 0.679290771 batch PCKh 0.5\n",
      "Trained batch 2275 batch loss 0.555885673 batch mAP 0.612670898 batch PCKh 0.375\n",
      "Trained batch 2276 batch loss 0.493053913 batch mAP 0.637268066 batch PCKh 0.6875\n",
      "Trained batch 2277 batch loss 0.505705833 batch mAP 0.667388916 batch PCKh 0.5\n",
      "Trained batch 2278 batch loss 0.505013525 batch mAP 0.604156494 batch PCKh 0.6875\n",
      "Trained batch 2279 batch loss 0.458049983 batch mAP 0.602905273 batch PCKh 0.375\n",
      "Trained batch 2280 batch loss 0.509210527 batch mAP 0.611236572 batch PCKh 0.0625\n",
      "Trained batch 2281 batch loss 0.337898314 batch mAP 0.670196533 batch PCKh 0.1875\n",
      "Trained batch 2282 batch loss 0.430467963 batch mAP 0.681488037 batch PCKh 0.4375\n",
      "Trained batch 2283 batch loss 0.378465593 batch mAP 0.750946045 batch PCKh 0.4375\n",
      "Trained batch 2284 batch loss 0.396043032 batch mAP 0.735870361 batch PCKh 0.5\n",
      "Trained batch 2285 batch loss 0.390382379 batch mAP 0.723175049 batch PCKh 0.375\n",
      "Trained batch 2286 batch loss 0.457076 batch mAP 0.692779541 batch PCKh 0.6875\n",
      "Trained batch 2287 batch loss 0.601977229 batch mAP 0.599395752 batch PCKh 0.4375\n",
      "Trained batch 2288 batch loss 0.6583606 batch mAP 0.54800415 batch PCKh 0.125\n",
      "Trained batch 2289 batch loss 0.60913527 batch mAP 0.569793701 batch PCKh 0.625\n",
      "Trained batch 2290 batch loss 0.601933 batch mAP 0.534057617 batch PCKh 0.5625\n",
      "Trained batch 2291 batch loss 0.628393292 batch mAP 0.581451416 batch PCKh 0.5625\n",
      "Trained batch 2292 batch loss 0.695161581 batch mAP 0.55178833 batch PCKh 0.625\n",
      "Trained batch 2293 batch loss 0.619311869 batch mAP 0.57800293 batch PCKh 0.625\n",
      "Trained batch 2294 batch loss 0.469029069 batch mAP 0.577087402 batch PCKh 0.75\n",
      "Trained batch 2295 batch loss 0.578068852 batch mAP 0.555603 batch PCKh 0.75\n",
      "Trained batch 2296 batch loss 0.586339712 batch mAP 0.55871582 batch PCKh 0.8125\n",
      "Trained batch 2297 batch loss 0.615714073 batch mAP 0.509460449 batch PCKh 0.6875\n",
      "Trained batch 2298 batch loss 0.59907949 batch mAP 0.536499 batch PCKh 0.75\n",
      "Trained batch 2299 batch loss 0.606913507 batch mAP 0.535766602 batch PCKh 0.6875\n",
      "Trained batch 2300 batch loss 0.530670345 batch mAP 0.582397461 batch PCKh 0\n",
      "Trained batch 2301 batch loss 0.525379 batch mAP 0.608764648 batch PCKh 0.4375\n",
      "Trained batch 2302 batch loss 0.513555527 batch mAP 0.67578125 batch PCKh 0.375\n",
      "Trained batch 2303 batch loss 0.472478062 batch mAP 0.634155273 batch PCKh 0.625\n",
      "Trained batch 2304 batch loss 0.602631032 batch mAP 0.57925415 batch PCKh 0.5\n",
      "Trained batch 2305 batch loss 0.57916 batch mAP 0.642334 batch PCKh 0.5\n",
      "Trained batch 2306 batch loss 0.460674405 batch mAP 0.691741943 batch PCKh 0.3125\n",
      "Trained batch 2307 batch loss 0.526990116 batch mAP 0.691223145 batch PCKh 0.625\n",
      "Trained batch 2308 batch loss 0.577760398 batch mAP 0.64944458 batch PCKh 0.4375\n",
      "Trained batch 2309 batch loss 0.553553939 batch mAP 0.611877441 batch PCKh 0.25\n",
      "Trained batch 2310 batch loss 0.557343 batch mAP 0.661712646 batch PCKh 0.375\n",
      "Trained batch 2311 batch loss 0.542576 batch mAP 0.618713379 batch PCKh 0.375\n",
      "Trained batch 2312 batch loss 0.489041269 batch mAP 0.724456787 batch PCKh 0.75\n",
      "Trained batch 2313 batch loss 0.431531668 batch mAP 0.690612793 batch PCKh 0.625\n",
      "Trained batch 2314 batch loss 0.531857073 batch mAP 0.70199585 batch PCKh 0.3125\n",
      "Trained batch 2315 batch loss 0.585728705 batch mAP 0.684326172 batch PCKh 0.375\n",
      "Trained batch 2316 batch loss 0.600000679 batch mAP 0.605957031 batch PCKh 0.625\n",
      "Trained batch 2317 batch loss 0.504448295 batch mAP 0.584960938 batch PCKh 0.625\n",
      "Trained batch 2318 batch loss 0.543504119 batch mAP 0.56918335 batch PCKh 0.875\n",
      "Trained batch 2319 batch loss 0.544726908 batch mAP 0.652252197 batch PCKh 0.6875\n",
      "Trained batch 2320 batch loss 0.554669201 batch mAP 0.576293945 batch PCKh 0.8125\n",
      "Trained batch 2321 batch loss 0.484023184 batch mAP 0.647216797 batch PCKh 0.6875\n",
      "Trained batch 2322 batch loss 0.586859703 batch mAP 0.563995361 batch PCKh 0.125\n",
      "Trained batch 2323 batch loss 0.472944915 batch mAP 0.564788818 batch PCKh 0.5625\n",
      "Trained batch 2324 batch loss 0.4669168 batch mAP 0.532135 batch PCKh 0\n",
      "Trained batch 2325 batch loss 0.519614 batch mAP 0.606048584 batch PCKh 0.625\n",
      "Trained batch 2326 batch loss 0.516749203 batch mAP 0.644561768 batch PCKh 0.6875\n",
      "Trained batch 2327 batch loss 0.537804365 batch mAP 0.640167236 batch PCKh 0.5625\n",
      "Trained batch 2328 batch loss 0.517101586 batch mAP 0.676818848 batch PCKh 0.375\n",
      "Trained batch 2329 batch loss 0.540419877 batch mAP 0.636169434 batch PCKh 0.6875\n",
      "Trained batch 2330 batch loss 0.562490702 batch mAP 0.572235107 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2331 batch loss 0.51170975 batch mAP 0.605072 batch PCKh 0.0625\n",
      "Trained batch 2332 batch loss 0.527656734 batch mAP 0.561767578 batch PCKh 0.1875\n",
      "Trained batch 2333 batch loss 0.637532115 batch mAP 0.596344 batch PCKh 0.5625\n",
      "Trained batch 2334 batch loss 0.61207056 batch mAP 0.603973389 batch PCKh 0.625\n",
      "Trained batch 2335 batch loss 0.63870132 batch mAP 0.561828613 batch PCKh 0.6875\n",
      "Trained batch 2336 batch loss 0.522871494 batch mAP 0.634338379 batch PCKh 0.875\n",
      "Trained batch 2337 batch loss 0.609622 batch mAP 0.587493896 batch PCKh 0.3125\n",
      "Trained batch 2338 batch loss 0.594752371 batch mAP 0.613891602 batch PCKh 0.8125\n",
      "Trained batch 2339 batch loss 0.554329872 batch mAP 0.588562 batch PCKh 0.5625\n",
      "Trained batch 2340 batch loss 0.49645412 batch mAP 0.641937256 batch PCKh 0.5625\n",
      "Trained batch 2341 batch loss 0.539938152 batch mAP 0.610565186 batch PCKh 0.125\n",
      "Trained batch 2342 batch loss 0.501089871 batch mAP 0.66293335 batch PCKh 0.1875\n",
      "Trained batch 2343 batch loss 0.489017725 batch mAP 0.632965088 batch PCKh 0.75\n",
      "Trained batch 2344 batch loss 0.427554965 batch mAP 0.649108887 batch PCKh 0.3125\n",
      "Trained batch 2345 batch loss 0.535405159 batch mAP 0.643310547 batch PCKh 0.5625\n",
      "Trained batch 2346 batch loss 0.479735255 batch mAP 0.635894775 batch PCKh 0.4375\n",
      "Trained batch 2347 batch loss 0.549074054 batch mAP 0.645477295 batch PCKh 0.625\n",
      "Trained batch 2348 batch loss 0.551983476 batch mAP 0.623596191 batch PCKh 0.75\n",
      "Trained batch 2349 batch loss 0.535024285 batch mAP 0.615448 batch PCKh 0.3125\n",
      "Trained batch 2350 batch loss 0.375385195 batch mAP 0.706420898 batch PCKh 0.25\n",
      "Trained batch 2351 batch loss 0.492647141 batch mAP 0.617401123 batch PCKh 0.25\n",
      "Trained batch 2352 batch loss 0.533128738 batch mAP 0.584350586 batch PCKh 0.75\n",
      "Trained batch 2353 batch loss 0.484774172 batch mAP 0.59487915 batch PCKh 0.125\n",
      "Trained batch 2354 batch loss 0.543865561 batch mAP 0.541931152 batch PCKh 0.625\n",
      "Trained batch 2355 batch loss 0.589874208 batch mAP 0.536010742 batch PCKh 0.5\n",
      "Trained batch 2356 batch loss 0.485300422 batch mAP 0.694397 batch PCKh 0.5\n",
      "Trained batch 2357 batch loss 0.463713825 batch mAP 0.60647583 batch PCKh 0.5625\n",
      "Trained batch 2358 batch loss 0.462969482 batch mAP 0.652648926 batch PCKh 0.5625\n",
      "Trained batch 2359 batch loss 0.458900094 batch mAP 0.60534668 batch PCKh 0.1875\n",
      "Trained batch 2360 batch loss 0.539713264 batch mAP 0.564544678 batch PCKh 0.4375\n",
      "Trained batch 2361 batch loss 0.459237874 batch mAP 0.515136719 batch PCKh 0.125\n",
      "Trained batch 2362 batch loss 0.534247756 batch mAP 0.553436279 batch PCKh 0.1875\n",
      "Trained batch 2363 batch loss 0.474967867 batch mAP 0.612518311 batch PCKh 0.5625\n",
      "Trained batch 2364 batch loss 0.466389358 batch mAP 0.622680664 batch PCKh 0.3125\n",
      "Trained batch 2365 batch loss 0.496459663 batch mAP 0.605926514 batch PCKh 0.3125\n",
      "Trained batch 2366 batch loss 0.473362058 batch mAP 0.611541748 batch PCKh 0.25\n",
      "Trained batch 2367 batch loss 0.464707315 batch mAP 0.620758057 batch PCKh 0.5\n",
      "Trained batch 2368 batch loss 0.465906769 batch mAP 0.605896 batch PCKh 0\n",
      "Trained batch 2369 batch loss 0.515412211 batch mAP 0.62512207 batch PCKh 0.3125\n",
      "Trained batch 2370 batch loss 0.5436728 batch mAP 0.625061035 batch PCKh 0.125\n",
      "Trained batch 2371 batch loss 0.499168962 batch mAP 0.639587402 batch PCKh 0.5\n",
      "Trained batch 2372 batch loss 0.530827701 batch mAP 0.652496338 batch PCKh 0.375\n",
      "Trained batch 2373 batch loss 0.583209515 batch mAP 0.623474121 batch PCKh 0.5\n",
      "Trained batch 2374 batch loss 0.541786432 batch mAP 0.666412354 batch PCKh 0.625\n",
      "Trained batch 2375 batch loss 0.504125059 batch mAP 0.672363281 batch PCKh 0.375\n",
      "Trained batch 2376 batch loss 0.523465216 batch mAP 0.658447266 batch PCKh 0.5\n",
      "Trained batch 2377 batch loss 0.609608293 batch mAP 0.643493652 batch PCKh 0.3125\n",
      "Trained batch 2378 batch loss 0.616204619 batch mAP 0.670959473 batch PCKh 0.25\n",
      "Trained batch 2379 batch loss 0.504845202 batch mAP 0.684143066 batch PCKh 0.3125\n",
      "Trained batch 2380 batch loss 0.59208405 batch mAP 0.585113525 batch PCKh 0.3125\n",
      "Trained batch 2381 batch loss 0.518295288 batch mAP 0.599273682 batch PCKh 0.5625\n",
      "Trained batch 2382 batch loss 0.59472847 batch mAP 0.576202393 batch PCKh 0.4375\n",
      "Trained batch 2383 batch loss 0.615969777 batch mAP 0.65234375 batch PCKh 0.375\n",
      "Trained batch 2384 batch loss 0.577544332 batch mAP 0.685668945 batch PCKh 0.75\n",
      "Trained batch 2385 batch loss 0.480790347 batch mAP 0.698516846 batch PCKh 0.4375\n",
      "Trained batch 2386 batch loss 0.614647865 batch mAP 0.590484619 batch PCKh 0.1875\n",
      "Trained batch 2387 batch loss 0.669099927 batch mAP 0.524627686 batch PCKh 0.5625\n",
      "Trained batch 2388 batch loss 0.620210469 batch mAP 0.546051 batch PCKh 0.625\n",
      "Trained batch 2389 batch loss 0.557594895 batch mAP 0.594268799 batch PCKh 0.3125\n",
      "Trained batch 2390 batch loss 0.54640168 batch mAP 0.536682129 batch PCKh 0.5625\n",
      "Trained batch 2391 batch loss 0.485685825 batch mAP 0.601501465 batch PCKh 0.25\n",
      "Trained batch 2392 batch loss 0.466718107 batch mAP 0.594482422 batch PCKh 0.5625\n",
      "Trained batch 2393 batch loss 0.426439673 batch mAP 0.587677 batch PCKh 0\n",
      "Trained batch 2394 batch loss 0.460282087 batch mAP 0.557739258 batch PCKh 0.125\n",
      "Trained batch 2395 batch loss 0.600997806 batch mAP 0.544372559 batch PCKh 0.25\n",
      "Trained batch 2396 batch loss 0.505423546 batch mAP 0.598815918 batch PCKh 0.1875\n",
      "Trained batch 2397 batch loss 0.572840571 batch mAP 0.555450439 batch PCKh 0.25\n",
      "Trained batch 2398 batch loss 0.589663565 batch mAP 0.575805664 batch PCKh 0.625\n",
      "Trained batch 2399 batch loss 0.566850603 batch mAP 0.599090576 batch PCKh 0.1875\n",
      "Trained batch 2400 batch loss 0.594036937 batch mAP 0.617462158 batch PCKh 0.8125\n",
      "Trained batch 2401 batch loss 0.54852432 batch mAP 0.553436279 batch PCKh 0.375\n",
      "Trained batch 2402 batch loss 0.576377332 batch mAP 0.598724365 batch PCKh 0.625\n",
      "Trained batch 2403 batch loss 0.488063514 batch mAP 0.583740234 batch PCKh 0.125\n",
      "Trained batch 2404 batch loss 0.596102476 batch mAP 0.646026611 batch PCKh 0.125\n",
      "Trained batch 2405 batch loss 0.555751741 batch mAP 0.682922363 batch PCKh 0.4375\n",
      "Trained batch 2406 batch loss 0.534405 batch mAP 0.673736572 batch PCKh 0.6875\n",
      "Trained batch 2407 batch loss 0.535944343 batch mAP 0.638000488 batch PCKh 0.3125\n",
      "Trained batch 2408 batch loss 0.562037885 batch mAP 0.631164551 batch PCKh 0.3125\n",
      "Trained batch 2409 batch loss 0.530906141 batch mAP 0.594696045 batch PCKh 0.25\n",
      "Trained batch 2410 batch loss 0.588306189 batch mAP 0.536468506 batch PCKh 0.4375\n",
      "Trained batch 2411 batch loss 0.5607391 batch mAP 0.484161377 batch PCKh 0.5625\n",
      "Trained batch 2412 batch loss 0.510011315 batch mAP 0.533172607 batch PCKh 0.125\n",
      "Trained batch 2413 batch loss 0.520664692 batch mAP 0.585235596 batch PCKh 0.625\n",
      "Trained batch 2414 batch loss 0.515547 batch mAP 0.602050781 batch PCKh 0.375\n",
      "Trained batch 2415 batch loss 0.54941988 batch mAP 0.598175049 batch PCKh 0.4375\n",
      "Trained batch 2416 batch loss 0.475900471 batch mAP 0.591430664 batch PCKh 0.75\n",
      "Trained batch 2417 batch loss 0.571386814 batch mAP 0.578704834 batch PCKh 0.75\n",
      "Trained batch 2418 batch loss 0.591704845 batch mAP 0.581207275 batch PCKh 0.75\n",
      "Trained batch 2419 batch loss 0.551037669 batch mAP 0.599700928 batch PCKh 0.75\n",
      "Trained batch 2420 batch loss 0.59449029 batch mAP 0.591186523 batch PCKh 0.625\n",
      "Trained batch 2421 batch loss 0.565693736 batch mAP 0.611633301 batch PCKh 0.375\n",
      "Trained batch 2422 batch loss 0.510319769 batch mAP 0.633728 batch PCKh 0.6875\n",
      "Trained batch 2423 batch loss 0.527941942 batch mAP 0.64956665 batch PCKh 0.6875\n",
      "Trained batch 2424 batch loss 0.592065632 batch mAP 0.60067749 batch PCKh 0.1875\n",
      "Trained batch 2425 batch loss 0.570377707 batch mAP 0.62878418 batch PCKh 0.5\n",
      "Trained batch 2426 batch loss 0.611305118 batch mAP 0.573730469 batch PCKh 0.1875\n",
      "Trained batch 2427 batch loss 0.564061642 batch mAP 0.682342529 batch PCKh 0.875\n",
      "Trained batch 2428 batch loss 0.57605 batch mAP 0.626190186 batch PCKh 0.5\n",
      "Trained batch 2429 batch loss 0.548532605 batch mAP 0.648712158 batch PCKh 0.5625\n",
      "Trained batch 2430 batch loss 0.555059433 batch mAP 0.629516602 batch PCKh 0.4375\n",
      "Trained batch 2431 batch loss 0.568266213 batch mAP 0.602264404 batch PCKh 0.625\n",
      "Trained batch 2432 batch loss 0.499712139 batch mAP 0.625549316 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2433 batch loss 0.548108578 batch mAP 0.570343 batch PCKh 0.6875\n",
      "Trained batch 2434 batch loss 0.474049389 batch mAP 0.633483887 batch PCKh 0.3125\n",
      "Trained batch 2435 batch loss 0.467472613 batch mAP 0.649475098 batch PCKh 0.625\n",
      "Trained batch 2436 batch loss 0.456643462 batch mAP 0.603790283 batch PCKh 0.5625\n",
      "Trained batch 2437 batch loss 0.470434666 batch mAP 0.613830566 batch PCKh 0.3125\n",
      "Trained batch 2438 batch loss 0.529945552 batch mAP 0.558105469 batch PCKh 0.25\n",
      "Trained batch 2439 batch loss 0.530198753 batch mAP 0.592407227 batch PCKh 0.125\n",
      "Trained batch 2440 batch loss 0.567619503 batch mAP 0.590911865 batch PCKh 0.375\n",
      "Trained batch 2441 batch loss 0.613092601 batch mAP 0.612976074 batch PCKh 0.125\n",
      "Trained batch 2442 batch loss 0.601954222 batch mAP 0.588165283 batch PCKh 0.1875\n",
      "Trained batch 2443 batch loss 0.659574389 batch mAP 0.546234131 batch PCKh 0.0625\n",
      "Trained batch 2444 batch loss 0.476090193 batch mAP 0.606414795 batch PCKh 0.75\n",
      "Trained batch 2445 batch loss 0.690834641 batch mAP 0.480773926 batch PCKh 0.125\n",
      "Trained batch 2446 batch loss 0.505807102 batch mAP 0.578613281 batch PCKh 0.875\n",
      "Trained batch 2447 batch loss 0.559786081 batch mAP 0.530151367 batch PCKh 0.625\n",
      "Trained batch 2448 batch loss 0.530978382 batch mAP 0.4609375 batch PCKh 0.875\n",
      "Trained batch 2449 batch loss 0.532959819 batch mAP 0.506897 batch PCKh 0.75\n",
      "Trained batch 2450 batch loss 0.564784467 batch mAP 0.517334 batch PCKh 0.75\n",
      "Trained batch 2451 batch loss 0.612170696 batch mAP 0.547210693 batch PCKh 0.875\n",
      "Trained batch 2452 batch loss 0.519494772 batch mAP 0.520324707 batch PCKh 0.75\n",
      "Trained batch 2453 batch loss 0.558933258 batch mAP 0.596557617 batch PCKh 0.8125\n",
      "Trained batch 2454 batch loss 0.513459623 batch mAP 0.576812744 batch PCKh 0.875\n",
      "Trained batch 2455 batch loss 0.452942431 batch mAP 0.617553711 batch PCKh 0.75\n",
      "Trained batch 2456 batch loss 0.579728305 batch mAP 0.589416504 batch PCKh 0.8125\n",
      "Trained batch 2457 batch loss 0.540633261 batch mAP 0.650085449 batch PCKh 0.5625\n",
      "Trained batch 2458 batch loss 0.418620378 batch mAP 0.664520264 batch PCKh 0.6875\n",
      "Trained batch 2459 batch loss 0.455438554 batch mAP 0.661895752 batch PCKh 0.4375\n",
      "Trained batch 2460 batch loss 0.445719838 batch mAP 0.67755127 batch PCKh 0.375\n",
      "Trained batch 2461 batch loss 0.565888643 batch mAP 0.679290771 batch PCKh 0.375\n",
      "Trained batch 2462 batch loss 0.546515584 batch mAP 0.593811035 batch PCKh 0.25\n",
      "Trained batch 2463 batch loss 0.528569341 batch mAP 0.599609375 batch PCKh 0.25\n",
      "Trained batch 2464 batch loss 0.508394122 batch mAP 0.652160645 batch PCKh 0.4375\n",
      "Trained batch 2465 batch loss 0.482880592 batch mAP 0.678161621 batch PCKh 0.6875\n",
      "Trained batch 2466 batch loss 0.50421685 batch mAP 0.60748291 batch PCKh 0.6875\n",
      "Trained batch 2467 batch loss 0.470999718 batch mAP 0.583221436 batch PCKh 0.625\n",
      "Trained batch 2468 batch loss 0.498178124 batch mAP 0.593994141 batch PCKh 0.5625\n",
      "Trained batch 2469 batch loss 0.44437 batch mAP 0.595977783 batch PCKh 0.75\n",
      "Trained batch 2470 batch loss 0.496846557 batch mAP 0.611084 batch PCKh 0.625\n",
      "Trained batch 2471 batch loss 0.524940312 batch mAP 0.607727051 batch PCKh 0.875\n",
      "Trained batch 2472 batch loss 0.547231734 batch mAP 0.592010498 batch PCKh 0.75\n",
      "Trained batch 2473 batch loss 0.510219097 batch mAP 0.60849 batch PCKh 0.6875\n",
      "Trained batch 2474 batch loss 0.53768158 batch mAP 0.643951416 batch PCKh 0.625\n",
      "Trained batch 2475 batch loss 0.609897256 batch mAP 0.625885 batch PCKh 0.75\n",
      "Trained batch 2476 batch loss 0.5149647 batch mAP 0.661438 batch PCKh 0.4375\n",
      "Trained batch 2477 batch loss 0.467564315 batch mAP 0.704620361 batch PCKh 0.4375\n",
      "Trained batch 2478 batch loss 0.48372677 batch mAP 0.703308105 batch PCKh 0.5\n",
      "Trained batch 2479 batch loss 0.537228167 batch mAP 0.65222168 batch PCKh 0.5625\n",
      "Trained batch 2480 batch loss 0.496765673 batch mAP 0.671569824 batch PCKh 0.6875\n",
      "Trained batch 2481 batch loss 0.616846919 batch mAP 0.51171875 batch PCKh 0.4375\n",
      "Trained batch 2482 batch loss 0.564317882 batch mAP 0.591827393 batch PCKh 0.6875\n",
      "Trained batch 2483 batch loss 0.508425355 batch mAP 0.595581055 batch PCKh 0.75\n",
      "Trained batch 2484 batch loss 0.54217118 batch mAP 0.616760254 batch PCKh 0.5625\n",
      "Trained batch 2485 batch loss 0.540437043 batch mAP 0.520385742 batch PCKh 0.375\n",
      "Trained batch 2486 batch loss 0.50344491 batch mAP 0.535949707 batch PCKh 0.75\n",
      "Trained batch 2487 batch loss 0.540519476 batch mAP 0.629333496 batch PCKh 0.4375\n",
      "Trained batch 2488 batch loss 0.594482243 batch mAP 0.511901855 batch PCKh 0.8125\n",
      "Trained batch 2489 batch loss 0.544017911 batch mAP 0.572174072 batch PCKh 0.5625\n",
      "Trained batch 2490 batch loss 0.547023237 batch mAP 0.567596436 batch PCKh 0.6875\n",
      "Trained batch 2491 batch loss 0.51350224 batch mAP 0.602935791 batch PCKh 0.6875\n",
      "Trained batch 2492 batch loss 0.526710749 batch mAP 0.541626 batch PCKh 0.75\n",
      "Trained batch 2493 batch loss 0.524187922 batch mAP 0.523284912 batch PCKh 0.375\n",
      "Trained batch 2494 batch loss 0.481206834 batch mAP 0.67565918 batch PCKh 0.75\n",
      "Trained batch 2495 batch loss 0.555617869 batch mAP 0.585296631 batch PCKh 0.625\n",
      "Trained batch 2496 batch loss 0.502114773 batch mAP 0.591644287 batch PCKh 0.5\n",
      "Trained batch 2497 batch loss 0.50772965 batch mAP 0.573761 batch PCKh 0.625\n",
      "Trained batch 2498 batch loss 0.642166376 batch mAP 0.536621094 batch PCKh 0.5625\n",
      "Trained batch 2499 batch loss 0.503451407 batch mAP 0.573425293 batch PCKh 0.75\n",
      "Trained batch 2500 batch loss 0.487215579 batch mAP 0.560394287 batch PCKh 0.75\n",
      "Trained batch 2501 batch loss 0.495921373 batch mAP 0.592071533 batch PCKh 0.5\n",
      "Trained batch 2502 batch loss 0.426810682 batch mAP 0.628387451 batch PCKh 0.6875\n",
      "Trained batch 2503 batch loss 0.477611959 batch mAP 0.619171143 batch PCKh 0.5625\n",
      "Trained batch 2504 batch loss 0.402162731 batch mAP 0.661315918 batch PCKh 0.625\n",
      "Trained batch 2505 batch loss 0.428017348 batch mAP 0.695098877 batch PCKh 0.4375\n",
      "Trained batch 2506 batch loss 0.381527662 batch mAP 0.636749268 batch PCKh 0.75\n",
      "Trained batch 2507 batch loss 0.561569095 batch mAP 0.63974 batch PCKh 0.875\n",
      "Trained batch 2508 batch loss 0.427185237 batch mAP 0.688903809 batch PCKh 0.625\n",
      "Trained batch 2509 batch loss 0.446155071 batch mAP 0.679077148 batch PCKh 0.5\n",
      "Trained batch 2510 batch loss 0.531936347 batch mAP 0.624511719 batch PCKh 0.75\n",
      "Trained batch 2511 batch loss 0.479296923 batch mAP 0.612518311 batch PCKh 0.6875\n",
      "Trained batch 2512 batch loss 0.498877645 batch mAP 0.596099854 batch PCKh 0.375\n",
      "Trained batch 2513 batch loss 0.405070215 batch mAP 0.656982422 batch PCKh 0.5625\n",
      "Trained batch 2514 batch loss 0.387824118 batch mAP 0.632904053 batch PCKh 0.625\n",
      "Trained batch 2515 batch loss 0.472569764 batch mAP 0.653137207 batch PCKh 0.875\n",
      "Trained batch 2516 batch loss 0.419153631 batch mAP 0.663360596 batch PCKh 0.25\n",
      "Trained batch 2517 batch loss 0.461152285 batch mAP 0.743560791 batch PCKh 0.4375\n",
      "Trained batch 2518 batch loss 0.377642184 batch mAP 0.74597168 batch PCKh 0.375\n",
      "Trained batch 2519 batch loss 0.461750448 batch mAP 0.729705811 batch PCKh 0.5\n",
      "Trained batch 2520 batch loss 0.425305814 batch mAP 0.730072 batch PCKh 0.3125\n",
      "Trained batch 2521 batch loss 0.533029199 batch mAP 0.674743652 batch PCKh 0.5\n",
      "Trained batch 2522 batch loss 0.39993161 batch mAP 0.680542 batch PCKh 0.6875\n",
      "Trained batch 2523 batch loss 0.449675649 batch mAP 0.655609131 batch PCKh 0.5625\n",
      "Trained batch 2524 batch loss 0.507017791 batch mAP 0.634918213 batch PCKh 0.5625\n",
      "Trained batch 2525 batch loss 0.510970116 batch mAP 0.635345459 batch PCKh 0.4375\n",
      "Trained batch 2526 batch loss 0.579421 batch mAP 0.567596436 batch PCKh 0.8125\n",
      "Trained batch 2527 batch loss 0.562766373 batch mAP 0.63961792 batch PCKh 0.375\n",
      "Trained batch 2528 batch loss 0.498197466 batch mAP 0.605255127 batch PCKh 0.625\n",
      "Trained batch 2529 batch loss 0.633830547 batch mAP 0.51083374 batch PCKh 0.4375\n",
      "Trained batch 2530 batch loss 0.539715707 batch mAP 0.552276611 batch PCKh 0.625\n",
      "Trained batch 2531 batch loss 0.532686174 batch mAP 0.557037354 batch PCKh 0.75\n",
      "Trained batch 2532 batch loss 0.62675035 batch mAP 0.521392822 batch PCKh 0.4375\n",
      "Trained batch 2533 batch loss 0.562222838 batch mAP 0.530334473 batch PCKh 0\n",
      "Trained batch 2534 batch loss 0.478411376 batch mAP 0.671325684 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2535 batch loss 0.562896967 batch mAP 0.620574951 batch PCKh 0.0625\n",
      "Trained batch 2536 batch loss 0.581308961 batch mAP 0.604675293 batch PCKh 0.375\n",
      "Trained batch 2537 batch loss 0.477229714 batch mAP 0.645019531 batch PCKh 0.4375\n",
      "Trained batch 2538 batch loss 0.539749 batch mAP 0.604370117 batch PCKh 0.375\n",
      "Trained batch 2539 batch loss 0.532672226 batch mAP 0.667358398 batch PCKh 0.5\n",
      "Trained batch 2540 batch loss 0.431520134 batch mAP 0.697265625 batch PCKh 0.5\n",
      "Trained batch 2541 batch loss 0.515677929 batch mAP 0.639556885 batch PCKh 0.5\n",
      "Trained batch 2542 batch loss 0.547606707 batch mAP 0.61819458 batch PCKh 0.5625\n",
      "Trained batch 2543 batch loss 0.606838822 batch mAP 0.587768555 batch PCKh 0.375\n",
      "Trained batch 2544 batch loss 0.560643494 batch mAP 0.551391602 batch PCKh 0.3125\n",
      "Trained batch 2545 batch loss 0.547439516 batch mAP 0.587432861 batch PCKh 0.4375\n",
      "Trained batch 2546 batch loss 0.586080432 batch mAP 0.56628418 batch PCKh 0.4375\n",
      "Trained batch 2547 batch loss 0.588009953 batch mAP 0.565795898 batch PCKh 0.4375\n",
      "Trained batch 2548 batch loss 0.466049284 batch mAP 0.552154541 batch PCKh 0.6875\n",
      "Trained batch 2549 batch loss 0.499148905 batch mAP 0.573699951 batch PCKh 0.3125\n",
      "Trained batch 2550 batch loss 0.513339639 batch mAP 0.538696289 batch PCKh 0.625\n",
      "Trained batch 2551 batch loss 0.621232748 batch mAP 0.571258545 batch PCKh 0.3125\n",
      "Trained batch 2552 batch loss 0.647831738 batch mAP 0.513855 batch PCKh 0.4375\n",
      "Trained batch 2553 batch loss 0.60050118 batch mAP 0.485778809 batch PCKh 0.625\n",
      "Trained batch 2554 batch loss 0.642984 batch mAP 0.455047607 batch PCKh 0.6875\n",
      "Trained batch 2555 batch loss 0.554119825 batch mAP 0.557037354 batch PCKh 0.375\n",
      "Trained batch 2556 batch loss 0.491665393 batch mAP 0.646209717 batch PCKh 0.6875\n",
      "Trained batch 2557 batch loss 0.462282568 batch mAP 0.682128906 batch PCKh 0.375\n",
      "Trained batch 2558 batch loss 0.534525871 batch mAP 0.641235352 batch PCKh 0.5625\n",
      "Trained batch 2559 batch loss 0.537040472 batch mAP 0.548248291 batch PCKh 0.6875\n",
      "Trained batch 2560 batch loss 0.578068316 batch mAP 0.618438721 batch PCKh 0.375\n",
      "Trained batch 2561 batch loss 0.591623068 batch mAP 0.529327393 batch PCKh 0.625\n",
      "Trained batch 2562 batch loss 0.620041549 batch mAP 0.512817383 batch PCKh 0.5\n",
      "Trained batch 2563 batch loss 0.529661238 batch mAP 0.608215332 batch PCKh 0.125\n",
      "Trained batch 2564 batch loss 0.559633672 batch mAP 0.624420166 batch PCKh 0.625\n",
      "Trained batch 2565 batch loss 0.591977596 batch mAP 0.521789551 batch PCKh 0.5625\n",
      "Trained batch 2566 batch loss 0.431971729 batch mAP 0.660125732 batch PCKh 0.75\n",
      "Trained batch 2567 batch loss 0.506940663 batch mAP 0.608154297 batch PCKh 0.75\n",
      "Trained batch 2568 batch loss 0.4271819 batch mAP 0.640869141 batch PCKh 0.1875\n",
      "Trained batch 2569 batch loss 0.506326854 batch mAP 0.571502686 batch PCKh 0.5\n",
      "Trained batch 2570 batch loss 0.462225884 batch mAP 0.533874512 batch PCKh 0.1875\n",
      "Trained batch 2571 batch loss 0.48075819 batch mAP 0.532989502 batch PCKh 0.75\n",
      "Trained batch 2572 batch loss 0.489170671 batch mAP 0.52545166 batch PCKh 0.6875\n",
      "Trained batch 2573 batch loss 0.481114239 batch mAP 0.565368652 batch PCKh 0.0625\n",
      "Trained batch 2574 batch loss 0.524091661 batch mAP 0.541534424 batch PCKh 0.625\n",
      "Trained batch 2575 batch loss 0.578794 batch mAP 0.558746338 batch PCKh 0.625\n",
      "Trained batch 2576 batch loss 0.522351503 batch mAP 0.626586914 batch PCKh 0.375\n",
      "Trained batch 2577 batch loss 0.506287932 batch mAP 0.661071777 batch PCKh 0.875\n",
      "Trained batch 2578 batch loss 0.500205398 batch mAP 0.662506104 batch PCKh 0.4375\n",
      "Trained batch 2579 batch loss 0.543394387 batch mAP 0.615081787 batch PCKh 0.3125\n",
      "Trained batch 2580 batch loss 0.533129513 batch mAP 0.629119873 batch PCKh 0.25\n",
      "Trained batch 2581 batch loss 0.548780918 batch mAP 0.68145752 batch PCKh 0.8125\n",
      "Trained batch 2582 batch loss 0.459747791 batch mAP 0.667663574 batch PCKh 0.375\n",
      "Trained batch 2583 batch loss 0.547416925 batch mAP 0.679290771 batch PCKh 0.6875\n",
      "Trained batch 2584 batch loss 0.444276601 batch mAP 0.671478271 batch PCKh 0.5625\n",
      "Trained batch 2585 batch loss 0.504747093 batch mAP 0.698028564 batch PCKh 0.625\n",
      "Trained batch 2586 batch loss 0.574296117 batch mAP 0.652404785 batch PCKh 0.75\n",
      "Trained batch 2587 batch loss 0.524088 batch mAP 0.601318359 batch PCKh 0.6875\n",
      "Trained batch 2588 batch loss 0.51181674 batch mAP 0.626190186 batch PCKh 0.6875\n",
      "Trained batch 2589 batch loss 0.48931247 batch mAP 0.574310303 batch PCKh 0.75\n",
      "Trained batch 2590 batch loss 0.432125449 batch mAP 0.589599609 batch PCKh 0.75\n",
      "Trained batch 2591 batch loss 0.479013622 batch mAP 0.533630371 batch PCKh 0.5\n",
      "Trained batch 2592 batch loss 0.586196423 batch mAP 0.567138672 batch PCKh 0.75\n",
      "Trained batch 2593 batch loss 0.561298311 batch mAP 0.63293457 batch PCKh 0.3125\n",
      "Trained batch 2594 batch loss 0.519561768 batch mAP 0.634002686 batch PCKh 0.5625\n",
      "Trained batch 2595 batch loss 0.546280384 batch mAP 0.635498047 batch PCKh 0\n",
      "Trained batch 2596 batch loss 0.557511091 batch mAP 0.65335083 batch PCKh 0.3125\n",
      "Trained batch 2597 batch loss 0.525584459 batch mAP 0.64050293 batch PCKh 0.5\n",
      "Trained batch 2598 batch loss 0.514223397 batch mAP 0.702392578 batch PCKh 0.375\n",
      "Trained batch 2599 batch loss 0.482369423 batch mAP 0.669647217 batch PCKh 0.625\n",
      "Trained batch 2600 batch loss 0.462703913 batch mAP 0.67288208 batch PCKh 0.4375\n",
      "Trained batch 2601 batch loss 0.457778394 batch mAP 0.687286377 batch PCKh 0.375\n",
      "Trained batch 2602 batch loss 0.506356657 batch mAP 0.719604492 batch PCKh 0.375\n",
      "Trained batch 2603 batch loss 0.496708661 batch mAP 0.666687 batch PCKh 0.4375\n",
      "Trained batch 2604 batch loss 0.531159639 batch mAP 0.604766846 batch PCKh 0.8125\n",
      "Trained batch 2605 batch loss 0.500011504 batch mAP 0.68081665 batch PCKh 0.8125\n",
      "Trained batch 2606 batch loss 0.520873368 batch mAP 0.620544434 batch PCKh 0.75\n",
      "Trained batch 2607 batch loss 0.569474101 batch mAP 0.643890381 batch PCKh 0.625\n",
      "Trained batch 2608 batch loss 0.505258739 batch mAP 0.619934082 batch PCKh 0.75\n",
      "Trained batch 2609 batch loss 0.48903358 batch mAP 0.634887695 batch PCKh 0.75\n",
      "Trained batch 2610 batch loss 0.510857284 batch mAP 0.668792725 batch PCKh 0.1875\n",
      "Trained batch 2611 batch loss 0.498046875 batch mAP 0.695617676 batch PCKh 0.3125\n",
      "Trained batch 2612 batch loss 0.477834821 batch mAP 0.690246582 batch PCKh 0.4375\n",
      "Trained batch 2613 batch loss 0.442693472 batch mAP 0.714569092 batch PCKh 0.875\n",
      "Trained batch 2614 batch loss 0.43392539 batch mAP 0.664306641 batch PCKh 0.5625\n",
      "Trained batch 2615 batch loss 0.498373657 batch mAP 0.669403076 batch PCKh 0.75\n",
      "Trained batch 2616 batch loss 0.473108202 batch mAP 0.634094238 batch PCKh 0.5625\n",
      "Trained batch 2617 batch loss 0.489898056 batch mAP 0.694152832 batch PCKh 0.625\n",
      "Trained batch 2618 batch loss 0.412887692 batch mAP 0.710601807 batch PCKh 0.75\n",
      "Trained batch 2619 batch loss 0.478936255 batch mAP 0.609771729 batch PCKh 0.25\n",
      "Trained batch 2620 batch loss 0.436914921 batch mAP 0.695251465 batch PCKh 0.25\n",
      "Trained batch 2621 batch loss 0.461647511 batch mAP 0.726043701 batch PCKh 0.625\n",
      "Trained batch 2622 batch loss 0.483353913 batch mAP 0.680114746 batch PCKh 0.625\n",
      "Trained batch 2623 batch loss 0.407806218 batch mAP 0.701507568 batch PCKh 0.25\n",
      "Trained batch 2624 batch loss 0.440579265 batch mAP 0.714202881 batch PCKh 0.75\n",
      "Trained batch 2625 batch loss 0.504602611 batch mAP 0.667999268 batch PCKh 0.5625\n",
      "Trained batch 2626 batch loss 0.429251879 batch mAP 0.683807373 batch PCKh 0.75\n",
      "Trained batch 2627 batch loss 0.416110873 batch mAP 0.731079102 batch PCKh 0.4375\n",
      "Trained batch 2628 batch loss 0.500652075 batch mAP 0.653289795 batch PCKh 0.6875\n",
      "Trained batch 2629 batch loss 0.497199059 batch mAP 0.656494141 batch PCKh 0.25\n",
      "Trained batch 2630 batch loss 0.576678157 batch mAP 0.539367676 batch PCKh 0\n",
      "Trained batch 2631 batch loss 0.612544835 batch mAP 0.610687256 batch PCKh 0\n",
      "Trained batch 2632 batch loss 0.518374622 batch mAP 0.64730835 batch PCKh 0.6875\n",
      "Trained batch 2633 batch loss 0.50773114 batch mAP 0.585876465 batch PCKh 0.1875\n",
      "Trained batch 2634 batch loss 0.576078176 batch mAP 0.553161621 batch PCKh 0.5625\n",
      "Trained batch 2635 batch loss 0.504973173 batch mAP 0.566772461 batch PCKh 0.1875\n",
      "Trained batch 2636 batch loss 0.490618646 batch mAP 0.537658691 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2637 batch loss 0.509882033 batch mAP 0.486877441 batch PCKh 0.25\n",
      "Trained batch 2638 batch loss 0.554356158 batch mAP 0.573303223 batch PCKh 0.25\n",
      "Trained batch 2639 batch loss 0.484916955 batch mAP 0.594665527 batch PCKh 0.5\n",
      "Trained batch 2640 batch loss 0.437483191 batch mAP 0.633453369 batch PCKh 0.6875\n",
      "Trained batch 2641 batch loss 0.491456181 batch mAP 0.661468506 batch PCKh 0.4375\n",
      "Trained batch 2642 batch loss 0.494340092 batch mAP 0.632049561 batch PCKh 0.75\n",
      "Trained batch 2643 batch loss 0.491780877 batch mAP 0.611358643 batch PCKh 0.3125\n",
      "Trained batch 2644 batch loss 0.575812817 batch mAP 0.585510254 batch PCKh 0.1875\n",
      "Trained batch 2645 batch loss 0.507960439 batch mAP 0.59854126 batch PCKh 0.75\n",
      "Trained batch 2646 batch loss 0.55738914 batch mAP 0.599884033 batch PCKh 0.5625\n",
      "Trained batch 2647 batch loss 0.52593267 batch mAP 0.654541 batch PCKh 0.75\n",
      "Trained batch 2648 batch loss 0.477499187 batch mAP 0.63470459 batch PCKh 0.6875\n",
      "Trained batch 2649 batch loss 0.498068273 batch mAP 0.654205322 batch PCKh 0.4375\n",
      "Trained batch 2650 batch loss 0.475987703 batch mAP 0.684539795 batch PCKh 0.375\n",
      "Trained batch 2651 batch loss 0.587948442 batch mAP 0.678009033 batch PCKh 0.625\n",
      "Trained batch 2652 batch loss 0.517662048 batch mAP 0.630371094 batch PCKh 0.3125\n",
      "Trained batch 2653 batch loss 0.539420426 batch mAP 0.645233154 batch PCKh 0.0625\n",
      "Trained batch 2654 batch loss 0.447808355 batch mAP 0.684509277 batch PCKh 0.4375\n",
      "Trained batch 2655 batch loss 0.410409391 batch mAP 0.749603271 batch PCKh 0.3125\n",
      "Trained batch 2656 batch loss 0.558893144 batch mAP 0.634277344 batch PCKh 0.125\n",
      "Trained batch 2657 batch loss 0.468318403 batch mAP 0.679412842 batch PCKh 0.5\n",
      "Trained batch 2658 batch loss 0.460874498 batch mAP 0.659759521 batch PCKh 0.5\n",
      "Trained batch 2659 batch loss 0.423635244 batch mAP 0.689544678 batch PCKh 0.3125\n",
      "Trained batch 2660 batch loss 0.44065541 batch mAP 0.683013916 batch PCKh 0.6875\n",
      "Trained batch 2661 batch loss 0.499584556 batch mAP 0.630218506 batch PCKh 0.75\n",
      "Trained batch 2662 batch loss 0.486818612 batch mAP 0.650787354 batch PCKh 0.75\n",
      "Trained batch 2663 batch loss 0.52436018 batch mAP 0.623138428 batch PCKh 0.75\n",
      "Trained batch 2664 batch loss 0.441768587 batch mAP 0.638549805 batch PCKh 0.6875\n",
      "Trained batch 2665 batch loss 0.551489532 batch mAP 0.580810547 batch PCKh 0.5\n",
      "Trained batch 2666 batch loss 0.484821796 batch mAP 0.684570312 batch PCKh 0.625\n",
      "Trained batch 2667 batch loss 0.514731646 batch mAP 0.666931152 batch PCKh 0.5\n",
      "Trained batch 2668 batch loss 0.546788037 batch mAP 0.675170898 batch PCKh 0.3125\n",
      "Trained batch 2669 batch loss 0.431613922 batch mAP 0.669677734 batch PCKh 0.3125\n",
      "Trained batch 2670 batch loss 0.480840832 batch mAP 0.653320312 batch PCKh 0.25\n",
      "Trained batch 2671 batch loss 0.563371658 batch mAP 0.590942383 batch PCKh 0.125\n",
      "Trained batch 2672 batch loss 0.48541984 batch mAP 0.634124756 batch PCKh 0.4375\n",
      "Trained batch 2673 batch loss 0.529261708 batch mAP 0.630218506 batch PCKh 0.5\n",
      "Trained batch 2674 batch loss 0.534153461 batch mAP 0.65246582 batch PCKh 0.5\n",
      "Trained batch 2675 batch loss 0.432622969 batch mAP 0.711975098 batch PCKh 0.5625\n",
      "Trained batch 2676 batch loss 0.534937143 batch mAP 0.655975342 batch PCKh 0.4375\n",
      "Trained batch 2677 batch loss 0.494104803 batch mAP 0.621246338 batch PCKh 0.625\n",
      "Trained batch 2678 batch loss 0.571638584 batch mAP 0.587341309 batch PCKh 0.5625\n",
      "Trained batch 2679 batch loss 0.608054519 batch mAP 0.607391357 batch PCKh 0.6875\n",
      "Trained batch 2680 batch loss 0.569363117 batch mAP 0.603973389 batch PCKh 0.4375\n",
      "Trained batch 2681 batch loss 0.462986171 batch mAP 0.600372314 batch PCKh 0.1875\n",
      "Trained batch 2682 batch loss 0.499090254 batch mAP 0.646362305 batch PCKh 0.75\n",
      "Trained batch 2683 batch loss 0.555316329 batch mAP 0.570220947 batch PCKh 0.375\n",
      "Trained batch 2684 batch loss 0.572228074 batch mAP 0.601928711 batch PCKh 0.1875\n",
      "Trained batch 2685 batch loss 0.4501127 batch mAP 0.585449219 batch PCKh 0.375\n",
      "Trained batch 2686 batch loss 0.491758764 batch mAP 0.674255371 batch PCKh 0.3125\n",
      "Trained batch 2687 batch loss 0.528522611 batch mAP 0.583496094 batch PCKh 0.625\n",
      "Trained batch 2688 batch loss 0.556189597 batch mAP 0.566772461 batch PCKh 0.3125\n",
      "Trained batch 2689 batch loss 0.579676867 batch mAP 0.561035156 batch PCKh 0.5625\n",
      "Trained batch 2690 batch loss 0.482023507 batch mAP 0.6277771 batch PCKh 0.6875\n",
      "Trained batch 2691 batch loss 0.522267222 batch mAP 0.579742432 batch PCKh 0.375\n",
      "Trained batch 2692 batch loss 0.544952869 batch mAP 0.626556396 batch PCKh 0.625\n",
      "Trained batch 2693 batch loss 0.493498296 batch mAP 0.646942139 batch PCKh 0.4375\n",
      "Trained batch 2694 batch loss 0.566397667 batch mAP 0.633026123 batch PCKh 0.3125\n",
      "Trained batch 2695 batch loss 0.542152584 batch mAP 0.651031494 batch PCKh 0.4375\n",
      "Trained batch 2696 batch loss 0.605409503 batch mAP 0.601379395 batch PCKh 0.375\n",
      "Trained batch 2697 batch loss 0.501556695 batch mAP 0.673034668 batch PCKh 0.4375\n",
      "Trained batch 2698 batch loss 0.530596375 batch mAP 0.640655518 batch PCKh 0.25\n",
      "Trained batch 2699 batch loss 0.620769739 batch mAP 0.541412354 batch PCKh 0.0625\n",
      "Trained batch 2700 batch loss 0.657604933 batch mAP 0.551025391 batch PCKh 0\n",
      "Trained batch 2701 batch loss 0.610358953 batch mAP 0.608398438 batch PCKh 0.5625\n",
      "Trained batch 2702 batch loss 0.539896309 batch mAP 0.642791748 batch PCKh 0.625\n",
      "Trained batch 2703 batch loss 0.499310762 batch mAP 0.639343262 batch PCKh 0.5625\n",
      "Trained batch 2704 batch loss 0.518069148 batch mAP 0.581451416 batch PCKh 0.125\n",
      "Trained batch 2705 batch loss 0.571964622 batch mAP 0.523498535 batch PCKh 0.25\n",
      "Trained batch 2706 batch loss 0.608433247 batch mAP 0.500976562 batch PCKh 0.375\n",
      "Trained batch 2707 batch loss 0.601487696 batch mAP 0.539917 batch PCKh 0.8125\n",
      "Trained batch 2708 batch loss 0.622044325 batch mAP 0.477203369 batch PCKh 0.75\n",
      "Trained batch 2709 batch loss 0.573528588 batch mAP 0.497802734 batch PCKh 0.5\n",
      "Trained batch 2710 batch loss 0.54491061 batch mAP 0.583099365 batch PCKh 0.625\n",
      "Trained batch 2711 batch loss 0.535080314 batch mAP 0.537200928 batch PCKh 0.375\n",
      "Trained batch 2712 batch loss 0.48902449 batch mAP 0.583374 batch PCKh 0.1875\n",
      "Trained batch 2713 batch loss 0.495046973 batch mAP 0.578491211 batch PCKh 0.625\n",
      "Trained batch 2714 batch loss 0.563273728 batch mAP 0.562713623 batch PCKh 0.5625\n",
      "Trained batch 2715 batch loss 0.491962671 batch mAP 0.552459717 batch PCKh 0.5625\n",
      "Trained batch 2716 batch loss 0.513456523 batch mAP 0.632507324 batch PCKh 0.25\n",
      "Trained batch 2717 batch loss 0.502343237 batch mAP 0.650817871 batch PCKh 0.6875\n",
      "Trained batch 2718 batch loss 0.522118032 batch mAP 0.551513672 batch PCKh 0.5\n",
      "Trained batch 2719 batch loss 0.582317591 batch mAP 0.623840332 batch PCKh 0.625\n",
      "Trained batch 2720 batch loss 0.580759406 batch mAP 0.605773926 batch PCKh 0.6875\n",
      "Trained batch 2721 batch loss 0.498993278 batch mAP 0.687469482 batch PCKh 0.4375\n",
      "Trained batch 2722 batch loss 0.578862548 batch mAP 0.570373535 batch PCKh 0.6875\n",
      "Trained batch 2723 batch loss 0.542199 batch mAP 0.557281494 batch PCKh 0.5\n",
      "Trained batch 2724 batch loss 0.536192656 batch mAP 0.664703369 batch PCKh 0.4375\n",
      "Trained batch 2725 batch loss 0.477792203 batch mAP 0.690917969 batch PCKh 0.3125\n",
      "Trained batch 2726 batch loss 0.479897588 batch mAP 0.667724609 batch PCKh 0.5625\n",
      "Trained batch 2727 batch loss 0.572434127 batch mAP 0.697876 batch PCKh 0.4375\n",
      "Trained batch 2728 batch loss 0.513760746 batch mAP 0.704040527 batch PCKh 0.625\n",
      "Trained batch 2729 batch loss 0.489391804 batch mAP 0.683502197 batch PCKh 0.25\n",
      "Trained batch 2730 batch loss 0.559271276 batch mAP 0.642486572 batch PCKh 0.5\n",
      "Trained batch 2731 batch loss 0.563874841 batch mAP 0.570831299 batch PCKh 0.4375\n",
      "Trained batch 2732 batch loss 0.54134655 batch mAP 0.547027588 batch PCKh 0.5625\n",
      "Trained batch 2733 batch loss 0.588959336 batch mAP 0.542205811 batch PCKh 0.5625\n",
      "Trained batch 2734 batch loss 0.494358808 batch mAP 0.516113281 batch PCKh 0.75\n",
      "Trained batch 2735 batch loss 0.564641237 batch mAP 0.565032959 batch PCKh 0.5\n",
      "Trained batch 2736 batch loss 0.494714826 batch mAP 0.659210205 batch PCKh 0.8125\n",
      "Trained batch 2737 batch loss 0.445075512 batch mAP 0.627594 batch PCKh 0.5\n",
      "Trained batch 2738 batch loss 0.579448342 batch mAP 0.637573242 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2739 batch loss 0.461144358 batch mAP 0.617157 batch PCKh 0.5625\n",
      "Trained batch 2740 batch loss 0.493682861 batch mAP 0.648101807 batch PCKh 0.6875\n",
      "Trained batch 2741 batch loss 0.508972 batch mAP 0.610595703 batch PCKh 0.5\n",
      "Trained batch 2742 batch loss 0.48927 batch mAP 0.644195557 batch PCKh 0.6875\n",
      "Trained batch 2743 batch loss 0.521149516 batch mAP 0.666259766 batch PCKh 0.5\n",
      "Trained batch 2744 batch loss 0.427561969 batch mAP 0.609924316 batch PCKh 0.25\n",
      "Trained batch 2745 batch loss 0.554574192 batch mAP 0.58013916 batch PCKh 0.4375\n",
      "Trained batch 2746 batch loss 0.570913315 batch mAP 0.528808594 batch PCKh 0.1875\n",
      "Trained batch 2747 batch loss 0.471017718 batch mAP 0.636749268 batch PCKh 0.25\n",
      "Trained batch 2748 batch loss 0.469582528 batch mAP 0.517028809 batch PCKh 0.25\n",
      "Trained batch 2749 batch loss 0.491928875 batch mAP 0.506073 batch PCKh 0.5625\n",
      "Trained batch 2750 batch loss 0.526905239 batch mAP 0.514312744 batch PCKh 0.3125\n",
      "Trained batch 2751 batch loss 0.398117959 batch mAP 0.534698486 batch PCKh 0.125\n",
      "Trained batch 2752 batch loss 0.299796045 batch mAP 0.625732422 batch PCKh 0\n",
      "Trained batch 2753 batch loss 0.34003973 batch mAP 0.647491455 batch PCKh 0\n",
      "Trained batch 2754 batch loss 0.442877471 batch mAP 0.653961182 batch PCKh 0\n",
      "Trained batch 2755 batch loss 0.469551474 batch mAP 0.640289307 batch PCKh 0.25\n",
      "Trained batch 2756 batch loss 0.595083296 batch mAP 0.605560303 batch PCKh 0.0625\n",
      "Trained batch 2757 batch loss 0.547912478 batch mAP 0.643188477 batch PCKh 0.1875\n",
      "Trained batch 2758 batch loss 0.498116314 batch mAP 0.63470459 batch PCKh 0.4375\n",
      "Trained batch 2759 batch loss 0.509443879 batch mAP 0.591125488 batch PCKh 0.8125\n",
      "Trained batch 2760 batch loss 0.443767786 batch mAP 0.597747803 batch PCKh 0\n",
      "Trained batch 2761 batch loss 0.552647769 batch mAP 0.520904541 batch PCKh 0.0625\n",
      "Trained batch 2762 batch loss 0.504008353 batch mAP 0.514526367 batch PCKh 0.125\n",
      "Trained batch 2763 batch loss 0.493444979 batch mAP 0.582702637 batch PCKh 0.875\n",
      "Trained batch 2764 batch loss 0.49859789 batch mAP 0.625030518 batch PCKh 0.875\n",
      "Trained batch 2765 batch loss 0.485633224 batch mAP 0.625732422 batch PCKh 0.4375\n",
      "Trained batch 2766 batch loss 0.403724313 batch mAP 0.655090332 batch PCKh 0.4375\n",
      "Trained batch 2767 batch loss 0.448662311 batch mAP 0.620941162 batch PCKh 0.6875\n",
      "Trained batch 2768 batch loss 0.441542655 batch mAP 0.592681885 batch PCKh 0.1875\n",
      "Trained batch 2769 batch loss 0.47134316 batch mAP 0.575775146 batch PCKh 0.75\n",
      "Trained batch 2770 batch loss 0.48947534 batch mAP 0.589050293 batch PCKh 0.4375\n",
      "Trained batch 2771 batch loss 0.489975214 batch mAP 0.575134277 batch PCKh 0.4375\n",
      "Trained batch 2772 batch loss 0.541261494 batch mAP 0.501373291 batch PCKh 0\n",
      "Trained batch 2773 batch loss 0.500348091 batch mAP 0.540313721 batch PCKh 0.5\n",
      "Trained batch 2774 batch loss 0.573973894 batch mAP 0.481201172 batch PCKh 0.5\n",
      "Trained batch 2775 batch loss 0.479444027 batch mAP 0.562103271 batch PCKh 0.4375\n",
      "Trained batch 2776 batch loss 0.4901357 batch mAP 0.684295654 batch PCKh 0.6875\n",
      "Epoch 8 train loss 0.518869161605835 train mAP 0.6059398055076599 train PCKh\n",
      "Validated batch 1 batch loss 0.563133955 batch mAP 0.643554688 batch PCKh 0.4375\n",
      "Validated batch 2 batch loss 0.578391492 batch mAP 0.531158447 batch PCKh 0.0625\n",
      "Validated batch 3 batch loss 0.497846514 batch mAP 0.620513916 batch PCKh 0.4375\n",
      "Validated batch 4 batch loss 0.551900625 batch mAP 0.658416748 batch PCKh 0.4375\n",
      "Validated batch 5 batch loss 0.557474613 batch mAP 0.648925781 batch PCKh 0.375\n",
      "Validated batch 6 batch loss 0.630638719 batch mAP 0.62979126 batch PCKh 0.625\n",
      "Validated batch 7 batch loss 0.47881341 batch mAP 0.733612061 batch PCKh 0.4375\n",
      "Validated batch 8 batch loss 0.55602473 batch mAP 0.68270874 batch PCKh 0.6875\n",
      "Validated batch 9 batch loss 0.563646436 batch mAP 0.607910156 batch PCKh 0.75\n",
      "Validated batch 10 batch loss 0.529482901 batch mAP 0.662994385 batch PCKh 0.5\n",
      "Validated batch 11 batch loss 0.579903185 batch mAP 0.638641357 batch PCKh 0.5\n",
      "Validated batch 12 batch loss 0.602831721 batch mAP 0.552429199 batch PCKh 0.3125\n",
      "Validated batch 13 batch loss 0.557887077 batch mAP 0.697570801 batch PCKh 0.75\n",
      "Validated batch 14 batch loss 0.603308618 batch mAP 0.56262207 batch PCKh 0.125\n",
      "Validated batch 15 batch loss 0.578497589 batch mAP 0.589477539 batch PCKh 0.5\n",
      "Validated batch 16 batch loss 0.64560312 batch mAP 0.534240723 batch PCKh 0.625\n",
      "Validated batch 17 batch loss 0.568063319 batch mAP 0.577911377 batch PCKh 0.625\n",
      "Validated batch 18 batch loss 0.619972944 batch mAP 0.656066895 batch PCKh 0.1875\n",
      "Validated batch 19 batch loss 0.561312258 batch mAP 0.604797363 batch PCKh 0.5\n",
      "Validated batch 20 batch loss 0.533478916 batch mAP 0.621459961 batch PCKh 0.6875\n",
      "Validated batch 21 batch loss 0.618861556 batch mAP 0.650085449 batch PCKh 0.4375\n",
      "Validated batch 22 batch loss 0.62367624 batch mAP 0.624786377 batch PCKh 0.75\n",
      "Validated batch 23 batch loss 0.666044056 batch mAP 0.533508301 batch PCKh 0.5625\n",
      "Validated batch 24 batch loss 0.612605393 batch mAP 0.639221191 batch PCKh 0.5625\n",
      "Validated batch 25 batch loss 0.685807765 batch mAP 0.545105 batch PCKh 0.375\n",
      "Validated batch 26 batch loss 0.630098343 batch mAP 0.633575439 batch PCKh 0.4375\n",
      "Validated batch 27 batch loss 0.641348302 batch mAP 0.642944336 batch PCKh 0.3125\n",
      "Validated batch 28 batch loss 0.649525523 batch mAP 0.668792725 batch PCKh 0.5625\n",
      "Validated batch 29 batch loss 0.5922001 batch mAP 0.648529053 batch PCKh 0.8125\n",
      "Validated batch 30 batch loss 0.669943213 batch mAP 0.588439941 batch PCKh 0.75\n",
      "Validated batch 31 batch loss 0.725495696 batch mAP 0.649902344 batch PCKh 0.6875\n",
      "Validated batch 32 batch loss 0.589084148 batch mAP 0.580444336 batch PCKh 0.6875\n",
      "Validated batch 33 batch loss 0.668476 batch mAP 0.627746582 batch PCKh 0.3125\n",
      "Validated batch 34 batch loss 0.601119757 batch mAP 0.684509277 batch PCKh 0.6875\n",
      "Validated batch 35 batch loss 0.556254447 batch mAP 0.676605225 batch PCKh 0.4375\n",
      "Validated batch 36 batch loss 0.542339504 batch mAP 0.667724609 batch PCKh 0.3125\n",
      "Validated batch 37 batch loss 0.576815784 batch mAP 0.685089111 batch PCKh 0.375\n",
      "Validated batch 38 batch loss 0.625102878 batch mAP 0.634613037 batch PCKh 0.3125\n",
      "Validated batch 39 batch loss 0.639318466 batch mAP 0.659790039 batch PCKh 0.4375\n",
      "Validated batch 40 batch loss 0.655244231 batch mAP 0.664245605 batch PCKh 0.3125\n",
      "Validated batch 41 batch loss 0.64921391 batch mAP 0.638946533 batch PCKh 0.75\n",
      "Validated batch 42 batch loss 0.541607201 batch mAP 0.667297363 batch PCKh 0.5\n",
      "Validated batch 43 batch loss 0.568680584 batch mAP 0.636749268 batch PCKh 0.3125\n",
      "Validated batch 44 batch loss 0.647831202 batch mAP 0.638092041 batch PCKh 0.3125\n",
      "Validated batch 45 batch loss 0.581202745 batch mAP 0.577209473 batch PCKh 0.5\n",
      "Validated batch 46 batch loss 0.581461668 batch mAP 0.662719727 batch PCKh 0.5625\n",
      "Validated batch 47 batch loss 0.523778617 batch mAP 0.618682861 batch PCKh 0.4375\n",
      "Validated batch 48 batch loss 0.653127789 batch mAP 0.614074707 batch PCKh 0.5625\n",
      "Validated batch 49 batch loss 0.57589817 batch mAP 0.653198242 batch PCKh 0.4375\n",
      "Validated batch 50 batch loss 0.558150589 batch mAP 0.653076172 batch PCKh 0.625\n",
      "Validated batch 51 batch loss 0.450155675 batch mAP 0.736602783 batch PCKh 0.5\n",
      "Validated batch 52 batch loss 0.668934941 batch mAP 0.552307129 batch PCKh 0.1875\n",
      "Validated batch 53 batch loss 0.513282895 batch mAP 0.640350342 batch PCKh 0.25\n",
      "Validated batch 54 batch loss 0.567701936 batch mAP 0.553710938 batch PCKh 0.5625\n",
      "Validated batch 55 batch loss 0.584437311 batch mAP 0.63571167 batch PCKh 0.625\n",
      "Validated batch 56 batch loss 0.536969781 batch mAP 0.66595459 batch PCKh 0.25\n",
      "Validated batch 57 batch loss 0.621586859 batch mAP 0.60244751 batch PCKh 0.75\n",
      "Validated batch 58 batch loss 0.579839051 batch mAP 0.695617676 batch PCKh 0.625\n",
      "Validated batch 59 batch loss 0.615435839 batch mAP 0.649078369 batch PCKh 0.4375\n",
      "Validated batch 60 batch loss 0.504258037 batch mAP 0.68347168 batch PCKh 0.25\n",
      "Validated batch 61 batch loss 0.573116899 batch mAP 0.586212158 batch PCKh 0.5625\n",
      "Validated batch 62 batch loss 0.588854313 batch mAP 0.658416748 batch PCKh 0.75\n",
      "Validated batch 63 batch loss 0.50506 batch mAP 0.636779785 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 64 batch loss 0.717240632 batch mAP 0.639770508 batch PCKh 0.3125\n",
      "Validated batch 65 batch loss 0.612959623 batch mAP 0.670410156 batch PCKh 0.0625\n",
      "Validated batch 66 batch loss 0.579299 batch mAP 0.674133301 batch PCKh 0.3125\n",
      "Validated batch 67 batch loss 0.563909471 batch mAP 0.680847168 batch PCKh 0.25\n",
      "Validated batch 68 batch loss 0.482337266 batch mAP 0.663116455 batch PCKh 0.5\n",
      "Validated batch 69 batch loss 0.470133483 batch mAP 0.684875488 batch PCKh 0.125\n",
      "Validated batch 70 batch loss 0.518743634 batch mAP 0.607910156 batch PCKh 0.75\n",
      "Validated batch 71 batch loss 0.603775859 batch mAP 0.565826416 batch PCKh 0.3125\n",
      "Validated batch 72 batch loss 0.58956778 batch mAP 0.627471924 batch PCKh 0.4375\n",
      "Validated batch 73 batch loss 0.644834161 batch mAP 0.557342529 batch PCKh 0.625\n",
      "Validated batch 74 batch loss 0.570037365 batch mAP 0.522796631 batch PCKh 0.125\n",
      "Validated batch 75 batch loss 0.698676169 batch mAP 0.505493164 batch PCKh 0.375\n",
      "Validated batch 76 batch loss 0.666244507 batch mAP 0.495788574 batch PCKh 0.4375\n",
      "Validated batch 77 batch loss 0.66667521 batch mAP 0.51663208 batch PCKh 0.4375\n",
      "Validated batch 78 batch loss 0.613015771 batch mAP 0.601318359 batch PCKh 0.625\n",
      "Validated batch 79 batch loss 0.567561626 batch mAP 0.639648438 batch PCKh 0.75\n",
      "Validated batch 80 batch loss 0.630473197 batch mAP 0.682281494 batch PCKh 0.3125\n",
      "Validated batch 81 batch loss 0.725083828 batch mAP 0.638671875 batch PCKh 0.75\n",
      "Validated batch 82 batch loss 0.600883305 batch mAP 0.528167725 batch PCKh 0.25\n",
      "Validated batch 83 batch loss 0.544084668 batch mAP 0.643463135 batch PCKh 0.5\n",
      "Validated batch 84 batch loss 0.603733301 batch mAP 0.723846436 batch PCKh 0.625\n",
      "Validated batch 85 batch loss 0.697974384 batch mAP 0.524047852 batch PCKh 0.5\n",
      "Validated batch 86 batch loss 0.489488155 batch mAP 0.639923096 batch PCKh 0.3125\n",
      "Validated batch 87 batch loss 0.620693564 batch mAP 0.631195068 batch PCKh 0.0625\n",
      "Validated batch 88 batch loss 0.514084 batch mAP 0.625732422 batch PCKh 0.75\n",
      "Validated batch 89 batch loss 0.431034863 batch mAP 0.709991455 batch PCKh 0.1875\n",
      "Validated batch 90 batch loss 0.457570165 batch mAP 0.655944824 batch PCKh 0.5625\n",
      "Validated batch 91 batch loss 0.465503812 batch mAP 0.674621582 batch PCKh 0.5\n",
      "Validated batch 92 batch loss 0.654657841 batch mAP 0.618560791 batch PCKh 0.375\n",
      "Validated batch 93 batch loss 0.545377612 batch mAP 0.67477417 batch PCKh 0.5\n",
      "Validated batch 94 batch loss 0.59098959 batch mAP 0.660644531 batch PCKh 0.875\n",
      "Validated batch 95 batch loss 0.589419961 batch mAP 0.600006104 batch PCKh 0.75\n",
      "Validated batch 96 batch loss 0.571924329 batch mAP 0.653564453 batch PCKh 0.6875\n",
      "Validated batch 97 batch loss 0.596973717 batch mAP 0.63293457 batch PCKh 0.125\n",
      "Validated batch 98 batch loss 0.523794413 batch mAP 0.671081543 batch PCKh 0.375\n",
      "Validated batch 99 batch loss 0.573157072 batch mAP 0.592132568 batch PCKh 0.3125\n",
      "Validated batch 100 batch loss 0.616012871 batch mAP 0.672699 batch PCKh 0.3125\n",
      "Validated batch 101 batch loss 0.541480064 batch mAP 0.670074463 batch PCKh 0.4375\n",
      "Validated batch 102 batch loss 0.632086873 batch mAP 0.655548096 batch PCKh 0.125\n",
      "Validated batch 103 batch loss 0.561608613 batch mAP 0.680084229 batch PCKh 0.8125\n",
      "Validated batch 104 batch loss 0.64986825 batch mAP 0.735870361 batch PCKh 0.875\n",
      "Validated batch 105 batch loss 0.590633214 batch mAP 0.618133545 batch PCKh 0.625\n",
      "Validated batch 106 batch loss 0.656295061 batch mAP 0.612487793 batch PCKh 0.1875\n",
      "Validated batch 107 batch loss 0.618592441 batch mAP 0.56036377 batch PCKh 0.3125\n",
      "Validated batch 108 batch loss 0.588615537 batch mAP 0.628570557 batch PCKh 0.6875\n",
      "Validated batch 109 batch loss 0.654869676 batch mAP 0.564605713 batch PCKh 0.0625\n",
      "Validated batch 110 batch loss 0.535066664 batch mAP 0.673370361 batch PCKh 0.375\n",
      "Validated batch 111 batch loss 0.541672945 batch mAP 0.678375244 batch PCKh 0.8125\n",
      "Validated batch 112 batch loss 0.556314349 batch mAP 0.705627441 batch PCKh 0.6875\n",
      "Validated batch 113 batch loss 0.594483852 batch mAP 0.726776123 batch PCKh 0.625\n",
      "Validated batch 114 batch loss 0.606376231 batch mAP 0.618164062 batch PCKh 0.375\n",
      "Validated batch 115 batch loss 0.582780182 batch mAP 0.657287598 batch PCKh 0.1875\n",
      "Validated batch 116 batch loss 0.567824125 batch mAP 0.562805176 batch PCKh 0.4375\n",
      "Validated batch 117 batch loss 0.551539421 batch mAP 0.577911377 batch PCKh 0.3125\n",
      "Validated batch 118 batch loss 0.617890835 batch mAP 0.643432617 batch PCKh 0.3125\n",
      "Validated batch 119 batch loss 0.62602216 batch mAP 0.599212646 batch PCKh 0.625\n",
      "Validated batch 120 batch loss 0.655361 batch mAP 0.564117432 batch PCKh 0.25\n",
      "Validated batch 121 batch loss 0.687480092 batch mAP 0.578887939 batch PCKh 0\n",
      "Validated batch 122 batch loss 0.619972527 batch mAP 0.6015625 batch PCKh 0.5625\n",
      "Validated batch 123 batch loss 0.560228229 batch mAP 0.627197266 batch PCKh 0.5625\n",
      "Validated batch 124 batch loss 0.595759332 batch mAP 0.63659668 batch PCKh 0.5\n",
      "Validated batch 125 batch loss 0.639092684 batch mAP 0.64553833 batch PCKh 0.5625\n",
      "Validated batch 126 batch loss 0.694941044 batch mAP 0.532653809 batch PCKh 0.0625\n",
      "Validated batch 127 batch loss 0.474078417 batch mAP 0.587768555 batch PCKh 0.5625\n",
      "Validated batch 128 batch loss 0.528875709 batch mAP 0.635345459 batch PCKh 0.125\n",
      "Validated batch 129 batch loss 0.570341885 batch mAP 0.63671875 batch PCKh 0.25\n",
      "Validated batch 130 batch loss 0.657493114 batch mAP 0.575256348 batch PCKh 0.625\n",
      "Validated batch 131 batch loss 0.498542368 batch mAP 0.715148926 batch PCKh 0.5\n",
      "Validated batch 132 batch loss 0.479553044 batch mAP 0.726074219 batch PCKh 0.375\n",
      "Validated batch 133 batch loss 0.474751174 batch mAP 0.641357422 batch PCKh 0.625\n",
      "Validated batch 134 batch loss 0.617439926 batch mAP 0.58392334 batch PCKh 0.625\n",
      "Validated batch 135 batch loss 0.610511959 batch mAP 0.682342529 batch PCKh 0\n",
      "Validated batch 136 batch loss 0.646226466 batch mAP 0.615814209 batch PCKh 0.75\n",
      "Validated batch 137 batch loss 0.470826954 batch mAP 0.653747559 batch PCKh 0.5625\n",
      "Validated batch 138 batch loss 0.53791666 batch mAP 0.674499512 batch PCKh 0.875\n",
      "Validated batch 139 batch loss 0.558155298 batch mAP 0.641693115 batch PCKh 0.8125\n",
      "Validated batch 140 batch loss 0.598966241 batch mAP 0.581390381 batch PCKh 0.3125\n",
      "Validated batch 141 batch loss 0.567762733 batch mAP 0.633056641 batch PCKh 0.6875\n",
      "Validated batch 142 batch loss 0.582288682 batch mAP 0.643829346 batch PCKh 0.625\n",
      "Validated batch 143 batch loss 0.555182755 batch mAP 0.641387939 batch PCKh 0.5\n",
      "Validated batch 144 batch loss 0.602492 batch mAP 0.617980957 batch PCKh 0.5625\n",
      "Validated batch 145 batch loss 0.51881516 batch mAP 0.665496826 batch PCKh 0.625\n",
      "Validated batch 146 batch loss 0.596508503 batch mAP 0.6199646 batch PCKh 0.625\n",
      "Validated batch 147 batch loss 0.632704437 batch mAP 0.584533691 batch PCKh 0.1875\n",
      "Validated batch 148 batch loss 0.583802164 batch mAP 0.572876 batch PCKh 0.5625\n",
      "Validated batch 149 batch loss 0.748076379 batch mAP 0.538818359 batch PCKh 0.5625\n",
      "Validated batch 150 batch loss 0.632846355 batch mAP 0.610290527 batch PCKh 0.375\n",
      "Validated batch 151 batch loss 0.529118419 batch mAP 0.692321777 batch PCKh 0.375\n",
      "Validated batch 152 batch loss 0.610552251 batch mAP 0.718444824 batch PCKh 0.375\n",
      "Validated batch 153 batch loss 0.586581588 batch mAP 0.635040283 batch PCKh 0.8125\n",
      "Validated batch 154 batch loss 0.621376395 batch mAP 0.641540527 batch PCKh 0.75\n",
      "Validated batch 155 batch loss 0.541293621 batch mAP 0.680664062 batch PCKh 0.3125\n",
      "Validated batch 156 batch loss 0.54893589 batch mAP 0.656799316 batch PCKh 0.625\n",
      "Validated batch 157 batch loss 0.626288652 batch mAP 0.601593 batch PCKh 0.1875\n",
      "Validated batch 158 batch loss 0.605950832 batch mAP 0.663818359 batch PCKh 0.75\n",
      "Validated batch 159 batch loss 0.59119314 batch mAP 0.721496582 batch PCKh 0.6875\n",
      "Validated batch 160 batch loss 0.595891893 batch mAP 0.644866943 batch PCKh 0.875\n",
      "Validated batch 161 batch loss 0.581372142 batch mAP 0.611114502 batch PCKh 0.6875\n",
      "Validated batch 162 batch loss 0.520472288 batch mAP 0.724517822 batch PCKh 0.5625\n",
      "Validated batch 163 batch loss 0.62992847 batch mAP 0.659484863 batch PCKh 0.75\n",
      "Validated batch 164 batch loss 0.716643155 batch mAP 0.639556885 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 165 batch loss 0.711765409 batch mAP 0.6925354 batch PCKh 0.0625\n",
      "Validated batch 166 batch loss 0.505743921 batch mAP 0.678772 batch PCKh 0.625\n",
      "Validated batch 167 batch loss 0.594458878 batch mAP 0.626586914 batch PCKh 0.3125\n",
      "Validated batch 168 batch loss 0.636062741 batch mAP 0.659393311 batch PCKh 0.4375\n",
      "Validated batch 169 batch loss 0.57857573 batch mAP 0.704620361 batch PCKh 0\n",
      "Validated batch 170 batch loss 0.658628643 batch mAP 0.730560303 batch PCKh 0.6875\n",
      "Validated batch 171 batch loss 0.596764505 batch mAP 0.672668457 batch PCKh 0.8125\n",
      "Validated batch 172 batch loss 0.635414481 batch mAP 0.554199219 batch PCKh 0.4375\n",
      "Validated batch 173 batch loss 0.615969539 batch mAP 0.626251221 batch PCKh 0.75\n",
      "Validated batch 174 batch loss 0.407740235 batch mAP 0.649292 batch PCKh 0.5625\n",
      "Validated batch 175 batch loss 0.609496593 batch mAP 0.656707764 batch PCKh 0.75\n",
      "Validated batch 176 batch loss 0.58851248 batch mAP 0.507659912 batch PCKh 0.6875\n",
      "Validated batch 177 batch loss 0.594570398 batch mAP 0.723510742 batch PCKh 0.5625\n",
      "Validated batch 178 batch loss 0.538912594 batch mAP 0.668212891 batch PCKh 0.4375\n",
      "Validated batch 179 batch loss 0.595308661 batch mAP 0.677215576 batch PCKh 0.625\n",
      "Validated batch 180 batch loss 0.593184769 batch mAP 0.668731689 batch PCKh 0.5625\n",
      "Validated batch 181 batch loss 0.607751787 batch mAP 0.671539307 batch PCKh 0.625\n",
      "Validated batch 182 batch loss 0.580687523 batch mAP 0.650970459 batch PCKh 0.25\n",
      "Validated batch 183 batch loss 0.564378679 batch mAP 0.65335083 batch PCKh 0.5625\n",
      "Validated batch 184 batch loss 0.573190451 batch mAP 0.612579346 batch PCKh 0.4375\n",
      "Validated batch 185 batch loss 0.6858899 batch mAP 0.598754883 batch PCKh 0.6875\n",
      "Validated batch 186 batch loss 0.466437101 batch mAP 0.660339355 batch PCKh 0.625\n",
      "Validated batch 187 batch loss 0.512007177 batch mAP 0.642425537 batch PCKh 0.75\n",
      "Validated batch 188 batch loss 0.552679896 batch mAP 0.650054932 batch PCKh 0.8125\n",
      "Validated batch 189 batch loss 0.629311085 batch mAP 0.617431641 batch PCKh 0.375\n",
      "Validated batch 190 batch loss 0.571713567 batch mAP 0.586853 batch PCKh 0.75\n",
      "Validated batch 191 batch loss 0.612170637 batch mAP 0.68371582 batch PCKh 0.0625\n",
      "Validated batch 192 batch loss 0.512754858 batch mAP 0.716918945 batch PCKh 0.4375\n",
      "Validated batch 193 batch loss 0.578369141 batch mAP 0.665405273 batch PCKh 0.625\n",
      "Validated batch 194 batch loss 0.664726 batch mAP 0.62979126 batch PCKh 0.3125\n",
      "Validated batch 195 batch loss 0.718366504 batch mAP 0.566436768 batch PCKh 0.25\n",
      "Validated batch 196 batch loss 0.654935 batch mAP 0.639709473 batch PCKh 0.75\n",
      "Validated batch 197 batch loss 0.587058 batch mAP 0.683563232 batch PCKh 0.8125\n",
      "Validated batch 198 batch loss 0.526316524 batch mAP 0.686248779 batch PCKh 0.875\n",
      "Validated batch 199 batch loss 0.574340224 batch mAP 0.703582764 batch PCKh 0.125\n",
      "Validated batch 200 batch loss 0.574165702 batch mAP 0.705474854 batch PCKh 0.5\n",
      "Validated batch 201 batch loss 0.606040716 batch mAP 0.607635498 batch PCKh 0.3125\n",
      "Validated batch 202 batch loss 0.482697845 batch mAP 0.643890381 batch PCKh 0.25\n",
      "Validated batch 203 batch loss 0.603012264 batch mAP 0.56073 batch PCKh 0.75\n",
      "Validated batch 204 batch loss 0.604825616 batch mAP 0.570526123 batch PCKh 0.5625\n",
      "Validated batch 205 batch loss 0.622326493 batch mAP 0.669586182 batch PCKh 0.1875\n",
      "Validated batch 206 batch loss 0.571243644 batch mAP 0.665679932 batch PCKh 0.5625\n",
      "Validated batch 207 batch loss 0.497026503 batch mAP 0.692871094 batch PCKh 0.5625\n",
      "Validated batch 208 batch loss 0.507785261 batch mAP 0.576416 batch PCKh 0.75\n",
      "Validated batch 209 batch loss 0.408573568 batch mAP 0.67401123 batch PCKh 0.5625\n",
      "Validated batch 210 batch loss 0.559236407 batch mAP 0.690429688 batch PCKh 0.5\n",
      "Validated batch 211 batch loss 0.56726712 batch mAP 0.697052 batch PCKh 0.875\n",
      "Validated batch 212 batch loss 0.543314695 batch mAP 0.686798096 batch PCKh 0.625\n",
      "Validated batch 213 batch loss 0.546264 batch mAP 0.698852539 batch PCKh 0.5\n",
      "Validated batch 214 batch loss 0.538792312 batch mAP 0.721893311 batch PCKh 0.5625\n",
      "Validated batch 215 batch loss 0.564815 batch mAP 0.655883789 batch PCKh 0.6875\n",
      "Validated batch 216 batch loss 0.56545645 batch mAP 0.612243652 batch PCKh 0.5625\n",
      "Validated batch 217 batch loss 0.632582068 batch mAP 0.636810303 batch PCKh 0.125\n",
      "Validated batch 218 batch loss 0.558960497 batch mAP 0.698486328 batch PCKh 0.875\n",
      "Validated batch 219 batch loss 0.48634094 batch mAP 0.736480713 batch PCKh 0.375\n",
      "Validated batch 220 batch loss 0.537691891 batch mAP 0.705841064 batch PCKh 0.8125\n",
      "Validated batch 221 batch loss 0.624364495 batch mAP 0.661010742 batch PCKh 0.125\n",
      "Validated batch 222 batch loss 0.558850884 batch mAP 0.682067871 batch PCKh 0.25\n",
      "Validated batch 223 batch loss 0.645713329 batch mAP 0.593322754 batch PCKh 0.0625\n",
      "Validated batch 224 batch loss 0.630048871 batch mAP 0.664276123 batch PCKh 0.3125\n",
      "Validated batch 225 batch loss 0.610874534 batch mAP 0.658935547 batch PCKh 0.375\n",
      "Validated batch 226 batch loss 0.594332814 batch mAP 0.750762939 batch PCKh 0.6875\n",
      "Validated batch 227 batch loss 0.641254961 batch mAP 0.714355469 batch PCKh 0.5\n",
      "Validated batch 228 batch loss 0.573973298 batch mAP 0.636566162 batch PCKh 0.625\n",
      "Validated batch 229 batch loss 0.519098163 batch mAP 0.613616943 batch PCKh 0.75\n",
      "Validated batch 230 batch loss 0.499134481 batch mAP 0.635284424 batch PCKh 0.25\n",
      "Validated batch 231 batch loss 0.605082273 batch mAP 0.705688477 batch PCKh 0.6875\n",
      "Validated batch 232 batch loss 0.581057429 batch mAP 0.621521 batch PCKh 0.4375\n",
      "Validated batch 233 batch loss 0.524063587 batch mAP 0.565643311 batch PCKh 0.3125\n",
      "Validated batch 234 batch loss 0.49416098 batch mAP 0.678863525 batch PCKh 0.25\n",
      "Validated batch 235 batch loss 0.554450512 batch mAP 0.609405518 batch PCKh 0.625\n",
      "Validated batch 236 batch loss 0.566357911 batch mAP 0.493835449 batch PCKh 0.375\n",
      "Validated batch 237 batch loss 0.555321693 batch mAP 0.645813 batch PCKh 0.1875\n",
      "Validated batch 238 batch loss 0.527697384 batch mAP 0.613708496 batch PCKh 0.75\n",
      "Validated batch 239 batch loss 0.514362752 batch mAP 0.626800537 batch PCKh 0.75\n",
      "Validated batch 240 batch loss 0.527946889 batch mAP 0.637756348 batch PCKh 0.75\n",
      "Validated batch 241 batch loss 0.590608239 batch mAP 0.607696533 batch PCKh 0.5625\n",
      "Validated batch 242 batch loss 0.583277941 batch mAP 0.58480835 batch PCKh 0.375\n",
      "Validated batch 243 batch loss 0.595174074 batch mAP 0.633331299 batch PCKh 0.6875\n",
      "Validated batch 244 batch loss 0.526616573 batch mAP 0.597747803 batch PCKh 0.4375\n",
      "Validated batch 245 batch loss 0.614568293 batch mAP 0.567169189 batch PCKh 0.0625\n",
      "Validated batch 246 batch loss 0.579186 batch mAP 0.688262939 batch PCKh 0.75\n",
      "Validated batch 247 batch loss 0.66228205 batch mAP 0.580627441 batch PCKh 0.625\n",
      "Validated batch 248 batch loss 0.542695761 batch mAP 0.614776611 batch PCKh 0.625\n",
      "Validated batch 249 batch loss 0.553857207 batch mAP 0.738494873 batch PCKh 0.3125\n",
      "Validated batch 250 batch loss 0.471990108 batch mAP 0.712188721 batch PCKh 0.75\n",
      "Validated batch 251 batch loss 0.708588481 batch mAP 0.599334717 batch PCKh 0\n",
      "Validated batch 252 batch loss 0.491093934 batch mAP 0.673828125 batch PCKh 0.8125\n",
      "Validated batch 253 batch loss 0.669249058 batch mAP 0.605621338 batch PCKh 0.1875\n",
      "Validated batch 254 batch loss 0.566256523 batch mAP 0.655334473 batch PCKh 0.5625\n",
      "Validated batch 255 batch loss 0.48164165 batch mAP 0.676361084 batch PCKh 0.1875\n",
      "Validated batch 256 batch loss 0.572147369 batch mAP 0.658721924 batch PCKh 0.25\n",
      "Validated batch 257 batch loss 0.567760229 batch mAP 0.620880127 batch PCKh 0.375\n",
      "Validated batch 258 batch loss 0.553080738 batch mAP 0.650512695 batch PCKh 0.75\n",
      "Validated batch 259 batch loss 0.595420361 batch mAP 0.606750488 batch PCKh 0.5625\n",
      "Validated batch 260 batch loss 0.513574719 batch mAP 0.657653809 batch PCKh 0.4375\n",
      "Validated batch 261 batch loss 0.536741614 batch mAP 0.687438965 batch PCKh 0.3125\n",
      "Validated batch 262 batch loss 0.543119192 batch mAP 0.629394531 batch PCKh 0.4375\n",
      "Validated batch 263 batch loss 0.613259315 batch mAP 0.612457275 batch PCKh 0.25\n",
      "Validated batch 264 batch loss 0.67534703 batch mAP 0.621887207 batch PCKh 0.3125\n",
      "Validated batch 265 batch loss 0.561256468 batch mAP 0.600311279 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 266 batch loss 0.652466416 batch mAP 0.621582031 batch PCKh 0.5625\n",
      "Validated batch 267 batch loss 0.644796848 batch mAP 0.627227783 batch PCKh 0.375\n",
      "Validated batch 268 batch loss 0.50202024 batch mAP 0.707336426 batch PCKh 0.625\n",
      "Validated batch 269 batch loss 0.483715177 batch mAP 0.661224365 batch PCKh 0.625\n",
      "Validated batch 270 batch loss 0.625978827 batch mAP 0.458465576 batch PCKh 0.5\n",
      "Validated batch 271 batch loss 0.578053176 batch mAP 0.583862305 batch PCKh 0.4375\n",
      "Validated batch 272 batch loss 0.543221354 batch mAP 0.656555176 batch PCKh 0.1875\n",
      "Validated batch 273 batch loss 0.612016082 batch mAP 0.601409912 batch PCKh 0.125\n",
      "Validated batch 274 batch loss 0.500987351 batch mAP 0.601776123 batch PCKh 0.625\n",
      "Validated batch 275 batch loss 0.459425718 batch mAP 0.680908203 batch PCKh 0.125\n",
      "Validated batch 276 batch loss 0.614188492 batch mAP 0.646514893 batch PCKh 0.5625\n",
      "Validated batch 277 batch loss 0.611848116 batch mAP 0.665557861 batch PCKh 0.625\n",
      "Validated batch 278 batch loss 0.62099278 batch mAP 0.596679688 batch PCKh 0.0625\n",
      "Validated batch 279 batch loss 0.587884486 batch mAP 0.624694824 batch PCKh 0.5\n",
      "Validated batch 280 batch loss 0.544469953 batch mAP 0.690124512 batch PCKh 0.5625\n",
      "Validated batch 281 batch loss 0.614625216 batch mAP 0.689086914 batch PCKh 0.5\n",
      "Validated batch 282 batch loss 0.542203367 batch mAP 0.657928467 batch PCKh 0.75\n",
      "Validated batch 283 batch loss 0.570660949 batch mAP 0.702270508 batch PCKh 0.5\n",
      "Validated batch 284 batch loss 0.601815701 batch mAP 0.720977783 batch PCKh 0.5625\n",
      "Validated batch 285 batch loss 0.582520604 batch mAP 0.696380615 batch PCKh 0.875\n",
      "Validated batch 286 batch loss 0.49004668 batch mAP 0.704772949 batch PCKh 0.625\n",
      "Validated batch 287 batch loss 0.608505368 batch mAP 0.64944458 batch PCKh 0.4375\n",
      "Validated batch 288 batch loss 0.593493402 batch mAP 0.675048828 batch PCKh 0.625\n",
      "Validated batch 289 batch loss 0.608807921 batch mAP 0.660339355 batch PCKh 0.5\n",
      "Validated batch 290 batch loss 0.668380857 batch mAP 0.613952637 batch PCKh 0.6875\n",
      "Validated batch 291 batch loss 0.594352841 batch mAP 0.697631836 batch PCKh 0.1875\n",
      "Validated batch 292 batch loss 0.518968403 batch mAP 0.621002197 batch PCKh 0.5625\n",
      "Validated batch 293 batch loss 0.54157728 batch mAP 0.655883789 batch PCKh 0.625\n",
      "Validated batch 294 batch loss 0.640384197 batch mAP 0.742004395 batch PCKh 0.375\n",
      "Validated batch 295 batch loss 0.619572878 batch mAP 0.666229248 batch PCKh 0.6875\n",
      "Validated batch 296 batch loss 0.621154606 batch mAP 0.662353516 batch PCKh 0.4375\n",
      "Validated batch 297 batch loss 0.603245497 batch mAP 0.709289551 batch PCKh 0.375\n",
      "Validated batch 298 batch loss 0.623786569 batch mAP 0.666442871 batch PCKh 0.25\n",
      "Validated batch 299 batch loss 0.658884048 batch mAP 0.572814941 batch PCKh 0.5\n",
      "Validated batch 300 batch loss 0.587441325 batch mAP 0.640594482 batch PCKh 0.625\n",
      "Validated batch 301 batch loss 0.575944245 batch mAP 0.604736328 batch PCKh 0.25\n",
      "Validated batch 302 batch loss 0.604149938 batch mAP 0.548095703 batch PCKh 0.5\n",
      "Validated batch 303 batch loss 0.491993 batch mAP 0.621459961 batch PCKh 0.5\n",
      "Validated batch 304 batch loss 0.537587702 batch mAP 0.634521484 batch PCKh 0.6875\n",
      "Validated batch 305 batch loss 0.618494034 batch mAP 0.588745117 batch PCKh 0.0625\n",
      "Validated batch 306 batch loss 0.47058928 batch mAP 0.712280273 batch PCKh 0.4375\n",
      "Validated batch 307 batch loss 0.590105891 batch mAP 0.586303711 batch PCKh 0.625\n",
      "Validated batch 308 batch loss 0.533586144 batch mAP 0.660339355 batch PCKh 0.6875\n",
      "Validated batch 309 batch loss 0.50915432 batch mAP 0.678466797 batch PCKh 0.25\n",
      "Validated batch 310 batch loss 0.614161789 batch mAP 0.659637451 batch PCKh 0.4375\n",
      "Validated batch 311 batch loss 0.542883277 batch mAP 0.567565918 batch PCKh 0.125\n",
      "Validated batch 312 batch loss 0.640179694 batch mAP 0.591430664 batch PCKh 0.5625\n",
      "Validated batch 313 batch loss 0.568100095 batch mAP 0.695800781 batch PCKh 0.4375\n",
      "Validated batch 314 batch loss 0.587169826 batch mAP 0.645721436 batch PCKh 0.375\n",
      "Validated batch 315 batch loss 0.661202073 batch mAP 0.634490967 batch PCKh 0.1875\n",
      "Validated batch 316 batch loss 0.693628311 batch mAP 0.537719727 batch PCKh 0.1875\n",
      "Validated batch 317 batch loss 0.70653522 batch mAP 0.575744629 batch PCKh 0.1875\n",
      "Validated batch 318 batch loss 0.603764713 batch mAP 0.638336182 batch PCKh 0.4375\n",
      "Validated batch 319 batch loss 0.560467839 batch mAP 0.70022583 batch PCKh 0.625\n",
      "Validated batch 320 batch loss 0.609969497 batch mAP 0.590698242 batch PCKh 0.625\n",
      "Validated batch 321 batch loss 0.536845624 batch mAP 0.560058594 batch PCKh 0.3125\n",
      "Validated batch 322 batch loss 0.53193146 batch mAP 0.706420898 batch PCKh 0.625\n",
      "Validated batch 323 batch loss 0.522233903 batch mAP 0.699493408 batch PCKh 0.5625\n",
      "Validated batch 324 batch loss 0.505747437 batch mAP 0.630096436 batch PCKh 0.625\n",
      "Validated batch 325 batch loss 0.593749285 batch mAP 0.666473389 batch PCKh 0.125\n",
      "Validated batch 326 batch loss 0.560154438 batch mAP 0.543457031 batch PCKh 0.5\n",
      "Validated batch 327 batch loss 0.537449419 batch mAP 0.603515625 batch PCKh 0.4375\n",
      "Validated batch 328 batch loss 0.578403831 batch mAP 0.576477051 batch PCKh 0.125\n",
      "Validated batch 329 batch loss 0.426835805 batch mAP 0.704040527 batch PCKh 0.25\n",
      "Validated batch 330 batch loss 0.573473334 batch mAP 0.603118896 batch PCKh 0.125\n",
      "Validated batch 331 batch loss 0.573104 batch mAP 0.709838867 batch PCKh 0.75\n",
      "Validated batch 332 batch loss 0.537398 batch mAP 0.699401855 batch PCKh 0.375\n",
      "Validated batch 333 batch loss 0.550168037 batch mAP 0.642272949 batch PCKh 0.4375\n",
      "Validated batch 334 batch loss 0.73547554 batch mAP 0.584259033 batch PCKh 0\n",
      "Validated batch 335 batch loss 0.516113222 batch mAP 0.653015137 batch PCKh 0.5625\n",
      "Validated batch 336 batch loss 0.499896437 batch mAP 0.669586182 batch PCKh 0.4375\n",
      "Validated batch 337 batch loss 0.589240074 batch mAP 0.607696533 batch PCKh 0.75\n",
      "Validated batch 338 batch loss 0.392669111 batch mAP 0.685180664 batch PCKh 0.4375\n",
      "Validated batch 339 batch loss 0.466510415 batch mAP 0.739471436 batch PCKh 0.5\n",
      "Validated batch 340 batch loss 0.542669356 batch mAP 0.66784668 batch PCKh 0.6875\n",
      "Validated batch 341 batch loss 0.500416398 batch mAP 0.714263916 batch PCKh 0.4375\n",
      "Validated batch 342 batch loss 0.633373201 batch mAP 0.59664917 batch PCKh 0.1875\n",
      "Validated batch 343 batch loss 0.478001386 batch mAP 0.710998535 batch PCKh 0.5\n",
      "Validated batch 344 batch loss 0.500524 batch mAP 0.7003479 batch PCKh 0.5625\n",
      "Validated batch 345 batch loss 0.553137362 batch mAP 0.612640381 batch PCKh 0.125\n",
      "Validated batch 346 batch loss 0.584851921 batch mAP 0.622406 batch PCKh 0.125\n",
      "Validated batch 347 batch loss 0.461077303 batch mAP 0.574859619 batch PCKh 0.125\n",
      "Validated batch 348 batch loss 0.438306272 batch mAP 0.708374 batch PCKh 0.5625\n",
      "Validated batch 349 batch loss 0.642980576 batch mAP 0.585845947 batch PCKh 0.5625\n",
      "Validated batch 350 batch loss 0.482676387 batch mAP 0.69317627 batch PCKh 0.3125\n",
      "Validated batch 351 batch loss 0.642000914 batch mAP 0.668609619 batch PCKh 0.875\n",
      "Validated batch 352 batch loss 0.657705367 batch mAP 0.619873047 batch PCKh 0.625\n",
      "Validated batch 353 batch loss 0.597962856 batch mAP 0.591491699 batch PCKh 0.25\n",
      "Validated batch 354 batch loss 0.640263319 batch mAP 0.623840332 batch PCKh 0.3125\n",
      "Validated batch 355 batch loss 0.510934651 batch mAP 0.676727295 batch PCKh 0.5625\n",
      "Validated batch 356 batch loss 0.548594534 batch mAP 0.627563477 batch PCKh 0.75\n",
      "Validated batch 357 batch loss 0.575421333 batch mAP 0.559875488 batch PCKh 0.3125\n",
      "Validated batch 358 batch loss 0.667877197 batch mAP 0.580474854 batch PCKh 0\n",
      "Validated batch 359 batch loss 0.510748863 batch mAP 0.717956543 batch PCKh 0.375\n",
      "Validated batch 360 batch loss 0.437561244 batch mAP 0.730041504 batch PCKh 0.4375\n",
      "Validated batch 361 batch loss 0.46866253 batch mAP 0.559417725 batch PCKh 0\n",
      "Validated batch 362 batch loss 0.47829181 batch mAP 0.711273193 batch PCKh 0.4375\n",
      "Validated batch 363 batch loss 0.643990159 batch mAP 0.547088623 batch PCKh 0.5\n",
      "Validated batch 364 batch loss 0.436377406 batch mAP 0.726409912 batch PCKh 0.8125\n",
      "Validated batch 365 batch loss 0.438980252 batch mAP 0.716186523 batch PCKh 0.625\n",
      "Validated batch 366 batch loss 0.533657 batch mAP 0.6668396 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 367 batch loss 0.628223658 batch mAP 0.535949707 batch PCKh 0.625\n",
      "Validated batch 368 batch loss 0.637196898 batch mAP 0.608917236 batch PCKh 0.625\n",
      "Validated batch 369 batch loss 0.624112427 batch mAP 0.544403076 batch PCKh 0.3125\n",
      "Epoch 8 val loss 0.5781797766685486 val mAP 0.6391614675521851 val PCKh\n",
      "Epoch 8 completed in 776.77 seconds\n",
      "Start epoch 9 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.412147373 batch mAP 0.662139893 batch PCKh 0.75\n",
      "Trained batch 2 batch loss 0.456965923 batch mAP 0.686279297 batch PCKh 0.375\n",
      "Trained batch 3 batch loss 0.40967834 batch mAP 0.68560791 batch PCKh 0.6875\n",
      "Trained batch 4 batch loss 0.378872097 batch mAP 0.682189941 batch PCKh 0.5\n",
      "Trained batch 5 batch loss 0.370401055 batch mAP 0.659515381 batch PCKh 0.5\n",
      "Trained batch 6 batch loss 0.39229697 batch mAP 0.68737793 batch PCKh 0.5\n",
      "Trained batch 7 batch loss 0.42915386 batch mAP 0.675109863 batch PCKh 0.75\n",
      "Trained batch 8 batch loss 0.509693623 batch mAP 0.632873535 batch PCKh 0.75\n",
      "Trained batch 9 batch loss 0.532822847 batch mAP 0.655639648 batch PCKh 0.5625\n",
      "Trained batch 10 batch loss 0.519555092 batch mAP 0.610626221 batch PCKh 0.6875\n",
      "Trained batch 11 batch loss 0.47817564 batch mAP 0.593933105 batch PCKh 0.1875\n",
      "Trained batch 12 batch loss 0.486586809 batch mAP 0.6534729 batch PCKh 0.4375\n",
      "Trained batch 13 batch loss 0.604632 batch mAP 0.555419922 batch PCKh 0.875\n",
      "Trained batch 14 batch loss 0.433249533 batch mAP 0.599060059 batch PCKh 0.75\n",
      "Trained batch 15 batch loss 0.440410554 batch mAP 0.626861572 batch PCKh 0.3125\n",
      "Trained batch 16 batch loss 0.458465815 batch mAP 0.600891113 batch PCKh 0.4375\n",
      "Trained batch 17 batch loss 0.381525397 batch mAP 0.567718506 batch PCKh 0.1875\n",
      "Trained batch 18 batch loss 0.444075167 batch mAP 0.587921143 batch PCKh 0.3125\n",
      "Trained batch 19 batch loss 0.462751746 batch mAP 0.578826904 batch PCKh 0.1875\n",
      "Trained batch 20 batch loss 0.434370875 batch mAP 0.625793457 batch PCKh 0.3125\n",
      "Trained batch 21 batch loss 0.45709306 batch mAP 0.625824 batch PCKh 0.625\n",
      "Trained batch 22 batch loss 0.441125184 batch mAP 0.647979736 batch PCKh 0.6875\n",
      "Trained batch 23 batch loss 0.520114899 batch mAP 0.663085938 batch PCKh 0.3125\n",
      "Trained batch 24 batch loss 0.455833495 batch mAP 0.614593506 batch PCKh 0.1875\n",
      "Trained batch 25 batch loss 0.582234144 batch mAP 0.602386475 batch PCKh 0.6875\n",
      "Trained batch 26 batch loss 0.582315862 batch mAP 0.604766846 batch PCKh 0.375\n",
      "Trained batch 27 batch loss 0.53011471 batch mAP 0.574859619 batch PCKh 0.6875\n",
      "Trained batch 28 batch loss 0.480253 batch mAP 0.681182861 batch PCKh 0.375\n",
      "Trained batch 29 batch loss 0.521181941 batch mAP 0.65927124 batch PCKh 0.375\n",
      "Trained batch 30 batch loss 0.509614 batch mAP 0.637420654 batch PCKh 0.75\n",
      "Trained batch 31 batch loss 0.428893209 batch mAP 0.646972656 batch PCKh 0.25\n",
      "Trained batch 32 batch loss 0.452783108 batch mAP 0.624633789 batch PCKh 0.5625\n",
      "Trained batch 33 batch loss 0.488262266 batch mAP 0.599121094 batch PCKh 0.4375\n",
      "Trained batch 34 batch loss 0.48419708 batch mAP 0.644744873 batch PCKh 0.625\n",
      "Trained batch 35 batch loss 0.483694375 batch mAP 0.62902832 batch PCKh 0.3125\n",
      "Trained batch 36 batch loss 0.483807504 batch mAP 0.651794434 batch PCKh 0.3125\n",
      "Trained batch 37 batch loss 0.448332548 batch mAP 0.668029785 batch PCKh 0.3125\n",
      "Trained batch 38 batch loss 0.539736509 batch mAP 0.582580566 batch PCKh 0.625\n",
      "Trained batch 39 batch loss 0.483246684 batch mAP 0.551635742 batch PCKh 0.4375\n",
      "Trained batch 40 batch loss 0.526666641 batch mAP 0.658447266 batch PCKh 0.4375\n",
      "Trained batch 41 batch loss 0.463204622 batch mAP 0.679443359 batch PCKh 0.5\n",
      "Trained batch 42 batch loss 0.583268 batch mAP 0.643554688 batch PCKh 0.75\n",
      "Trained batch 43 batch loss 0.473896921 batch mAP 0.62008667 batch PCKh 0.1875\n",
      "Trained batch 44 batch loss 0.511532843 batch mAP 0.584442139 batch PCKh 0.4375\n",
      "Trained batch 45 batch loss 0.539483368 batch mAP 0.526519775 batch PCKh 0.3125\n",
      "Trained batch 46 batch loss 0.555221 batch mAP 0.592529297 batch PCKh 0.75\n",
      "Trained batch 47 batch loss 0.517483592 batch mAP 0.487945557 batch PCKh 0.8125\n",
      "Trained batch 48 batch loss 0.472253859 batch mAP 0.542785645 batch PCKh 0.3125\n",
      "Trained batch 49 batch loss 0.462906957 batch mAP 0.627410889 batch PCKh 0.4375\n",
      "Trained batch 50 batch loss 0.528725147 batch mAP 0.587493896 batch PCKh 0.6875\n",
      "Trained batch 51 batch loss 0.510692894 batch mAP 0.578399658 batch PCKh 0.375\n",
      "Trained batch 52 batch loss 0.577001333 batch mAP 0.534912109 batch PCKh 0.25\n",
      "Trained batch 53 batch loss 0.506947875 batch mAP 0.601959229 batch PCKh 0.375\n",
      "Trained batch 54 batch loss 0.513801277 batch mAP 0.575164795 batch PCKh 0.6875\n",
      "Trained batch 55 batch loss 0.535160065 batch mAP 0.52154541 batch PCKh 0.8125\n",
      "Trained batch 56 batch loss 0.494146466 batch mAP 0.507080078 batch PCKh 0.5\n",
      "Trained batch 57 batch loss 0.390036345 batch mAP 0.610595703 batch PCKh 0.75\n",
      "Trained batch 58 batch loss 0.477414697 batch mAP 0.636627197 batch PCKh 0.4375\n",
      "Trained batch 59 batch loss 0.473338306 batch mAP 0.602935791 batch PCKh 0.6875\n",
      "Trained batch 60 batch loss 0.432207137 batch mAP 0.597076416 batch PCKh 0.5625\n",
      "Trained batch 61 batch loss 0.400735021 batch mAP 0.674163818 batch PCKh 0.4375\n",
      "Trained batch 62 batch loss 0.409446716 batch mAP 0.620880127 batch PCKh 0.8125\n",
      "Trained batch 63 batch loss 0.360454172 batch mAP 0.644348145 batch PCKh 0.6875\n",
      "Trained batch 64 batch loss 0.370691329 batch mAP 0.62991333 batch PCKh 0.6875\n",
      "Trained batch 65 batch loss 0.394953966 batch mAP 0.646179199 batch PCKh 0.6875\n",
      "Trained batch 66 batch loss 0.446396202 batch mAP 0.629547119 batch PCKh 0.875\n",
      "Trained batch 67 batch loss 0.485545218 batch mAP 0.59185791 batch PCKh 0.75\n",
      "Trained batch 68 batch loss 0.460295975 batch mAP 0.623565674 batch PCKh 0.625\n",
      "Trained batch 69 batch loss 0.489827514 batch mAP 0.614349365 batch PCKh 0.6875\n",
      "Trained batch 70 batch loss 0.481775403 batch mAP 0.623321533 batch PCKh 0.5625\n",
      "Trained batch 71 batch loss 0.425475299 batch mAP 0.662139893 batch PCKh 0.75\n",
      "Trained batch 72 batch loss 0.388928413 batch mAP 0.630371094 batch PCKh 0.4375\n",
      "Trained batch 73 batch loss 0.474605799 batch mAP 0.673950195 batch PCKh 0.6875\n",
      "Trained batch 74 batch loss 0.517338514 batch mAP 0.650146484 batch PCKh 0.5625\n",
      "Trained batch 75 batch loss 0.419574142 batch mAP 0.70047 batch PCKh 0.6875\n",
      "Trained batch 76 batch loss 0.471601874 batch mAP 0.674346924 batch PCKh 0.875\n",
      "Trained batch 77 batch loss 0.390544713 batch mAP 0.6847229 batch PCKh 0.75\n",
      "Trained batch 78 batch loss 0.392923534 batch mAP 0.662872314 batch PCKh 0.4375\n",
      "Trained batch 79 batch loss 0.555799723 batch mAP 0.678741455 batch PCKh 0.6875\n",
      "Trained batch 80 batch loss 0.571818 batch mAP 0.668701172 batch PCKh 0.4375\n",
      "Trained batch 81 batch loss 0.54841423 batch mAP 0.674804688 batch PCKh 0.625\n",
      "Trained batch 82 batch loss 0.516722441 batch mAP 0.602813721 batch PCKh 0.4375\n",
      "Trained batch 83 batch loss 0.528288305 batch mAP 0.652130127 batch PCKh 0.625\n",
      "Trained batch 84 batch loss 0.539938092 batch mAP 0.678192139 batch PCKh 0.6875\n",
      "Trained batch 85 batch loss 0.499533 batch mAP 0.679595947 batch PCKh 0.5\n",
      "Trained batch 86 batch loss 0.512472272 batch mAP 0.558563232 batch PCKh 0.5625\n",
      "Trained batch 87 batch loss 0.534028888 batch mAP 0.562347412 batch PCKh 0.375\n",
      "Trained batch 88 batch loss 0.5994367 batch mAP 0.583557129 batch PCKh 0.5\n",
      "Trained batch 89 batch loss 0.536827803 batch mAP 0.675109863 batch PCKh 0.8125\n",
      "Trained batch 90 batch loss 0.460951746 batch mAP 0.660125732 batch PCKh 0.5625\n",
      "Trained batch 91 batch loss 0.545402765 batch mAP 0.660949707 batch PCKh 0.4375\n",
      "Trained batch 92 batch loss 0.490882635 batch mAP 0.659545898 batch PCKh 0.4375\n",
      "Trained batch 93 batch loss 0.451788545 batch mAP 0.680633545 batch PCKh 0.5\n",
      "Trained batch 94 batch loss 0.454093605 batch mAP 0.694519043 batch PCKh 0.8125\n",
      "Trained batch 95 batch loss 0.525049388 batch mAP 0.596557617 batch PCKh 0.1875\n",
      "Trained batch 96 batch loss 0.530312121 batch mAP 0.580200195 batch PCKh 0.8125\n",
      "Trained batch 97 batch loss 0.461366624 batch mAP 0.58996582 batch PCKh 0.75\n",
      "Trained batch 98 batch loss 0.445293069 batch mAP 0.631652832 batch PCKh 0.375\n",
      "Trained batch 99 batch loss 0.574664 batch mAP 0.597442627 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 100 batch loss 0.555368066 batch mAP 0.57208252 batch PCKh 0.625\n",
      "Trained batch 101 batch loss 0.431306273 batch mAP 0.570159912 batch PCKh 0.75\n",
      "Trained batch 102 batch loss 0.500240684 batch mAP 0.578949 batch PCKh 0.25\n",
      "Trained batch 103 batch loss 0.499928176 batch mAP 0.575836182 batch PCKh 0.875\n",
      "Trained batch 104 batch loss 0.473085195 batch mAP 0.582305908 batch PCKh 0.5625\n",
      "Trained batch 105 batch loss 0.499915481 batch mAP 0.587158203 batch PCKh 0.625\n",
      "Trained batch 106 batch loss 0.422531903 batch mAP 0.590240479 batch PCKh 0.6875\n",
      "Trained batch 107 batch loss 0.507768691 batch mAP 0.57321167 batch PCKh 0.75\n",
      "Trained batch 108 batch loss 0.475319445 batch mAP 0.630065918 batch PCKh 0.375\n",
      "Trained batch 109 batch loss 0.497341216 batch mAP 0.615264893 batch PCKh 0.625\n",
      "Trained batch 110 batch loss 0.473564386 batch mAP 0.633453369 batch PCKh 0.875\n",
      "Trained batch 111 batch loss 0.541328609 batch mAP 0.597869873 batch PCKh 0.8125\n",
      "Trained batch 112 batch loss 0.531861484 batch mAP 0.553405762 batch PCKh 0.4375\n",
      "Trained batch 113 batch loss 0.505491555 batch mAP 0.596008301 batch PCKh 0.6875\n",
      "Trained batch 114 batch loss 0.470964223 batch mAP 0.638702393 batch PCKh 0.625\n",
      "Trained batch 115 batch loss 0.453701764 batch mAP 0.553344727 batch PCKh 0.75\n",
      "Trained batch 116 batch loss 0.482981682 batch mAP 0.606079102 batch PCKh 0.3125\n",
      "Trained batch 117 batch loss 0.396410286 batch mAP 0.705444336 batch PCKh 0.3125\n",
      "Trained batch 118 batch loss 0.424199432 batch mAP 0.649780273 batch PCKh 0.75\n",
      "Trained batch 119 batch loss 0.394787043 batch mAP 0.71887207 batch PCKh 0.8125\n",
      "Trained batch 120 batch loss 0.368619561 batch mAP 0.72064209 batch PCKh 0.6875\n",
      "Trained batch 121 batch loss 0.448516965 batch mAP 0.610687256 batch PCKh 0.625\n",
      "Trained batch 122 batch loss 0.441493 batch mAP 0.690582275 batch PCKh 0.75\n",
      "Trained batch 123 batch loss 0.406439304 batch mAP 0.708953857 batch PCKh 0.375\n",
      "Trained batch 124 batch loss 0.4871521 batch mAP 0.618743896 batch PCKh 0.6875\n",
      "Trained batch 125 batch loss 0.440950304 batch mAP 0.625274658 batch PCKh 0.75\n",
      "Trained batch 126 batch loss 0.427082688 batch mAP 0.64855957 batch PCKh 0.75\n",
      "Trained batch 127 batch loss 0.380912215 batch mAP 0.675689697 batch PCKh 0.4375\n",
      "Trained batch 128 batch loss 0.341695398 batch mAP 0.675567627 batch PCKh 0.25\n",
      "Trained batch 129 batch loss 0.400703847 batch mAP 0.651977539 batch PCKh 0.625\n",
      "Trained batch 130 batch loss 0.461532027 batch mAP 0.612365723 batch PCKh 0.625\n",
      "Trained batch 131 batch loss 0.418959051 batch mAP 0.556365967 batch PCKh 0\n",
      "Trained batch 132 batch loss 0.454980403 batch mAP 0.589050293 batch PCKh 0.4375\n",
      "Trained batch 133 batch loss 0.481254965 batch mAP 0.625213623 batch PCKh 0.1875\n",
      "Trained batch 134 batch loss 0.530031085 batch mAP 0.500854492 batch PCKh 0.5\n",
      "Trained batch 135 batch loss 0.552283406 batch mAP 0.510498047 batch PCKh 0.4375\n",
      "Trained batch 136 batch loss 0.733298779 batch mAP 0.467163086 batch PCKh 0.0625\n",
      "Trained batch 137 batch loss 0.505639195 batch mAP 0.665496826 batch PCKh 0.1875\n",
      "Trained batch 138 batch loss 0.430515289 batch mAP 0.649902344 batch PCKh 0.3125\n",
      "Trained batch 139 batch loss 0.44595179 batch mAP 0.649658203 batch PCKh 0.0625\n",
      "Trained batch 140 batch loss 0.411760092 batch mAP 0.675994873 batch PCKh 0.0625\n",
      "Trained batch 141 batch loss 0.304343849 batch mAP 0.743652344 batch PCKh 0.5625\n",
      "Trained batch 142 batch loss 0.399723589 batch mAP 0.669128418 batch PCKh 0.8125\n",
      "Trained batch 143 batch loss 0.370601505 batch mAP 0.645202637 batch PCKh 0.1875\n",
      "Trained batch 144 batch loss 0.475477636 batch mAP 0.678955078 batch PCKh 0.3125\n",
      "Trained batch 145 batch loss 0.481179804 batch mAP 0.690094 batch PCKh 0.4375\n",
      "Trained batch 146 batch loss 0.480923533 batch mAP 0.666595459 batch PCKh 0.75\n",
      "Trained batch 147 batch loss 0.583438575 batch mAP 0.593078613 batch PCKh 0.625\n",
      "Trained batch 148 batch loss 0.53655386 batch mAP 0.608520508 batch PCKh 0.3125\n",
      "Trained batch 149 batch loss 0.526903749 batch mAP 0.619781494 batch PCKh 0.25\n",
      "Trained batch 150 batch loss 0.593545377 batch mAP 0.559448242 batch PCKh 0.4375\n",
      "Trained batch 151 batch loss 0.431707293 batch mAP 0.658660889 batch PCKh 0.1875\n",
      "Trained batch 152 batch loss 0.536744475 batch mAP 0.672576904 batch PCKh 0.3125\n",
      "Trained batch 153 batch loss 0.486884296 batch mAP 0.678131104 batch PCKh 0.25\n",
      "Trained batch 154 batch loss 0.487319022 batch mAP 0.639923096 batch PCKh 0.375\n",
      "Trained batch 155 batch loss 0.481831551 batch mAP 0.649383545 batch PCKh 0.375\n",
      "Trained batch 156 batch loss 0.453371823 batch mAP 0.708404541 batch PCKh 0.375\n",
      "Trained batch 157 batch loss 0.478626579 batch mAP 0.628631592 batch PCKh 0.1875\n",
      "Trained batch 158 batch loss 0.511971295 batch mAP 0.562652588 batch PCKh 0.625\n",
      "Trained batch 159 batch loss 0.567778111 batch mAP 0.594512939 batch PCKh 0.375\n",
      "Trained batch 160 batch loss 0.435480952 batch mAP 0.63974 batch PCKh 0.6875\n",
      "Trained batch 161 batch loss 0.565442324 batch mAP 0.538330078 batch PCKh 0.125\n",
      "Trained batch 162 batch loss 0.52184844 batch mAP 0.577392578 batch PCKh 0.0625\n",
      "Trained batch 163 batch loss 0.57806164 batch mAP 0.574035645 batch PCKh 0.75\n",
      "Trained batch 164 batch loss 0.525539 batch mAP 0.62323 batch PCKh 0.3125\n",
      "Trained batch 165 batch loss 0.465231538 batch mAP 0.620361328 batch PCKh 0.875\n",
      "Trained batch 166 batch loss 0.440356582 batch mAP 0.59954834 batch PCKh 0.875\n",
      "Trained batch 167 batch loss 0.415339172 batch mAP 0.570861816 batch PCKh 0.5625\n",
      "Trained batch 168 batch loss 0.44089815 batch mAP 0.50302124 batch PCKh 0.625\n",
      "Trained batch 169 batch loss 0.43663168 batch mAP 0.559539795 batch PCKh 0.375\n",
      "Trained batch 170 batch loss 0.376367062 batch mAP 0.657653809 batch PCKh 0.3125\n",
      "Trained batch 171 batch loss 0.403919727 batch mAP 0.510925293 batch PCKh 0.5625\n",
      "Trained batch 172 batch loss 0.38045758 batch mAP 0.563781738 batch PCKh 0.375\n",
      "Trained batch 173 batch loss 0.555387437 batch mAP 0.623931885 batch PCKh 0.125\n",
      "Trained batch 174 batch loss 0.592752934 batch mAP 0.626983643 batch PCKh 0.5625\n",
      "Trained batch 175 batch loss 0.5205006 batch mAP 0.639953613 batch PCKh 0.5625\n",
      "Trained batch 176 batch loss 0.542437673 batch mAP 0.604187 batch PCKh 0.75\n",
      "Trained batch 177 batch loss 0.535050333 batch mAP 0.573730469 batch PCKh 0.3125\n",
      "Trained batch 178 batch loss 0.433973879 batch mAP 0.663147 batch PCKh 0.375\n",
      "Trained batch 179 batch loss 0.436294 batch mAP 0.666046143 batch PCKh 0.5\n",
      "Trained batch 180 batch loss 0.468433022 batch mAP 0.68347168 batch PCKh 0.25\n",
      "Trained batch 181 batch loss 0.489444733 batch mAP 0.634185791 batch PCKh 0.625\n",
      "Trained batch 182 batch loss 0.43828848 batch mAP 0.577362061 batch PCKh 0.625\n",
      "Trained batch 183 batch loss 0.498232603 batch mAP 0.593078613 batch PCKh 0.1875\n",
      "Trained batch 184 batch loss 0.506871 batch mAP 0.654876709 batch PCKh 0.5\n",
      "Trained batch 185 batch loss 0.532798886 batch mAP 0.665924072 batch PCKh 0.0625\n",
      "Trained batch 186 batch loss 0.57461381 batch mAP 0.622192383 batch PCKh 0.1875\n",
      "Trained batch 187 batch loss 0.648083687 batch mAP 0.566375732 batch PCKh 0.125\n",
      "Trained batch 188 batch loss 0.487893224 batch mAP 0.611053467 batch PCKh 0.6875\n",
      "Trained batch 189 batch loss 0.594083309 batch mAP 0.534118652 batch PCKh 0.1875\n",
      "Trained batch 190 batch loss 0.536981642 batch mAP 0.592315674 batch PCKh 0.4375\n",
      "Trained batch 191 batch loss 0.494353086 batch mAP 0.588470459 batch PCKh 0.6875\n",
      "Trained batch 192 batch loss 0.476627409 batch mAP 0.564239502 batch PCKh 0.875\n",
      "Trained batch 193 batch loss 0.483970881 batch mAP 0.515167236 batch PCKh 0.75\n",
      "Trained batch 194 batch loss 0.544288397 batch mAP 0.528839111 batch PCKh 0.75\n",
      "Trained batch 195 batch loss 0.580876291 batch mAP 0.540710449 batch PCKh 0.875\n",
      "Trained batch 196 batch loss 0.53676 batch mAP 0.592132568 batch PCKh 0.75\n",
      "Trained batch 197 batch loss 0.495996952 batch mAP 0.596618652 batch PCKh 0.625\n",
      "Trained batch 198 batch loss 0.440218478 batch mAP 0.581085205 batch PCKh 0.75\n",
      "Trained batch 199 batch loss 0.456374973 batch mAP 0.577453613 batch PCKh 0.75\n",
      "Trained batch 200 batch loss 0.465272963 batch mAP 0.661315918 batch PCKh 0.625\n",
      "Trained batch 201 batch loss 0.538510382 batch mAP 0.587219238 batch PCKh 0.625\n",
      "Trained batch 202 batch loss 0.469313085 batch mAP 0.671691895 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 203 batch loss 0.45215863 batch mAP 0.669830322 batch PCKh 0.6875\n",
      "Trained batch 204 batch loss 0.463746786 batch mAP 0.642334 batch PCKh 0.375\n",
      "Trained batch 205 batch loss 0.402745843 batch mAP 0.709899902 batch PCKh 0.75\n",
      "Trained batch 206 batch loss 0.460724682 batch mAP 0.661132812 batch PCKh 0.1875\n",
      "Trained batch 207 batch loss 0.505031109 batch mAP 0.671600342 batch PCKh 0.4375\n",
      "Trained batch 208 batch loss 0.532790303 batch mAP 0.655395508 batch PCKh 0.6875\n",
      "Trained batch 209 batch loss 0.468356043 batch mAP 0.681488037 batch PCKh 0.4375\n",
      "Trained batch 210 batch loss 0.409241796 batch mAP 0.701019287 batch PCKh 0.5625\n",
      "Trained batch 211 batch loss 0.461855471 batch mAP 0.594085693 batch PCKh 0.75\n",
      "Trained batch 212 batch loss 0.489485919 batch mAP 0.581634521 batch PCKh 0.6875\n",
      "Trained batch 213 batch loss 0.397742391 batch mAP 0.602203369 batch PCKh 0.6875\n",
      "Trained batch 214 batch loss 0.421212673 batch mAP 0.638793945 batch PCKh 0.8125\n",
      "Trained batch 215 batch loss 0.513989925 batch mAP 0.597381592 batch PCKh 0.6875\n",
      "Trained batch 216 batch loss 0.368334085 batch mAP 0.641479492 batch PCKh 0.75\n",
      "Trained batch 217 batch loss 0.45319432 batch mAP 0.600494385 batch PCKh 0.625\n",
      "Trained batch 218 batch loss 0.550660372 batch mAP 0.635528564 batch PCKh 0.6875\n",
      "Trained batch 219 batch loss 0.546884298 batch mAP 0.552459717 batch PCKh 0.8125\n",
      "Trained batch 220 batch loss 0.495084524 batch mAP 0.692657471 batch PCKh 0.875\n",
      "Trained batch 221 batch loss 0.538014591 batch mAP 0.701721191 batch PCKh 0.4375\n",
      "Trained batch 222 batch loss 0.547144294 batch mAP 0.661712646 batch PCKh 0.375\n",
      "Trained batch 223 batch loss 0.450174779 batch mAP 0.701324463 batch PCKh 0.8125\n",
      "Trained batch 224 batch loss 0.46823284 batch mAP 0.695068359 batch PCKh 0.25\n",
      "Trained batch 225 batch loss 0.563650608 batch mAP 0.64465332 batch PCKh 0.6875\n",
      "Trained batch 226 batch loss 0.608478308 batch mAP 0.555297852 batch PCKh 0.5\n",
      "Trained batch 227 batch loss 0.601105213 batch mAP 0.532745361 batch PCKh 0.3125\n",
      "Trained batch 228 batch loss 0.612770438 batch mAP 0.558074951 batch PCKh 0.375\n",
      "Trained batch 229 batch loss 0.540128946 batch mAP 0.582428 batch PCKh 0.625\n",
      "Trained batch 230 batch loss 0.532113314 batch mAP 0.593780518 batch PCKh 0.875\n",
      "Trained batch 231 batch loss 0.556548655 batch mAP 0.540283203 batch PCKh 0.875\n",
      "Trained batch 232 batch loss 0.457795173 batch mAP 0.621551514 batch PCKh 0.75\n",
      "Trained batch 233 batch loss 0.509250581 batch mAP 0.574646 batch PCKh 0.875\n",
      "Trained batch 234 batch loss 0.479494691 batch mAP 0.615692139 batch PCKh 0.875\n",
      "Trained batch 235 batch loss 0.479129672 batch mAP 0.646820068 batch PCKh 0.875\n",
      "Trained batch 236 batch loss 0.454243034 batch mAP 0.608551 batch PCKh 0.4375\n",
      "Trained batch 237 batch loss 0.431811541 batch mAP 0.664306641 batch PCKh 0.5625\n",
      "Trained batch 238 batch loss 0.493906 batch mAP 0.624481201 batch PCKh 0.75\n",
      "Trained batch 239 batch loss 0.599628925 batch mAP 0.591796875 batch PCKh 0.5\n",
      "Trained batch 240 batch loss 0.619970202 batch mAP 0.606414795 batch PCKh 0.25\n",
      "Trained batch 241 batch loss 0.531498194 batch mAP 0.629211426 batch PCKh 0.3125\n",
      "Trained batch 242 batch loss 0.583918333 batch mAP 0.546661377 batch PCKh 0.625\n",
      "Trained batch 243 batch loss 0.618953645 batch mAP 0.56854248 batch PCKh 0.125\n",
      "Trained batch 244 batch loss 0.607533216 batch mAP 0.508178711 batch PCKh 0.6875\n",
      "Trained batch 245 batch loss 0.612777829 batch mAP 0.52456665 batch PCKh 0.25\n",
      "Trained batch 246 batch loss 0.498250067 batch mAP 0.667938232 batch PCKh 0.75\n",
      "Trained batch 247 batch loss 0.528003752 batch mAP 0.632141113 batch PCKh 0.1875\n",
      "Trained batch 248 batch loss 0.454442829 batch mAP 0.661987305 batch PCKh 0.5\n",
      "Trained batch 249 batch loss 0.500763535 batch mAP 0.630340576 batch PCKh 0.0625\n",
      "Trained batch 250 batch loss 0.526702583 batch mAP 0.634857178 batch PCKh 0.375\n",
      "Trained batch 251 batch loss 0.524223745 batch mAP 0.635986328 batch PCKh 0.375\n",
      "Trained batch 252 batch loss 0.433983147 batch mAP 0.658935547 batch PCKh 0.4375\n",
      "Trained batch 253 batch loss 0.587712049 batch mAP 0.588775635 batch PCKh 0.375\n",
      "Trained batch 254 batch loss 0.538893223 batch mAP 0.593261719 batch PCKh 0.4375\n",
      "Trained batch 255 batch loss 0.588942 batch mAP 0.525054932 batch PCKh 0.3125\n",
      "Trained batch 256 batch loss 0.660072446 batch mAP 0.538116455 batch PCKh 0.0625\n",
      "Trained batch 257 batch loss 0.483567566 batch mAP 0.631530762 batch PCKh 0.5\n",
      "Trained batch 258 batch loss 0.505667567 batch mAP 0.674835205 batch PCKh 0.4375\n",
      "Trained batch 259 batch loss 0.376574695 batch mAP 0.66922 batch PCKh 0.4375\n",
      "Trained batch 260 batch loss 0.374685347 batch mAP 0.606170654 batch PCKh 0.625\n",
      "Trained batch 261 batch loss 0.474998385 batch mAP 0.607971191 batch PCKh 0.25\n",
      "Trained batch 262 batch loss 0.445739061 batch mAP 0.602355957 batch PCKh 0.375\n",
      "Trained batch 263 batch loss 0.432106555 batch mAP 0.617340088 batch PCKh 0.4375\n",
      "Trained batch 264 batch loss 0.527992964 batch mAP 0.568756104 batch PCKh 0.4375\n",
      "Trained batch 265 batch loss 0.532851696 batch mAP 0.542938232 batch PCKh 0.375\n",
      "Trained batch 266 batch loss 0.518317103 batch mAP 0.541900635 batch PCKh 0.375\n",
      "Trained batch 267 batch loss 0.545994759 batch mAP 0.543457031 batch PCKh 0.8125\n",
      "Trained batch 268 batch loss 0.479551554 batch mAP 0.619689941 batch PCKh 0.875\n",
      "Trained batch 269 batch loss 0.554711103 batch mAP 0.655944824 batch PCKh 0.375\n",
      "Trained batch 270 batch loss 0.70375067 batch mAP 0.500976562 batch PCKh 0.375\n",
      "Trained batch 271 batch loss 0.584491611 batch mAP 0.573852539 batch PCKh 0.3125\n",
      "Trained batch 272 batch loss 0.57776475 batch mAP 0.56628418 batch PCKh 0.375\n",
      "Trained batch 273 batch loss 0.643684328 batch mAP 0.501281738 batch PCKh 0.75\n",
      "Trained batch 274 batch loss 0.521730125 batch mAP 0.585113525 batch PCKh 0.6875\n",
      "Trained batch 275 batch loss 0.430957675 batch mAP 0.612854 batch PCKh 0.5625\n",
      "Trained batch 276 batch loss 0.504255295 batch mAP 0.533996582 batch PCKh 0.625\n",
      "Trained batch 277 batch loss 0.456536382 batch mAP 0.651641846 batch PCKh 0.6875\n",
      "Trained batch 278 batch loss 0.381025642 batch mAP 0.670715332 batch PCKh 0.1875\n",
      "Trained batch 279 batch loss 0.339410812 batch mAP 0.645141602 batch PCKh 0.3125\n",
      "Trained batch 280 batch loss 0.290381551 batch mAP 0.655517578 batch PCKh 0\n",
      "Trained batch 281 batch loss 0.462628782 batch mAP 0.655914307 batch PCKh 0.5\n",
      "Trained batch 282 batch loss 0.51783669 batch mAP 0.625335693 batch PCKh 0.625\n",
      "Trained batch 283 batch loss 0.48158288 batch mAP 0.570983887 batch PCKh 0.625\n",
      "Trained batch 284 batch loss 0.470523179 batch mAP 0.551940918 batch PCKh 0.0625\n",
      "Trained batch 285 batch loss 0.394735336 batch mAP 0.621551514 batch PCKh 0.3125\n",
      "Trained batch 286 batch loss 0.273746848 batch mAP 0.593841553 batch PCKh 0\n",
      "Trained batch 287 batch loss 0.278635204 batch mAP 0.627990723 batch PCKh 0\n",
      "Trained batch 288 batch loss 0.391422153 batch mAP 0.61050415 batch PCKh 0\n",
      "Trained batch 289 batch loss 0.339047879 batch mAP 0.569580078 batch PCKh 0\n",
      "Trained batch 290 batch loss 0.333224326 batch mAP 0.590393066 batch PCKh 0.625\n",
      "Trained batch 291 batch loss 0.375286967 batch mAP 0.562713623 batch PCKh 0.75\n",
      "Trained batch 292 batch loss 0.388961941 batch mAP 0.626709 batch PCKh 0.5625\n",
      "Trained batch 293 batch loss 0.412175626 batch mAP 0.620117188 batch PCKh 0.625\n",
      "Trained batch 294 batch loss 0.453517616 batch mAP 0.612854 batch PCKh 0.8125\n",
      "Trained batch 295 batch loss 0.478038847 batch mAP 0.585662842 batch PCKh 0.875\n",
      "Trained batch 296 batch loss 0.413428903 batch mAP 0.616027832 batch PCKh 0.5625\n",
      "Trained batch 297 batch loss 0.515907288 batch mAP 0.641387939 batch PCKh 0\n",
      "Trained batch 298 batch loss 0.506627917 batch mAP 0.606781 batch PCKh 0.3125\n",
      "Trained batch 299 batch loss 0.349561095 batch mAP 0.662994385 batch PCKh 0.75\n",
      "Trained batch 300 batch loss 0.480842888 batch mAP 0.652923584 batch PCKh 0.375\n",
      "Trained batch 301 batch loss 0.449278891 batch mAP 0.633667 batch PCKh 0.6875\n",
      "Trained batch 302 batch loss 0.458164871 batch mAP 0.659851074 batch PCKh 0.6875\n",
      "Trained batch 303 batch loss 0.481702119 batch mAP 0.656768799 batch PCKh 0.625\n",
      "Trained batch 304 batch loss 0.487143815 batch mAP 0.575866699 batch PCKh 0.0625\n",
      "Trained batch 305 batch loss 0.537396908 batch mAP 0.566253662 batch PCKh 0.125\n",
      "Trained batch 306 batch loss 0.581117749 batch mAP 0.571655273 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 307 batch loss 0.626533628 batch mAP 0.607330322 batch PCKh 0.25\n",
      "Trained batch 308 batch loss 0.716581225 batch mAP 0.595581055 batch PCKh 0.1875\n",
      "Trained batch 309 batch loss 0.608502746 batch mAP 0.622406 batch PCKh 0.3125\n",
      "Trained batch 310 batch loss 0.647990227 batch mAP 0.661773682 batch PCKh 0.3125\n",
      "Trained batch 311 batch loss 0.597005 batch mAP 0.471710205 batch PCKh 0.5\n",
      "Trained batch 312 batch loss 0.610312581 batch mAP 0.217315674 batch PCKh 0.5625\n",
      "Trained batch 313 batch loss 0.583542466 batch mAP 0.134216309 batch PCKh 0.5\n",
      "Trained batch 314 batch loss 0.569663286 batch mAP 0.0230102539 batch PCKh 0.5625\n",
      "Trained batch 315 batch loss 0.57812953 batch mAP 0.0185546875 batch PCKh 0.625\n",
      "Trained batch 316 batch loss 0.575323582 batch mAP 0.0403137207 batch PCKh 0.875\n",
      "Trained batch 317 batch loss 0.537275672 batch mAP 0.0223388672 batch PCKh 0.5625\n",
      "Trained batch 318 batch loss 0.558470607 batch mAP 0.0436706543 batch PCKh 0.75\n",
      "Trained batch 319 batch loss 0.526389837 batch mAP 0.0338439941 batch PCKh 0.1875\n",
      "Trained batch 320 batch loss 0.522059321 batch mAP 0.135253906 batch PCKh 0.5625\n",
      "Trained batch 321 batch loss 0.553755045 batch mAP 0.0889892578 batch PCKh 0.5\n",
      "Trained batch 322 batch loss 0.641245723 batch mAP 0.22555542 batch PCKh 0.375\n",
      "Trained batch 323 batch loss 0.562226892 batch mAP 0.302032471 batch PCKh 0.625\n",
      "Trained batch 324 batch loss 0.58620739 batch mAP 0.280944824 batch PCKh 0.625\n",
      "Trained batch 325 batch loss 0.495706886 batch mAP 0.28302002 batch PCKh 0.125\n",
      "Trained batch 326 batch loss 0.553476572 batch mAP 0.237701416 batch PCKh 0.5\n",
      "Trained batch 327 batch loss 0.505434 batch mAP 0.381195068 batch PCKh 0.4375\n",
      "Trained batch 328 batch loss 0.476540923 batch mAP 0.377807617 batch PCKh 0.5\n",
      "Trained batch 329 batch loss 0.536739111 batch mAP 0.45526123 batch PCKh 0.75\n",
      "Trained batch 330 batch loss 0.509033084 batch mAP 0.292541504 batch PCKh 0.75\n",
      "Trained batch 331 batch loss 0.482759774 batch mAP 0.408111572 batch PCKh 0.4375\n",
      "Trained batch 332 batch loss 0.491516918 batch mAP 0.384857178 batch PCKh 0.1875\n",
      "Trained batch 333 batch loss 0.4920654 batch mAP 0.507507324 batch PCKh 0.3125\n",
      "Trained batch 334 batch loss 0.51949662 batch mAP 0.517242432 batch PCKh 0.75\n",
      "Trained batch 335 batch loss 0.387455404 batch mAP 0.532165527 batch PCKh 0.625\n",
      "Trained batch 336 batch loss 0.405318856 batch mAP 0.533508301 batch PCKh 0.75\n",
      "Trained batch 337 batch loss 0.422088 batch mAP 0.517425537 batch PCKh 0.75\n",
      "Trained batch 338 batch loss 0.433785617 batch mAP 0.529083252 batch PCKh 0.75\n",
      "Trained batch 339 batch loss 0.505941629 batch mAP 0.463623047 batch PCKh 0.6875\n",
      "Trained batch 340 batch loss 0.59798938 batch mAP 0.492218018 batch PCKh 0.6875\n",
      "Trained batch 341 batch loss 0.46021843 batch mAP 0.512359619 batch PCKh 0.625\n",
      "Trained batch 342 batch loss 0.578364432 batch mAP 0.480896 batch PCKh 0.75\n",
      "Trained batch 343 batch loss 0.521296263 batch mAP 0.410705566 batch PCKh 0.75\n",
      "Trained batch 344 batch loss 0.569495559 batch mAP 0.473114 batch PCKh 0.8125\n",
      "Trained batch 345 batch loss 0.564991415 batch mAP 0.483459473 batch PCKh 0.875\n",
      "Trained batch 346 batch loss 0.585655212 batch mAP 0.463989258 batch PCKh 0.375\n",
      "Trained batch 347 batch loss 0.470987856 batch mAP 0.497833252 batch PCKh 0.625\n",
      "Trained batch 348 batch loss 0.474687636 batch mAP 0.481842041 batch PCKh 0.375\n",
      "Trained batch 349 batch loss 0.488661736 batch mAP 0.503845215 batch PCKh 0.3125\n",
      "Trained batch 350 batch loss 0.53874141 batch mAP 0.484832764 batch PCKh 0.625\n",
      "Trained batch 351 batch loss 0.450475484 batch mAP 0.57611084 batch PCKh 0.4375\n",
      "Trained batch 352 batch loss 0.439173698 batch mAP 0.598968506 batch PCKh 0.4375\n",
      "Trained batch 353 batch loss 0.427953541 batch mAP 0.653595 batch PCKh 0.75\n",
      "Trained batch 354 batch loss 0.42727685 batch mAP 0.607971191 batch PCKh 0.25\n",
      "Trained batch 355 batch loss 0.425014615 batch mAP 0.67099 batch PCKh 0.1875\n",
      "Trained batch 356 batch loss 0.457672268 batch mAP 0.679595947 batch PCKh 0.3125\n",
      "Trained batch 357 batch loss 0.436059624 batch mAP 0.692321777 batch PCKh 0.4375\n",
      "Trained batch 358 batch loss 0.439372778 batch mAP 0.711975098 batch PCKh 0.625\n",
      "Trained batch 359 batch loss 0.435527444 batch mAP 0.703430176 batch PCKh 0.4375\n",
      "Trained batch 360 batch loss 0.401314437 batch mAP 0.705749512 batch PCKh 0.375\n",
      "Trained batch 361 batch loss 0.600809872 batch mAP 0.582275391 batch PCKh 0.5\n",
      "Trained batch 362 batch loss 0.412527084 batch mAP 0.665679932 batch PCKh 0.4375\n",
      "Trained batch 363 batch loss 0.489075541 batch mAP 0.632141113 batch PCKh 0.875\n",
      "Trained batch 364 batch loss 0.457256436 batch mAP 0.62008667 batch PCKh 0.625\n",
      "Trained batch 365 batch loss 0.456348658 batch mAP 0.657226562 batch PCKh 0.5\n",
      "Trained batch 366 batch loss 0.402872115 batch mAP 0.670013428 batch PCKh 0.5\n",
      "Trained batch 367 batch loss 0.410031468 batch mAP 0.708251953 batch PCKh 0.375\n",
      "Trained batch 368 batch loss 0.465452552 batch mAP 0.65222168 batch PCKh 0.4375\n",
      "Trained batch 369 batch loss 0.47459054 batch mAP 0.661438 batch PCKh 0.25\n",
      "Trained batch 370 batch loss 0.425167441 batch mAP 0.665039062 batch PCKh 0.25\n",
      "Trained batch 371 batch loss 0.467025906 batch mAP 0.728881836 batch PCKh 0.8125\n",
      "Trained batch 372 batch loss 0.363677472 batch mAP 0.699829102 batch PCKh 0.3125\n",
      "Trained batch 373 batch loss 0.499063522 batch mAP 0.541748047 batch PCKh 0.3125\n",
      "Trained batch 374 batch loss 0.657891512 batch mAP 0.502716064 batch PCKh 0\n",
      "Trained batch 375 batch loss 0.585098863 batch mAP 0.557037354 batch PCKh 0.6875\n",
      "Trained batch 376 batch loss 0.620108664 batch mAP 0.550231934 batch PCKh 0\n",
      "Trained batch 377 batch loss 0.53062439 batch mAP 0.586242676 batch PCKh 0.125\n",
      "Trained batch 378 batch loss 0.483622551 batch mAP 0.693908691 batch PCKh 0.4375\n",
      "Trained batch 379 batch loss 0.510488331 batch mAP 0.663421631 batch PCKh 0.6875\n",
      "Trained batch 380 batch loss 0.489091158 batch mAP 0.622375488 batch PCKh 0.625\n",
      "Trained batch 381 batch loss 0.548868537 batch mAP 0.623352051 batch PCKh 0.3125\n",
      "Trained batch 382 batch loss 0.627849698 batch mAP 0.552276611 batch PCKh 0.1875\n",
      "Trained batch 383 batch loss 0.518821299 batch mAP 0.561218262 batch PCKh 0.4375\n",
      "Trained batch 384 batch loss 0.487603277 batch mAP 0.603515625 batch PCKh 0.6875\n",
      "Trained batch 385 batch loss 0.541196942 batch mAP 0.59942627 batch PCKh 0.5\n",
      "Trained batch 386 batch loss 0.500461459 batch mAP 0.594024658 batch PCKh 0.625\n",
      "Trained batch 387 batch loss 0.560628653 batch mAP 0.615905762 batch PCKh 0.1875\n",
      "Trained batch 388 batch loss 0.440526575 batch mAP 0.713165283 batch PCKh 0.8125\n",
      "Trained batch 389 batch loss 0.476619273 batch mAP 0.585968 batch PCKh 0.75\n",
      "Trained batch 390 batch loss 0.423369765 batch mAP 0.606170654 batch PCKh 0.875\n",
      "Trained batch 391 batch loss 0.448182583 batch mAP 0.604431152 batch PCKh 0.625\n",
      "Trained batch 392 batch loss 0.517747223 batch mAP 0.560455322 batch PCKh 0.75\n",
      "Trained batch 393 batch loss 0.576443911 batch mAP 0.6065979 batch PCKh 0.6875\n",
      "Trained batch 394 batch loss 0.54942143 batch mAP 0.568573 batch PCKh 0.625\n",
      "Trained batch 395 batch loss 0.41679129 batch mAP 0.626556396 batch PCKh 0.6875\n",
      "Trained batch 396 batch loss 0.456866443 batch mAP 0.657897949 batch PCKh 0.625\n",
      "Trained batch 397 batch loss 0.324817181 batch mAP 0.643768311 batch PCKh 0\n",
      "Trained batch 398 batch loss 0.42714569 batch mAP 0.638397217 batch PCKh 0.75\n",
      "Trained batch 399 batch loss 0.381844163 batch mAP 0.637908936 batch PCKh 0.6875\n",
      "Trained batch 400 batch loss 0.385980904 batch mAP 0.613098145 batch PCKh 0.75\n",
      "Trained batch 401 batch loss 0.434844434 batch mAP 0.629760742 batch PCKh 0.75\n",
      "Trained batch 402 batch loss 0.552140594 batch mAP 0.601470947 batch PCKh 0.8125\n",
      "Trained batch 403 batch loss 0.540530086 batch mAP 0.574310303 batch PCKh 0.5\n",
      "Trained batch 404 batch loss 0.57686162 batch mAP 0.531951904 batch PCKh 0.6875\n",
      "Trained batch 405 batch loss 0.478214025 batch mAP 0.574584961 batch PCKh 0.6875\n",
      "Trained batch 406 batch loss 0.542879879 batch mAP 0.577148438 batch PCKh 0.375\n",
      "Trained batch 407 batch loss 0.540928483 batch mAP 0.655853271 batch PCKh 0.25\n",
      "Trained batch 408 batch loss 0.560156822 batch mAP 0.599029541 batch PCKh 0.4375\n",
      "Trained batch 409 batch loss 0.582868934 batch mAP 0.610870361 batch PCKh 0.5\n",
      "Trained batch 410 batch loss 0.389244139 batch mAP 0.726654053 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 411 batch loss 0.379905939 batch mAP 0.705963135 batch PCKh 0.625\n",
      "Trained batch 412 batch loss 0.41256234 batch mAP 0.670379639 batch PCKh 0.5\n",
      "Trained batch 413 batch loss 0.411674 batch mAP 0.689575195 batch PCKh 0.6875\n",
      "Trained batch 414 batch loss 0.53775537 batch mAP 0.611358643 batch PCKh 0.5625\n",
      "Trained batch 415 batch loss 0.50360918 batch mAP 0.639404297 batch PCKh 0.4375\n",
      "Trained batch 416 batch loss 0.554252446 batch mAP 0.576141357 batch PCKh 0.75\n",
      "Trained batch 417 batch loss 0.489420056 batch mAP 0.660736084 batch PCKh 0.75\n",
      "Trained batch 418 batch loss 0.56582433 batch mAP 0.590789795 batch PCKh 0.5625\n",
      "Trained batch 419 batch loss 0.553141534 batch mAP 0.640594482 batch PCKh 0.4375\n",
      "Trained batch 420 batch loss 0.435968339 batch mAP 0.650299072 batch PCKh 0.875\n",
      "Trained batch 421 batch loss 0.456971169 batch mAP 0.625610352 batch PCKh 0.625\n",
      "Trained batch 422 batch loss 0.463888019 batch mAP 0.66519165 batch PCKh 0.625\n",
      "Trained batch 423 batch loss 0.59253931 batch mAP 0.598419189 batch PCKh 0.6875\n",
      "Trained batch 424 batch loss 0.626172721 batch mAP 0.566650391 batch PCKh 0.4375\n",
      "Trained batch 425 batch loss 0.603272855 batch mAP 0.484039307 batch PCKh 0.875\n",
      "Trained batch 426 batch loss 0.486276627 batch mAP 0.63684082 batch PCKh 0.875\n",
      "Trained batch 427 batch loss 0.537157059 batch mAP 0.586181641 batch PCKh 0.8125\n",
      "Trained batch 428 batch loss 0.555987954 batch mAP 0.621368408 batch PCKh 0.6875\n",
      "Trained batch 429 batch loss 0.5329808 batch mAP 0.623413086 batch PCKh 0.75\n",
      "Trained batch 430 batch loss 0.512813807 batch mAP 0.571502686 batch PCKh 0.625\n",
      "Trained batch 431 batch loss 0.456515372 batch mAP 0.603057861 batch PCKh 0.25\n",
      "Trained batch 432 batch loss 0.393023461 batch mAP 0.65411377 batch PCKh 0.75\n",
      "Trained batch 433 batch loss 0.416140497 batch mAP 0.601074219 batch PCKh 0.625\n",
      "Trained batch 434 batch loss 0.464816511 batch mAP 0.628967285 batch PCKh 0.5625\n",
      "Trained batch 435 batch loss 0.502707124 batch mAP 0.614501953 batch PCKh 0.1875\n",
      "Trained batch 436 batch loss 0.610329 batch mAP 0.586395264 batch PCKh 0.25\n",
      "Trained batch 437 batch loss 0.57301873 batch mAP 0.576629639 batch PCKh 0.3125\n",
      "Trained batch 438 batch loss 0.431268394 batch mAP 0.6121521 batch PCKh 0.375\n",
      "Trained batch 439 batch loss 0.527458906 batch mAP 0.655944824 batch PCKh 0.5625\n",
      "Trained batch 440 batch loss 0.49480778 batch mAP 0.647247314 batch PCKh 0.25\n",
      "Trained batch 441 batch loss 0.527739 batch mAP 0.639221191 batch PCKh 0.5\n",
      "Trained batch 442 batch loss 0.597227097 batch mAP 0.590942383 batch PCKh 0.375\n",
      "Trained batch 443 batch loss 0.535216868 batch mAP 0.55178833 batch PCKh 0.3125\n",
      "Trained batch 444 batch loss 0.557106733 batch mAP 0.512420654 batch PCKh 0.1875\n",
      "Trained batch 445 batch loss 0.411794275 batch mAP 0.502929688 batch PCKh 0.0625\n",
      "Trained batch 446 batch loss 0.50552392 batch mAP 0.517150879 batch PCKh 0.375\n",
      "Trained batch 447 batch loss 0.53652966 batch mAP 0.550048828 batch PCKh 0.125\n",
      "Trained batch 448 batch loss 0.60819304 batch mAP 0.490081787 batch PCKh 0.25\n",
      "Trained batch 449 batch loss 0.643333077 batch mAP 0.48526 batch PCKh 0.75\n",
      "Trained batch 450 batch loss 0.614087105 batch mAP 0.546783447 batch PCKh 0.3125\n",
      "Trained batch 451 batch loss 0.602433264 batch mAP 0.581420898 batch PCKh 0.1875\n",
      "Trained batch 452 batch loss 0.453828871 batch mAP 0.638427734 batch PCKh 0.5\n",
      "Trained batch 453 batch loss 0.355168313 batch mAP 0.734100342 batch PCKh 0.4375\n",
      "Trained batch 454 batch loss 0.431229472 batch mAP 0.734008789 batch PCKh 0.5\n",
      "Trained batch 455 batch loss 0.393303156 batch mAP 0.690765381 batch PCKh 0.5625\n",
      "Trained batch 456 batch loss 0.450657278 batch mAP 0.67489624 batch PCKh 0.75\n",
      "Trained batch 457 batch loss 0.418353379 batch mAP 0.699951172 batch PCKh 0.5\n",
      "Trained batch 458 batch loss 0.488672078 batch mAP 0.647827148 batch PCKh 0.5\n",
      "Trained batch 459 batch loss 0.40790081 batch mAP 0.688659668 batch PCKh 0.5625\n",
      "Trained batch 460 batch loss 0.447477907 batch mAP 0.653778076 batch PCKh 0.5\n",
      "Trained batch 461 batch loss 0.411388963 batch mAP 0.700958252 batch PCKh 0.4375\n",
      "Trained batch 462 batch loss 0.419769853 batch mAP 0.725158691 batch PCKh 0.75\n",
      "Trained batch 463 batch loss 0.433002949 batch mAP 0.65322876 batch PCKh 0.125\n",
      "Trained batch 464 batch loss 0.443501472 batch mAP 0.688110352 batch PCKh 0.625\n",
      "Trained batch 465 batch loss 0.444610655 batch mAP 0.637390137 batch PCKh 0.5\n",
      "Trained batch 466 batch loss 0.473863661 batch mAP 0.621734619 batch PCKh 0.625\n",
      "Trained batch 467 batch loss 0.438304484 batch mAP 0.626678467 batch PCKh 0.875\n",
      "Trained batch 468 batch loss 0.505082667 batch mAP 0.573120117 batch PCKh 0.75\n",
      "Trained batch 469 batch loss 0.423240095 batch mAP 0.671813965 batch PCKh 0.6875\n",
      "Trained batch 470 batch loss 0.507446527 batch mAP 0.636810303 batch PCKh 0.8125\n",
      "Trained batch 471 batch loss 0.480336487 batch mAP 0.555236816 batch PCKh 0.75\n",
      "Trained batch 472 batch loss 0.55093354 batch mAP 0.53314209 batch PCKh 0.875\n",
      "Trained batch 473 batch loss 0.519354582 batch mAP 0.572998047 batch PCKh 0.875\n",
      "Trained batch 474 batch loss 0.502576828 batch mAP 0.565582275 batch PCKh 0.6875\n",
      "Trained batch 475 batch loss 0.549530506 batch mAP 0.499725342 batch PCKh 0.6875\n",
      "Trained batch 476 batch loss 0.554656863 batch mAP 0.547058105 batch PCKh 0.5625\n",
      "Trained batch 477 batch loss 0.492852271 batch mAP 0.544677734 batch PCKh 0.5\n",
      "Trained batch 478 batch loss 0.523568869 batch mAP 0.556671143 batch PCKh 0.5625\n",
      "Trained batch 479 batch loss 0.503525257 batch mAP 0.567840576 batch PCKh 0.625\n",
      "Trained batch 480 batch loss 0.536841273 batch mAP 0.557281494 batch PCKh 0.625\n",
      "Trained batch 481 batch loss 0.520529211 batch mAP 0.557525635 batch PCKh 0.875\n",
      "Trained batch 482 batch loss 0.473999023 batch mAP 0.603271484 batch PCKh 0.875\n",
      "Trained batch 483 batch loss 0.594934881 batch mAP 0.607299805 batch PCKh 0.8125\n",
      "Trained batch 484 batch loss 0.556833446 batch mAP 0.572662354 batch PCKh 0.5625\n",
      "Trained batch 485 batch loss 0.465391308 batch mAP 0.589508057 batch PCKh 0.375\n",
      "Trained batch 486 batch loss 0.486636519 batch mAP 0.656524658 batch PCKh 0.4375\n",
      "Trained batch 487 batch loss 0.469392627 batch mAP 0.653015137 batch PCKh 0.5\n",
      "Trained batch 488 batch loss 0.538372517 batch mAP 0.612823486 batch PCKh 0.375\n",
      "Trained batch 489 batch loss 0.586760879 batch mAP 0.639526367 batch PCKh 0.3125\n",
      "Trained batch 490 batch loss 0.558370173 batch mAP 0.644287109 batch PCKh 0.375\n",
      "Trained batch 491 batch loss 0.456676 batch mAP 0.652099609 batch PCKh 0.875\n",
      "Trained batch 492 batch loss 0.42954576 batch mAP 0.633331299 batch PCKh 0.625\n",
      "Trained batch 493 batch loss 0.490887344 batch mAP 0.646850586 batch PCKh 0.6875\n",
      "Trained batch 494 batch loss 0.374036908 batch mAP 0.693695068 batch PCKh 0.6875\n",
      "Trained batch 495 batch loss 0.553674102 batch mAP 0.640899658 batch PCKh 0.5625\n",
      "Trained batch 496 batch loss 0.556121111 batch mAP 0.575622559 batch PCKh 0.4375\n",
      "Trained batch 497 batch loss 0.572269619 batch mAP 0.577972412 batch PCKh 0.4375\n",
      "Trained batch 498 batch loss 0.555131555 batch mAP 0.619873047 batch PCKh 0.1875\n",
      "Trained batch 499 batch loss 0.601538777 batch mAP 0.630249 batch PCKh 0.3125\n",
      "Trained batch 500 batch loss 0.53738451 batch mAP 0.623687744 batch PCKh 0.1875\n",
      "Trained batch 501 batch loss 0.497198164 batch mAP 0.649627686 batch PCKh 0.3125\n",
      "Trained batch 502 batch loss 0.522055447 batch mAP 0.639892578 batch PCKh 0.5625\n",
      "Trained batch 503 batch loss 0.500180125 batch mAP 0.673095703 batch PCKh 0.375\n",
      "Trained batch 504 batch loss 0.463931412 batch mAP 0.709533691 batch PCKh 0.5\n",
      "Trained batch 505 batch loss 0.462098598 batch mAP 0.685424805 batch PCKh 0.625\n",
      "Trained batch 506 batch loss 0.63177681 batch mAP 0.607208252 batch PCKh 0.25\n",
      "Trained batch 507 batch loss 0.556955278 batch mAP 0.667114258 batch PCKh 0.5\n",
      "Trained batch 508 batch loss 0.415115684 batch mAP 0.630950928 batch PCKh 0.25\n",
      "Trained batch 509 batch loss 0.517244339 batch mAP 0.572357178 batch PCKh 0.875\n",
      "Trained batch 510 batch loss 0.556294322 batch mAP 0.566925049 batch PCKh 0.8125\n",
      "Trained batch 511 batch loss 0.597285092 batch mAP 0.489685059 batch PCKh 0.875\n",
      "Trained batch 512 batch loss 0.566104591 batch mAP 0.464080811 batch PCKh 0.75\n",
      "Trained batch 513 batch loss 0.60405165 batch mAP 0.493804932 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 514 batch loss 0.557102 batch mAP 0.5284729 batch PCKh 0.0625\n",
      "Trained batch 515 batch loss 0.567273855 batch mAP 0.551086426 batch PCKh 0.75\n",
      "Trained batch 516 batch loss 0.504242539 batch mAP 0.562133789 batch PCKh 0.8125\n",
      "Trained batch 517 batch loss 0.43258512 batch mAP 0.609222412 batch PCKh 0.3125\n",
      "Trained batch 518 batch loss 0.418704271 batch mAP 0.546630859 batch PCKh 0.375\n",
      "Trained batch 519 batch loss 0.388620138 batch mAP 0.607849121 batch PCKh 0.4375\n",
      "Trained batch 520 batch loss 0.418539047 batch mAP 0.544158936 batch PCKh 0.25\n",
      "Trained batch 521 batch loss 0.485810369 batch mAP 0.658294678 batch PCKh 0.375\n",
      "Trained batch 522 batch loss 0.448635578 batch mAP 0.647796631 batch PCKh 0.25\n",
      "Trained batch 523 batch loss 0.544168115 batch mAP 0.664825439 batch PCKh 0.375\n",
      "Trained batch 524 batch loss 0.480063498 batch mAP 0.691833496 batch PCKh 0.625\n",
      "Trained batch 525 batch loss 0.532927096 batch mAP 0.700134277 batch PCKh 0.4375\n",
      "Trained batch 526 batch loss 0.515797496 batch mAP 0.69052124 batch PCKh 0.5\n",
      "Trained batch 527 batch loss 0.476833522 batch mAP 0.683532715 batch PCKh 0.3125\n",
      "Trained batch 528 batch loss 0.42965126 batch mAP 0.73550415 batch PCKh 0.3125\n",
      "Trained batch 529 batch loss 0.502576888 batch mAP 0.677032471 batch PCKh 0.375\n",
      "Trained batch 530 batch loss 0.461324096 batch mAP 0.627868652 batch PCKh 0.6875\n",
      "Trained batch 531 batch loss 0.50697124 batch mAP 0.645050049 batch PCKh 0.25\n",
      "Trained batch 532 batch loss 0.609576106 batch mAP 0.499023438 batch PCKh 0.4375\n",
      "Trained batch 533 batch loss 0.49491924 batch mAP 0.592895508 batch PCKh 0.6875\n",
      "Trained batch 534 batch loss 0.466699868 batch mAP 0.645355225 batch PCKh 0.6875\n",
      "Trained batch 535 batch loss 0.435883135 batch mAP 0.597076416 batch PCKh 0.625\n",
      "Trained batch 536 batch loss 0.390439421 batch mAP 0.585998535 batch PCKh 0.0625\n",
      "Trained batch 537 batch loss 0.509762466 batch mAP 0.548492432 batch PCKh 0.25\n",
      "Trained batch 538 batch loss 0.48213312 batch mAP 0.536743164 batch PCKh 0.25\n",
      "Trained batch 539 batch loss 0.461442202 batch mAP 0.419372559 batch PCKh 0.1875\n",
      "Trained batch 540 batch loss 0.516221404 batch mAP 0.416717529 batch PCKh 0.3125\n",
      "Trained batch 541 batch loss 0.474568576 batch mAP 0.494781494 batch PCKh 0.5\n",
      "Trained batch 542 batch loss 0.489847243 batch mAP 0.538635254 batch PCKh 0.4375\n",
      "Trained batch 543 batch loss 0.448946834 batch mAP 0.586181641 batch PCKh 0.375\n",
      "Trained batch 544 batch loss 0.434969515 batch mAP 0.610168457 batch PCKh 0.25\n",
      "Trained batch 545 batch loss 0.476468325 batch mAP 0.549438477 batch PCKh 0\n",
      "Trained batch 546 batch loss 0.428553522 batch mAP 0.550445557 batch PCKh 0.5625\n",
      "Trained batch 547 batch loss 0.482153833 batch mAP 0.581054688 batch PCKh 0.4375\n",
      "Trained batch 548 batch loss 0.492894292 batch mAP 0.629943848 batch PCKh 0.0625\n",
      "Trained batch 549 batch loss 0.504312098 batch mAP 0.651977539 batch PCKh 0.8125\n",
      "Trained batch 550 batch loss 0.565890551 batch mAP 0.655700684 batch PCKh 0.1875\n",
      "Trained batch 551 batch loss 0.564696491 batch mAP 0.634674072 batch PCKh 0.6875\n",
      "Trained batch 552 batch loss 0.494127303 batch mAP 0.658996582 batch PCKh 0.375\n",
      "Trained batch 553 batch loss 0.473263741 batch mAP 0.689178467 batch PCKh 0.375\n",
      "Trained batch 554 batch loss 0.527702212 batch mAP 0.702423096 batch PCKh 0.25\n",
      "Trained batch 555 batch loss 0.506610215 batch mAP 0.707305908 batch PCKh 0.75\n",
      "Trained batch 556 batch loss 0.55559206 batch mAP 0.657714844 batch PCKh 0.25\n",
      "Trained batch 557 batch loss 0.531548738 batch mAP 0.707214355 batch PCKh 0.3125\n",
      "Trained batch 558 batch loss 0.537518799 batch mAP 0.643737793 batch PCKh 0.375\n",
      "Trained batch 559 batch loss 0.545684814 batch mAP 0.576385498 batch PCKh 0.5625\n",
      "Trained batch 560 batch loss 0.569443 batch mAP 0.647644043 batch PCKh 0.4375\n",
      "Trained batch 561 batch loss 0.594273329 batch mAP 0.667785645 batch PCKh 0.4375\n",
      "Trained batch 562 batch loss 0.561879456 batch mAP 0.657562256 batch PCKh 0.5625\n",
      "Trained batch 563 batch loss 0.532776833 batch mAP 0.674102783 batch PCKh 0.6875\n",
      "Trained batch 564 batch loss 0.491861403 batch mAP 0.659545898 batch PCKh 0.375\n",
      "Trained batch 565 batch loss 0.507648289 batch mAP 0.662200928 batch PCKh 0.4375\n",
      "Trained batch 566 batch loss 0.496109843 batch mAP 0.604827881 batch PCKh 0.375\n",
      "Trained batch 567 batch loss 0.486914784 batch mAP 0.683837891 batch PCKh 0.875\n",
      "Trained batch 568 batch loss 0.480013341 batch mAP 0.673034668 batch PCKh 0.4375\n",
      "Trained batch 569 batch loss 0.533372223 batch mAP 0.669708252 batch PCKh 0.375\n",
      "Trained batch 570 batch loss 0.490567237 batch mAP 0.638366699 batch PCKh 0.4375\n",
      "Trained batch 571 batch loss 0.501443386 batch mAP 0.651184082 batch PCKh 0.5\n",
      "Trained batch 572 batch loss 0.487853616 batch mAP 0.679351807 batch PCKh 0.3125\n",
      "Trained batch 573 batch loss 0.536325574 batch mAP 0.642089844 batch PCKh 0.375\n",
      "Trained batch 574 batch loss 0.510104179 batch mAP 0.637512207 batch PCKh 0.5\n",
      "Trained batch 575 batch loss 0.461165965 batch mAP 0.720184326 batch PCKh 0.25\n",
      "Trained batch 576 batch loss 0.498745918 batch mAP 0.66619873 batch PCKh 0.4375\n",
      "Trained batch 577 batch loss 0.488351882 batch mAP 0.681121826 batch PCKh 0.875\n",
      "Trained batch 578 batch loss 0.471293569 batch mAP 0.694732666 batch PCKh 0.5625\n",
      "Trained batch 579 batch loss 0.461590171 batch mAP 0.7215271 batch PCKh 0.4375\n",
      "Trained batch 580 batch loss 0.431986034 batch mAP 0.694549561 batch PCKh 0.375\n",
      "Trained batch 581 batch loss 0.577689886 batch mAP 0.628875732 batch PCKh 0.4375\n",
      "Trained batch 582 batch loss 0.56364429 batch mAP 0.630615234 batch PCKh 0.75\n",
      "Trained batch 583 batch loss 0.47718066 batch mAP 0.645080566 batch PCKh 0.75\n",
      "Trained batch 584 batch loss 0.51638943 batch mAP 0.635894775 batch PCKh 0.625\n",
      "Trained batch 585 batch loss 0.517517924 batch mAP 0.590515137 batch PCKh 0.875\n",
      "Trained batch 586 batch loss 0.493105054 batch mAP 0.664428711 batch PCKh 0.5\n",
      "Trained batch 587 batch loss 0.456907243 batch mAP 0.68270874 batch PCKh 0.5625\n",
      "Trained batch 588 batch loss 0.500500321 batch mAP 0.573486328 batch PCKh 0.5625\n",
      "Trained batch 589 batch loss 0.493457 batch mAP 0.583862305 batch PCKh 0.6875\n",
      "Trained batch 590 batch loss 0.469885826 batch mAP 0.614837646 batch PCKh 0.5625\n",
      "Trained batch 591 batch loss 0.467793941 batch mAP 0.682861328 batch PCKh 0.3125\n",
      "Trained batch 592 batch loss 0.501863956 batch mAP 0.690094 batch PCKh 0.4375\n",
      "Trained batch 593 batch loss 0.499470741 batch mAP 0.678772 batch PCKh 0.5625\n",
      "Trained batch 594 batch loss 0.512426913 batch mAP 0.64730835 batch PCKh 0.5\n",
      "Trained batch 595 batch loss 0.491021693 batch mAP 0.59375 batch PCKh 0.625\n",
      "Trained batch 596 batch loss 0.556732416 batch mAP 0.590454102 batch PCKh 0.3125\n",
      "Trained batch 597 batch loss 0.487968475 batch mAP 0.587036133 batch PCKh 0.3125\n",
      "Trained batch 598 batch loss 0.608227 batch mAP 0.620697 batch PCKh 0.5625\n",
      "Trained batch 599 batch loss 0.566390932 batch mAP 0.613800049 batch PCKh 0.8125\n",
      "Trained batch 600 batch loss 0.595484853 batch mAP 0.588439941 batch PCKh 0.8125\n",
      "Trained batch 601 batch loss 0.570881486 batch mAP 0.587524414 batch PCKh 0.6875\n",
      "Trained batch 602 batch loss 0.593159616 batch mAP 0.591217041 batch PCKh 0.3125\n",
      "Trained batch 603 batch loss 0.543649197 batch mAP 0.593170166 batch PCKh 0.8125\n",
      "Trained batch 604 batch loss 0.537896156 batch mAP 0.596405 batch PCKh 0.125\n",
      "Trained batch 605 batch loss 0.514724851 batch mAP 0.6144104 batch PCKh 0.625\n",
      "Trained batch 606 batch loss 0.438443542 batch mAP 0.680206299 batch PCKh 0.625\n",
      "Trained batch 607 batch loss 0.506669044 batch mAP 0.612243652 batch PCKh 0.625\n",
      "Trained batch 608 batch loss 0.510998249 batch mAP 0.569549561 batch PCKh 0.75\n",
      "Trained batch 609 batch loss 0.52970314 batch mAP 0.537078857 batch PCKh 0.3125\n",
      "Trained batch 610 batch loss 0.431194931 batch mAP 0.564758301 batch PCKh 0.25\n",
      "Trained batch 611 batch loss 0.531107903 batch mAP 0.515045166 batch PCKh 0.4375\n",
      "Trained batch 612 batch loss 0.496617675 batch mAP 0.475738525 batch PCKh 0\n",
      "Trained batch 613 batch loss 0.606893063 batch mAP 0.48727417 batch PCKh 0.1875\n",
      "Trained batch 614 batch loss 0.604581416 batch mAP 0.55645752 batch PCKh 0.1875\n",
      "Trained batch 615 batch loss 0.471518487 batch mAP 0.58984375 batch PCKh 0.5625\n",
      "Trained batch 616 batch loss 0.487235844 batch mAP 0.553100586 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 617 batch loss 0.596167564 batch mAP 0.579406738 batch PCKh 0.5\n",
      "Trained batch 618 batch loss 0.507086277 batch mAP 0.582855225 batch PCKh 0.0625\n",
      "Trained batch 619 batch loss 0.552409172 batch mAP 0.630889893 batch PCKh 0.75\n",
      "Trained batch 620 batch loss 0.409882843 batch mAP 0.635314941 batch PCKh 0.75\n",
      "Trained batch 621 batch loss 0.460694 batch mAP 0.645507812 batch PCKh 0.875\n",
      "Trained batch 622 batch loss 0.428141564 batch mAP 0.618865967 batch PCKh 0.4375\n",
      "Trained batch 623 batch loss 0.4540295 batch mAP 0.637298584 batch PCKh 0.5\n",
      "Trained batch 624 batch loss 0.43893376 batch mAP 0.594146729 batch PCKh 0.75\n",
      "Trained batch 625 batch loss 0.451493591 batch mAP 0.588195801 batch PCKh 0.75\n",
      "Trained batch 626 batch loss 0.520793319 batch mAP 0.537353516 batch PCKh 0.8125\n",
      "Trained batch 627 batch loss 0.509568036 batch mAP 0.630096436 batch PCKh 0.5625\n",
      "Trained batch 628 batch loss 0.517347097 batch mAP 0.602844238 batch PCKh 0.75\n",
      "Trained batch 629 batch loss 0.479875118 batch mAP 0.637542725 batch PCKh 0.3125\n",
      "Trained batch 630 batch loss 0.533964097 batch mAP 0.664001465 batch PCKh 0.5\n",
      "Trained batch 631 batch loss 0.455821246 batch mAP 0.689849854 batch PCKh 0.5\n",
      "Trained batch 632 batch loss 0.439025432 batch mAP 0.654602051 batch PCKh 0.4375\n",
      "Trained batch 633 batch loss 0.388221294 batch mAP 0.607421875 batch PCKh 0.6875\n",
      "Trained batch 634 batch loss 0.474398077 batch mAP 0.546081543 batch PCKh 0.5\n",
      "Trained batch 635 batch loss 0.423879355 batch mAP 0.608306885 batch PCKh 0.75\n",
      "Trained batch 636 batch loss 0.490015984 batch mAP 0.610137939 batch PCKh 0.6875\n",
      "Trained batch 637 batch loss 0.565678179 batch mAP 0.555236816 batch PCKh 0.4375\n",
      "Trained batch 638 batch loss 0.682888508 batch mAP 0.50100708 batch PCKh 0.5625\n",
      "Trained batch 639 batch loss 0.540600479 batch mAP 0.577148438 batch PCKh 0.625\n",
      "Trained batch 640 batch loss 0.467844665 batch mAP 0.606414795 batch PCKh 0.5625\n",
      "Trained batch 641 batch loss 0.410859108 batch mAP 0.607208252 batch PCKh 0.6875\n",
      "Trained batch 642 batch loss 0.453518182 batch mAP 0.581207275 batch PCKh 0.6875\n",
      "Trained batch 643 batch loss 0.538679123 batch mAP 0.521698 batch PCKh 0.75\n",
      "Trained batch 644 batch loss 0.447889984 batch mAP 0.542419434 batch PCKh 0.625\n",
      "Trained batch 645 batch loss 0.321336746 batch mAP 0.700531 batch PCKh 0.625\n",
      "Trained batch 646 batch loss 0.319889516 batch mAP 0.767578125 batch PCKh 0.5625\n",
      "Trained batch 647 batch loss 0.298554301 batch mAP 0.740570068 batch PCKh 0.5\n",
      "Trained batch 648 batch loss 0.34549737 batch mAP 0.761749268 batch PCKh 0.75\n",
      "Trained batch 649 batch loss 0.387488 batch mAP 0.729553223 batch PCKh 0.5625\n",
      "Trained batch 650 batch loss 0.340823412 batch mAP 0.733154297 batch PCKh 0.3125\n",
      "Trained batch 651 batch loss 0.397950709 batch mAP 0.702636719 batch PCKh 0.625\n",
      "Trained batch 652 batch loss 0.494134605 batch mAP 0.647979736 batch PCKh 0.5\n",
      "Trained batch 653 batch loss 0.48062551 batch mAP 0.655395508 batch PCKh 0.3125\n",
      "Trained batch 654 batch loss 0.562606633 batch mAP 0.621673584 batch PCKh 0.6875\n",
      "Trained batch 655 batch loss 0.50711441 batch mAP 0.63394165 batch PCKh 0.75\n",
      "Trained batch 656 batch loss 0.583847821 batch mAP 0.639556885 batch PCKh 0.5\n",
      "Trained batch 657 batch loss 0.617071629 batch mAP 0.60446167 batch PCKh 0.375\n",
      "Trained batch 658 batch loss 0.545692 batch mAP 0.574584961 batch PCKh 0.75\n",
      "Trained batch 659 batch loss 0.514259934 batch mAP 0.629119873 batch PCKh 0.875\n",
      "Trained batch 660 batch loss 0.511587381 batch mAP 0.608154297 batch PCKh 0.6875\n",
      "Trained batch 661 batch loss 0.516631305 batch mAP 0.569915771 batch PCKh 0.5625\n",
      "Trained batch 662 batch loss 0.49333483 batch mAP 0.574554443 batch PCKh 0.5\n",
      "Trained batch 663 batch loss 0.63065207 batch mAP 0.549133301 batch PCKh 0.875\n",
      "Trained batch 664 batch loss 0.596784294 batch mAP 0.560760498 batch PCKh 0.8125\n",
      "Trained batch 665 batch loss 0.488909721 batch mAP 0.651092529 batch PCKh 0.6875\n",
      "Trained batch 666 batch loss 0.611646652 batch mAP 0.587310791 batch PCKh 0.75\n",
      "Trained batch 667 batch loss 0.529433 batch mAP 0.592346191 batch PCKh 0.8125\n",
      "Trained batch 668 batch loss 0.560124159 batch mAP 0.545623779 batch PCKh 0.0625\n",
      "Trained batch 669 batch loss 0.521666646 batch mAP 0.596740723 batch PCKh 0.625\n",
      "Trained batch 670 batch loss 0.542694449 batch mAP 0.61328125 batch PCKh 0.6875\n",
      "Trained batch 671 batch loss 0.454953939 batch mAP 0.586975098 batch PCKh 0.125\n",
      "Trained batch 672 batch loss 0.55138123 batch mAP 0.601196289 batch PCKh 0.5\n",
      "Trained batch 673 batch loss 0.536768615 batch mAP 0.578430176 batch PCKh 0.625\n",
      "Trained batch 674 batch loss 0.586351633 batch mAP 0.582519531 batch PCKh 0.25\n",
      "Trained batch 675 batch loss 0.533453882 batch mAP 0.573852539 batch PCKh 0.875\n",
      "Trained batch 676 batch loss 0.60814923 batch mAP 0.610565186 batch PCKh 0.4375\n",
      "Trained batch 677 batch loss 0.550590277 batch mAP 0.665008545 batch PCKh 0.4375\n",
      "Trained batch 678 batch loss 0.548839152 batch mAP 0.577209473 batch PCKh 0.5625\n",
      "Trained batch 679 batch loss 0.544145882 batch mAP 0.609069824 batch PCKh 0.75\n",
      "Trained batch 680 batch loss 0.550994 batch mAP 0.679321289 batch PCKh 0.625\n",
      "Trained batch 681 batch loss 0.514902 batch mAP 0.669647217 batch PCKh 0.5625\n",
      "Trained batch 682 batch loss 0.432608187 batch mAP 0.70010376 batch PCKh 0.375\n",
      "Trained batch 683 batch loss 0.332736731 batch mAP 0.694641113 batch PCKh 0.3125\n",
      "Trained batch 684 batch loss 0.398638338 batch mAP 0.736084 batch PCKh 0.4375\n",
      "Trained batch 685 batch loss 0.392716229 batch mAP 0.708068848 batch PCKh 0.25\n",
      "Trained batch 686 batch loss 0.477122128 batch mAP 0.693573 batch PCKh 0.3125\n",
      "Trained batch 687 batch loss 0.432168454 batch mAP 0.7003479 batch PCKh 0.3125\n",
      "Trained batch 688 batch loss 0.516760111 batch mAP 0.692626953 batch PCKh 0.1875\n",
      "Trained batch 689 batch loss 0.507341087 batch mAP 0.637023926 batch PCKh 0.5\n",
      "Trained batch 690 batch loss 0.532476962 batch mAP 0.593780518 batch PCKh 0.6875\n",
      "Trained batch 691 batch loss 0.634303212 batch mAP 0.590209961 batch PCKh 0.125\n",
      "Trained batch 692 batch loss 0.555407465 batch mAP 0.495666504 batch PCKh 0.1875\n",
      "Trained batch 693 batch loss 0.555411816 batch mAP 0.622497559 batch PCKh 0.1875\n",
      "Trained batch 694 batch loss 0.544624448 batch mAP 0.591430664 batch PCKh 0.25\n",
      "Trained batch 695 batch loss 0.693028092 batch mAP 0.584075928 batch PCKh 0.1875\n",
      "Trained batch 696 batch loss 0.651525259 batch mAP 0.527008057 batch PCKh 0.5\n",
      "Trained batch 697 batch loss 0.564875901 batch mAP 0.662414551 batch PCKh 0.75\n",
      "Trained batch 698 batch loss 0.501767755 batch mAP 0.664825439 batch PCKh 0.8125\n",
      "Trained batch 699 batch loss 0.636859179 batch mAP 0.595916748 batch PCKh 0.4375\n",
      "Trained batch 700 batch loss 0.537282825 batch mAP 0.615264893 batch PCKh 0.75\n",
      "Trained batch 701 batch loss 0.578689814 batch mAP 0.597808838 batch PCKh 0.5625\n",
      "Trained batch 702 batch loss 0.645897269 batch mAP 0.524230957 batch PCKh 0.5625\n",
      "Trained batch 703 batch loss 0.579385281 batch mAP 0.515380859 batch PCKh 0.25\n",
      "Trained batch 704 batch loss 0.555541039 batch mAP 0.581207275 batch PCKh 0.75\n",
      "Trained batch 705 batch loss 0.645459592 batch mAP 0.568603516 batch PCKh 0.75\n",
      "Trained batch 706 batch loss 0.532324 batch mAP 0.575286865 batch PCKh 0.5625\n",
      "Trained batch 707 batch loss 0.457587838 batch mAP 0.578369141 batch PCKh 0.3125\n",
      "Trained batch 708 batch loss 0.4944655 batch mAP 0.569397 batch PCKh 0.25\n",
      "Trained batch 709 batch loss 0.503233314 batch mAP 0.645080566 batch PCKh 0.5\n",
      "Trained batch 710 batch loss 0.518733084 batch mAP 0.609161377 batch PCKh 0.3125\n",
      "Trained batch 711 batch loss 0.562690496 batch mAP 0.573120117 batch PCKh 0.3125\n",
      "Trained batch 712 batch loss 0.459932745 batch mAP 0.583496094 batch PCKh 0.75\n",
      "Trained batch 713 batch loss 0.494976729 batch mAP 0.605194092 batch PCKh 0.3125\n",
      "Trained batch 714 batch loss 0.542867184 batch mAP 0.537323 batch PCKh 0.3125\n",
      "Trained batch 715 batch loss 0.507985294 batch mAP 0.584442139 batch PCKh 0.625\n",
      "Trained batch 716 batch loss 0.555649459 batch mAP 0.590606689 batch PCKh 0.875\n",
      "Trained batch 717 batch loss 0.511326432 batch mAP 0.617980957 batch PCKh 0.625\n",
      "Trained batch 718 batch loss 0.585057735 batch mAP 0.561981201 batch PCKh 0.75\n",
      "Trained batch 719 batch loss 0.546644807 batch mAP 0.569488525 batch PCKh 0.3125\n",
      "Trained batch 720 batch loss 0.593591869 batch mAP 0.581390381 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 721 batch loss 0.571222246 batch mAP 0.558837891 batch PCKh 0.4375\n",
      "Trained batch 722 batch loss 0.518647075 batch mAP 0.588989258 batch PCKh 0.4375\n",
      "Trained batch 723 batch loss 0.473593116 batch mAP 0.624511719 batch PCKh 0.5\n",
      "Trained batch 724 batch loss 0.550537646 batch mAP 0.601013184 batch PCKh 0.75\n",
      "Trained batch 725 batch loss 0.616642177 batch mAP 0.592712402 batch PCKh 0.375\n",
      "Trained batch 726 batch loss 0.465387285 batch mAP 0.71295166 batch PCKh 0.5625\n",
      "Trained batch 727 batch loss 0.433155537 batch mAP 0.674346924 batch PCKh 0.5\n",
      "Trained batch 728 batch loss 0.413477272 batch mAP 0.682830811 batch PCKh 0.625\n",
      "Trained batch 729 batch loss 0.400076181 batch mAP 0.671234131 batch PCKh 0.625\n",
      "Trained batch 730 batch loss 0.397884905 batch mAP 0.637115479 batch PCKh 0.625\n",
      "Trained batch 731 batch loss 0.517519116 batch mAP 0.58682251 batch PCKh 0.3125\n",
      "Trained batch 732 batch loss 0.554393828 batch mAP 0.501403809 batch PCKh 0.5\n",
      "Trained batch 733 batch loss 0.495788664 batch mAP 0.577453613 batch PCKh 0.5625\n",
      "Trained batch 734 batch loss 0.576603889 batch mAP 0.52746582 batch PCKh 0.125\n",
      "Trained batch 735 batch loss 0.522782505 batch mAP 0.603027344 batch PCKh 0.1875\n",
      "Trained batch 736 batch loss 0.52554214 batch mAP 0.691436768 batch PCKh 0.1875\n",
      "Trained batch 737 batch loss 0.493650198 batch mAP 0.704284668 batch PCKh 0.1875\n",
      "Trained batch 738 batch loss 0.514666438 batch mAP 0.618530273 batch PCKh 0.8125\n",
      "Trained batch 739 batch loss 0.449476242 batch mAP 0.648345947 batch PCKh 0.625\n",
      "Trained batch 740 batch loss 0.521659732 batch mAP 0.68170166 batch PCKh 0.5\n",
      "Trained batch 741 batch loss 0.438997865 batch mAP 0.706848145 batch PCKh 0.8125\n",
      "Trained batch 742 batch loss 0.463745415 batch mAP 0.661132812 batch PCKh 0.5625\n",
      "Trained batch 743 batch loss 0.523519754 batch mAP 0.62020874 batch PCKh 0.5\n",
      "Trained batch 744 batch loss 0.406569839 batch mAP 0.70880127 batch PCKh 0.4375\n",
      "Trained batch 745 batch loss 0.43366611 batch mAP 0.684295654 batch PCKh 0.5\n",
      "Trained batch 746 batch loss 0.567245185 batch mAP 0.652404785 batch PCKh 0.75\n",
      "Trained batch 747 batch loss 0.466046393 batch mAP 0.691803 batch PCKh 0.4375\n",
      "Trained batch 748 batch loss 0.391590029 batch mAP 0.7315979 batch PCKh 0.4375\n",
      "Trained batch 749 batch loss 0.470129877 batch mAP 0.650360107 batch PCKh 0.5\n",
      "Trained batch 750 batch loss 0.549824893 batch mAP 0.569885254 batch PCKh 0.5\n",
      "Trained batch 751 batch loss 0.488738418 batch mAP 0.549804688 batch PCKh 0\n",
      "Trained batch 752 batch loss 0.495943189 batch mAP 0.66998291 batch PCKh 0.375\n",
      "Trained batch 753 batch loss 0.426919341 batch mAP 0.502075195 batch PCKh 0.1875\n",
      "Trained batch 754 batch loss 0.463311851 batch mAP 0.4737854 batch PCKh 0.1875\n",
      "Trained batch 755 batch loss 0.402959406 batch mAP 0.532012939 batch PCKh 0.1875\n",
      "Trained batch 756 batch loss 0.406009108 batch mAP 0.523284912 batch PCKh 0.375\n",
      "Trained batch 757 batch loss 0.296977401 batch mAP 0.589050293 batch PCKh 0\n",
      "Trained batch 758 batch loss 0.416148543 batch mAP 0.585784912 batch PCKh 0.3125\n",
      "Trained batch 759 batch loss 0.390316486 batch mAP 0.670715332 batch PCKh 0.125\n",
      "Trained batch 760 batch loss 0.531616092 batch mAP 0.634277344 batch PCKh 0.0625\n",
      "Trained batch 761 batch loss 0.565726519 batch mAP 0.621459961 batch PCKh 0.125\n",
      "Trained batch 762 batch loss 0.542723477 batch mAP 0.599151611 batch PCKh 0.6875\n",
      "Trained batch 763 batch loss 0.477874577 batch mAP 0.645813 batch PCKh 0.1875\n",
      "Trained batch 764 batch loss 0.445623457 batch mAP 0.650970459 batch PCKh 0.375\n",
      "Trained batch 765 batch loss 0.535218 batch mAP 0.614654541 batch PCKh 0.25\n",
      "Trained batch 766 batch loss 0.440036446 batch mAP 0.519928 batch PCKh 0.375\n",
      "Trained batch 767 batch loss 0.46166116 batch mAP 0.547302246 batch PCKh 0.5625\n",
      "Trained batch 768 batch loss 0.495794594 batch mAP 0.614074707 batch PCKh 0.75\n",
      "Trained batch 769 batch loss 0.397272766 batch mAP 0.651824951 batch PCKh 0.75\n",
      "Trained batch 770 batch loss 0.493915617 batch mAP 0.606872559 batch PCKh 0.5625\n",
      "Trained batch 771 batch loss 0.372458786 batch mAP 0.661590576 batch PCKh 0.6875\n",
      "Trained batch 772 batch loss 0.433490217 batch mAP 0.573761 batch PCKh 0.625\n",
      "Trained batch 773 batch loss 0.434639126 batch mAP 0.608917236 batch PCKh 0.6875\n",
      "Trained batch 774 batch loss 0.421431 batch mAP 0.574066162 batch PCKh 0.75\n",
      "Trained batch 775 batch loss 0.424862057 batch mAP 0.592865 batch PCKh 0.375\n",
      "Trained batch 776 batch loss 0.521960676 batch mAP 0.574707031 batch PCKh 0.125\n",
      "Trained batch 777 batch loss 0.481311023 batch mAP 0.533844 batch PCKh 0.25\n",
      "Trained batch 778 batch loss 0.49561 batch mAP 0.540679932 batch PCKh 0.5625\n",
      "Trained batch 779 batch loss 0.519860506 batch mAP 0.520385742 batch PCKh 0.5\n",
      "Trained batch 780 batch loss 0.532358289 batch mAP 0.560333252 batch PCKh 0.4375\n",
      "Trained batch 781 batch loss 0.495431036 batch mAP 0.65448 batch PCKh 0.625\n",
      "Trained batch 782 batch loss 0.55128485 batch mAP 0.690185547 batch PCKh 0.8125\n",
      "Trained batch 783 batch loss 0.620043159 batch mAP 0.640960693 batch PCKh 0.8125\n",
      "Trained batch 784 batch loss 0.545854032 batch mAP 0.632843 batch PCKh 0.4375\n",
      "Trained batch 785 batch loss 0.535891533 batch mAP 0.647216797 batch PCKh 0.5625\n",
      "Trained batch 786 batch loss 0.512148 batch mAP 0.654968262 batch PCKh 0.75\n",
      "Trained batch 787 batch loss 0.594802558 batch mAP 0.660400391 batch PCKh 0.4375\n",
      "Trained batch 788 batch loss 0.568677723 batch mAP 0.604095459 batch PCKh 0.3125\n",
      "Trained batch 789 batch loss 0.543056965 batch mAP 0.63861084 batch PCKh 0.625\n",
      "Trained batch 790 batch loss 0.487051487 batch mAP 0.61517334 batch PCKh 0.875\n",
      "Trained batch 791 batch loss 0.555160284 batch mAP 0.639465332 batch PCKh 0.8125\n",
      "Trained batch 792 batch loss 0.535912693 batch mAP 0.607513428 batch PCKh 0.4375\n",
      "Trained batch 793 batch loss 0.592870355 batch mAP 0.544036865 batch PCKh 0.4375\n",
      "Trained batch 794 batch loss 0.482857615 batch mAP 0.53717041 batch PCKh 0.875\n",
      "Trained batch 795 batch loss 0.48753649 batch mAP 0.551757812 batch PCKh 0.6875\n",
      "Trained batch 796 batch loss 0.478814185 batch mAP 0.598876953 batch PCKh 0.5\n",
      "Trained batch 797 batch loss 0.4782058 batch mAP 0.586700439 batch PCKh 0.5\n",
      "Trained batch 798 batch loss 0.413892567 batch mAP 0.602325439 batch PCKh 0.5\n",
      "Trained batch 799 batch loss 0.488947392 batch mAP 0.53237915 batch PCKh 0.625\n",
      "Trained batch 800 batch loss 0.503171563 batch mAP 0.589538574 batch PCKh 0.75\n",
      "Trained batch 801 batch loss 0.522968054 batch mAP 0.537628174 batch PCKh 0.5625\n",
      "Trained batch 802 batch loss 0.523512781 batch mAP 0.568939209 batch PCKh 0.25\n",
      "Trained batch 803 batch loss 0.467731297 batch mAP 0.569732666 batch PCKh 0.75\n",
      "Trained batch 804 batch loss 0.557806492 batch mAP 0.606628418 batch PCKh 0.875\n",
      "Trained batch 805 batch loss 0.572121322 batch mAP 0.556182861 batch PCKh 0.625\n",
      "Trained batch 806 batch loss 0.5446257 batch mAP 0.56552124 batch PCKh 0.25\n",
      "Trained batch 807 batch loss 0.51158917 batch mAP 0.570343 batch PCKh 0.1875\n",
      "Trained batch 808 batch loss 0.491916716 batch mAP 0.593994141 batch PCKh 0.375\n",
      "Trained batch 809 batch loss 0.488386065 batch mAP 0.670715332 batch PCKh 0.75\n",
      "Trained batch 810 batch loss 0.497962475 batch mAP 0.654907227 batch PCKh 0.5625\n",
      "Trained batch 811 batch loss 0.515898824 batch mAP 0.637390137 batch PCKh 0.6875\n",
      "Trained batch 812 batch loss 0.523822069 batch mAP 0.578918457 batch PCKh 0.625\n",
      "Trained batch 813 batch loss 0.481960595 batch mAP 0.63571167 batch PCKh 0.5\n",
      "Trained batch 814 batch loss 0.49588418 batch mAP 0.592010498 batch PCKh 0.6875\n",
      "Trained batch 815 batch loss 0.471411884 batch mAP 0.592498779 batch PCKh 0.5\n",
      "Trained batch 816 batch loss 0.394891202 batch mAP 0.619262695 batch PCKh 0.1875\n",
      "Trained batch 817 batch loss 0.420584351 batch mAP 0.640319824 batch PCKh 0.75\n",
      "Trained batch 818 batch loss 0.548045635 batch mAP 0.573425293 batch PCKh 0.6875\n",
      "Trained batch 819 batch loss 0.526863217 batch mAP 0.584533691 batch PCKh 0.6875\n",
      "Trained batch 820 batch loss 0.559011459 batch mAP 0.568634033 batch PCKh 0.1875\n",
      "Trained batch 821 batch loss 0.615734935 batch mAP 0.524749756 batch PCKh 0.6875\n",
      "Trained batch 822 batch loss 0.503510475 batch mAP 0.549743652 batch PCKh 0.3125\n",
      "Trained batch 823 batch loss 0.537056506 batch mAP 0.610809326 batch PCKh 0.1875\n",
      "Trained batch 824 batch loss 0.506743968 batch mAP 0.686859131 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 825 batch loss 0.45050928 batch mAP 0.669494629 batch PCKh 0.75\n",
      "Trained batch 826 batch loss 0.494521171 batch mAP 0.639526367 batch PCKh 0.875\n",
      "Trained batch 827 batch loss 0.507610321 batch mAP 0.611602783 batch PCKh 0.8125\n",
      "Trained batch 828 batch loss 0.530716 batch mAP 0.562133789 batch PCKh 0.75\n",
      "Trained batch 829 batch loss 0.513576865 batch mAP 0.620483398 batch PCKh 0.75\n",
      "Trained batch 830 batch loss 0.529738784 batch mAP 0.604217529 batch PCKh 0.25\n",
      "Trained batch 831 batch loss 0.528507769 batch mAP 0.625488281 batch PCKh 0.75\n",
      "Trained batch 832 batch loss 0.561931849 batch mAP 0.666229248 batch PCKh 0.5625\n",
      "Trained batch 833 batch loss 0.598838389 batch mAP 0.594085693 batch PCKh 0.625\n",
      "Trained batch 834 batch loss 0.534227073 batch mAP 0.6171875 batch PCKh 0.75\n",
      "Trained batch 835 batch loss 0.616322279 batch mAP 0.581573486 batch PCKh 0.25\n",
      "Trained batch 836 batch loss 0.609495938 batch mAP 0.57547 batch PCKh 0.75\n",
      "Trained batch 837 batch loss 0.671733618 batch mAP 0.588378906 batch PCKh 0.0625\n",
      "Trained batch 838 batch loss 0.659460425 batch mAP 0.600219727 batch PCKh 0.125\n",
      "Trained batch 839 batch loss 0.621656418 batch mAP 0.590271 batch PCKh 0.625\n",
      "Trained batch 840 batch loss 0.452771425 batch mAP 0.483215332 batch PCKh 0.3125\n",
      "Trained batch 841 batch loss 0.44627887 batch mAP 0.501342773 batch PCKh 0.25\n",
      "Trained batch 842 batch loss 0.35071066 batch mAP 0.597686768 batch PCKh 0.1875\n",
      "Trained batch 843 batch loss 0.556232095 batch mAP 0.559783936 batch PCKh 0.3125\n",
      "Trained batch 844 batch loss 0.51578033 batch mAP 0.603607178 batch PCKh 0.5\n",
      "Trained batch 845 batch loss 0.542020857 batch mAP 0.594421387 batch PCKh 0.375\n",
      "Trained batch 846 batch loss 0.473345846 batch mAP 0.625244141 batch PCKh 0.5\n",
      "Trained batch 847 batch loss 0.470275819 batch mAP 0.63192749 batch PCKh 0.5625\n",
      "Trained batch 848 batch loss 0.478427 batch mAP 0.617767334 batch PCKh 0.3125\n",
      "Trained batch 849 batch loss 0.396509498 batch mAP 0.658233643 batch PCKh 0.0625\n",
      "Trained batch 850 batch loss 0.436900616 batch mAP 0.689666748 batch PCKh 0.25\n",
      "Trained batch 851 batch loss 0.417337537 batch mAP 0.687591553 batch PCKh 0.875\n",
      "Trained batch 852 batch loss 0.421412706 batch mAP 0.727783203 batch PCKh 0.3125\n",
      "Trained batch 853 batch loss 0.383612901 batch mAP 0.723846436 batch PCKh 0.375\n",
      "Trained batch 854 batch loss 0.426995635 batch mAP 0.675964355 batch PCKh 0.375\n",
      "Trained batch 855 batch loss 0.421946466 batch mAP 0.685302734 batch PCKh 0.3125\n",
      "Trained batch 856 batch loss 0.348388195 batch mAP 0.727050781 batch PCKh 0.375\n",
      "Trained batch 857 batch loss 0.400168777 batch mAP 0.723724365 batch PCKh 0.25\n",
      "Trained batch 858 batch loss 0.454390913 batch mAP 0.677337646 batch PCKh 0.25\n",
      "Trained batch 859 batch loss 0.411132187 batch mAP 0.685699463 batch PCKh 0.5625\n",
      "Trained batch 860 batch loss 0.467322499 batch mAP 0.632171631 batch PCKh 0.3125\n",
      "Trained batch 861 batch loss 0.603617907 batch mAP 0.59274292 batch PCKh 0.4375\n",
      "Trained batch 862 batch loss 0.535366178 batch mAP 0.624969482 batch PCKh 0.5\n",
      "Trained batch 863 batch loss 0.506168425 batch mAP 0.639068604 batch PCKh 0.375\n",
      "Trained batch 864 batch loss 0.394082278 batch mAP 0.702270508 batch PCKh 0.3125\n",
      "Trained batch 865 batch loss 0.368224263 batch mAP 0.694488525 batch PCKh 0.375\n",
      "Trained batch 866 batch loss 0.383574158 batch mAP 0.696563721 batch PCKh 0.375\n",
      "Trained batch 867 batch loss 0.468592942 batch mAP 0.625152588 batch PCKh 0.5625\n",
      "Trained batch 868 batch loss 0.519471884 batch mAP 0.645690918 batch PCKh 0.3125\n",
      "Trained batch 869 batch loss 0.589324832 batch mAP 0.644104 batch PCKh 0.3125\n",
      "Trained batch 870 batch loss 0.605446637 batch mAP 0.641723633 batch PCKh 0.4375\n",
      "Trained batch 871 batch loss 0.533434033 batch mAP 0.615783691 batch PCKh 0.1875\n",
      "Trained batch 872 batch loss 0.480275214 batch mAP 0.657867432 batch PCKh 0.6875\n",
      "Trained batch 873 batch loss 0.534822702 batch mAP 0.650817871 batch PCKh 0.375\n",
      "Trained batch 874 batch loss 0.564621627 batch mAP 0.594848633 batch PCKh 0.375\n",
      "Trained batch 875 batch loss 0.539518714 batch mAP 0.683013916 batch PCKh 0.6875\n",
      "Trained batch 876 batch loss 0.599174857 batch mAP 0.615966797 batch PCKh 0.8125\n",
      "Trained batch 877 batch loss 0.537329197 batch mAP 0.615631104 batch PCKh 0.25\n",
      "Trained batch 878 batch loss 0.524749219 batch mAP 0.587921143 batch PCKh 0.25\n",
      "Trained batch 879 batch loss 0.507505715 batch mAP 0.647064209 batch PCKh 0.1875\n",
      "Trained batch 880 batch loss 0.598585248 batch mAP 0.530639648 batch PCKh 0.3125\n",
      "Trained batch 881 batch loss 0.568632662 batch mAP 0.595855713 batch PCKh 0.375\n",
      "Trained batch 882 batch loss 0.526037276 batch mAP 0.55166626 batch PCKh 0.75\n",
      "Trained batch 883 batch loss 0.456442475 batch mAP 0.645446777 batch PCKh 0.875\n",
      "Trained batch 884 batch loss 0.563530564 batch mAP 0.577056885 batch PCKh 0.5\n",
      "Trained batch 885 batch loss 0.576252818 batch mAP 0.555328369 batch PCKh 0.5625\n",
      "Trained batch 886 batch loss 0.617158532 batch mAP 0.466674805 batch PCKh 0.5\n",
      "Trained batch 887 batch loss 0.571326137 batch mAP 0.556793213 batch PCKh 0.5\n",
      "Trained batch 888 batch loss 0.639371097 batch mAP 0.533111572 batch PCKh 0.4375\n",
      "Trained batch 889 batch loss 0.569211066 batch mAP 0.575286865 batch PCKh 0.625\n",
      "Trained batch 890 batch loss 0.594822526 batch mAP 0.532745361 batch PCKh 0.6875\n",
      "Trained batch 891 batch loss 0.512268603 batch mAP 0.42590332 batch PCKh 0.625\n",
      "Trained batch 892 batch loss 0.492714047 batch mAP 0.449920654 batch PCKh 0.5\n",
      "Trained batch 893 batch loss 0.516308069 batch mAP 0.47064209 batch PCKh 0.75\n",
      "Trained batch 894 batch loss 0.497785866 batch mAP 0.547271729 batch PCKh 0.5\n",
      "Trained batch 895 batch loss 0.517256618 batch mAP 0.528900146 batch PCKh 0.75\n",
      "Trained batch 896 batch loss 0.511493444 batch mAP 0.584106445 batch PCKh 0.625\n",
      "Trained batch 897 batch loss 0.579389691 batch mAP 0.514709473 batch PCKh 0.5\n",
      "Trained batch 898 batch loss 0.507810295 batch mAP 0.577941895 batch PCKh 0.6875\n",
      "Trained batch 899 batch loss 0.528880715 batch mAP 0.626525879 batch PCKh 0.25\n",
      "Trained batch 900 batch loss 0.517018437 batch mAP 0.573150635 batch PCKh 0.5\n",
      "Trained batch 901 batch loss 0.564590096 batch mAP 0.595306396 batch PCKh 0.75\n",
      "Trained batch 902 batch loss 0.533229709 batch mAP 0.614807129 batch PCKh 0.5\n",
      "Trained batch 903 batch loss 0.536050439 batch mAP 0.595977783 batch PCKh 0.4375\n",
      "Trained batch 904 batch loss 0.508542776 batch mAP 0.627838135 batch PCKh 0.25\n",
      "Trained batch 905 batch loss 0.51610297 batch mAP 0.567962646 batch PCKh 0.75\n",
      "Trained batch 906 batch loss 0.645458698 batch mAP 0.543426514 batch PCKh 0.25\n",
      "Trained batch 907 batch loss 0.516819239 batch mAP 0.667511 batch PCKh 0.625\n",
      "Trained batch 908 batch loss 0.57058841 batch mAP 0.59161377 batch PCKh 0.1875\n",
      "Trained batch 909 batch loss 0.527821541 batch mAP 0.657867432 batch PCKh 0.375\n",
      "Trained batch 910 batch loss 0.5072577 batch mAP 0.633178711 batch PCKh 0.5625\n",
      "Trained batch 911 batch loss 0.637092292 batch mAP 0.544250488 batch PCKh 0.375\n",
      "Trained batch 912 batch loss 0.573895037 batch mAP 0.617462158 batch PCKh 0.375\n",
      "Trained batch 913 batch loss 0.622138441 batch mAP 0.611206055 batch PCKh 0.3125\n",
      "Trained batch 914 batch loss 0.590453744 batch mAP 0.608337402 batch PCKh 0.5\n",
      "Trained batch 915 batch loss 0.615069449 batch mAP 0.547699 batch PCKh 0.4375\n",
      "Trained batch 916 batch loss 0.559188485 batch mAP 0.612060547 batch PCKh 0.75\n",
      "Trained batch 917 batch loss 0.556354463 batch mAP 0.569152832 batch PCKh 0.625\n",
      "Trained batch 918 batch loss 0.60485816 batch mAP 0.641326904 batch PCKh 0.5\n",
      "Trained batch 919 batch loss 0.451326549 batch mAP 0.551391602 batch PCKh 0.625\n",
      "Trained batch 920 batch loss 0.520229697 batch mAP 0.345550537 batch PCKh 0.25\n",
      "Trained batch 921 batch loss 0.527742267 batch mAP 0.370758057 batch PCKh 0.5\n",
      "Trained batch 922 batch loss 0.528400481 batch mAP 0.31628418 batch PCKh 0.75\n",
      "Trained batch 923 batch loss 0.52251929 batch mAP 0.348480225 batch PCKh 0.75\n",
      "Trained batch 924 batch loss 0.54555136 batch mAP 0.376220703 batch PCKh 0.375\n",
      "Trained batch 925 batch loss 0.489627 batch mAP 0.540496826 batch PCKh 0.4375\n",
      "Trained batch 926 batch loss 0.578081489 batch mAP 0.488739 batch PCKh 0.1875\n",
      "Trained batch 927 batch loss 0.440517098 batch mAP 0.594482422 batch PCKh 0.75\n",
      "Trained batch 928 batch loss 0.498776436 batch mAP 0.583313 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 929 batch loss 0.564479113 batch mAP 0.598266602 batch PCKh 0.3125\n",
      "Trained batch 930 batch loss 0.579343557 batch mAP 0.570465088 batch PCKh 0.5\n",
      "Trained batch 931 batch loss 0.615864754 batch mAP 0.566619873 batch PCKh 0.5\n",
      "Trained batch 932 batch loss 0.49870652 batch mAP 0.612670898 batch PCKh 0.125\n",
      "Trained batch 933 batch loss 0.543945193 batch mAP 0.622009277 batch PCKh 0.6875\n",
      "Trained batch 934 batch loss 0.463842213 batch mAP 0.599609375 batch PCKh 0.5625\n",
      "Trained batch 935 batch loss 0.54031533 batch mAP 0.60736084 batch PCKh 0.1875\n",
      "Trained batch 936 batch loss 0.529979587 batch mAP 0.633483887 batch PCKh 0.4375\n",
      "Trained batch 937 batch loss 0.521701276 batch mAP 0.643554688 batch PCKh 0.6875\n",
      "Trained batch 938 batch loss 0.490943134 batch mAP 0.647583 batch PCKh 0.4375\n",
      "Trained batch 939 batch loss 0.39007166 batch mAP 0.69128418 batch PCKh 0.5625\n",
      "Trained batch 940 batch loss 0.392385662 batch mAP 0.696167 batch PCKh 0.5\n",
      "Trained batch 941 batch loss 0.489373654 batch mAP 0.655273438 batch PCKh 0.5625\n",
      "Trained batch 942 batch loss 0.498638839 batch mAP 0.660553 batch PCKh 0.5625\n",
      "Trained batch 943 batch loss 0.52872324 batch mAP 0.665466309 batch PCKh 0.625\n",
      "Trained batch 944 batch loss 0.484605134 batch mAP 0.675262451 batch PCKh 0.8125\n",
      "Trained batch 945 batch loss 0.46024856 batch mAP 0.701843262 batch PCKh 0.625\n",
      "Trained batch 946 batch loss 0.445722 batch mAP 0.74017334 batch PCKh 0.5\n",
      "Trained batch 947 batch loss 0.39092508 batch mAP 0.738189697 batch PCKh 0.4375\n",
      "Trained batch 948 batch loss 0.493381262 batch mAP 0.711486816 batch PCKh 0.4375\n",
      "Trained batch 949 batch loss 0.505838394 batch mAP 0.710662842 batch PCKh 0.4375\n",
      "Trained batch 950 batch loss 0.465262949 batch mAP 0.728820801 batch PCKh 0.8125\n",
      "Trained batch 951 batch loss 0.44817555 batch mAP 0.674072266 batch PCKh 0.625\n",
      "Trained batch 952 batch loss 0.479926765 batch mAP 0.687316895 batch PCKh 0.75\n",
      "Trained batch 953 batch loss 0.475503206 batch mAP 0.658660889 batch PCKh 0.875\n",
      "Trained batch 954 batch loss 0.564636827 batch mAP 0.594177246 batch PCKh 0.6875\n",
      "Trained batch 955 batch loss 0.49509269 batch mAP 0.699707031 batch PCKh 0.6875\n",
      "Trained batch 956 batch loss 0.52170831 batch mAP 0.672149658 batch PCKh 0.75\n",
      "Trained batch 957 batch loss 0.483314514 batch mAP 0.703521729 batch PCKh 0.625\n",
      "Trained batch 958 batch loss 0.522705078 batch mAP 0.665466309 batch PCKh 0.375\n",
      "Trained batch 959 batch loss 0.507499456 batch mAP 0.625335693 batch PCKh 0.625\n",
      "Trained batch 960 batch loss 0.516158521 batch mAP 0.631195068 batch PCKh 0.3125\n",
      "Trained batch 961 batch loss 0.53128016 batch mAP 0.671081543 batch PCKh 0.5625\n",
      "Trained batch 962 batch loss 0.553159952 batch mAP 0.621582031 batch PCKh 0.625\n",
      "Trained batch 963 batch loss 0.564015567 batch mAP 0.556152344 batch PCKh 0.75\n",
      "Trained batch 964 batch loss 0.541978419 batch mAP 0.597503662 batch PCKh 0.1875\n",
      "Trained batch 965 batch loss 0.527083158 batch mAP 0.554992676 batch PCKh 0.75\n",
      "Trained batch 966 batch loss 0.518566728 batch mAP 0.63369751 batch PCKh 0.8125\n",
      "Trained batch 967 batch loss 0.482566237 batch mAP 0.637786865 batch PCKh 0.75\n",
      "Trained batch 968 batch loss 0.54200232 batch mAP 0.632415771 batch PCKh 0.75\n",
      "Trained batch 969 batch loss 0.571595609 batch mAP 0.567382812 batch PCKh 0.5\n",
      "Trained batch 970 batch loss 0.593463778 batch mAP 0.611999512 batch PCKh 0.5625\n",
      "Trained batch 971 batch loss 0.550854445 batch mAP 0.594940186 batch PCKh 0.6875\n",
      "Trained batch 972 batch loss 0.558200359 batch mAP 0.634735107 batch PCKh 0.875\n",
      "Trained batch 973 batch loss 0.458071947 batch mAP 0.664154053 batch PCKh 0.625\n",
      "Trained batch 974 batch loss 0.511962414 batch mAP 0.624267578 batch PCKh 0.75\n",
      "Trained batch 975 batch loss 0.57058084 batch mAP 0.54699707 batch PCKh 0.125\n",
      "Trained batch 976 batch loss 0.574291229 batch mAP 0.578033447 batch PCKh 0.4375\n",
      "Trained batch 977 batch loss 0.580043316 batch mAP 0.551361084 batch PCKh 0.5625\n",
      "Trained batch 978 batch loss 0.521177351 batch mAP 0.646514893 batch PCKh 0.4375\n",
      "Trained batch 979 batch loss 0.521387935 batch mAP 0.635742188 batch PCKh 0.75\n",
      "Trained batch 980 batch loss 0.467008144 batch mAP 0.639007568 batch PCKh 0.75\n",
      "Trained batch 981 batch loss 0.576600194 batch mAP 0.610351562 batch PCKh 0.5625\n",
      "Trained batch 982 batch loss 0.557938933 batch mAP 0.531616211 batch PCKh 0.6875\n",
      "Trained batch 983 batch loss 0.48250255 batch mAP 0.597869873 batch PCKh 0.375\n",
      "Trained batch 984 batch loss 0.495966285 batch mAP 0.617828369 batch PCKh 0.875\n",
      "Trained batch 985 batch loss 0.519808948 batch mAP 0.624542236 batch PCKh 0.75\n",
      "Trained batch 986 batch loss 0.523185372 batch mAP 0.604064941 batch PCKh 0.625\n",
      "Trained batch 987 batch loss 0.491204321 batch mAP 0.560852051 batch PCKh 0.375\n",
      "Trained batch 988 batch loss 0.518593907 batch mAP 0.484710693 batch PCKh 0.1875\n",
      "Trained batch 989 batch loss 0.488325596 batch mAP 0.560211182 batch PCKh 0.5625\n",
      "Trained batch 990 batch loss 0.562205911 batch mAP 0.584320068 batch PCKh 0.75\n",
      "Trained batch 991 batch loss 0.553837121 batch mAP 0.53125 batch PCKh 0.25\n",
      "Trained batch 992 batch loss 0.480502725 batch mAP 0.589355469 batch PCKh 0.5625\n",
      "Trained batch 993 batch loss 0.487622768 batch mAP 0.574981689 batch PCKh 0.75\n",
      "Trained batch 994 batch loss 0.539160609 batch mAP 0.580932617 batch PCKh 0.625\n",
      "Trained batch 995 batch loss 0.465652287 batch mAP 0.565307617 batch PCKh 0.625\n",
      "Trained batch 996 batch loss 0.393418431 batch mAP 0.614013672 batch PCKh 0.5\n",
      "Trained batch 997 batch loss 0.466043413 batch mAP 0.625366211 batch PCKh 0.25\n",
      "Trained batch 998 batch loss 0.44963938 batch mAP 0.608306885 batch PCKh 0.5625\n",
      "Trained batch 999 batch loss 0.488684237 batch mAP 0.62097168 batch PCKh 0.75\n",
      "Trained batch 1000 batch loss 0.487658054 batch mAP 0.656677246 batch PCKh 0.5\n",
      "Trained batch 1001 batch loss 0.499157548 batch mAP 0.690887451 batch PCKh 0.5625\n",
      "Trained batch 1002 batch loss 0.417281598 batch mAP 0.68145752 batch PCKh 0.375\n",
      "Trained batch 1003 batch loss 0.414390862 batch mAP 0.724884033 batch PCKh 0.5\n",
      "Trained batch 1004 batch loss 0.439350367 batch mAP 0.684845 batch PCKh 0.375\n",
      "Trained batch 1005 batch loss 0.423791289 batch mAP 0.705413818 batch PCKh 0.5\n",
      "Trained batch 1006 batch loss 0.588441312 batch mAP 0.57131958 batch PCKh 0.4375\n",
      "Trained batch 1007 batch loss 0.52564007 batch mAP 0.625274658 batch PCKh 0.6875\n",
      "Trained batch 1008 batch loss 0.520589888 batch mAP 0.585479736 batch PCKh 0.625\n",
      "Trained batch 1009 batch loss 0.539043 batch mAP 0.634368896 batch PCKh 0.4375\n",
      "Trained batch 1010 batch loss 0.515363455 batch mAP 0.617034912 batch PCKh 0.4375\n",
      "Trained batch 1011 batch loss 0.578088 batch mAP 0.622497559 batch PCKh 0.375\n",
      "Trained batch 1012 batch loss 0.489993751 batch mAP 0.615142822 batch PCKh 0.75\n",
      "Trained batch 1013 batch loss 0.485259056 batch mAP 0.647033691 batch PCKh 0.6875\n",
      "Trained batch 1014 batch loss 0.53199029 batch mAP 0.594390869 batch PCKh 0.625\n",
      "Trained batch 1015 batch loss 0.478627503 batch mAP 0.605834961 batch PCKh 0.1875\n",
      "Trained batch 1016 batch loss 0.577296138 batch mAP 0.589599609 batch PCKh 0.5625\n",
      "Trained batch 1017 batch loss 0.599501133 batch mAP 0.58480835 batch PCKh 0.3125\n",
      "Trained batch 1018 batch loss 0.543599069 batch mAP 0.590057373 batch PCKh 0.3125\n",
      "Trained batch 1019 batch loss 0.587676883 batch mAP 0.580047607 batch PCKh 0.5625\n",
      "Trained batch 1020 batch loss 0.46273157 batch mAP 0.64932251 batch PCKh 0.8125\n",
      "Trained batch 1021 batch loss 0.402432978 batch mAP 0.630371094 batch PCKh 0.375\n",
      "Trained batch 1022 batch loss 0.483625 batch mAP 0.565582275 batch PCKh 0.1875\n",
      "Trained batch 1023 batch loss 0.466589421 batch mAP 0.684936523 batch PCKh 0.375\n",
      "Trained batch 1024 batch loss 0.487619132 batch mAP 0.641784668 batch PCKh 0.5\n",
      "Trained batch 1025 batch loss 0.431196958 batch mAP 0.648345947 batch PCKh 0.1875\n",
      "Trained batch 1026 batch loss 0.547460437 batch mAP 0.545318604 batch PCKh 0.8125\n",
      "Trained batch 1027 batch loss 0.460290045 batch mAP 0.504547119 batch PCKh 0.5\n",
      "Trained batch 1028 batch loss 0.416818202 batch mAP 0.588195801 batch PCKh 0.5625\n",
      "Trained batch 1029 batch loss 0.421531022 batch mAP 0.612335205 batch PCKh 0.375\n",
      "Trained batch 1030 batch loss 0.354669631 batch mAP 0.630889893 batch PCKh 0.25\n",
      "Trained batch 1031 batch loss 0.526621878 batch mAP 0.514251709 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1032 batch loss 0.403046787 batch mAP 0.577850342 batch PCKh 0.4375\n",
      "Trained batch 1033 batch loss 0.472414851 batch mAP 0.628845215 batch PCKh 0.75\n",
      "Trained batch 1034 batch loss 0.371132731 batch mAP 0.652191162 batch PCKh 0.5625\n",
      "Trained batch 1035 batch loss 0.39760384 batch mAP 0.67590332 batch PCKh 0.3125\n",
      "Trained batch 1036 batch loss 0.423957467 batch mAP 0.697937 batch PCKh 0.375\n",
      "Trained batch 1037 batch loss 0.414553344 batch mAP 0.689941406 batch PCKh 0.625\n",
      "Trained batch 1038 batch loss 0.365992934 batch mAP 0.643249512 batch PCKh 0.5625\n",
      "Trained batch 1039 batch loss 0.469142854 batch mAP 0.630157471 batch PCKh 0.75\n",
      "Trained batch 1040 batch loss 0.524822593 batch mAP 0.610717773 batch PCKh 0.75\n",
      "Trained batch 1041 batch loss 0.497550666 batch mAP 0.61227417 batch PCKh 0.75\n",
      "Trained batch 1042 batch loss 0.454320818 batch mAP 0.586181641 batch PCKh 0.25\n",
      "Trained batch 1043 batch loss 0.588473797 batch mAP 0.615356445 batch PCKh 0.75\n",
      "Trained batch 1044 batch loss 0.54712224 batch mAP 0.608978271 batch PCKh 0.25\n",
      "Trained batch 1045 batch loss 0.461071521 batch mAP 0.697784424 batch PCKh 0.3125\n",
      "Trained batch 1046 batch loss 0.462691724 batch mAP 0.682891846 batch PCKh 0.625\n",
      "Trained batch 1047 batch loss 0.489557981 batch mAP 0.676879883 batch PCKh 0.625\n",
      "Trained batch 1048 batch loss 0.502774954 batch mAP 0.644104 batch PCKh 0.625\n",
      "Trained batch 1049 batch loss 0.5887 batch mAP 0.620330811 batch PCKh 0.5625\n",
      "Trained batch 1050 batch loss 0.518069267 batch mAP 0.601715088 batch PCKh 0.5\n",
      "Trained batch 1051 batch loss 0.451524138 batch mAP 0.59564209 batch PCKh 0.5625\n",
      "Trained batch 1052 batch loss 0.524415851 batch mAP 0.641113281 batch PCKh 0.4375\n",
      "Trained batch 1053 batch loss 0.472290933 batch mAP 0.619232178 batch PCKh 0.8125\n",
      "Trained batch 1054 batch loss 0.417942166 batch mAP 0.655609131 batch PCKh 0.625\n",
      "Trained batch 1055 batch loss 0.520028532 batch mAP 0.600219727 batch PCKh 0.75\n",
      "Trained batch 1056 batch loss 0.482704103 batch mAP 0.603668213 batch PCKh 0.5625\n",
      "Trained batch 1057 batch loss 0.478381515 batch mAP 0.664032 batch PCKh 0.25\n",
      "Trained batch 1058 batch loss 0.553822517 batch mAP 0.644561768 batch PCKh 0.8125\n",
      "Trained batch 1059 batch loss 0.489776522 batch mAP 0.576843262 batch PCKh 0.75\n",
      "Trained batch 1060 batch loss 0.473333836 batch mAP 0.573577881 batch PCKh 0.75\n",
      "Trained batch 1061 batch loss 0.648181 batch mAP 0.493560791 batch PCKh 0.625\n",
      "Trained batch 1062 batch loss 0.551625669 batch mAP 0.560546875 batch PCKh 0.375\n",
      "Trained batch 1063 batch loss 0.453241348 batch mAP 0.572143555 batch PCKh 0.375\n",
      "Trained batch 1064 batch loss 0.497563094 batch mAP 0.624389648 batch PCKh 0.625\n",
      "Trained batch 1065 batch loss 0.460533261 batch mAP 0.620300293 batch PCKh 0.6875\n",
      "Trained batch 1066 batch loss 0.42054683 batch mAP 0.582946777 batch PCKh 0.3125\n",
      "Trained batch 1067 batch loss 0.469760299 batch mAP 0.595825195 batch PCKh 0.125\n",
      "Trained batch 1068 batch loss 0.461810499 batch mAP 0.614379883 batch PCKh 0.625\n",
      "Trained batch 1069 batch loss 0.478274465 batch mAP 0.679260254 batch PCKh 0.625\n",
      "Trained batch 1070 batch loss 0.496315777 batch mAP 0.679534912 batch PCKh 0.5625\n",
      "Trained batch 1071 batch loss 0.519272387 batch mAP 0.619384766 batch PCKh 0.625\n",
      "Trained batch 1072 batch loss 0.50552547 batch mAP 0.670349121 batch PCKh 0.625\n",
      "Trained batch 1073 batch loss 0.524413645 batch mAP 0.613311768 batch PCKh 0.8125\n",
      "Trained batch 1074 batch loss 0.542315185 batch mAP 0.630249 batch PCKh 0.4375\n",
      "Trained batch 1075 batch loss 0.516808808 batch mAP 0.66116333 batch PCKh 0.75\n",
      "Trained batch 1076 batch loss 0.511497259 batch mAP 0.657684326 batch PCKh 0.4375\n",
      "Trained batch 1077 batch loss 0.475412905 batch mAP 0.625091553 batch PCKh 0.5625\n",
      "Trained batch 1078 batch loss 0.421796858 batch mAP 0.691131592 batch PCKh 0.5625\n",
      "Trained batch 1079 batch loss 0.474686921 batch mAP 0.64453125 batch PCKh 0.8125\n",
      "Trained batch 1080 batch loss 0.419588655 batch mAP 0.651947 batch PCKh 0.5\n",
      "Trained batch 1081 batch loss 0.425164938 batch mAP 0.637634277 batch PCKh 0.5625\n",
      "Trained batch 1082 batch loss 0.541125536 batch mAP 0.670776367 batch PCKh 0.8125\n",
      "Trained batch 1083 batch loss 0.51125288 batch mAP 0.594604492 batch PCKh 0.75\n",
      "Trained batch 1084 batch loss 0.369566977 batch mAP 0.616790771 batch PCKh 0.1875\n",
      "Trained batch 1085 batch loss 0.41384685 batch mAP 0.630096436 batch PCKh 0.5625\n",
      "Trained batch 1086 batch loss 0.52022934 batch mAP 0.588897705 batch PCKh 0.625\n",
      "Trained batch 1087 batch loss 0.542601109 batch mAP 0.585632324 batch PCKh 0.4375\n",
      "Trained batch 1088 batch loss 0.477026 batch mAP 0.616485596 batch PCKh 0.8125\n",
      "Trained batch 1089 batch loss 0.439020336 batch mAP 0.615753174 batch PCKh 0.625\n",
      "Trained batch 1090 batch loss 0.510526061 batch mAP 0.574432373 batch PCKh 0.5625\n",
      "Trained batch 1091 batch loss 0.489896625 batch mAP 0.591461182 batch PCKh 0.6875\n",
      "Trained batch 1092 batch loss 0.577350378 batch mAP 0.529083252 batch PCKh 0.375\n",
      "Trained batch 1093 batch loss 0.526623845 batch mAP 0.505310059 batch PCKh 0.8125\n",
      "Trained batch 1094 batch loss 0.481217802 batch mAP 0.527038574 batch PCKh 0\n",
      "Trained batch 1095 batch loss 0.420934051 batch mAP 0.596862793 batch PCKh 0.5\n",
      "Trained batch 1096 batch loss 0.425344765 batch mAP 0.620361328 batch PCKh 0.6875\n",
      "Trained batch 1097 batch loss 0.406873018 batch mAP 0.69543457 batch PCKh 0.8125\n",
      "Trained batch 1098 batch loss 0.433555812 batch mAP 0.658782959 batch PCKh 0.5\n",
      "Trained batch 1099 batch loss 0.4711546 batch mAP 0.672424316 batch PCKh 0.5\n",
      "Trained batch 1100 batch loss 0.507029295 batch mAP 0.713348389 batch PCKh 0.8125\n",
      "Trained batch 1101 batch loss 0.560342669 batch mAP 0.689422607 batch PCKh 0.5625\n",
      "Trained batch 1102 batch loss 0.524246931 batch mAP 0.648071289 batch PCKh 0.5625\n",
      "Trained batch 1103 batch loss 0.527085364 batch mAP 0.565979 batch PCKh 0.3125\n",
      "Trained batch 1104 batch loss 0.59120965 batch mAP 0.514099121 batch PCKh 0.5625\n",
      "Trained batch 1105 batch loss 0.562259912 batch mAP 0.619445801 batch PCKh 0.6875\n",
      "Trained batch 1106 batch loss 0.562945 batch mAP 0.608123779 batch PCKh 0.3125\n",
      "Trained batch 1107 batch loss 0.599498689 batch mAP 0.581726074 batch PCKh 0.625\n",
      "Trained batch 1108 batch loss 0.537592709 batch mAP 0.615478516 batch PCKh 0.5625\n",
      "Trained batch 1109 batch loss 0.551377535 batch mAP 0.619873047 batch PCKh 0.125\n",
      "Trained batch 1110 batch loss 0.57986784 batch mAP 0.659057617 batch PCKh 0.4375\n",
      "Trained batch 1111 batch loss 0.497639269 batch mAP 0.642242432 batch PCKh 0.3125\n",
      "Trained batch 1112 batch loss 0.516879439 batch mAP 0.628234863 batch PCKh 0.25\n",
      "Trained batch 1113 batch loss 0.602377176 batch mAP 0.588989258 batch PCKh 0.125\n",
      "Trained batch 1114 batch loss 0.54275769 batch mAP 0.545227051 batch PCKh 0.375\n",
      "Trained batch 1115 batch loss 0.476709872 batch mAP 0.621795654 batch PCKh 0.875\n",
      "Trained batch 1116 batch loss 0.491601169 batch mAP 0.621337891 batch PCKh 0.8125\n",
      "Trained batch 1117 batch loss 0.518209219 batch mAP 0.649047852 batch PCKh 0.625\n",
      "Trained batch 1118 batch loss 0.622539878 batch mAP 0.538513184 batch PCKh 0.1875\n",
      "Trained batch 1119 batch loss 0.595824718 batch mAP 0.556304932 batch PCKh 0.5\n",
      "Trained batch 1120 batch loss 0.514443278 batch mAP 0.562744141 batch PCKh 0.4375\n",
      "Trained batch 1121 batch loss 0.553979218 batch mAP 0.556243896 batch PCKh 0.5625\n",
      "Trained batch 1122 batch loss 0.576522112 batch mAP 0.588409424 batch PCKh 0.625\n",
      "Trained batch 1123 batch loss 0.56018889 batch mAP 0.611419678 batch PCKh 0.3125\n",
      "Trained batch 1124 batch loss 0.592432678 batch mAP 0.574707031 batch PCKh 0.4375\n",
      "Trained batch 1125 batch loss 0.529688179 batch mAP 0.563049316 batch PCKh 0.625\n",
      "Trained batch 1126 batch loss 0.511317849 batch mAP 0.570648193 batch PCKh 0.4375\n",
      "Trained batch 1127 batch loss 0.583585858 batch mAP 0.455566406 batch PCKh 0.125\n",
      "Trained batch 1128 batch loss 0.456379354 batch mAP 0.630371094 batch PCKh 0.75\n",
      "Trained batch 1129 batch loss 0.504913 batch mAP 0.703125 batch PCKh 0.3125\n",
      "Trained batch 1130 batch loss 0.497407705 batch mAP 0.66305542 batch PCKh 0.75\n",
      "Trained batch 1131 batch loss 0.477563709 batch mAP 0.664032 batch PCKh 0.25\n",
      "Trained batch 1132 batch loss 0.45671314 batch mAP 0.696685791 batch PCKh 0.75\n",
      "Trained batch 1133 batch loss 0.389337182 batch mAP 0.634979248 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1134 batch loss 0.435480475 batch mAP 0.617584229 batch PCKh 0.375\n",
      "Trained batch 1135 batch loss 0.415725052 batch mAP 0.597045898 batch PCKh 0\n",
      "Trained batch 1136 batch loss 0.428739667 batch mAP 0.621368408 batch PCKh 0.25\n",
      "Trained batch 1137 batch loss 0.448669434 batch mAP 0.62020874 batch PCKh 0.1875\n",
      "Trained batch 1138 batch loss 0.563717961 batch mAP 0.611572266 batch PCKh 0.4375\n",
      "Trained batch 1139 batch loss 0.564009845 batch mAP 0.577789307 batch PCKh 0.75\n",
      "Trained batch 1140 batch loss 0.538428426 batch mAP 0.636322 batch PCKh 0.1875\n",
      "Trained batch 1141 batch loss 0.476756513 batch mAP 0.617706299 batch PCKh 0.3125\n",
      "Trained batch 1142 batch loss 0.502562046 batch mAP 0.639007568 batch PCKh 0.4375\n",
      "Trained batch 1143 batch loss 0.565074205 batch mAP 0.622345 batch PCKh 0.1875\n",
      "Trained batch 1144 batch loss 0.547564149 batch mAP 0.574157715 batch PCKh 0.0625\n",
      "Trained batch 1145 batch loss 0.618396163 batch mAP 0.517852783 batch PCKh 0.125\n",
      "Trained batch 1146 batch loss 0.591573954 batch mAP 0.579589844 batch PCKh 0.5625\n",
      "Trained batch 1147 batch loss 0.523978293 batch mAP 0.569519043 batch PCKh 0.3125\n",
      "Trained batch 1148 batch loss 0.470362544 batch mAP 0.65725708 batch PCKh 0.125\n",
      "Trained batch 1149 batch loss 0.498411775 batch mAP 0.685577393 batch PCKh 0.6875\n",
      "Trained batch 1150 batch loss 0.557249188 batch mAP 0.655853271 batch PCKh 0.375\n",
      "Trained batch 1151 batch loss 0.487266839 batch mAP 0.734436035 batch PCKh 0.5\n",
      "Trained batch 1152 batch loss 0.530598283 batch mAP 0.662811279 batch PCKh 0.375\n",
      "Trained batch 1153 batch loss 0.481377423 batch mAP 0.704437256 batch PCKh 0.625\n",
      "Trained batch 1154 batch loss 0.418709278 batch mAP 0.659240723 batch PCKh 0.125\n",
      "Trained batch 1155 batch loss 0.634935141 batch mAP 0.559173584 batch PCKh 0.5\n",
      "Trained batch 1156 batch loss 0.537981391 batch mAP 0.652282715 batch PCKh 0.4375\n",
      "Trained batch 1157 batch loss 0.523724198 batch mAP 0.612091064 batch PCKh 0.1875\n",
      "Trained batch 1158 batch loss 0.388686657 batch mAP 0.604522705 batch PCKh 0.25\n",
      "Trained batch 1159 batch loss 0.35523513 batch mAP 0.639068604 batch PCKh 0.3125\n",
      "Trained batch 1160 batch loss 0.365758389 batch mAP 0.531524658 batch PCKh 0\n",
      "Trained batch 1161 batch loss 0.379655421 batch mAP 0.552703857 batch PCKh 0\n",
      "Trained batch 1162 batch loss 0.517665327 batch mAP 0.681976318 batch PCKh 0.8125\n",
      "Trained batch 1163 batch loss 0.450058281 batch mAP 0.64730835 batch PCKh 0\n",
      "Trained batch 1164 batch loss 0.542340159 batch mAP 0.725219727 batch PCKh 0.8125\n",
      "Trained batch 1165 batch loss 0.527371645 batch mAP 0.714874268 batch PCKh 0.3125\n",
      "Trained batch 1166 batch loss 0.5837816 batch mAP 0.720367432 batch PCKh 0.75\n",
      "Trained batch 1167 batch loss 0.506349683 batch mAP 0.703918457 batch PCKh 0.5\n",
      "Trained batch 1168 batch loss 0.614878297 batch mAP 0.623504639 batch PCKh 0.25\n",
      "Trained batch 1169 batch loss 0.617068052 batch mAP 0.600067139 batch PCKh 0.3125\n",
      "Trained batch 1170 batch loss 0.554769635 batch mAP 0.620513916 batch PCKh 0.5625\n",
      "Trained batch 1171 batch loss 0.52084744 batch mAP 0.647918701 batch PCKh 0.6875\n",
      "Trained batch 1172 batch loss 0.570262551 batch mAP 0.595825195 batch PCKh 0.1875\n",
      "Trained batch 1173 batch loss 0.548330069 batch mAP 0.616638184 batch PCKh 0.25\n",
      "Trained batch 1174 batch loss 0.564437747 batch mAP 0.635101318 batch PCKh 0.1875\n",
      "Trained batch 1175 batch loss 0.514281094 batch mAP 0.681335449 batch PCKh 0.375\n",
      "Trained batch 1176 batch loss 0.514475405 batch mAP 0.678070068 batch PCKh 0.25\n",
      "Trained batch 1177 batch loss 0.506868 batch mAP 0.597961426 batch PCKh 0.6875\n",
      "Trained batch 1178 batch loss 0.532371819 batch mAP 0.580719 batch PCKh 0.375\n",
      "Trained batch 1179 batch loss 0.470397 batch mAP 0.612518311 batch PCKh 0.5625\n",
      "Trained batch 1180 batch loss 0.52581358 batch mAP 0.643890381 batch PCKh 0.5\n",
      "Trained batch 1181 batch loss 0.551257193 batch mAP 0.633087158 batch PCKh 0.4375\n",
      "Trained batch 1182 batch loss 0.53542316 batch mAP 0.508483887 batch PCKh 0.6875\n",
      "Trained batch 1183 batch loss 0.573666811 batch mAP 0.580657959 batch PCKh 0.6875\n",
      "Trained batch 1184 batch loss 0.556661367 batch mAP 0.571655273 batch PCKh 0.625\n",
      "Trained batch 1185 batch loss 0.652172446 batch mAP 0.480682373 batch PCKh 0.25\n",
      "Trained batch 1186 batch loss 0.609281719 batch mAP 0.514282227 batch PCKh 0.25\n",
      "Trained batch 1187 batch loss 0.523270905 batch mAP 0.590332031 batch PCKh 0.5625\n",
      "Trained batch 1188 batch loss 0.484134883 batch mAP 0.658172607 batch PCKh 0.6875\n",
      "Trained batch 1189 batch loss 0.521077514 batch mAP 0.628143311 batch PCKh 0.0625\n",
      "Trained batch 1190 batch loss 0.567837059 batch mAP 0.681335449 batch PCKh 0.25\n",
      "Trained batch 1191 batch loss 0.575503469 batch mAP 0.562011719 batch PCKh 0.625\n",
      "Trained batch 1192 batch loss 0.565489113 batch mAP 0.544769287 batch PCKh 0.1875\n",
      "Trained batch 1193 batch loss 0.475025892 batch mAP 0.60369873 batch PCKh 0.375\n",
      "Trained batch 1194 batch loss 0.494977057 batch mAP 0.590057373 batch PCKh 0.625\n",
      "Trained batch 1195 batch loss 0.546475232 batch mAP 0.595611572 batch PCKh 0.8125\n",
      "Trained batch 1196 batch loss 0.500483513 batch mAP 0.542449951 batch PCKh 0.375\n",
      "Trained batch 1197 batch loss 0.552329242 batch mAP 0.552063 batch PCKh 0.6875\n",
      "Trained batch 1198 batch loss 0.532284498 batch mAP 0.59072876 batch PCKh 0.4375\n",
      "Trained batch 1199 batch loss 0.520373344 batch mAP 0.573577881 batch PCKh 0.8125\n",
      "Trained batch 1200 batch loss 0.556738615 batch mAP 0.541229248 batch PCKh 0.5625\n",
      "Trained batch 1201 batch loss 0.545670748 batch mAP 0.5574646 batch PCKh 0.8125\n",
      "Trained batch 1202 batch loss 0.473818183 batch mAP 0.606140137 batch PCKh 0.5625\n",
      "Trained batch 1203 batch loss 0.542226 batch mAP 0.564697266 batch PCKh 0.5\n",
      "Trained batch 1204 batch loss 0.647262454 batch mAP 0.491943359 batch PCKh 0.125\n",
      "Trained batch 1205 batch loss 0.623045564 batch mAP 0.569061279 batch PCKh 0.625\n",
      "Trained batch 1206 batch loss 0.56465739 batch mAP 0.571014404 batch PCKh 0.875\n",
      "Trained batch 1207 batch loss 0.531167269 batch mAP 0.61151123 batch PCKh 0.625\n",
      "Trained batch 1208 batch loss 0.517775655 batch mAP 0.541473389 batch PCKh 0.4375\n",
      "Trained batch 1209 batch loss 0.602719605 batch mAP 0.523590088 batch PCKh 0.25\n",
      "Trained batch 1210 batch loss 0.5981372 batch mAP 0.587188721 batch PCKh 0.375\n",
      "Trained batch 1211 batch loss 0.587828159 batch mAP 0.546051 batch PCKh 0.5625\n",
      "Trained batch 1212 batch loss 0.546132922 batch mAP 0.554382324 batch PCKh 0.5625\n",
      "Trained batch 1213 batch loss 0.576111257 batch mAP 0.471679688 batch PCKh 0.375\n",
      "Trained batch 1214 batch loss 0.474911869 batch mAP 0.604126 batch PCKh 0.625\n",
      "Trained batch 1215 batch loss 0.540552557 batch mAP 0.508148193 batch PCKh 0.75\n",
      "Trained batch 1216 batch loss 0.590440035 batch mAP 0.48059082 batch PCKh 0.0625\n",
      "Trained batch 1217 batch loss 0.588025331 batch mAP 0.551971436 batch PCKh 0.3125\n",
      "Trained batch 1218 batch loss 0.554313898 batch mAP 0.587799072 batch PCKh 0.5625\n",
      "Trained batch 1219 batch loss 0.493001759 batch mAP 0.60672 batch PCKh 0.625\n",
      "Trained batch 1220 batch loss 0.491542399 batch mAP 0.562683105 batch PCKh 0.3125\n",
      "Trained batch 1221 batch loss 0.411894679 batch mAP 0.625396729 batch PCKh 0.625\n",
      "Trained batch 1222 batch loss 0.397685051 batch mAP 0.63873291 batch PCKh 0.125\n",
      "Trained batch 1223 batch loss 0.399194121 batch mAP 0.710235596 batch PCKh 0.5625\n",
      "Trained batch 1224 batch loss 0.481859088 batch mAP 0.659820557 batch PCKh 0.625\n",
      "Trained batch 1225 batch loss 0.471742541 batch mAP 0.619873047 batch PCKh 0.5625\n",
      "Trained batch 1226 batch loss 0.394902766 batch mAP 0.703186035 batch PCKh 0.4375\n",
      "Trained batch 1227 batch loss 0.487372041 batch mAP 0.650848389 batch PCKh 0.75\n",
      "Trained batch 1228 batch loss 0.469138026 batch mAP 0.605499268 batch PCKh 0.75\n",
      "Trained batch 1229 batch loss 0.427261204 batch mAP 0.650604248 batch PCKh 0.75\n",
      "Trained batch 1230 batch loss 0.412109494 batch mAP 0.632476807 batch PCKh 0.5625\n",
      "Trained batch 1231 batch loss 0.467563331 batch mAP 0.552001953 batch PCKh 0.0625\n",
      "Trained batch 1232 batch loss 0.548304737 batch mAP 0.545532227 batch PCKh 0.3125\n",
      "Trained batch 1233 batch loss 0.545765519 batch mAP 0.617340088 batch PCKh 0.25\n",
      "Trained batch 1234 batch loss 0.444173545 batch mAP 0.599304199 batch PCKh 0.1875\n",
      "Trained batch 1235 batch loss 0.428298831 batch mAP 0.656158447 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1236 batch loss 0.481948704 batch mAP 0.647338867 batch PCKh 0.75\n",
      "Trained batch 1237 batch loss 0.540219486 batch mAP 0.629638672 batch PCKh 0.75\n",
      "Trained batch 1238 batch loss 0.538043082 batch mAP 0.638000488 batch PCKh 0.6875\n",
      "Trained batch 1239 batch loss 0.47813794 batch mAP 0.627075195 batch PCKh 0.375\n",
      "Trained batch 1240 batch loss 0.50873363 batch mAP 0.622833252 batch PCKh 0.5\n",
      "Trained batch 1241 batch loss 0.59617883 batch mAP 0.605651855 batch PCKh 0.5625\n",
      "Trained batch 1242 batch loss 0.616738915 batch mAP 0.563720703 batch PCKh 0.5\n",
      "Trained batch 1243 batch loss 0.661655784 batch mAP 0.558380127 batch PCKh 0.1875\n",
      "Trained batch 1244 batch loss 0.618206203 batch mAP 0.550415039 batch PCKh 0.0625\n",
      "Trained batch 1245 batch loss 0.656101584 batch mAP 0.549591064 batch PCKh 0.25\n",
      "Trained batch 1246 batch loss 0.63727057 batch mAP 0.55960083 batch PCKh 0.625\n",
      "Trained batch 1247 batch loss 0.452090383 batch mAP 0.561126709 batch PCKh 0.1875\n",
      "Trained batch 1248 batch loss 0.484866917 batch mAP 0.563476562 batch PCKh 0.5\n",
      "Trained batch 1249 batch loss 0.448992908 batch mAP 0.511871338 batch PCKh 0.0625\n",
      "Trained batch 1250 batch loss 0.364492029 batch mAP 0.561004639 batch PCKh 0.25\n",
      "Trained batch 1251 batch loss 0.547601163 batch mAP 0.519744873 batch PCKh 0.25\n",
      "Trained batch 1252 batch loss 0.550853372 batch mAP 0.541748047 batch PCKh 0.5\n",
      "Trained batch 1253 batch loss 0.607542515 batch mAP 0.534942627 batch PCKh 0.3125\n",
      "Trained batch 1254 batch loss 0.560053289 batch mAP 0.545379639 batch PCKh 0.5\n",
      "Trained batch 1255 batch loss 0.645545721 batch mAP 0.505889893 batch PCKh 0\n",
      "Trained batch 1256 batch loss 0.57333231 batch mAP 0.493499756 batch PCKh 0.4375\n",
      "Trained batch 1257 batch loss 0.525309265 batch mAP 0.603515625 batch PCKh 0.6875\n",
      "Trained batch 1258 batch loss 0.517118 batch mAP 0.589447 batch PCKh 0.125\n",
      "Trained batch 1259 batch loss 0.468907923 batch mAP 0.649200439 batch PCKh 0.875\n",
      "Trained batch 1260 batch loss 0.552325845 batch mAP 0.642791748 batch PCKh 0.9375\n",
      "Trained batch 1261 batch loss 0.498900741 batch mAP 0.674194336 batch PCKh 0.75\n",
      "Trained batch 1262 batch loss 0.502693892 batch mAP 0.645599365 batch PCKh 0.5625\n",
      "Trained batch 1263 batch loss 0.531247377 batch mAP 0.578216553 batch PCKh 0.625\n",
      "Trained batch 1264 batch loss 0.422567248 batch mAP 0.703338623 batch PCKh 0.8125\n",
      "Trained batch 1265 batch loss 0.35377273 batch mAP 0.740142822 batch PCKh 0.5\n",
      "Trained batch 1266 batch loss 0.437008023 batch mAP 0.693878174 batch PCKh 0.375\n",
      "Trained batch 1267 batch loss 0.398369908 batch mAP 0.73324585 batch PCKh 0.625\n",
      "Trained batch 1268 batch loss 0.385212213 batch mAP 0.777923584 batch PCKh 0.625\n",
      "Trained batch 1269 batch loss 0.390500784 batch mAP 0.779327393 batch PCKh 0.5625\n",
      "Trained batch 1270 batch loss 0.475929022 batch mAP 0.688690186 batch PCKh 0.5625\n",
      "Trained batch 1271 batch loss 0.516506493 batch mAP 0.635040283 batch PCKh 0.3125\n",
      "Trained batch 1272 batch loss 0.539318681 batch mAP 0.655059814 batch PCKh 0.1875\n",
      "Trained batch 1273 batch loss 0.510527 batch mAP 0.657226562 batch PCKh 0.25\n",
      "Trained batch 1274 batch loss 0.488343596 batch mAP 0.669677734 batch PCKh 0.375\n",
      "Trained batch 1275 batch loss 0.536245346 batch mAP 0.607666 batch PCKh 0.3125\n",
      "Trained batch 1276 batch loss 0.632937491 batch mAP 0.556030273 batch PCKh 0.5625\n",
      "Trained batch 1277 batch loss 0.579710186 batch mAP 0.584228516 batch PCKh 0.1875\n",
      "Trained batch 1278 batch loss 0.646922946 batch mAP 0.519744873 batch PCKh 0.1875\n",
      "Trained batch 1279 batch loss 0.579868674 batch mAP 0.630096436 batch PCKh 0.5625\n",
      "Trained batch 1280 batch loss 0.516299129 batch mAP 0.604949951 batch PCKh 0.6875\n",
      "Trained batch 1281 batch loss 0.529179 batch mAP 0.63772583 batch PCKh 0.4375\n",
      "Trained batch 1282 batch loss 0.558228374 batch mAP 0.638397217 batch PCKh 0.0625\n",
      "Trained batch 1283 batch loss 0.531232059 batch mAP 0.659362793 batch PCKh 0.375\n",
      "Trained batch 1284 batch loss 0.481890321 batch mAP 0.683959961 batch PCKh 0.6875\n",
      "Trained batch 1285 batch loss 0.444008648 batch mAP 0.699554443 batch PCKh 0.625\n",
      "Trained batch 1286 batch loss 0.484614611 batch mAP 0.719940186 batch PCKh 0.8125\n",
      "Trained batch 1287 batch loss 0.480487943 batch mAP 0.704895 batch PCKh 0.75\n",
      "Trained batch 1288 batch loss 0.436954707 batch mAP 0.668762207 batch PCKh 0.6875\n",
      "Trained batch 1289 batch loss 0.452828318 batch mAP 0.617340088 batch PCKh 0.125\n",
      "Trained batch 1290 batch loss 0.525937498 batch mAP 0.618743896 batch PCKh 0\n",
      "Trained batch 1291 batch loss 0.498533547 batch mAP 0.68649292 batch PCKh 0.375\n",
      "Trained batch 1292 batch loss 0.515316367 batch mAP 0.679901123 batch PCKh 0.5\n",
      "Trained batch 1293 batch loss 0.532157063 batch mAP 0.641296387 batch PCKh 0.75\n",
      "Trained batch 1294 batch loss 0.541962445 batch mAP 0.70892334 batch PCKh 0.6875\n",
      "Trained batch 1295 batch loss 0.527172446 batch mAP 0.724365234 batch PCKh 0.8125\n",
      "Trained batch 1296 batch loss 0.534811497 batch mAP 0.695526123 batch PCKh 0.75\n",
      "Trained batch 1297 batch loss 0.511292398 batch mAP 0.687561035 batch PCKh 0.875\n",
      "Trained batch 1298 batch loss 0.555208206 batch mAP 0.707214355 batch PCKh 0.6875\n",
      "Trained batch 1299 batch loss 0.484399557 batch mAP 0.66104126 batch PCKh 0.6875\n",
      "Trained batch 1300 batch loss 0.452152699 batch mAP 0.647705078 batch PCKh 0.625\n",
      "Trained batch 1301 batch loss 0.556794286 batch mAP 0.623352051 batch PCKh 0.6875\n",
      "Trained batch 1302 batch loss 0.469417214 batch mAP 0.615203857 batch PCKh 0.8125\n",
      "Trained batch 1303 batch loss 0.47687158 batch mAP 0.609893799 batch PCKh 0.875\n",
      "Trained batch 1304 batch loss 0.541727901 batch mAP 0.574462891 batch PCKh 0.8125\n",
      "Trained batch 1305 batch loss 0.525364161 batch mAP 0.517547607 batch PCKh 0.75\n",
      "Trained batch 1306 batch loss 0.466455936 batch mAP 0.564117432 batch PCKh 0.6875\n",
      "Trained batch 1307 batch loss 0.549075961 batch mAP 0.637084961 batch PCKh 0.125\n",
      "Trained batch 1308 batch loss 0.376204073 batch mAP 0.723327637 batch PCKh 0.5\n",
      "Trained batch 1309 batch loss 0.371336699 batch mAP 0.707519531 batch PCKh 0.625\n",
      "Trained batch 1310 batch loss 0.418252468 batch mAP 0.689209 batch PCKh 0.5625\n",
      "Trained batch 1311 batch loss 0.396045238 batch mAP 0.763763428 batch PCKh 0.4375\n",
      "Trained batch 1312 batch loss 0.440451235 batch mAP 0.72744751 batch PCKh 0.5\n",
      "Trained batch 1313 batch loss 0.499291658 batch mAP 0.686950684 batch PCKh 0.4375\n",
      "Trained batch 1314 batch loss 0.491075516 batch mAP 0.678283691 batch PCKh 0.4375\n",
      "Trained batch 1315 batch loss 0.51199615 batch mAP 0.674957275 batch PCKh 0.1875\n",
      "Trained batch 1316 batch loss 0.445420414 batch mAP 0.718139648 batch PCKh 0.8125\n",
      "Trained batch 1317 batch loss 0.485941142 batch mAP 0.676879883 batch PCKh 0.3125\n",
      "Trained batch 1318 batch loss 0.480229497 batch mAP 0.586425781 batch PCKh 0.5\n",
      "Trained batch 1319 batch loss 0.465388358 batch mAP 0.668182373 batch PCKh 0.4375\n",
      "Trained batch 1320 batch loss 0.506617665 batch mAP 0.616668701 batch PCKh 0.5\n",
      "Trained batch 1321 batch loss 0.539168179 batch mAP 0.690734863 batch PCKh 0.3125\n",
      "Trained batch 1322 batch loss 0.613068342 batch mAP 0.658294678 batch PCKh 0.6875\n",
      "Trained batch 1323 batch loss 0.548602939 batch mAP 0.658538818 batch PCKh 0.5\n",
      "Trained batch 1324 batch loss 0.51415205 batch mAP 0.639221191 batch PCKh 0.6875\n",
      "Trained batch 1325 batch loss 0.582844853 batch mAP 0.514648438 batch PCKh 0.5625\n",
      "Trained batch 1326 batch loss 0.528363168 batch mAP 0.566009521 batch PCKh 0.5\n",
      "Trained batch 1327 batch loss 0.6508829 batch mAP 0.536865234 batch PCKh 0.6875\n",
      "Trained batch 1328 batch loss 0.533902466 batch mAP 0.516021729 batch PCKh 0.5\n",
      "Trained batch 1329 batch loss 0.464173764 batch mAP 0.502746582 batch PCKh 0.25\n",
      "Trained batch 1330 batch loss 0.440811664 batch mAP 0.661224365 batch PCKh 0\n",
      "Trained batch 1331 batch loss 0.412347734 batch mAP 0.712646484 batch PCKh 0.3125\n",
      "Trained batch 1332 batch loss 0.469706446 batch mAP 0.607177734 batch PCKh 0.3125\n",
      "Trained batch 1333 batch loss 0.503049 batch mAP 0.613708496 batch PCKh 0.875\n",
      "Trained batch 1334 batch loss 0.516607165 batch mAP 0.58807373 batch PCKh 0.875\n",
      "Trained batch 1335 batch loss 0.555908442 batch mAP 0.603057861 batch PCKh 0.5\n",
      "Trained batch 1336 batch loss 0.546784699 batch mAP 0.602752686 batch PCKh 0.5625\n",
      "Trained batch 1337 batch loss 0.630374312 batch mAP 0.604980469 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1338 batch loss 0.610186696 batch mAP 0.628997803 batch PCKh 0.5625\n",
      "Trained batch 1339 batch loss 0.496594369 batch mAP 0.743713379 batch PCKh 0.625\n",
      "Trained batch 1340 batch loss 0.524614334 batch mAP 0.702911377 batch PCKh 0.5\n",
      "Trained batch 1341 batch loss 0.5427472 batch mAP 0.608306885 batch PCKh 0.25\n",
      "Trained batch 1342 batch loss 0.508650362 batch mAP 0.594421387 batch PCKh 0.5\n",
      "Trained batch 1343 batch loss 0.55274874 batch mAP 0.598907471 batch PCKh 0.25\n",
      "Trained batch 1344 batch loss 0.453256249 batch mAP 0.543304443 batch PCKh 0.25\n",
      "Trained batch 1345 batch loss 0.508496523 batch mAP 0.553070068 batch PCKh 0.1875\n",
      "Trained batch 1346 batch loss 0.484457135 batch mAP 0.560119629 batch PCKh 0.25\n",
      "Trained batch 1347 batch loss 0.497552514 batch mAP 0.565979 batch PCKh 0.6875\n",
      "Trained batch 1348 batch loss 0.46923992 batch mAP 0.626220703 batch PCKh 0.5\n",
      "Trained batch 1349 batch loss 0.495939076 batch mAP 0.592254639 batch PCKh 0.5625\n",
      "Trained batch 1350 batch loss 0.496399909 batch mAP 0.63974 batch PCKh 0.4375\n",
      "Trained batch 1351 batch loss 0.532841444 batch mAP 0.651886 batch PCKh 0.5\n",
      "Trained batch 1352 batch loss 0.585993767 batch mAP 0.562225342 batch PCKh 0.4375\n",
      "Trained batch 1353 batch loss 0.500537276 batch mAP 0.61907959 batch PCKh 0.4375\n",
      "Trained batch 1354 batch loss 0.454799682 batch mAP 0.615905762 batch PCKh 0.1875\n",
      "Trained batch 1355 batch loss 0.448918164 batch mAP 0.570098877 batch PCKh 0.5\n",
      "Trained batch 1356 batch loss 0.445049971 batch mAP 0.701080322 batch PCKh 0.4375\n",
      "Trained batch 1357 batch loss 0.491036534 batch mAP 0.666168213 batch PCKh 0.25\n",
      "Trained batch 1358 batch loss 0.53139782 batch mAP 0.659332275 batch PCKh 0.8125\n",
      "Trained batch 1359 batch loss 0.484026819 batch mAP 0.652557373 batch PCKh 0.625\n",
      "Trained batch 1360 batch loss 0.483808637 batch mAP 0.705230713 batch PCKh 0.4375\n",
      "Trained batch 1361 batch loss 0.442351699 batch mAP 0.678436279 batch PCKh 0.4375\n",
      "Trained batch 1362 batch loss 0.450999767 batch mAP 0.703704834 batch PCKh 0.4375\n",
      "Trained batch 1363 batch loss 0.348505497 batch mAP 0.733093262 batch PCKh 0.5\n",
      "Trained batch 1364 batch loss 0.394810587 batch mAP 0.732513428 batch PCKh 0.625\n",
      "Trained batch 1365 batch loss 0.374124974 batch mAP 0.700714111 batch PCKh 0.4375\n",
      "Trained batch 1366 batch loss 0.448560059 batch mAP 0.667785645 batch PCKh 0.875\n",
      "Trained batch 1367 batch loss 0.363448203 batch mAP 0.6824646 batch PCKh 0.75\n",
      "Trained batch 1368 batch loss 0.458924413 batch mAP 0.651702881 batch PCKh 0.5625\n",
      "Trained batch 1369 batch loss 0.518833756 batch mAP 0.632873535 batch PCKh 0.625\n",
      "Trained batch 1370 batch loss 0.414358199 batch mAP 0.639068604 batch PCKh 0.5\n",
      "Trained batch 1371 batch loss 0.439969 batch mAP 0.655365 batch PCKh 0.25\n",
      "Trained batch 1372 batch loss 0.491848558 batch mAP 0.685089111 batch PCKh 0.5625\n",
      "Trained batch 1373 batch loss 0.423805326 batch mAP 0.628173828 batch PCKh 0.5625\n",
      "Trained batch 1374 batch loss 0.473385394 batch mAP 0.660522461 batch PCKh 0.375\n",
      "Trained batch 1375 batch loss 0.421956182 batch mAP 0.644226074 batch PCKh 0.75\n",
      "Trained batch 1376 batch loss 0.491104245 batch mAP 0.597625732 batch PCKh 0.625\n",
      "Trained batch 1377 batch loss 0.438889742 batch mAP 0.692626953 batch PCKh 0.625\n",
      "Trained batch 1378 batch loss 0.439336598 batch mAP 0.691314697 batch PCKh 0.5\n",
      "Trained batch 1379 batch loss 0.538729489 batch mAP 0.643188477 batch PCKh 0.3125\n",
      "Trained batch 1380 batch loss 0.494643092 batch mAP 0.643890381 batch PCKh 0.125\n",
      "Trained batch 1381 batch loss 0.442538083 batch mAP 0.674194336 batch PCKh 0.6875\n",
      "Trained batch 1382 batch loss 0.55777657 batch mAP 0.655548096 batch PCKh 0.375\n",
      "Trained batch 1383 batch loss 0.487762868 batch mAP 0.683532715 batch PCKh 0.4375\n",
      "Trained batch 1384 batch loss 0.478492558 batch mAP 0.68850708 batch PCKh 0.375\n",
      "Trained batch 1385 batch loss 0.564827323 batch mAP 0.711090088 batch PCKh 0.375\n",
      "Trained batch 1386 batch loss 0.61111778 batch mAP 0.648407 batch PCKh 0.4375\n",
      "Trained batch 1387 batch loss 0.495510697 batch mAP 0.673126221 batch PCKh 0.125\n",
      "Trained batch 1388 batch loss 0.492352188 batch mAP 0.615509033 batch PCKh 0.625\n",
      "Trained batch 1389 batch loss 0.491583139 batch mAP 0.571136475 batch PCKh 0.6875\n",
      "Trained batch 1390 batch loss 0.426565647 batch mAP 0.553344727 batch PCKh 0.6875\n",
      "Trained batch 1391 batch loss 0.424533725 batch mAP 0.587493896 batch PCKh 0.6875\n",
      "Trained batch 1392 batch loss 0.46857053 batch mAP 0.553314209 batch PCKh 0.375\n",
      "Trained batch 1393 batch loss 0.457147658 batch mAP 0.703460693 batch PCKh 0.875\n",
      "Trained batch 1394 batch loss 0.509022951 batch mAP 0.646484375 batch PCKh 0.625\n",
      "Trained batch 1395 batch loss 0.530205846 batch mAP 0.666229248 batch PCKh 0.0625\n",
      "Trained batch 1396 batch loss 0.534115911 batch mAP 0.619415283 batch PCKh 0.4375\n",
      "Trained batch 1397 batch loss 0.472205907 batch mAP 0.611175537 batch PCKh 0.4375\n",
      "Trained batch 1398 batch loss 0.552500844 batch mAP 0.681060791 batch PCKh 0.1875\n",
      "Trained batch 1399 batch loss 0.506136656 batch mAP 0.663024902 batch PCKh 0.6875\n",
      "Trained batch 1400 batch loss 0.574113727 batch mAP 0.635528564 batch PCKh 0.3125\n",
      "Trained batch 1401 batch loss 0.548921704 batch mAP 0.622314453 batch PCKh 0.0625\n",
      "Trained batch 1402 batch loss 0.527395 batch mAP 0.665161133 batch PCKh 0.8125\n",
      "Trained batch 1403 batch loss 0.608989596 batch mAP 0.547149658 batch PCKh 0\n",
      "Trained batch 1404 batch loss 0.611034751 batch mAP 0.567993164 batch PCKh 0\n",
      "Trained batch 1405 batch loss 0.627529621 batch mAP 0.504150391 batch PCKh 0.3125\n",
      "Trained batch 1406 batch loss 0.567428112 batch mAP 0.606750488 batch PCKh 0.375\n",
      "Trained batch 1407 batch loss 0.615624607 batch mAP 0.497375488 batch PCKh 0.125\n",
      "Trained batch 1408 batch loss 0.606335938 batch mAP 0.502868652 batch PCKh 0\n",
      "Trained batch 1409 batch loss 0.593474329 batch mAP 0.533630371 batch PCKh 0.25\n",
      "Trained batch 1410 batch loss 0.549043179 batch mAP 0.606414795 batch PCKh 0.5\n",
      "Trained batch 1411 batch loss 0.499419808 batch mAP 0.593658447 batch PCKh 0.25\n",
      "Trained batch 1412 batch loss 0.46025613 batch mAP 0.630523682 batch PCKh 0.25\n",
      "Trained batch 1413 batch loss 0.459007829 batch mAP 0.623352051 batch PCKh 0.6875\n",
      "Trained batch 1414 batch loss 0.615018 batch mAP 0.460693359 batch PCKh 0.625\n",
      "Trained batch 1415 batch loss 0.553983688 batch mAP 0.574859619 batch PCKh 0.3125\n",
      "Trained batch 1416 batch loss 0.623041451 batch mAP 0.567230225 batch PCKh 0.3125\n",
      "Trained batch 1417 batch loss 0.523645282 batch mAP 0.632232666 batch PCKh 0.3125\n",
      "Trained batch 1418 batch loss 0.484164 batch mAP 0.662750244 batch PCKh 0.5\n",
      "Trained batch 1419 batch loss 0.484459758 batch mAP 0.712310791 batch PCKh 0.6875\n",
      "Trained batch 1420 batch loss 0.503825307 batch mAP 0.721466064 batch PCKh 0.6875\n",
      "Trained batch 1421 batch loss 0.447064698 batch mAP 0.705108643 batch PCKh 0.8125\n",
      "Trained batch 1422 batch loss 0.4410972 batch mAP 0.68536377 batch PCKh 0.8125\n",
      "Trained batch 1423 batch loss 0.497378588 batch mAP 0.689697266 batch PCKh 0.5625\n",
      "Trained batch 1424 batch loss 0.56314379 batch mAP 0.636291504 batch PCKh 0.4375\n",
      "Trained batch 1425 batch loss 0.445000738 batch mAP 0.705963135 batch PCKh 0.5\n",
      "Trained batch 1426 batch loss 0.487811387 batch mAP 0.674224854 batch PCKh 0.4375\n",
      "Trained batch 1427 batch loss 0.443418503 batch mAP 0.728424072 batch PCKh 0.5625\n",
      "Trained batch 1428 batch loss 0.499658048 batch mAP 0.683685303 batch PCKh 0.0625\n",
      "Trained batch 1429 batch loss 0.466261625 batch mAP 0.685974121 batch PCKh 0.625\n",
      "Trained batch 1430 batch loss 0.49443233 batch mAP 0.675445557 batch PCKh 0.375\n",
      "Trained batch 1431 batch loss 0.500985682 batch mAP 0.705108643 batch PCKh 0.4375\n",
      "Trained batch 1432 batch loss 0.507670045 batch mAP 0.628082275 batch PCKh 0.4375\n",
      "Trained batch 1433 batch loss 0.510550261 batch mAP 0.641052246 batch PCKh 0.25\n",
      "Trained batch 1434 batch loss 0.503881633 batch mAP 0.618896484 batch PCKh 0.6875\n",
      "Trained batch 1435 batch loss 0.536941051 batch mAP 0.659515381 batch PCKh 0.6875\n",
      "Trained batch 1436 batch loss 0.525477767 batch mAP 0.646331787 batch PCKh 0.4375\n",
      "Trained batch 1437 batch loss 0.548310399 batch mAP 0.661560059 batch PCKh 0.4375\n",
      "Trained batch 1438 batch loss 0.560444832 batch mAP 0.669708252 batch PCKh 0.25\n",
      "Trained batch 1439 batch loss 0.498445451 batch mAP 0.68170166 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1440 batch loss 0.520982742 batch mAP 0.669281 batch PCKh 0.375\n",
      "Trained batch 1441 batch loss 0.602948189 batch mAP 0.576293945 batch PCKh 0.125\n",
      "Trained batch 1442 batch loss 0.595399499 batch mAP 0.617034912 batch PCKh 0.1875\n",
      "Trained batch 1443 batch loss 0.696988702 batch mAP 0.543060303 batch PCKh 0.125\n",
      "Trained batch 1444 batch loss 0.508111 batch mAP 0.659576416 batch PCKh 0.625\n",
      "Trained batch 1445 batch loss 0.431543708 batch mAP 0.662902832 batch PCKh 0.75\n",
      "Trained batch 1446 batch loss 0.495433539 batch mAP 0.566741943 batch PCKh 0.25\n",
      "Trained batch 1447 batch loss 0.662127137 batch mAP 0.533172607 batch PCKh 0.5\n",
      "Trained batch 1448 batch loss 0.563581 batch mAP 0.559814453 batch PCKh 0.125\n",
      "Trained batch 1449 batch loss 0.635903 batch mAP 0.567596436 batch PCKh 0.625\n",
      "Trained batch 1450 batch loss 0.576715648 batch mAP 0.57409668 batch PCKh 0.875\n",
      "Trained batch 1451 batch loss 0.486735642 batch mAP 0.581115723 batch PCKh 0.5\n",
      "Trained batch 1452 batch loss 0.558005929 batch mAP 0.598693848 batch PCKh 0.6875\n",
      "Trained batch 1453 batch loss 0.466222823 batch mAP 0.624053955 batch PCKh 0.375\n",
      "Trained batch 1454 batch loss 0.507774293 batch mAP 0.593505859 batch PCKh 0.4375\n",
      "Trained batch 1455 batch loss 0.475731492 batch mAP 0.59954834 batch PCKh 0.5\n",
      "Trained batch 1456 batch loss 0.447083145 batch mAP 0.546417236 batch PCKh 0.5\n",
      "Trained batch 1457 batch loss 0.520984352 batch mAP 0.635467529 batch PCKh 0.25\n",
      "Trained batch 1458 batch loss 0.400868893 batch mAP 0.647949219 batch PCKh 0.625\n",
      "Trained batch 1459 batch loss 0.488720298 batch mAP 0.610137939 batch PCKh 0.375\n",
      "Trained batch 1460 batch loss 0.555948436 batch mAP 0.593139648 batch PCKh 0.625\n",
      "Trained batch 1461 batch loss 0.503643155 batch mAP 0.646759033 batch PCKh 0.625\n",
      "Trained batch 1462 batch loss 0.547991276 batch mAP 0.666717529 batch PCKh 0.625\n",
      "Trained batch 1463 batch loss 0.522844791 batch mAP 0.660644531 batch PCKh 0.5625\n",
      "Trained batch 1464 batch loss 0.494045228 batch mAP 0.615112305 batch PCKh 0.625\n",
      "Trained batch 1465 batch loss 0.533369 batch mAP 0.582458496 batch PCKh 0.375\n",
      "Trained batch 1466 batch loss 0.577898622 batch mAP 0.674407959 batch PCKh 0.375\n",
      "Trained batch 1467 batch loss 0.481506228 batch mAP 0.68850708 batch PCKh 0.5\n",
      "Trained batch 1468 batch loss 0.465814561 batch mAP 0.716491699 batch PCKh 0.375\n",
      "Trained batch 1469 batch loss 0.55708456 batch mAP 0.697509766 batch PCKh 0.75\n",
      "Trained batch 1470 batch loss 0.586670041 batch mAP 0.688354492 batch PCKh 0.3125\n",
      "Trained batch 1471 batch loss 0.505194247 batch mAP 0.684631348 batch PCKh 0.5\n",
      "Trained batch 1472 batch loss 0.515874267 batch mAP 0.689666748 batch PCKh 0.25\n",
      "Trained batch 1473 batch loss 0.507562339 batch mAP 0.603668213 batch PCKh 0.5\n",
      "Trained batch 1474 batch loss 0.552935183 batch mAP 0.550506592 batch PCKh 0.375\n",
      "Trained batch 1475 batch loss 0.542643964 batch mAP 0.573364258 batch PCKh 0.6875\n",
      "Trained batch 1476 batch loss 0.467167318 batch mAP 0.556732178 batch PCKh 0\n",
      "Trained batch 1477 batch loss 0.531027853 batch mAP 0.570770264 batch PCKh 0.25\n",
      "Trained batch 1478 batch loss 0.548073173 batch mAP 0.535003662 batch PCKh 0.6875\n",
      "Trained batch 1479 batch loss 0.613721609 batch mAP 0.529022217 batch PCKh 0.6875\n",
      "Trained batch 1480 batch loss 0.590597093 batch mAP 0.598144531 batch PCKh 0.8125\n",
      "Trained batch 1481 batch loss 0.47982654 batch mAP 0.638793945 batch PCKh 0\n",
      "Trained batch 1482 batch loss 0.586421847 batch mAP 0.677032471 batch PCKh 0.75\n",
      "Trained batch 1483 batch loss 0.52112329 batch mAP 0.668762207 batch PCKh 0.875\n",
      "Trained batch 1484 batch loss 0.539464593 batch mAP 0.676727295 batch PCKh 0.75\n",
      "Trained batch 1485 batch loss 0.588304758 batch mAP 0.626068115 batch PCKh 0.75\n",
      "Trained batch 1486 batch loss 0.483350039 batch mAP 0.640686035 batch PCKh 0.6875\n",
      "Trained batch 1487 batch loss 0.477733493 batch mAP 0.664276123 batch PCKh 0.8125\n",
      "Trained batch 1488 batch loss 0.569282591 batch mAP 0.627166748 batch PCKh 0.625\n",
      "Trained batch 1489 batch loss 0.464063942 batch mAP 0.633575439 batch PCKh 0.5625\n",
      "Trained batch 1490 batch loss 0.53667593 batch mAP 0.657745361 batch PCKh 0.5625\n",
      "Trained batch 1491 batch loss 0.583275914 batch mAP 0.620941162 batch PCKh 0.5625\n",
      "Trained batch 1492 batch loss 0.559262872 batch mAP 0.599182129 batch PCKh 0.3125\n",
      "Trained batch 1493 batch loss 0.624743164 batch mAP 0.589599609 batch PCKh 0.625\n",
      "Trained batch 1494 batch loss 0.62765789 batch mAP 0.589294434 batch PCKh 0.25\n",
      "Trained batch 1495 batch loss 0.584912419 batch mAP 0.579925537 batch PCKh 0.625\n",
      "Trained batch 1496 batch loss 0.50025326 batch mAP 0.637573242 batch PCKh 0.375\n",
      "Trained batch 1497 batch loss 0.500212908 batch mAP 0.632080078 batch PCKh 0.75\n",
      "Trained batch 1498 batch loss 0.486262411 batch mAP 0.636077881 batch PCKh 0.5\n",
      "Trained batch 1499 batch loss 0.553042471 batch mAP 0.566436768 batch PCKh 0.5625\n",
      "Trained batch 1500 batch loss 0.547666311 batch mAP 0.625824 batch PCKh 0.5625\n",
      "Trained batch 1501 batch loss 0.503410161 batch mAP 0.614013672 batch PCKh 0.375\n",
      "Trained batch 1502 batch loss 0.446973115 batch mAP 0.658447266 batch PCKh 0.375\n",
      "Trained batch 1503 batch loss 0.599260807 batch mAP 0.607940674 batch PCKh 0.25\n",
      "Trained batch 1504 batch loss 0.535469234 batch mAP 0.669708252 batch PCKh 0.125\n",
      "Trained batch 1505 batch loss 0.533603728 batch mAP 0.706115723 batch PCKh 0.5\n",
      "Trained batch 1506 batch loss 0.556234896 batch mAP 0.67779541 batch PCKh 0.3125\n",
      "Trained batch 1507 batch loss 0.491199702 batch mAP 0.660797119 batch PCKh 0.6875\n",
      "Trained batch 1508 batch loss 0.528006792 batch mAP 0.612365723 batch PCKh 0.8125\n",
      "Trained batch 1509 batch loss 0.480849653 batch mAP 0.627532959 batch PCKh 0.75\n",
      "Trained batch 1510 batch loss 0.557706416 batch mAP 0.617401123 batch PCKh 0.6875\n",
      "Trained batch 1511 batch loss 0.539757729 batch mAP 0.640441895 batch PCKh 0.8125\n",
      "Trained batch 1512 batch loss 0.437449813 batch mAP 0.615661621 batch PCKh 0.375\n",
      "Trained batch 1513 batch loss 0.601708055 batch mAP 0.586334229 batch PCKh 0.5\n",
      "Trained batch 1514 batch loss 0.576776564 batch mAP 0.586792 batch PCKh 0.4375\n",
      "Trained batch 1515 batch loss 0.515233278 batch mAP 0.630004883 batch PCKh 0.375\n",
      "Trained batch 1516 batch loss 0.533969402 batch mAP 0.68447876 batch PCKh 0.375\n",
      "Trained batch 1517 batch loss 0.530496776 batch mAP 0.620178223 batch PCKh 0.6875\n",
      "Trained batch 1518 batch loss 0.527645469 batch mAP 0.640960693 batch PCKh 0.6875\n",
      "Trained batch 1519 batch loss 0.560013652 batch mAP 0.630310059 batch PCKh 0.8125\n",
      "Trained batch 1520 batch loss 0.46871382 batch mAP 0.66293335 batch PCKh 0.75\n",
      "Trained batch 1521 batch loss 0.525274396 batch mAP 0.640289307 batch PCKh 0.6875\n",
      "Trained batch 1522 batch loss 0.520644069 batch mAP 0.642730713 batch PCKh 0.75\n",
      "Trained batch 1523 batch loss 0.579413056 batch mAP 0.6222229 batch PCKh 0.875\n",
      "Trained batch 1524 batch loss 0.427897751 batch mAP 0.63180542 batch PCKh 0.75\n",
      "Trained batch 1525 batch loss 0.517400861 batch mAP 0.621154785 batch PCKh 0.625\n",
      "Trained batch 1526 batch loss 0.528699279 batch mAP 0.702484131 batch PCKh 0.4375\n",
      "Trained batch 1527 batch loss 0.470646918 batch mAP 0.67779541 batch PCKh 0.375\n",
      "Trained batch 1528 batch loss 0.454001248 batch mAP 0.710022 batch PCKh 0.875\n",
      "Trained batch 1529 batch loss 0.451427877 batch mAP 0.651031494 batch PCKh 0.5625\n",
      "Trained batch 1530 batch loss 0.418995947 batch mAP 0.709075928 batch PCKh 0.75\n",
      "Trained batch 1531 batch loss 0.457435608 batch mAP 0.678741455 batch PCKh 0.875\n",
      "Trained batch 1532 batch loss 0.495542705 batch mAP 0.675842285 batch PCKh 0.5625\n",
      "Trained batch 1533 batch loss 0.407587588 batch mAP 0.683990479 batch PCKh 0.5625\n",
      "Trained batch 1534 batch loss 0.47303161 batch mAP 0.711364746 batch PCKh 0.75\n",
      "Trained batch 1535 batch loss 0.526565909 batch mAP 0.653503418 batch PCKh 0.5625\n",
      "Trained batch 1536 batch loss 0.421959162 batch mAP 0.690124512 batch PCKh 0.3125\n",
      "Trained batch 1537 batch loss 0.455215693 batch mAP 0.707702637 batch PCKh 0.375\n",
      "Trained batch 1538 batch loss 0.439166754 batch mAP 0.707061768 batch PCKh 0.6875\n",
      "Trained batch 1539 batch loss 0.432684481 batch mAP 0.718017578 batch PCKh 0.6875\n",
      "Trained batch 1540 batch loss 0.41931814 batch mAP 0.667358398 batch PCKh 0.1875\n",
      "Trained batch 1541 batch loss 0.428454757 batch mAP 0.699401855 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1542 batch loss 0.405369341 batch mAP 0.710968 batch PCKh 0.25\n",
      "Trained batch 1543 batch loss 0.466279358 batch mAP 0.691436768 batch PCKh 0.75\n",
      "Trained batch 1544 batch loss 0.505589 batch mAP 0.663574219 batch PCKh 0.5625\n",
      "Trained batch 1545 batch loss 0.48554492 batch mAP 0.670928955 batch PCKh 0.125\n",
      "Trained batch 1546 batch loss 0.653909564 batch mAP 0.591766357 batch PCKh 0.3125\n",
      "Trained batch 1547 batch loss 0.492401719 batch mAP 0.594055176 batch PCKh 0.1875\n",
      "Trained batch 1548 batch loss 0.476607174 batch mAP 0.620727539 batch PCKh 0.625\n",
      "Trained batch 1549 batch loss 0.461689532 batch mAP 0.61819458 batch PCKh 0.4375\n",
      "Trained batch 1550 batch loss 0.440375775 batch mAP 0.605011 batch PCKh 0\n",
      "Trained batch 1551 batch loss 0.513783455 batch mAP 0.553466797 batch PCKh 0.3125\n",
      "Trained batch 1552 batch loss 0.491439402 batch mAP 0.590820312 batch PCKh 0.4375\n",
      "Trained batch 1553 batch loss 0.509027243 batch mAP 0.570526123 batch PCKh 0.6875\n",
      "Trained batch 1554 batch loss 0.491167188 batch mAP 0.563324 batch PCKh 0.0625\n",
      "Trained batch 1555 batch loss 0.430561244 batch mAP 0.634735107 batch PCKh 0.25\n",
      "Trained batch 1556 batch loss 0.433417499 batch mAP 0.684753418 batch PCKh 0.75\n",
      "Trained batch 1557 batch loss 0.409398943 batch mAP 0.674133301 batch PCKh 0.625\n",
      "Trained batch 1558 batch loss 0.443176448 batch mAP 0.628295898 batch PCKh 0.5625\n",
      "Trained batch 1559 batch loss 0.566205382 batch mAP 0.643249512 batch PCKh 0.3125\n",
      "Trained batch 1560 batch loss 0.550770581 batch mAP 0.598053 batch PCKh 0.625\n",
      "Trained batch 1561 batch loss 0.485802621 batch mAP 0.607025146 batch PCKh 0.5\n",
      "Trained batch 1562 batch loss 0.488052964 batch mAP 0.669006348 batch PCKh 0.875\n",
      "Trained batch 1563 batch loss 0.521727204 batch mAP 0.615203857 batch PCKh 0.4375\n",
      "Trained batch 1564 batch loss 0.537678361 batch mAP 0.643005371 batch PCKh 0.1875\n",
      "Trained batch 1565 batch loss 0.497987241 batch mAP 0.580627441 batch PCKh 0.625\n",
      "Trained batch 1566 batch loss 0.525095224 batch mAP 0.641876221 batch PCKh 0.5\n",
      "Trained batch 1567 batch loss 0.538499117 batch mAP 0.637664795 batch PCKh 0.25\n",
      "Trained batch 1568 batch loss 0.507964194 batch mAP 0.660888672 batch PCKh 0.4375\n",
      "Trained batch 1569 batch loss 0.518572 batch mAP 0.600708 batch PCKh 0.1875\n",
      "Trained batch 1570 batch loss 0.538769841 batch mAP 0.660125732 batch PCKh 0.25\n",
      "Trained batch 1571 batch loss 0.492873132 batch mAP 0.59161377 batch PCKh 0.3125\n",
      "Trained batch 1572 batch loss 0.541568637 batch mAP 0.619354248 batch PCKh 0.875\n",
      "Trained batch 1573 batch loss 0.494137377 batch mAP 0.57547 batch PCKh 0.625\n",
      "Trained batch 1574 batch loss 0.524260044 batch mAP 0.652984619 batch PCKh 0.3125\n",
      "Trained batch 1575 batch loss 0.468119383 batch mAP 0.73349 batch PCKh 0.8125\n",
      "Trained batch 1576 batch loss 0.413693041 batch mAP 0.703521729 batch PCKh 0.8125\n",
      "Trained batch 1577 batch loss 0.487870097 batch mAP 0.63797 batch PCKh 0.5\n",
      "Trained batch 1578 batch loss 0.455283642 batch mAP 0.684387207 batch PCKh 0.8125\n",
      "Trained batch 1579 batch loss 0.427560061 batch mAP 0.702026367 batch PCKh 0.3125\n",
      "Trained batch 1580 batch loss 0.380875945 batch mAP 0.741546631 batch PCKh 0.75\n",
      "Trained batch 1581 batch loss 0.402976096 batch mAP 0.745666504 batch PCKh 0.5625\n",
      "Trained batch 1582 batch loss 0.489770502 batch mAP 0.657806396 batch PCKh 0.625\n",
      "Trained batch 1583 batch loss 0.530502439 batch mAP 0.63067627 batch PCKh 0.3125\n",
      "Trained batch 1584 batch loss 0.514531076 batch mAP 0.625366211 batch PCKh 0.625\n",
      "Trained batch 1585 batch loss 0.505042374 batch mAP 0.654296875 batch PCKh 0.75\n",
      "Trained batch 1586 batch loss 0.532683253 batch mAP 0.623809814 batch PCKh 0.4375\n",
      "Trained batch 1587 batch loss 0.501777351 batch mAP 0.68069458 batch PCKh 0.1875\n",
      "Trained batch 1588 batch loss 0.50635004 batch mAP 0.634490967 batch PCKh 0.5625\n",
      "Trained batch 1589 batch loss 0.498691738 batch mAP 0.671020508 batch PCKh 0.5625\n",
      "Trained batch 1590 batch loss 0.504706204 batch mAP 0.650390625 batch PCKh 0.625\n",
      "Trained batch 1591 batch loss 0.524287939 batch mAP 0.649993896 batch PCKh 0.75\n",
      "Trained batch 1592 batch loss 0.529528856 batch mAP 0.613067627 batch PCKh 0.4375\n",
      "Trained batch 1593 batch loss 0.570797086 batch mAP 0.609344482 batch PCKh 0.4375\n",
      "Trained batch 1594 batch loss 0.501341939 batch mAP 0.642944336 batch PCKh 0.8125\n",
      "Trained batch 1595 batch loss 0.516354859 batch mAP 0.602661133 batch PCKh 0.125\n",
      "Trained batch 1596 batch loss 0.432494938 batch mAP 0.663269043 batch PCKh 0.4375\n",
      "Trained batch 1597 batch loss 0.481872588 batch mAP 0.690826416 batch PCKh 0.4375\n",
      "Trained batch 1598 batch loss 0.527451754 batch mAP 0.677734375 batch PCKh 0.6875\n",
      "Trained batch 1599 batch loss 0.509138823 batch mAP 0.650604248 batch PCKh 0.375\n",
      "Trained batch 1600 batch loss 0.392938048 batch mAP 0.675231934 batch PCKh 0.8125\n",
      "Trained batch 1601 batch loss 0.437108368 batch mAP 0.660125732 batch PCKh 0.375\n",
      "Trained batch 1602 batch loss 0.383557856 batch mAP 0.709960938 batch PCKh 0.6875\n",
      "Trained batch 1603 batch loss 0.489510775 batch mAP 0.668823242 batch PCKh 0.875\n",
      "Trained batch 1604 batch loss 0.4844594 batch mAP 0.646850586 batch PCKh 0.625\n",
      "Trained batch 1605 batch loss 0.401766628 batch mAP 0.688964844 batch PCKh 0.75\n",
      "Trained batch 1606 batch loss 0.430587232 batch mAP 0.641571045 batch PCKh 0.875\n",
      "Trained batch 1607 batch loss 0.470489025 batch mAP 0.595092773 batch PCKh 0.5625\n",
      "Trained batch 1608 batch loss 0.511924207 batch mAP 0.641052246 batch PCKh 0.875\n",
      "Trained batch 1609 batch loss 0.568579555 batch mAP 0.536651611 batch PCKh 0.0625\n",
      "Trained batch 1610 batch loss 0.6453107 batch mAP 0.568206787 batch PCKh 0.5\n",
      "Trained batch 1611 batch loss 0.582072 batch mAP 0.62197876 batch PCKh 0.8125\n",
      "Trained batch 1612 batch loss 0.58725059 batch mAP 0.622436523 batch PCKh 0.625\n",
      "Trained batch 1613 batch loss 0.58575052 batch mAP 0.603393555 batch PCKh 0.25\n",
      "Trained batch 1614 batch loss 0.583409369 batch mAP 0.659393311 batch PCKh 0.625\n",
      "Trained batch 1615 batch loss 0.558818221 batch mAP 0.649505615 batch PCKh 0.5\n",
      "Trained batch 1616 batch loss 0.501084507 batch mAP 0.692047119 batch PCKh 0.625\n",
      "Trained batch 1617 batch loss 0.573473573 batch mAP 0.624511719 batch PCKh 0.5625\n",
      "Trained batch 1618 batch loss 0.570907176 batch mAP 0.593414307 batch PCKh 0.25\n",
      "Trained batch 1619 batch loss 0.573496282 batch mAP 0.54876709 batch PCKh 0.0625\n",
      "Trained batch 1620 batch loss 0.546369195 batch mAP 0.615539551 batch PCKh 0.5625\n",
      "Trained batch 1621 batch loss 0.532344937 batch mAP 0.64175415 batch PCKh 0.5\n",
      "Trained batch 1622 batch loss 0.461558223 batch mAP 0.692169189 batch PCKh 0.75\n",
      "Trained batch 1623 batch loss 0.389670074 batch mAP 0.660247803 batch PCKh 0.6875\n",
      "Trained batch 1624 batch loss 0.391529977 batch mAP 0.63079834 batch PCKh 0.25\n",
      "Trained batch 1625 batch loss 0.478938 batch mAP 0.586853 batch PCKh 0.625\n",
      "Trained batch 1626 batch loss 0.552339911 batch mAP 0.665313721 batch PCKh 0.875\n",
      "Trained batch 1627 batch loss 0.537309229 batch mAP 0.696258545 batch PCKh 0.375\n",
      "Trained batch 1628 batch loss 0.47011131 batch mAP 0.594207764 batch PCKh 0.0625\n",
      "Trained batch 1629 batch loss 0.578952193 batch mAP 0.563385 batch PCKh 0.3125\n",
      "Trained batch 1630 batch loss 0.566852808 batch mAP 0.605499268 batch PCKh 0.5625\n",
      "Trained batch 1631 batch loss 0.472799957 batch mAP 0.596191406 batch PCKh 0.5625\n",
      "Trained batch 1632 batch loss 0.552144825 batch mAP 0.585022 batch PCKh 0.5\n",
      "Trained batch 1633 batch loss 0.496794522 batch mAP 0.624816895 batch PCKh 0.6875\n",
      "Trained batch 1634 batch loss 0.465036273 batch mAP 0.632049561 batch PCKh 0.625\n",
      "Trained batch 1635 batch loss 0.437216759 batch mAP 0.543182373 batch PCKh 0.0625\n",
      "Trained batch 1636 batch loss 0.43042329 batch mAP 0.639343262 batch PCKh 0.8125\n",
      "Trained batch 1637 batch loss 0.421738178 batch mAP 0.667175293 batch PCKh 0.1875\n",
      "Trained batch 1638 batch loss 0.43094027 batch mAP 0.723968506 batch PCKh 0.5625\n",
      "Trained batch 1639 batch loss 0.367357492 batch mAP 0.704162598 batch PCKh 0.25\n",
      "Trained batch 1640 batch loss 0.365459353 batch mAP 0.733306885 batch PCKh 0.375\n",
      "Trained batch 1641 batch loss 0.480212271 batch mAP 0.605011 batch PCKh 0.625\n",
      "Trained batch 1642 batch loss 0.426971823 batch mAP 0.692565918 batch PCKh 0.625\n",
      "Trained batch 1643 batch loss 0.507907033 batch mAP 0.67300415 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1644 batch loss 0.489005387 batch mAP 0.639038086 batch PCKh 0.5625\n",
      "Trained batch 1645 batch loss 0.426273197 batch mAP 0.687255859 batch PCKh 0.625\n",
      "Trained batch 1646 batch loss 0.493372619 batch mAP 0.656921387 batch PCKh 0.3125\n",
      "Trained batch 1647 batch loss 0.519209623 batch mAP 0.7137146 batch PCKh 0.375\n",
      "Trained batch 1648 batch loss 0.53718257 batch mAP 0.677063 batch PCKh 0.375\n",
      "Trained batch 1649 batch loss 0.631480694 batch mAP 0.575500488 batch PCKh 0.4375\n",
      "Trained batch 1650 batch loss 0.558259249 batch mAP 0.609649658 batch PCKh 0.6875\n",
      "Trained batch 1651 batch loss 0.681173325 batch mAP 0.62310791 batch PCKh 0.375\n",
      "Trained batch 1652 batch loss 0.490689814 batch mAP 0.653625488 batch PCKh 0.6875\n",
      "Trained batch 1653 batch loss 0.492053598 batch mAP 0.564331055 batch PCKh 0.125\n",
      "Trained batch 1654 batch loss 0.448557377 batch mAP 0.609802246 batch PCKh 0.5\n",
      "Trained batch 1655 batch loss 0.500112891 batch mAP 0.610199 batch PCKh 0.1875\n",
      "Trained batch 1656 batch loss 0.378876388 batch mAP 0.642364502 batch PCKh 0.4375\n",
      "Trained batch 1657 batch loss 0.44810608 batch mAP 0.623260498 batch PCKh 0.125\n",
      "Trained batch 1658 batch loss 0.562778711 batch mAP 0.565368652 batch PCKh 0.1875\n",
      "Trained batch 1659 batch loss 0.599308848 batch mAP 0.567382812 batch PCKh 0.25\n",
      "Trained batch 1660 batch loss 0.553835213 batch mAP 0.609863281 batch PCKh 0.5625\n",
      "Trained batch 1661 batch loss 0.579894543 batch mAP 0.563781738 batch PCKh 0.25\n",
      "Trained batch 1662 batch loss 0.563865423 batch mAP 0.622161865 batch PCKh 0.5625\n",
      "Trained batch 1663 batch loss 0.537821233 batch mAP 0.59967041 batch PCKh 0.1875\n",
      "Trained batch 1664 batch loss 0.60817945 batch mAP 0.571777344 batch PCKh 0.75\n",
      "Trained batch 1665 batch loss 0.503192425 batch mAP 0.634796143 batch PCKh 0.5\n",
      "Trained batch 1666 batch loss 0.519895434 batch mAP 0.587890625 batch PCKh 0.4375\n",
      "Trained batch 1667 batch loss 0.545535088 batch mAP 0.668457031 batch PCKh 0.3125\n",
      "Trained batch 1668 batch loss 0.56323 batch mAP 0.667511 batch PCKh 0.375\n",
      "Trained batch 1669 batch loss 0.561565399 batch mAP 0.616912842 batch PCKh 0.75\n",
      "Trained batch 1670 batch loss 0.517731786 batch mAP 0.663360596 batch PCKh 0.25\n",
      "Trained batch 1671 batch loss 0.523016155 batch mAP 0.590332031 batch PCKh 0.375\n",
      "Trained batch 1672 batch loss 0.550786793 batch mAP 0.54498291 batch PCKh 0.5625\n",
      "Trained batch 1673 batch loss 0.457555175 batch mAP 0.585327148 batch PCKh 0.1875\n",
      "Trained batch 1674 batch loss 0.559775949 batch mAP 0.519928 batch PCKh 0.25\n",
      "Trained batch 1675 batch loss 0.484984547 batch mAP 0.554992676 batch PCKh 0.1875\n",
      "Trained batch 1676 batch loss 0.510836601 batch mAP 0.516601562 batch PCKh 0.375\n",
      "Trained batch 1677 batch loss 0.503721237 batch mAP 0.635314941 batch PCKh 0.4375\n",
      "Trained batch 1678 batch loss 0.536251962 batch mAP 0.578796387 batch PCKh 0.5625\n",
      "Trained batch 1679 batch loss 0.527626872 batch mAP 0.63848877 batch PCKh 0.875\n",
      "Trained batch 1680 batch loss 0.483856082 batch mAP 0.633911133 batch PCKh 0.75\n",
      "Trained batch 1681 batch loss 0.542583 batch mAP 0.644958496 batch PCKh 0.75\n",
      "Trained batch 1682 batch loss 0.569226205 batch mAP 0.609100342 batch PCKh 0.75\n",
      "Trained batch 1683 batch loss 0.523829579 batch mAP 0.620666504 batch PCKh 0.4375\n",
      "Trained batch 1684 batch loss 0.548921704 batch mAP 0.663848877 batch PCKh 0.375\n",
      "Trained batch 1685 batch loss 0.563804507 batch mAP 0.607788086 batch PCKh 0.6875\n",
      "Trained batch 1686 batch loss 0.533597946 batch mAP 0.632659912 batch PCKh 0.875\n",
      "Trained batch 1687 batch loss 0.547911525 batch mAP 0.628753662 batch PCKh 0.1875\n",
      "Trained batch 1688 batch loss 0.579038858 batch mAP 0.612243652 batch PCKh 0.375\n",
      "Trained batch 1689 batch loss 0.569841504 batch mAP 0.619445801 batch PCKh 0.25\n",
      "Trained batch 1690 batch loss 0.546021 batch mAP 0.67678833 batch PCKh 0.75\n",
      "Trained batch 1691 batch loss 0.553529859 batch mAP 0.664215088 batch PCKh 0.4375\n",
      "Trained batch 1692 batch loss 0.510319471 batch mAP 0.644104 batch PCKh 0.4375\n",
      "Trained batch 1693 batch loss 0.511113405 batch mAP 0.600769043 batch PCKh 0.625\n",
      "Trained batch 1694 batch loss 0.529164076 batch mAP 0.618591309 batch PCKh 0.5\n",
      "Trained batch 1695 batch loss 0.442759573 batch mAP 0.653442383 batch PCKh 0.4375\n",
      "Trained batch 1696 batch loss 0.446884423 batch mAP 0.661895752 batch PCKh 0.625\n",
      "Trained batch 1697 batch loss 0.484843016 batch mAP 0.670593262 batch PCKh 0.75\n",
      "Trained batch 1698 batch loss 0.510957778 batch mAP 0.611297607 batch PCKh 0.25\n",
      "Trained batch 1699 batch loss 0.532437086 batch mAP 0.65802 batch PCKh 0.625\n",
      "Trained batch 1700 batch loss 0.521649122 batch mAP 0.685821533 batch PCKh 0.5625\n",
      "Trained batch 1701 batch loss 0.489331186 batch mAP 0.654785156 batch PCKh 0.5625\n",
      "Trained batch 1702 batch loss 0.505349398 batch mAP 0.568817139 batch PCKh 0.6875\n",
      "Trained batch 1703 batch loss 0.59065187 batch mAP 0.588439941 batch PCKh 0.8125\n",
      "Trained batch 1704 batch loss 0.537682116 batch mAP 0.514312744 batch PCKh 0.25\n",
      "Trained batch 1705 batch loss 0.588647783 batch mAP 0.541442871 batch PCKh 0.75\n",
      "Trained batch 1706 batch loss 0.592212558 batch mAP 0.52645874 batch PCKh 0\n",
      "Trained batch 1707 batch loss 0.577952921 batch mAP 0.577178955 batch PCKh 0.75\n",
      "Trained batch 1708 batch loss 0.55361259 batch mAP 0.54586792 batch PCKh 0.1875\n",
      "Trained batch 1709 batch loss 0.57790792 batch mAP 0.530914307 batch PCKh 0.3125\n",
      "Trained batch 1710 batch loss 0.554244518 batch mAP 0.636474609 batch PCKh 0.5625\n",
      "Trained batch 1711 batch loss 0.521567106 batch mAP 0.614990234 batch PCKh 0.6875\n",
      "Trained batch 1712 batch loss 0.477429271 batch mAP 0.683654785 batch PCKh 0.6875\n",
      "Trained batch 1713 batch loss 0.483614266 batch mAP 0.703735352 batch PCKh 0.5\n",
      "Trained batch 1714 batch loss 0.475626558 batch mAP 0.691619873 batch PCKh 0.5625\n",
      "Trained batch 1715 batch loss 0.514059186 batch mAP 0.558197 batch PCKh 0.6875\n",
      "Trained batch 1716 batch loss 0.433916777 batch mAP 0.625 batch PCKh 0.6875\n",
      "Trained batch 1717 batch loss 0.440300226 batch mAP 0.638214111 batch PCKh 0.5625\n",
      "Trained batch 1718 batch loss 0.474912614 batch mAP 0.602661133 batch PCKh 0.75\n",
      "Trained batch 1719 batch loss 0.43112272 batch mAP 0.598571777 batch PCKh 0.625\n",
      "Trained batch 1720 batch loss 0.561642289 batch mAP 0.611938477 batch PCKh 0.1875\n",
      "Trained batch 1721 batch loss 0.518980145 batch mAP 0.560333252 batch PCKh 0.6875\n",
      "Trained batch 1722 batch loss 0.563327432 batch mAP 0.55255127 batch PCKh 0.75\n",
      "Trained batch 1723 batch loss 0.579324245 batch mAP 0.555145264 batch PCKh 0.8125\n",
      "Trained batch 1724 batch loss 0.538013935 batch mAP 0.61328125 batch PCKh 0.75\n",
      "Trained batch 1725 batch loss 0.632594645 batch mAP 0.589050293 batch PCKh 0.3125\n",
      "Trained batch 1726 batch loss 0.486988723 batch mAP 0.618377686 batch PCKh 0.5625\n",
      "Trained batch 1727 batch loss 0.53750664 batch mAP 0.626678467 batch PCKh 0.75\n",
      "Trained batch 1728 batch loss 0.49413237 batch mAP 0.612335205 batch PCKh 0.0625\n",
      "Trained batch 1729 batch loss 0.444378823 batch mAP 0.57043457 batch PCKh 0.375\n",
      "Trained batch 1730 batch loss 0.499826044 batch mAP 0.56729126 batch PCKh 0.25\n",
      "Trained batch 1731 batch loss 0.526572168 batch mAP 0.66394043 batch PCKh 0.4375\n",
      "Trained batch 1732 batch loss 0.489471 batch mAP 0.661224365 batch PCKh 0.875\n",
      "Trained batch 1733 batch loss 0.413589239 batch mAP 0.646270752 batch PCKh 0.3125\n",
      "Trained batch 1734 batch loss 0.434133857 batch mAP 0.626312256 batch PCKh 0.1875\n",
      "Trained batch 1735 batch loss 0.477994591 batch mAP 0.668212891 batch PCKh 0.375\n",
      "Trained batch 1736 batch loss 0.427073181 batch mAP 0.737213135 batch PCKh 0.8125\n",
      "Trained batch 1737 batch loss 0.402233362 batch mAP 0.696655273 batch PCKh 0.25\n",
      "Trained batch 1738 batch loss 0.497990251 batch mAP 0.663787842 batch PCKh 0.6875\n",
      "Trained batch 1739 batch loss 0.529390812 batch mAP 0.646728516 batch PCKh 0.75\n",
      "Trained batch 1740 batch loss 0.506124198 batch mAP 0.686920166 batch PCKh 0.3125\n",
      "Trained batch 1741 batch loss 0.450749427 batch mAP 0.714630127 batch PCKh 0.5\n",
      "Trained batch 1742 batch loss 0.376316309 batch mAP 0.716918945 batch PCKh 0.75\n",
      "Trained batch 1743 batch loss 0.467961788 batch mAP 0.688903809 batch PCKh 0.3125\n",
      "Trained batch 1744 batch loss 0.644987941 batch mAP 0.602630615 batch PCKh 0.375\n",
      "Trained batch 1745 batch loss 0.514795244 batch mAP 0.642578125 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1746 batch loss 0.49873063 batch mAP 0.693237305 batch PCKh 0.625\n",
      "Trained batch 1747 batch loss 0.545060515 batch mAP 0.603118896 batch PCKh 0.5625\n",
      "Trained batch 1748 batch loss 0.509549558 batch mAP 0.616790771 batch PCKh 0.3125\n",
      "Trained batch 1749 batch loss 0.525515676 batch mAP 0.57699585 batch PCKh 0.5625\n",
      "Trained batch 1750 batch loss 0.526014924 batch mAP 0.583679199 batch PCKh 0.4375\n",
      "Trained batch 1751 batch loss 0.518963039 batch mAP 0.581268311 batch PCKh 0.375\n",
      "Trained batch 1752 batch loss 0.59998244 batch mAP 0.525268555 batch PCKh 0.5625\n",
      "Trained batch 1753 batch loss 0.634507358 batch mAP 0.568817139 batch PCKh 0.375\n",
      "Trained batch 1754 batch loss 0.497675449 batch mAP 0.605743408 batch PCKh 0.3125\n",
      "Trained batch 1755 batch loss 0.487588257 batch mAP 0.566619873 batch PCKh 0.75\n",
      "Trained batch 1756 batch loss 0.553294241 batch mAP 0.567260742 batch PCKh 0.4375\n",
      "Trained batch 1757 batch loss 0.558447719 batch mAP 0.553009033 batch PCKh 0.625\n",
      "Trained batch 1758 batch loss 0.578191459 batch mAP 0.513244629 batch PCKh 0.75\n",
      "Trained batch 1759 batch loss 0.459157705 batch mAP 0.568847656 batch PCKh 0.125\n",
      "Trained batch 1760 batch loss 0.526928961 batch mAP 0.556060791 batch PCKh 0.8125\n",
      "Trained batch 1761 batch loss 0.416454703 batch mAP 0.633148193 batch PCKh 0.5\n",
      "Trained batch 1762 batch loss 0.447224855 batch mAP 0.646453857 batch PCKh 0.4375\n",
      "Trained batch 1763 batch loss 0.442404956 batch mAP 0.61630249 batch PCKh 0.25\n",
      "Trained batch 1764 batch loss 0.523678899 batch mAP 0.594726562 batch PCKh 0.6875\n",
      "Trained batch 1765 batch loss 0.514549375 batch mAP 0.638671875 batch PCKh 0.1875\n",
      "Trained batch 1766 batch loss 0.547582448 batch mAP 0.633605957 batch PCKh 0.4375\n",
      "Trained batch 1767 batch loss 0.525899291 batch mAP 0.673126221 batch PCKh 0.5625\n",
      "Trained batch 1768 batch loss 0.53143841 batch mAP 0.642822266 batch PCKh 0.25\n",
      "Trained batch 1769 batch loss 0.627963543 batch mAP 0.664459229 batch PCKh 0.4375\n",
      "Trained batch 1770 batch loss 0.630305767 batch mAP 0.655792236 batch PCKh 0.375\n",
      "Trained batch 1771 batch loss 0.552335739 batch mAP 0.683288574 batch PCKh 0.8125\n",
      "Trained batch 1772 batch loss 0.632297397 batch mAP 0.576324463 batch PCKh 0.625\n",
      "Trained batch 1773 batch loss 0.558900833 batch mAP 0.587371826 batch PCKh 0.5625\n",
      "Trained batch 1774 batch loss 0.415134609 batch mAP 0.630249 batch PCKh 0.25\n",
      "Trained batch 1775 batch loss 0.457665056 batch mAP 0.570007324 batch PCKh 0.375\n",
      "Trained batch 1776 batch loss 0.539192 batch mAP 0.594604492 batch PCKh 0.625\n",
      "Trained batch 1777 batch loss 0.506472528 batch mAP 0.6434021 batch PCKh 0.75\n",
      "Trained batch 1778 batch loss 0.504274786 batch mAP 0.597351074 batch PCKh 0.625\n",
      "Trained batch 1779 batch loss 0.491167426 batch mAP 0.614044189 batch PCKh 0.4375\n",
      "Trained batch 1780 batch loss 0.51776886 batch mAP 0.550598145 batch PCKh 0.75\n",
      "Trained batch 1781 batch loss 0.564814568 batch mAP 0.583984375 batch PCKh 0.25\n",
      "Trained batch 1782 batch loss 0.499592513 batch mAP 0.611053467 batch PCKh 0.3125\n",
      "Trained batch 1783 batch loss 0.578379512 batch mAP 0.562072754 batch PCKh 0.5625\n",
      "Trained batch 1784 batch loss 0.515360832 batch mAP 0.614532471 batch PCKh 0.6875\n",
      "Trained batch 1785 batch loss 0.560589314 batch mAP 0.571563721 batch PCKh 0.6875\n",
      "Trained batch 1786 batch loss 0.541082621 batch mAP 0.63684082 batch PCKh 0.6875\n",
      "Trained batch 1787 batch loss 0.558039606 batch mAP 0.63293457 batch PCKh 0.8125\n",
      "Trained batch 1788 batch loss 0.518432498 batch mAP 0.645629883 batch PCKh 0.75\n",
      "Trained batch 1789 batch loss 0.64325732 batch mAP 0.647705078 batch PCKh 0.8125\n",
      "Trained batch 1790 batch loss 0.578686237 batch mAP 0.613891602 batch PCKh 0.5625\n",
      "Trained batch 1791 batch loss 0.501235366 batch mAP 0.672851562 batch PCKh 0.5625\n",
      "Trained batch 1792 batch loss 0.490876555 batch mAP 0.637359619 batch PCKh 0.5\n",
      "Trained batch 1793 batch loss 0.441943407 batch mAP 0.709960938 batch PCKh 0.5625\n",
      "Trained batch 1794 batch loss 0.515428424 batch mAP 0.6434021 batch PCKh 0.4375\n",
      "Trained batch 1795 batch loss 0.475690186 batch mAP 0.580047607 batch PCKh 0.25\n",
      "Trained batch 1796 batch loss 0.412480891 batch mAP 0.637023926 batch PCKh 0.5\n",
      "Trained batch 1797 batch loss 0.439145416 batch mAP 0.607696533 batch PCKh 0.4375\n",
      "Trained batch 1798 batch loss 0.412106872 batch mAP 0.669372559 batch PCKh 0.5625\n",
      "Trained batch 1799 batch loss 0.424263328 batch mAP 0.614257812 batch PCKh 0.5\n",
      "Trained batch 1800 batch loss 0.420548081 batch mAP 0.640686035 batch PCKh 0.5625\n",
      "Trained batch 1801 batch loss 0.463197291 batch mAP 0.58416748 batch PCKh 0.6875\n",
      "Trained batch 1802 batch loss 0.465557516 batch mAP 0.592926 batch PCKh 0.75\n",
      "Trained batch 1803 batch loss 0.456017792 batch mAP 0.570892334 batch PCKh 0.75\n",
      "Trained batch 1804 batch loss 0.404444277 batch mAP 0.595336914 batch PCKh 0.75\n",
      "Trained batch 1805 batch loss 0.339538336 batch mAP 0.650360107 batch PCKh 0\n",
      "Trained batch 1806 batch loss 0.370095909 batch mAP 0.639007568 batch PCKh 0.75\n",
      "Trained batch 1807 batch loss 0.345342636 batch mAP 0.664001465 batch PCKh 0\n",
      "Trained batch 1808 batch loss 0.390610576 batch mAP 0.586334229 batch PCKh 0.5625\n",
      "Trained batch 1809 batch loss 0.363557279 batch mAP 0.642883301 batch PCKh 0\n",
      "Trained batch 1810 batch loss 0.315569043 batch mAP 0.678314209 batch PCKh 0\n",
      "Trained batch 1811 batch loss 0.407115579 batch mAP 0.650421143 batch PCKh 0.5625\n",
      "Trained batch 1812 batch loss 0.467749059 batch mAP 0.619751 batch PCKh 0.75\n",
      "Trained batch 1813 batch loss 0.445476979 batch mAP 0.661560059 batch PCKh 0.5625\n",
      "Trained batch 1814 batch loss 0.541752577 batch mAP 0.672576904 batch PCKh 0.375\n",
      "Trained batch 1815 batch loss 0.47202903 batch mAP 0.686737061 batch PCKh 0.5\n",
      "Trained batch 1816 batch loss 0.440142453 batch mAP 0.710235596 batch PCKh 0.625\n",
      "Trained batch 1817 batch loss 0.435460329 batch mAP 0.673278809 batch PCKh 0.1875\n",
      "Trained batch 1818 batch loss 0.497768164 batch mAP 0.666931152 batch PCKh 0.3125\n",
      "Trained batch 1819 batch loss 0.479595065 batch mAP 0.661743164 batch PCKh 0.5\n",
      "Trained batch 1820 batch loss 0.465009749 batch mAP 0.674469 batch PCKh 0.5625\n",
      "Trained batch 1821 batch loss 0.556931555 batch mAP 0.66519165 batch PCKh 0.625\n",
      "Trained batch 1822 batch loss 0.50509572 batch mAP 0.621459961 batch PCKh 0.125\n",
      "Trained batch 1823 batch loss 0.49098748 batch mAP 0.638916 batch PCKh 0.4375\n",
      "Trained batch 1824 batch loss 0.525886536 batch mAP 0.666442871 batch PCKh 0.375\n",
      "Trained batch 1825 batch loss 0.532274604 batch mAP 0.541534424 batch PCKh 0.75\n",
      "Trained batch 1826 batch loss 0.504893 batch mAP 0.577056885 batch PCKh 0.4375\n",
      "Trained batch 1827 batch loss 0.54461658 batch mAP 0.616546631 batch PCKh 0.5625\n",
      "Trained batch 1828 batch loss 0.520129502 batch mAP 0.62713623 batch PCKh 0.125\n",
      "Trained batch 1829 batch loss 0.454875231 batch mAP 0.660308838 batch PCKh 0.625\n",
      "Trained batch 1830 batch loss 0.423152447 batch mAP 0.596069336 batch PCKh 0.375\n",
      "Trained batch 1831 batch loss 0.438085943 batch mAP 0.692260742 batch PCKh 0.75\n",
      "Trained batch 1832 batch loss 0.477269 batch mAP 0.682342529 batch PCKh 0.375\n",
      "Trained batch 1833 batch loss 0.507446051 batch mAP 0.721740723 batch PCKh 0.1875\n",
      "Trained batch 1834 batch loss 0.470727831 batch mAP 0.692718506 batch PCKh 0.5\n",
      "Trained batch 1835 batch loss 0.443483502 batch mAP 0.697906494 batch PCKh 0.4375\n",
      "Trained batch 1836 batch loss 0.376284301 batch mAP 0.688049316 batch PCKh 0.5\n",
      "Trained batch 1837 batch loss 0.449355662 batch mAP 0.717254639 batch PCKh 0.75\n",
      "Trained batch 1838 batch loss 0.449187934 batch mAP 0.688415527 batch PCKh 0.8125\n",
      "Trained batch 1839 batch loss 0.482798517 batch mAP 0.66104126 batch PCKh 0.3125\n",
      "Trained batch 1840 batch loss 0.425280184 batch mAP 0.696929932 batch PCKh 0.4375\n",
      "Trained batch 1841 batch loss 0.504473805 batch mAP 0.626403809 batch PCKh 0.25\n",
      "Trained batch 1842 batch loss 0.49182719 batch mAP 0.671112061 batch PCKh 0.6875\n",
      "Trained batch 1843 batch loss 0.554133356 batch mAP 0.59274292 batch PCKh 0.625\n",
      "Trained batch 1844 batch loss 0.383605182 batch mAP 0.702453613 batch PCKh 0.125\n",
      "Trained batch 1845 batch loss 0.463820368 batch mAP 0.674835205 batch PCKh 0.625\n",
      "Trained batch 1846 batch loss 0.448096514 batch mAP 0.571716309 batch PCKh 0.3125\n",
      "Trained batch 1847 batch loss 0.481413066 batch mAP 0.602478 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1848 batch loss 0.548594952 batch mAP 0.640960693 batch PCKh 0.625\n",
      "Trained batch 1849 batch loss 0.485407025 batch mAP 0.647064209 batch PCKh 0.625\n",
      "Trained batch 1850 batch loss 0.531494856 batch mAP 0.663848877 batch PCKh 0.6875\n",
      "Trained batch 1851 batch loss 0.554796159 batch mAP 0.618652344 batch PCKh 0.4375\n",
      "Trained batch 1852 batch loss 0.487340808 batch mAP 0.66204834 batch PCKh 0.5\n",
      "Trained batch 1853 batch loss 0.561579704 batch mAP 0.610778809 batch PCKh 0.375\n",
      "Trained batch 1854 batch loss 0.594756722 batch mAP 0.583648682 batch PCKh 0.5\n",
      "Trained batch 1855 batch loss 0.560552716 batch mAP 0.62310791 batch PCKh 0.25\n",
      "Trained batch 1856 batch loss 0.515683353 batch mAP 0.600616455 batch PCKh 0.5\n",
      "Trained batch 1857 batch loss 0.446156472 batch mAP 0.63470459 batch PCKh 0.25\n",
      "Trained batch 1858 batch loss 0.510274529 batch mAP 0.511169434 batch PCKh 0.3125\n",
      "Trained batch 1859 batch loss 0.508545458 batch mAP 0.630218506 batch PCKh 0.5\n",
      "Trained batch 1860 batch loss 0.450848103 batch mAP 0.65234375 batch PCKh 0.75\n",
      "Trained batch 1861 batch loss 0.545096636 batch mAP 0.588501 batch PCKh 0.625\n",
      "Trained batch 1862 batch loss 0.512522221 batch mAP 0.585540771 batch PCKh 0.3125\n",
      "Trained batch 1863 batch loss 0.494934857 batch mAP 0.637329102 batch PCKh 0.375\n",
      "Trained batch 1864 batch loss 0.408499777 batch mAP 0.689788818 batch PCKh 0.875\n",
      "Trained batch 1865 batch loss 0.489939302 batch mAP 0.63369751 batch PCKh 0.5\n",
      "Trained batch 1866 batch loss 0.645166934 batch mAP 0.66784668 batch PCKh 0.5\n",
      "Trained batch 1867 batch loss 0.683741 batch mAP 0.595459 batch PCKh 0\n",
      "Trained batch 1868 batch loss 0.596327186 batch mAP 0.63583374 batch PCKh 0.5\n",
      "Trained batch 1869 batch loss 0.654359818 batch mAP 0.639648438 batch PCKh 0.1875\n",
      "Trained batch 1870 batch loss 0.63890934 batch mAP 0.681304932 batch PCKh 0.3125\n",
      "Trained batch 1871 batch loss 0.51879245 batch mAP 0.663604736 batch PCKh 0.5\n",
      "Trained batch 1872 batch loss 0.451608241 batch mAP 0.673553467 batch PCKh 0.8125\n",
      "Trained batch 1873 batch loss 0.474665523 batch mAP 0.665557861 batch PCKh 0.5\n",
      "Trained batch 1874 batch loss 0.538421273 batch mAP 0.619384766 batch PCKh 0.5625\n",
      "Trained batch 1875 batch loss 0.453553885 batch mAP 0.636810303 batch PCKh 0.6875\n",
      "Trained batch 1876 batch loss 0.456875712 batch mAP 0.635192871 batch PCKh 0.625\n",
      "Trained batch 1877 batch loss 0.535792172 batch mAP 0.57131958 batch PCKh 0.875\n",
      "Trained batch 1878 batch loss 0.504409432 batch mAP 0.563324 batch PCKh 0.5\n",
      "Trained batch 1879 batch loss 0.650736868 batch mAP 0.600036621 batch PCKh 0.75\n",
      "Trained batch 1880 batch loss 0.509587586 batch mAP 0.519012451 batch PCKh 0.25\n",
      "Trained batch 1881 batch loss 0.575039148 batch mAP 0.588104248 batch PCKh 0.375\n",
      "Trained batch 1882 batch loss 0.506987095 batch mAP 0.5027771 batch PCKh 0.5625\n",
      "Trained batch 1883 batch loss 0.545010746 batch mAP 0.490325928 batch PCKh 0.375\n",
      "Trained batch 1884 batch loss 0.594461918 batch mAP 0.516265869 batch PCKh 0.5\n",
      "Trained batch 1885 batch loss 0.559354901 batch mAP 0.548828125 batch PCKh 0.3125\n",
      "Trained batch 1886 batch loss 0.612253308 batch mAP 0.530578613 batch PCKh 0.25\n",
      "Trained batch 1887 batch loss 0.539958596 batch mAP 0.565582275 batch PCKh 0.6875\n",
      "Trained batch 1888 batch loss 0.616790354 batch mAP 0.489898682 batch PCKh 0.4375\n",
      "Trained batch 1889 batch loss 0.627137601 batch mAP 0.528839111 batch PCKh 0.5625\n",
      "Trained batch 1890 batch loss 0.607578397 batch mAP 0.557159424 batch PCKh 0.625\n",
      "Trained batch 1891 batch loss 0.501105785 batch mAP 0.704834 batch PCKh 0.6875\n",
      "Trained batch 1892 batch loss 0.47731787 batch mAP 0.65737915 batch PCKh 0.4375\n",
      "Trained batch 1893 batch loss 0.590750933 batch mAP 0.6355896 batch PCKh 0.4375\n",
      "Trained batch 1894 batch loss 0.50619483 batch mAP 0.603851318 batch PCKh 0.4375\n",
      "Trained batch 1895 batch loss 0.409158 batch mAP 0.632171631 batch PCKh 0.75\n",
      "Trained batch 1896 batch loss 0.500359535 batch mAP 0.635223389 batch PCKh 0.375\n",
      "Trained batch 1897 batch loss 0.559786439 batch mAP 0.6144104 batch PCKh 0.625\n",
      "Trained batch 1898 batch loss 0.623450637 batch mAP 0.554962158 batch PCKh 0.375\n",
      "Trained batch 1899 batch loss 0.551998675 batch mAP 0.58303833 batch PCKh 0.1875\n",
      "Trained batch 1900 batch loss 0.541124463 batch mAP 0.637115479 batch PCKh 0.1875\n",
      "Trained batch 1901 batch loss 0.503281832 batch mAP 0.667907715 batch PCKh 0.5625\n",
      "Trained batch 1902 batch loss 0.55413717 batch mAP 0.628570557 batch PCKh 0.3125\n",
      "Trained batch 1903 batch loss 0.479799092 batch mAP 0.66317749 batch PCKh 0.625\n",
      "Trained batch 1904 batch loss 0.55817771 batch mAP 0.698120117 batch PCKh 0.875\n",
      "Trained batch 1905 batch loss 0.505090833 batch mAP 0.682098389 batch PCKh 0.5\n",
      "Trained batch 1906 batch loss 0.497015089 batch mAP 0.615386963 batch PCKh 0.75\n",
      "Trained batch 1907 batch loss 0.541200221 batch mAP 0.599456787 batch PCKh 0.375\n",
      "Trained batch 1908 batch loss 0.451842248 batch mAP 0.59375 batch PCKh 0.5\n",
      "Trained batch 1909 batch loss 0.555480242 batch mAP 0.594512939 batch PCKh 0.5625\n",
      "Trained batch 1910 batch loss 0.602176607 batch mAP 0.573669434 batch PCKh 0.625\n",
      "Trained batch 1911 batch loss 0.491571188 batch mAP 0.590667725 batch PCKh 0.75\n",
      "Trained batch 1912 batch loss 0.49190855 batch mAP 0.560821533 batch PCKh 0.5625\n",
      "Trained batch 1913 batch loss 0.43592611 batch mAP 0.584991455 batch PCKh 0.375\n",
      "Trained batch 1914 batch loss 0.502105117 batch mAP 0.611755371 batch PCKh 0.4375\n",
      "Trained batch 1915 batch loss 0.49344191 batch mAP 0.613647461 batch PCKh 0.375\n",
      "Trained batch 1916 batch loss 0.541282654 batch mAP 0.550415039 batch PCKh 0.625\n",
      "Trained batch 1917 batch loss 0.538669288 batch mAP 0.532135 batch PCKh 0.5\n",
      "Trained batch 1918 batch loss 0.53009963 batch mAP 0.580322266 batch PCKh 0.75\n",
      "Trained batch 1919 batch loss 0.477678239 batch mAP 0.568206787 batch PCKh 0.5625\n",
      "Trained batch 1920 batch loss 0.439011 batch mAP 0.670593262 batch PCKh 0.75\n",
      "Trained batch 1921 batch loss 0.437450379 batch mAP 0.564697266 batch PCKh 0\n",
      "Trained batch 1922 batch loss 0.526289523 batch mAP 0.671051 batch PCKh 0.3125\n",
      "Trained batch 1923 batch loss 0.518136382 batch mAP 0.630126953 batch PCKh 0.1875\n",
      "Trained batch 1924 batch loss 0.52156955 batch mAP 0.575012207 batch PCKh 0.6875\n",
      "Trained batch 1925 batch loss 0.559446335 batch mAP 0.562225342 batch PCKh 0.5625\n",
      "Trained batch 1926 batch loss 0.507890701 batch mAP 0.62210083 batch PCKh 0.75\n",
      "Trained batch 1927 batch loss 0.497001916 batch mAP 0.599395752 batch PCKh 0.1875\n",
      "Trained batch 1928 batch loss 0.538998246 batch mAP 0.587310791 batch PCKh 0.6875\n",
      "Trained batch 1929 batch loss 0.464304686 batch mAP 0.655670166 batch PCKh 0.6875\n",
      "Trained batch 1930 batch loss 0.565930068 batch mAP 0.623077393 batch PCKh 0.5625\n",
      "Trained batch 1931 batch loss 0.601403117 batch mAP 0.623352051 batch PCKh 0.5625\n",
      "Trained batch 1932 batch loss 0.539689302 batch mAP 0.642608643 batch PCKh 0.5\n",
      "Trained batch 1933 batch loss 0.648996472 batch mAP 0.617034912 batch PCKh 0.6875\n",
      "Trained batch 1934 batch loss 0.397177 batch mAP 0.570831299 batch PCKh 0.75\n",
      "Trained batch 1935 batch loss 0.509019673 batch mAP 0.567779541 batch PCKh 0.6875\n",
      "Trained batch 1936 batch loss 0.456054688 batch mAP 0.592529297 batch PCKh 0.5625\n",
      "Trained batch 1937 batch loss 0.494327664 batch mAP 0.552429199 batch PCKh 0.6875\n",
      "Trained batch 1938 batch loss 0.51437521 batch mAP 0.498626709 batch PCKh 0.5\n",
      "Trained batch 1939 batch loss 0.57692951 batch mAP 0.50982666 batch PCKh 0.5625\n",
      "Trained batch 1940 batch loss 0.498508573 batch mAP 0.559570312 batch PCKh 0.5\n",
      "Trained batch 1941 batch loss 0.536853909 batch mAP 0.61932373 batch PCKh 0.25\n",
      "Trained batch 1942 batch loss 0.522139549 batch mAP 0.645446777 batch PCKh 0.75\n",
      "Trained batch 1943 batch loss 0.514795661 batch mAP 0.609130859 batch PCKh 0.625\n",
      "Trained batch 1944 batch loss 0.460045457 batch mAP 0.601074219 batch PCKh 0.25\n",
      "Trained batch 1945 batch loss 0.527256 batch mAP 0.577362061 batch PCKh 0.5625\n",
      "Trained batch 1946 batch loss 0.485343754 batch mAP 0.630401611 batch PCKh 0.5625\n",
      "Trained batch 1947 batch loss 0.430353105 batch mAP 0.657592773 batch PCKh 0.6875\n",
      "Trained batch 1948 batch loss 0.37497285 batch mAP 0.6847229 batch PCKh 0.5625\n",
      "Trained batch 1949 batch loss 0.463128626 batch mAP 0.569244385 batch PCKh 0.5\n",
      "Trained batch 1950 batch loss 0.580062747 batch mAP 0.468170166 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1951 batch loss 0.574482203 batch mAP 0.549804688 batch PCKh 0.1875\n",
      "Trained batch 1952 batch loss 0.532496512 batch mAP 0.630249 batch PCKh 0.25\n",
      "Trained batch 1953 batch loss 0.546309114 batch mAP 0.552856445 batch PCKh 0.4375\n",
      "Trained batch 1954 batch loss 0.485262692 batch mAP 0.633667 batch PCKh 0.1875\n",
      "Trained batch 1955 batch loss 0.551670074 batch mAP 0.671630859 batch PCKh 0.6875\n",
      "Trained batch 1956 batch loss 0.504286289 batch mAP 0.676940918 batch PCKh 0.4375\n",
      "Trained batch 1957 batch loss 0.571742833 batch mAP 0.599761963 batch PCKh 0.8125\n",
      "Trained batch 1958 batch loss 0.494687736 batch mAP 0.597900391 batch PCKh 0.5625\n",
      "Trained batch 1959 batch loss 0.466618419 batch mAP 0.59085083 batch PCKh 0.0625\n",
      "Trained batch 1960 batch loss 0.420445979 batch mAP 0.642150879 batch PCKh 0.4375\n",
      "Trained batch 1961 batch loss 0.470674247 batch mAP 0.617889404 batch PCKh 0.6875\n",
      "Trained batch 1962 batch loss 0.540658116 batch mAP 0.606689453 batch PCKh 0.75\n",
      "Trained batch 1963 batch loss 0.510537 batch mAP 0.608642578 batch PCKh 0.6875\n",
      "Trained batch 1964 batch loss 0.501511693 batch mAP 0.598815918 batch PCKh 0.75\n",
      "Trained batch 1965 batch loss 0.552643716 batch mAP 0.588043213 batch PCKh 0.375\n",
      "Trained batch 1966 batch loss 0.578212082 batch mAP 0.569824219 batch PCKh 0.5625\n",
      "Trained batch 1967 batch loss 0.571882546 batch mAP 0.598083496 batch PCKh 0.0625\n",
      "Trained batch 1968 batch loss 0.580134 batch mAP 0.604156494 batch PCKh 0.5\n",
      "Trained batch 1969 batch loss 0.492398828 batch mAP 0.625183105 batch PCKh 0.5625\n",
      "Trained batch 1970 batch loss 0.495394737 batch mAP 0.645568848 batch PCKh 0.625\n",
      "Trained batch 1971 batch loss 0.567779064 batch mAP 0.63394165 batch PCKh 0.3125\n",
      "Trained batch 1972 batch loss 0.437760383 batch mAP 0.683227539 batch PCKh 0.625\n",
      "Trained batch 1973 batch loss 0.497558594 batch mAP 0.689117432 batch PCKh 0.75\n",
      "Trained batch 1974 batch loss 0.516148508 batch mAP 0.591827393 batch PCKh 0.5\n",
      "Trained batch 1975 batch loss 0.57997489 batch mAP 0.560058594 batch PCKh 0.8125\n",
      "Trained batch 1976 batch loss 0.583159089 batch mAP 0.635345459 batch PCKh 0.3125\n",
      "Trained batch 1977 batch loss 0.547426 batch mAP 0.662750244 batch PCKh 0.5\n",
      "Trained batch 1978 batch loss 0.493024528 batch mAP 0.596893311 batch PCKh 0.875\n",
      "Trained batch 1979 batch loss 0.507595837 batch mAP 0.658569336 batch PCKh 0.8125\n",
      "Trained batch 1980 batch loss 0.53509742 batch mAP 0.58795166 batch PCKh 0.5625\n",
      "Trained batch 1981 batch loss 0.574090898 batch mAP 0.588470459 batch PCKh 0.625\n",
      "Trained batch 1982 batch loss 0.524275839 batch mAP 0.629272461 batch PCKh 0.6875\n",
      "Trained batch 1983 batch loss 0.561010897 batch mAP 0.551055908 batch PCKh 0.5625\n",
      "Trained batch 1984 batch loss 0.5308972 batch mAP 0.576568604 batch PCKh 0.4375\n",
      "Trained batch 1985 batch loss 0.463907301 batch mAP 0.544952393 batch PCKh 0\n",
      "Trained batch 1986 batch loss 0.49138093 batch mAP 0.59552 batch PCKh 0.6875\n",
      "Trained batch 1987 batch loss 0.499404073 batch mAP 0.603485107 batch PCKh 0.5\n",
      "Trained batch 1988 batch loss 0.452414304 batch mAP 0.642852783 batch PCKh 0.3125\n",
      "Trained batch 1989 batch loss 0.510207176 batch mAP 0.62878418 batch PCKh 0.5625\n",
      "Trained batch 1990 batch loss 0.472481906 batch mAP 0.590087891 batch PCKh 0.5625\n",
      "Trained batch 1991 batch loss 0.595321536 batch mAP 0.58782959 batch PCKh 0.875\n",
      "Trained batch 1992 batch loss 0.612432837 batch mAP 0.581512451 batch PCKh 0\n",
      "Trained batch 1993 batch loss 0.515530825 batch mAP 0.631286621 batch PCKh 0.6875\n",
      "Trained batch 1994 batch loss 0.54711628 batch mAP 0.582428 batch PCKh 0.4375\n",
      "Trained batch 1995 batch loss 0.44450295 batch mAP 0.621063232 batch PCKh 0.5625\n",
      "Trained batch 1996 batch loss 0.433072478 batch mAP 0.614379883 batch PCKh 0.3125\n",
      "Trained batch 1997 batch loss 0.449453861 batch mAP 0.62121582 batch PCKh 0.4375\n",
      "Trained batch 1998 batch loss 0.424954653 batch mAP 0.648162842 batch PCKh 0.5\n",
      "Trained batch 1999 batch loss 0.40839833 batch mAP 0.625457764 batch PCKh 0.3125\n",
      "Trained batch 2000 batch loss 0.523179233 batch mAP 0.653442383 batch PCKh 0.375\n",
      "Trained batch 2001 batch loss 0.420945972 batch mAP 0.687683105 batch PCKh 0.625\n",
      "Trained batch 2002 batch loss 0.560334086 batch mAP 0.666351318 batch PCKh 0.875\n",
      "Trained batch 2003 batch loss 0.469438344 batch mAP 0.665618896 batch PCKh 0.4375\n",
      "Trained batch 2004 batch loss 0.486161411 batch mAP 0.673309326 batch PCKh 0.5625\n",
      "Trained batch 2005 batch loss 0.439877152 batch mAP 0.723419189 batch PCKh 0.9375\n",
      "Trained batch 2006 batch loss 0.472027659 batch mAP 0.74017334 batch PCKh 0.4375\n",
      "Trained batch 2007 batch loss 0.490701914 batch mAP 0.722686768 batch PCKh 0.5625\n",
      "Trained batch 2008 batch loss 0.429148316 batch mAP 0.761810303 batch PCKh 0.6875\n",
      "Trained batch 2009 batch loss 0.41689837 batch mAP 0.749816895 batch PCKh 0.75\n",
      "Trained batch 2010 batch loss 0.525186718 batch mAP 0.538391113 batch PCKh 0.1875\n",
      "Trained batch 2011 batch loss 0.52498287 batch mAP 0.585022 batch PCKh 0.5625\n",
      "Trained batch 2012 batch loss 0.544791281 batch mAP 0.556304932 batch PCKh 0.4375\n",
      "Trained batch 2013 batch loss 0.530116379 batch mAP 0.533905 batch PCKh 0.5625\n",
      "Trained batch 2014 batch loss 0.493130922 batch mAP 0.659759521 batch PCKh 0.4375\n",
      "Trained batch 2015 batch loss 0.397174478 batch mAP 0.720855713 batch PCKh 0.4375\n",
      "Trained batch 2016 batch loss 0.547845 batch mAP 0.547607422 batch PCKh 0.4375\n",
      "Trained batch 2017 batch loss 0.439795762 batch mAP 0.570831299 batch PCKh 0.125\n",
      "Trained batch 2018 batch loss 0.545002699 batch mAP 0.611816406 batch PCKh 0\n",
      "Trained batch 2019 batch loss 0.362543643 batch mAP 0.663116455 batch PCKh 0.75\n",
      "Trained batch 2020 batch loss 0.403265655 batch mAP 0.634094238 batch PCKh 0.5\n",
      "Trained batch 2021 batch loss 0.549994409 batch mAP 0.636657715 batch PCKh 0.3125\n",
      "Trained batch 2022 batch loss 0.522937179 batch mAP 0.590454102 batch PCKh 0.25\n",
      "Trained batch 2023 batch loss 0.391841322 batch mAP 0.656066895 batch PCKh 0.6875\n",
      "Trained batch 2024 batch loss 0.486047655 batch mAP 0.650543213 batch PCKh 0.5\n",
      "Trained batch 2025 batch loss 0.511592329 batch mAP 0.636901855 batch PCKh 0.5625\n",
      "Trained batch 2026 batch loss 0.557917833 batch mAP 0.618164062 batch PCKh 0.4375\n",
      "Trained batch 2027 batch loss 0.46033 batch mAP 0.623443604 batch PCKh 0.625\n",
      "Trained batch 2028 batch loss 0.586419404 batch mAP 0.55960083 batch PCKh 0.375\n",
      "Trained batch 2029 batch loss 0.615565896 batch mAP 0.5887146 batch PCKh 0.1875\n",
      "Trained batch 2030 batch loss 0.612348914 batch mAP 0.566223145 batch PCKh 0.75\n",
      "Trained batch 2031 batch loss 0.630005479 batch mAP 0.556976318 batch PCKh 0.5\n",
      "Trained batch 2032 batch loss 0.55302918 batch mAP 0.545288086 batch PCKh 0.3125\n",
      "Trained batch 2033 batch loss 0.431721687 batch mAP 0.565185547 batch PCKh 0.5625\n",
      "Trained batch 2034 batch loss 0.494142264 batch mAP 0.571258545 batch PCKh 0.8125\n",
      "Trained batch 2035 batch loss 0.474181026 batch mAP 0.597930908 batch PCKh 0.4375\n",
      "Trained batch 2036 batch loss 0.555622101 batch mAP 0.569946289 batch PCKh 0.625\n",
      "Trained batch 2037 batch loss 0.404249132 batch mAP 0.581817627 batch PCKh 0.5625\n",
      "Trained batch 2038 batch loss 0.527950406 batch mAP 0.538421631 batch PCKh 0.25\n",
      "Trained batch 2039 batch loss 0.46119082 batch mAP 0.578613281 batch PCKh 0.375\n",
      "Trained batch 2040 batch loss 0.493492335 batch mAP 0.49798584 batch PCKh 0.75\n",
      "Trained batch 2041 batch loss 0.559616685 batch mAP 0.494537354 batch PCKh 0.6875\n",
      "Trained batch 2042 batch loss 0.54079628 batch mAP 0.51348877 batch PCKh 0.6875\n",
      "Trained batch 2043 batch loss 0.527685881 batch mAP 0.600921631 batch PCKh 0.625\n",
      "Trained batch 2044 batch loss 0.561324239 batch mAP 0.601898193 batch PCKh 0.25\n",
      "Trained batch 2045 batch loss 0.492918968 batch mAP 0.664611816 batch PCKh 0.4375\n",
      "Trained batch 2046 batch loss 0.418870509 batch mAP 0.711486816 batch PCKh 0.375\n",
      "Trained batch 2047 batch loss 0.506251216 batch mAP 0.681640625 batch PCKh 0.3125\n",
      "Trained batch 2048 batch loss 0.481677115 batch mAP 0.671142578 batch PCKh 0.6875\n",
      "Trained batch 2049 batch loss 0.402722418 batch mAP 0.631866455 batch PCKh 0.5625\n",
      "Trained batch 2050 batch loss 0.495769382 batch mAP 0.661865234 batch PCKh 0.6875\n",
      "Trained batch 2051 batch loss 0.615470767 batch mAP 0.616973877 batch PCKh 0.75\n",
      "Trained batch 2052 batch loss 0.566634476 batch mAP 0.588043213 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2053 batch loss 0.524783611 batch mAP 0.583740234 batch PCKh 0.5625\n",
      "Trained batch 2054 batch loss 0.506800771 batch mAP 0.570404053 batch PCKh 0.625\n",
      "Trained batch 2055 batch loss 0.523827136 batch mAP 0.565826416 batch PCKh 0.4375\n",
      "Trained batch 2056 batch loss 0.458928227 batch mAP 0.613189697 batch PCKh 0.5\n",
      "Trained batch 2057 batch loss 0.620141506 batch mAP 0.535827637 batch PCKh 0.625\n",
      "Trained batch 2058 batch loss 0.508153737 batch mAP 0.591339111 batch PCKh 0.6875\n",
      "Trained batch 2059 batch loss 0.612701774 batch mAP 0.558959961 batch PCKh 0.5\n",
      "Trained batch 2060 batch loss 0.559805572 batch mAP 0.61505127 batch PCKh 0.75\n",
      "Trained batch 2061 batch loss 0.610444784 batch mAP 0.577148438 batch PCKh 0.0625\n",
      "Trained batch 2062 batch loss 0.538494468 batch mAP 0.608856201 batch PCKh 0.375\n",
      "Trained batch 2063 batch loss 0.567657292 batch mAP 0.578582764 batch PCKh 0.0625\n",
      "Trained batch 2064 batch loss 0.499573052 batch mAP 0.569030762 batch PCKh 0.625\n",
      "Trained batch 2065 batch loss 0.61150527 batch mAP 0.542480469 batch PCKh 0.625\n",
      "Trained batch 2066 batch loss 0.58509016 batch mAP 0.520019531 batch PCKh 0.125\n",
      "Trained batch 2067 batch loss 0.609776378 batch mAP 0.553253174 batch PCKh 0.125\n",
      "Trained batch 2068 batch loss 0.587175906 batch mAP 0.513214111 batch PCKh 0.375\n",
      "Trained batch 2069 batch loss 0.460979521 batch mAP 0.483428955 batch PCKh 0.5625\n",
      "Trained batch 2070 batch loss 0.485726833 batch mAP 0.619384766 batch PCKh 0.3125\n",
      "Trained batch 2071 batch loss 0.437697619 batch mAP 0.604309082 batch PCKh 0.3125\n",
      "Trained batch 2072 batch loss 0.488471091 batch mAP 0.629943848 batch PCKh 0.4375\n",
      "Trained batch 2073 batch loss 0.537557781 batch mAP 0.631713867 batch PCKh 0.375\n",
      "Trained batch 2074 batch loss 0.495456964 batch mAP 0.607971191 batch PCKh 0.3125\n",
      "Trained batch 2075 batch loss 0.442731053 batch mAP 0.639221191 batch PCKh 0.4375\n",
      "Trained batch 2076 batch loss 0.422199458 batch mAP 0.681762695 batch PCKh 0.6875\n",
      "Trained batch 2077 batch loss 0.450533569 batch mAP 0.653991699 batch PCKh 0.5625\n",
      "Trained batch 2078 batch loss 0.523808241 batch mAP 0.618530273 batch PCKh 0.375\n",
      "Trained batch 2079 batch loss 0.406353354 batch mAP 0.656219482 batch PCKh 0.25\n",
      "Trained batch 2080 batch loss 0.564978123 batch mAP 0.618927 batch PCKh 0.875\n",
      "Trained batch 2081 batch loss 0.443044454 batch mAP 0.625824 batch PCKh 0.4375\n",
      "Trained batch 2082 batch loss 0.600354135 batch mAP 0.59487915 batch PCKh 0.625\n",
      "Trained batch 2083 batch loss 0.502158046 batch mAP 0.560455322 batch PCKh 0.5625\n",
      "Trained batch 2084 batch loss 0.480089366 batch mAP 0.648010254 batch PCKh 0.5625\n",
      "Trained batch 2085 batch loss 0.568013966 batch mAP 0.613861084 batch PCKh 0.25\n",
      "Trained batch 2086 batch loss 0.493920505 batch mAP 0.666992188 batch PCKh 0.875\n",
      "Trained batch 2087 batch loss 0.554790735 batch mAP 0.675872803 batch PCKh 0.4375\n",
      "Trained batch 2088 batch loss 0.487230152 batch mAP 0.668731689 batch PCKh 0.5\n",
      "Trained batch 2089 batch loss 0.534064531 batch mAP 0.652374268 batch PCKh 0.5625\n",
      "Trained batch 2090 batch loss 0.566560507 batch mAP 0.638092041 batch PCKh 0.625\n",
      "Trained batch 2091 batch loss 0.57329309 batch mAP 0.605163574 batch PCKh 0.375\n",
      "Trained batch 2092 batch loss 0.513780653 batch mAP 0.662353516 batch PCKh 0.25\n",
      "Trained batch 2093 batch loss 0.567621529 batch mAP 0.667144775 batch PCKh 0.3125\n",
      "Trained batch 2094 batch loss 0.566345155 batch mAP 0.668487549 batch PCKh 0.375\n",
      "Trained batch 2095 batch loss 0.510126233 batch mAP 0.62689209 batch PCKh 0.3125\n",
      "Trained batch 2096 batch loss 0.482902378 batch mAP 0.641784668 batch PCKh 0.625\n",
      "Trained batch 2097 batch loss 0.501686454 batch mAP 0.657806396 batch PCKh 0.5\n",
      "Trained batch 2098 batch loss 0.469249278 batch mAP 0.657440186 batch PCKh 0.5\n",
      "Trained batch 2099 batch loss 0.428275347 batch mAP 0.675415039 batch PCKh 0.75\n",
      "Trained batch 2100 batch loss 0.434716582 batch mAP 0.693481445 batch PCKh 0.4375\n",
      "Trained batch 2101 batch loss 0.473368883 batch mAP 0.671966553 batch PCKh 0.625\n",
      "Trained batch 2102 batch loss 0.471659243 batch mAP 0.678314209 batch PCKh 0.625\n",
      "Trained batch 2103 batch loss 0.439290226 batch mAP 0.718994141 batch PCKh 0.8125\n",
      "Trained batch 2104 batch loss 0.378627479 batch mAP 0.748168945 batch PCKh 0.75\n",
      "Trained batch 2105 batch loss 0.516210914 batch mAP 0.675415039 batch PCKh 0.5625\n",
      "Trained batch 2106 batch loss 0.50481385 batch mAP 0.680999756 batch PCKh 0.75\n",
      "Trained batch 2107 batch loss 0.468369126 batch mAP 0.6953125 batch PCKh 0.3125\n",
      "Trained batch 2108 batch loss 0.547019124 batch mAP 0.641845703 batch PCKh 0.75\n",
      "Trained batch 2109 batch loss 0.484210789 batch mAP 0.658691406 batch PCKh 0.4375\n",
      "Trained batch 2110 batch loss 0.413061321 batch mAP 0.661376953 batch PCKh 0.4375\n",
      "Trained batch 2111 batch loss 0.429258525 batch mAP 0.691680908 batch PCKh 0.625\n",
      "Trained batch 2112 batch loss 0.448307902 batch mAP 0.692382812 batch PCKh 0.4375\n",
      "Trained batch 2113 batch loss 0.395370692 batch mAP 0.749816895 batch PCKh 0.5\n",
      "Trained batch 2114 batch loss 0.356454909 batch mAP 0.677185059 batch PCKh 0.5\n",
      "Trained batch 2115 batch loss 0.493080884 batch mAP 0.664154053 batch PCKh 0.6875\n",
      "Trained batch 2116 batch loss 0.489765525 batch mAP 0.636688232 batch PCKh 0.75\n",
      "Trained batch 2117 batch loss 0.59387213 batch mAP 0.618591309 batch PCKh 0.3125\n",
      "Trained batch 2118 batch loss 0.501568794 batch mAP 0.601104736 batch PCKh 0.375\n",
      "Trained batch 2119 batch loss 0.510779619 batch mAP 0.560516357 batch PCKh 0.625\n",
      "Trained batch 2120 batch loss 0.546744049 batch mAP 0.647369385 batch PCKh 0.75\n",
      "Trained batch 2121 batch loss 0.487738311 batch mAP 0.617675781 batch PCKh 0.3125\n",
      "Trained batch 2122 batch loss 0.48593691 batch mAP 0.554351807 batch PCKh 0.4375\n",
      "Trained batch 2123 batch loss 0.574840307 batch mAP 0.545349121 batch PCKh 0.875\n",
      "Trained batch 2124 batch loss 0.541953683 batch mAP 0.576599121 batch PCKh 0.5625\n",
      "Trained batch 2125 batch loss 0.502902269 batch mAP 0.590820312 batch PCKh 0.4375\n",
      "Trained batch 2126 batch loss 0.628463089 batch mAP 0.545623779 batch PCKh 0.75\n",
      "Trained batch 2127 batch loss 0.506271243 batch mAP 0.64440918 batch PCKh 0.625\n",
      "Trained batch 2128 batch loss 0.50314647 batch mAP 0.636383057 batch PCKh 0.5\n",
      "Trained batch 2129 batch loss 0.586475253 batch mAP 0.538970947 batch PCKh 0.125\n",
      "Trained batch 2130 batch loss 0.514701426 batch mAP 0.52230835 batch PCKh 0.4375\n",
      "Trained batch 2131 batch loss 0.445469409 batch mAP 0.518341064 batch PCKh 0.8125\n",
      "Trained batch 2132 batch loss 0.490859598 batch mAP 0.560516357 batch PCKh 0.3125\n",
      "Trained batch 2133 batch loss 0.39871186 batch mAP 0.645935059 batch PCKh 0.75\n",
      "Trained batch 2134 batch loss 0.401763141 batch mAP 0.631530762 batch PCKh 0.375\n",
      "Trained batch 2135 batch loss 0.430262625 batch mAP 0.565032959 batch PCKh 0.4375\n",
      "Trained batch 2136 batch loss 0.446502268 batch mAP 0.684875488 batch PCKh 0.5\n",
      "Trained batch 2137 batch loss 0.525996327 batch mAP 0.610229492 batch PCKh 0.3125\n",
      "Trained batch 2138 batch loss 0.64822197 batch mAP 0.582702637 batch PCKh 0.0625\n",
      "Trained batch 2139 batch loss 0.64652884 batch mAP 0.600830078 batch PCKh 0.1875\n",
      "Trained batch 2140 batch loss 0.555273116 batch mAP 0.621063232 batch PCKh 0.5\n",
      "Trained batch 2141 batch loss 0.523518562 batch mAP 0.714599609 batch PCKh 0.625\n",
      "Trained batch 2142 batch loss 0.490005016 batch mAP 0.651367188 batch PCKh 0.625\n",
      "Trained batch 2143 batch loss 0.484335363 batch mAP 0.582885742 batch PCKh 0.8125\n",
      "Trained batch 2144 batch loss 0.455784589 batch mAP 0.661865234 batch PCKh 0.3125\n",
      "Trained batch 2145 batch loss 0.459560931 batch mAP 0.645965576 batch PCKh 0.6875\n",
      "Trained batch 2146 batch loss 0.544024 batch mAP 0.597259521 batch PCKh 0.25\n",
      "Trained batch 2147 batch loss 0.53574 batch mAP 0.643768311 batch PCKh 0.5\n",
      "Trained batch 2148 batch loss 0.410830021 batch mAP 0.683227539 batch PCKh 0.3125\n",
      "Trained batch 2149 batch loss 0.477720588 batch mAP 0.697601318 batch PCKh 0.8125\n",
      "Trained batch 2150 batch loss 0.409842342 batch mAP 0.679901123 batch PCKh 0.375\n",
      "Trained batch 2151 batch loss 0.42361024 batch mAP 0.680847168 batch PCKh 0.6875\n",
      "Trained batch 2152 batch loss 0.527610421 batch mAP 0.562316895 batch PCKh 0.8125\n",
      "Trained batch 2153 batch loss 0.485967368 batch mAP 0.663848877 batch PCKh 0.625\n",
      "Trained batch 2154 batch loss 0.522997797 batch mAP 0.687164307 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2155 batch loss 0.489648223 batch mAP 0.624908447 batch PCKh 0.4375\n",
      "Trained batch 2156 batch loss 0.483438879 batch mAP 0.623657227 batch PCKh 0.4375\n",
      "Trained batch 2157 batch loss 0.50125432 batch mAP 0.67565918 batch PCKh 0.75\n",
      "Trained batch 2158 batch loss 0.561356366 batch mAP 0.641998291 batch PCKh 0.6875\n",
      "Trained batch 2159 batch loss 0.513622 batch mAP 0.622070312 batch PCKh 0.5625\n",
      "Trained batch 2160 batch loss 0.52954 batch mAP 0.637176514 batch PCKh 0.4375\n",
      "Trained batch 2161 batch loss 0.526858687 batch mAP 0.641418457 batch PCKh 0.625\n",
      "Trained batch 2162 batch loss 0.428544581 batch mAP 0.683105469 batch PCKh 0.1875\n",
      "Trained batch 2163 batch loss 0.496842265 batch mAP 0.648284912 batch PCKh 0.375\n",
      "Trained batch 2164 batch loss 0.440639138 batch mAP 0.698761 batch PCKh 0.625\n",
      "Trained batch 2165 batch loss 0.513861895 batch mAP 0.726837158 batch PCKh 0.5\n",
      "Trained batch 2166 batch loss 0.474441171 batch mAP 0.697052 batch PCKh 0.4375\n",
      "Trained batch 2167 batch loss 0.484654844 batch mAP 0.711456299 batch PCKh 0.1875\n",
      "Trained batch 2168 batch loss 0.493433952 batch mAP 0.725738525 batch PCKh 0.6875\n",
      "Trained batch 2169 batch loss 0.571686387 batch mAP 0.62097168 batch PCKh 0.5\n",
      "Trained batch 2170 batch loss 0.540195 batch mAP 0.639404297 batch PCKh 0.8125\n",
      "Trained batch 2171 batch loss 0.523784637 batch mAP 0.636138916 batch PCKh 0.625\n",
      "Trained batch 2172 batch loss 0.598737836 batch mAP 0.615753174 batch PCKh 0.25\n",
      "Trained batch 2173 batch loss 0.49010849 batch mAP 0.594116211 batch PCKh 0.125\n",
      "Trained batch 2174 batch loss 0.597197413 batch mAP 0.579193115 batch PCKh 0.6875\n",
      "Trained batch 2175 batch loss 0.463689566 batch mAP 0.529815674 batch PCKh 0.5\n",
      "Trained batch 2176 batch loss 0.512845039 batch mAP 0.632110596 batch PCKh 0.75\n",
      "Trained batch 2177 batch loss 0.50403142 batch mAP 0.64163208 batch PCKh 0.5\n",
      "Trained batch 2178 batch loss 0.557255149 batch mAP 0.632995605 batch PCKh 0.625\n",
      "Trained batch 2179 batch loss 0.583753884 batch mAP 0.61227417 batch PCKh 0.6875\n",
      "Trained batch 2180 batch loss 0.561440825 batch mAP 0.606842041 batch PCKh 0.75\n",
      "Trained batch 2181 batch loss 0.426686 batch mAP 0.628875732 batch PCKh 0.3125\n",
      "Trained batch 2182 batch loss 0.46168831 batch mAP 0.611358643 batch PCKh 0.4375\n",
      "Trained batch 2183 batch loss 0.543692231 batch mAP 0.608612061 batch PCKh 0.6875\n",
      "Trained batch 2184 batch loss 0.546331048 batch mAP 0.623626709 batch PCKh 0.6875\n",
      "Trained batch 2185 batch loss 0.451820195 batch mAP 0.634490967 batch PCKh 0.4375\n",
      "Trained batch 2186 batch loss 0.460402846 batch mAP 0.584228516 batch PCKh 0.375\n",
      "Trained batch 2187 batch loss 0.448931813 batch mAP 0.627105713 batch PCKh 0.5\n",
      "Trained batch 2188 batch loss 0.545937359 batch mAP 0.589324951 batch PCKh 0.5625\n",
      "Trained batch 2189 batch loss 0.444107056 batch mAP 0.635772705 batch PCKh 0.5\n",
      "Trained batch 2190 batch loss 0.472669899 batch mAP 0.728118896 batch PCKh 0.3125\n",
      "Trained batch 2191 batch loss 0.501714528 batch mAP 0.675567627 batch PCKh 0.75\n",
      "Trained batch 2192 batch loss 0.455123544 batch mAP 0.725006104 batch PCKh 0.6875\n",
      "Trained batch 2193 batch loss 0.48744303 batch mAP 0.728668213 batch PCKh 0.4375\n",
      "Trained batch 2194 batch loss 0.553197145 batch mAP 0.658355713 batch PCKh 0.6875\n",
      "Trained batch 2195 batch loss 0.505782604 batch mAP 0.627380371 batch PCKh 0.625\n",
      "Trained batch 2196 batch loss 0.625139952 batch mAP 0.522613525 batch PCKh 0.625\n",
      "Trained batch 2197 batch loss 0.617133379 batch mAP 0.587524414 batch PCKh 0.3125\n",
      "Trained batch 2198 batch loss 0.557625771 batch mAP 0.587402344 batch PCKh 0.75\n",
      "Trained batch 2199 batch loss 0.572389781 batch mAP 0.598053 batch PCKh 0.3125\n",
      "Trained batch 2200 batch loss 0.5333395 batch mAP 0.603668213 batch PCKh 0.375\n",
      "Trained batch 2201 batch loss 0.569507 batch mAP 0.566314697 batch PCKh 0.625\n",
      "Trained batch 2202 batch loss 0.556893408 batch mAP 0.593505859 batch PCKh 0.625\n",
      "Trained batch 2203 batch loss 0.545087695 batch mAP 0.549926758 batch PCKh 0.625\n",
      "Trained batch 2204 batch loss 0.5003106 batch mAP 0.548736572 batch PCKh 0.625\n",
      "Trained batch 2205 batch loss 0.487691134 batch mAP 0.539642334 batch PCKh 0.75\n",
      "Trained batch 2206 batch loss 0.425618857 batch mAP 0.672668457 batch PCKh 0.5\n",
      "Trained batch 2207 batch loss 0.400238156 batch mAP 0.565460205 batch PCKh 0\n",
      "Trained batch 2208 batch loss 0.408932984 batch mAP 0.6121521 batch PCKh 0\n",
      "Trained batch 2209 batch loss 0.389860451 batch mAP 0.595855713 batch PCKh 0.125\n",
      "Trained batch 2210 batch loss 0.409598798 batch mAP 0.634887695 batch PCKh 0.6875\n",
      "Trained batch 2211 batch loss 0.403302521 batch mAP 0.65826416 batch PCKh 0.75\n",
      "Trained batch 2212 batch loss 0.468028843 batch mAP 0.627868652 batch PCKh 0.625\n",
      "Trained batch 2213 batch loss 0.483211786 batch mAP 0.687469482 batch PCKh 0.875\n",
      "Trained batch 2214 batch loss 0.441037267 batch mAP 0.675048828 batch PCKh 0.5625\n",
      "Trained batch 2215 batch loss 0.471044183 batch mAP 0.632232666 batch PCKh 0.625\n",
      "Trained batch 2216 batch loss 0.534276068 batch mAP 0.647613525 batch PCKh 0.625\n",
      "Trained batch 2217 batch loss 0.456656069 batch mAP 0.656646729 batch PCKh 0.4375\n",
      "Trained batch 2218 batch loss 0.566478968 batch mAP 0.67477417 batch PCKh 0.3125\n",
      "Trained batch 2219 batch loss 0.559735298 batch mAP 0.642669678 batch PCKh 0.375\n",
      "Trained batch 2220 batch loss 0.482534677 batch mAP 0.624725342 batch PCKh 0.375\n",
      "Trained batch 2221 batch loss 0.433992803 batch mAP 0.687591553 batch PCKh 0.5\n",
      "Trained batch 2222 batch loss 0.423437059 batch mAP 0.717681885 batch PCKh 0.375\n",
      "Trained batch 2223 batch loss 0.5128901 batch mAP 0.630554199 batch PCKh 0.4375\n",
      "Trained batch 2224 batch loss 0.417853534 batch mAP 0.690246582 batch PCKh 0.375\n",
      "Trained batch 2225 batch loss 0.467799783 batch mAP 0.640350342 batch PCKh 0.5625\n",
      "Trained batch 2226 batch loss 0.424388 batch mAP 0.649261475 batch PCKh 0.5625\n",
      "Trained batch 2227 batch loss 0.487487972 batch mAP 0.628234863 batch PCKh 0.75\n",
      "Trained batch 2228 batch loss 0.517187834 batch mAP 0.641723633 batch PCKh 0.8125\n",
      "Trained batch 2229 batch loss 0.429217964 batch mAP 0.63873291 batch PCKh 0.75\n",
      "Trained batch 2230 batch loss 0.498280942 batch mAP 0.653839111 batch PCKh 0.625\n",
      "Trained batch 2231 batch loss 0.504202843 batch mAP 0.65411377 batch PCKh 0.5\n",
      "Trained batch 2232 batch loss 0.553190768 batch mAP 0.654510498 batch PCKh 0.6875\n",
      "Trained batch 2233 batch loss 0.49731797 batch mAP 0.69821167 batch PCKh 0.75\n",
      "Trained batch 2234 batch loss 0.537714839 batch mAP 0.68081665 batch PCKh 0.125\n",
      "Trained batch 2235 batch loss 0.462837249 batch mAP 0.701416 batch PCKh 0.3125\n",
      "Trained batch 2236 batch loss 0.424699962 batch mAP 0.69708252 batch PCKh 0.1875\n",
      "Trained batch 2237 batch loss 0.479650438 batch mAP 0.631713867 batch PCKh 0.3125\n",
      "Trained batch 2238 batch loss 0.4633407 batch mAP 0.617218 batch PCKh 0.4375\n",
      "Trained batch 2239 batch loss 0.480130732 batch mAP 0.645446777 batch PCKh 0.5\n",
      "Trained batch 2240 batch loss 0.490072608 batch mAP 0.702148438 batch PCKh 0.6875\n",
      "Trained batch 2241 batch loss 0.421786457 batch mAP 0.688140869 batch PCKh 0.5\n",
      "Trained batch 2242 batch loss 0.511155486 batch mAP 0.690032959 batch PCKh 0.875\n",
      "Trained batch 2243 batch loss 0.502249 batch mAP 0.643463135 batch PCKh 0.75\n",
      "Trained batch 2244 batch loss 0.519953132 batch mAP 0.650146484 batch PCKh 0.625\n",
      "Trained batch 2245 batch loss 0.54637 batch mAP 0.566711426 batch PCKh 0.4375\n",
      "Trained batch 2246 batch loss 0.673854947 batch mAP 0.575866699 batch PCKh 0.4375\n",
      "Trained batch 2247 batch loss 0.377324969 batch mAP 0.689086914 batch PCKh 0.25\n",
      "Trained batch 2248 batch loss 0.478336096 batch mAP 0.653961182 batch PCKh 0.625\n",
      "Trained batch 2249 batch loss 0.526884079 batch mAP 0.5887146 batch PCKh 0.25\n",
      "Trained batch 2250 batch loss 0.505513251 batch mAP 0.580596924 batch PCKh 0.625\n",
      "Trained batch 2251 batch loss 0.44622466 batch mAP 0.612701416 batch PCKh 0.3125\n",
      "Trained batch 2252 batch loss 0.529534221 batch mAP 0.626190186 batch PCKh 0.5625\n",
      "Trained batch 2253 batch loss 0.471276104 batch mAP 0.65234375 batch PCKh 0.75\n",
      "Trained batch 2254 batch loss 0.464190394 batch mAP 0.641143799 batch PCKh 0.875\n",
      "Trained batch 2255 batch loss 0.534967124 batch mAP 0.5809021 batch PCKh 0.5625\n",
      "Trained batch 2256 batch loss 0.563101649 batch mAP 0.652099609 batch PCKh 0.4375\n",
      "Trained batch 2257 batch loss 0.486806601 batch mAP 0.694702148 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2258 batch loss 0.525054276 batch mAP 0.640655518 batch PCKh 0.5\n",
      "Trained batch 2259 batch loss 0.502391875 batch mAP 0.699768066 batch PCKh 0.625\n",
      "Trained batch 2260 batch loss 0.518291 batch mAP 0.635467529 batch PCKh 0.5\n",
      "Trained batch 2261 batch loss 0.520882964 batch mAP 0.585723877 batch PCKh 0.0625\n",
      "Trained batch 2262 batch loss 0.582499623 batch mAP 0.585327148 batch PCKh 0.125\n",
      "Trained batch 2263 batch loss 0.409856945 batch mAP 0.641937256 batch PCKh 0.75\n",
      "Trained batch 2264 batch loss 0.479989946 batch mAP 0.599304199 batch PCKh 0.75\n",
      "Trained batch 2265 batch loss 0.591446459 batch mAP 0.593811035 batch PCKh 0.75\n",
      "Trained batch 2266 batch loss 0.545132279 batch mAP 0.61340332 batch PCKh 0.5\n",
      "Trained batch 2267 batch loss 0.488851607 batch mAP 0.692749 batch PCKh 0.6875\n",
      "Trained batch 2268 batch loss 0.524005055 batch mAP 0.598510742 batch PCKh 0.5625\n",
      "Trained batch 2269 batch loss 0.481743813 batch mAP 0.668121338 batch PCKh 0.25\n",
      "Trained batch 2270 batch loss 0.451034546 batch mAP 0.615814209 batch PCKh 0.5\n",
      "Trained batch 2271 batch loss 0.49907 batch mAP 0.645965576 batch PCKh 0.75\n",
      "Trained batch 2272 batch loss 0.49569875 batch mAP 0.634765625 batch PCKh 0.6875\n",
      "Trained batch 2273 batch loss 0.501922429 batch mAP 0.61907959 batch PCKh 0.625\n",
      "Trained batch 2274 batch loss 0.641800463 batch mAP 0.499267578 batch PCKh 0.75\n",
      "Trained batch 2275 batch loss 0.536659956 batch mAP 0.562194824 batch PCKh 0.625\n",
      "Trained batch 2276 batch loss 0.607124865 batch mAP 0.508087158 batch PCKh 0.75\n",
      "Trained batch 2277 batch loss 0.471634328 batch mAP 0.578308105 batch PCKh 0.6875\n",
      "Trained batch 2278 batch loss 0.552164912 batch mAP 0.569976807 batch PCKh 0.875\n",
      "Trained batch 2279 batch loss 0.530845761 batch mAP 0.5105896 batch PCKh 0.75\n",
      "Trained batch 2280 batch loss 0.577576697 batch mAP 0.5652771 batch PCKh 0.875\n",
      "Trained batch 2281 batch loss 0.612705 batch mAP 0.536956787 batch PCKh 0.625\n",
      "Trained batch 2282 batch loss 0.531529844 batch mAP 0.652587891 batch PCKh 0.3125\n",
      "Trained batch 2283 batch loss 0.474291563 batch mAP 0.678985596 batch PCKh 0.5\n",
      "Trained batch 2284 batch loss 0.526939 batch mAP 0.601623535 batch PCKh 0.75\n",
      "Trained batch 2285 batch loss 0.6322999 batch mAP 0.577789307 batch PCKh 0.625\n",
      "Trained batch 2286 batch loss 0.607063353 batch mAP 0.616027832 batch PCKh 0.4375\n",
      "Trained batch 2287 batch loss 0.52421248 batch mAP 0.62197876 batch PCKh 0.6875\n",
      "Trained batch 2288 batch loss 0.512726367 batch mAP 0.593139648 batch PCKh 0.5625\n",
      "Trained batch 2289 batch loss 0.578132093 batch mAP 0.555603 batch PCKh 0.5\n",
      "Trained batch 2290 batch loss 0.579436183 batch mAP 0.599639893 batch PCKh 0.5\n",
      "Trained batch 2291 batch loss 0.409431159 batch mAP 0.682128906 batch PCKh 0.625\n",
      "Trained batch 2292 batch loss 0.519834518 batch mAP 0.568908691 batch PCKh 0.5\n",
      "Trained batch 2293 batch loss 0.534004688 batch mAP 0.595031738 batch PCKh 0.1875\n",
      "Trained batch 2294 batch loss 0.47960043 batch mAP 0.629608154 batch PCKh 0.3125\n",
      "Trained batch 2295 batch loss 0.488740087 batch mAP 0.616638184 batch PCKh 0.875\n",
      "Trained batch 2296 batch loss 0.441746652 batch mAP 0.635742188 batch PCKh 0.75\n",
      "Trained batch 2297 batch loss 0.413598806 batch mAP 0.609588623 batch PCKh 0.5\n",
      "Trained batch 2298 batch loss 0.412651598 batch mAP 0.596130371 batch PCKh 0.75\n",
      "Trained batch 2299 batch loss 0.512558281 batch mAP 0.623962402 batch PCKh 0.75\n",
      "Trained batch 2300 batch loss 0.440621346 batch mAP 0.61428833 batch PCKh 0.625\n",
      "Trained batch 2301 batch loss 0.481630534 batch mAP 0.648345947 batch PCKh 0.5\n",
      "Trained batch 2302 batch loss 0.48943314 batch mAP 0.66885376 batch PCKh 0.5\n",
      "Trained batch 2303 batch loss 0.537754834 batch mAP 0.614929199 batch PCKh 0.6875\n",
      "Trained batch 2304 batch loss 0.609772682 batch mAP 0.550354 batch PCKh 0\n",
      "Trained batch 2305 batch loss 0.610784471 batch mAP 0.5809021 batch PCKh 0.3125\n",
      "Trained batch 2306 batch loss 0.558713555 batch mAP 0.601318359 batch PCKh 0.3125\n",
      "Trained batch 2307 batch loss 0.441210538 batch mAP 0.657501221 batch PCKh 0.4375\n",
      "Trained batch 2308 batch loss 0.573404253 batch mAP 0.619903564 batch PCKh 0.6875\n",
      "Trained batch 2309 batch loss 0.544053 batch mAP 0.601409912 batch PCKh 0.6875\n",
      "Trained batch 2310 batch loss 0.518923283 batch mAP 0.615722656 batch PCKh 0.5\n",
      "Trained batch 2311 batch loss 0.464062095 batch mAP 0.615783691 batch PCKh 0.625\n",
      "Trained batch 2312 batch loss 0.464855611 batch mAP 0.630157471 batch PCKh 0.5625\n",
      "Trained batch 2313 batch loss 0.502428174 batch mAP 0.599731445 batch PCKh 0.1875\n",
      "Trained batch 2314 batch loss 0.510176718 batch mAP 0.547943115 batch PCKh 0.5\n",
      "Trained batch 2315 batch loss 0.450665385 batch mAP 0.632019043 batch PCKh 0.375\n",
      "Trained batch 2316 batch loss 0.457825303 batch mAP 0.694824219 batch PCKh 0.75\n",
      "Trained batch 2317 batch loss 0.516322732 batch mAP 0.682312 batch PCKh 0.625\n",
      "Trained batch 2318 batch loss 0.537113309 batch mAP 0.635223389 batch PCKh 0.375\n",
      "Trained batch 2319 batch loss 0.423186302 batch mAP 0.678527832 batch PCKh 0.75\n",
      "Trained batch 2320 batch loss 0.517657161 batch mAP 0.626617432 batch PCKh 0.6875\n",
      "Trained batch 2321 batch loss 0.404690981 batch mAP 0.643707275 batch PCKh 0.5\n",
      "Trained batch 2322 batch loss 0.437943816 batch mAP 0.620239258 batch PCKh 0.1875\n",
      "Trained batch 2323 batch loss 0.424448788 batch mAP 0.671386719 batch PCKh 0.3125\n",
      "Trained batch 2324 batch loss 0.445283353 batch mAP 0.671691895 batch PCKh 0.375\n",
      "Trained batch 2325 batch loss 0.40171513 batch mAP 0.692077637 batch PCKh 0.125\n",
      "Trained batch 2326 batch loss 0.425284594 batch mAP 0.741668701 batch PCKh 0.5\n",
      "Trained batch 2327 batch loss 0.405302405 batch mAP 0.691314697 batch PCKh 0.375\n",
      "Trained batch 2328 batch loss 0.422285259 batch mAP 0.745117188 batch PCKh 0.5\n",
      "Trained batch 2329 batch loss 0.472527564 batch mAP 0.687408447 batch PCKh 0.5625\n",
      "Trained batch 2330 batch loss 0.569493115 batch mAP 0.645263672 batch PCKh 0.5625\n",
      "Trained batch 2331 batch loss 0.461813 batch mAP 0.664703369 batch PCKh 0.4375\n",
      "Trained batch 2332 batch loss 0.675036848 batch mAP 0.565826416 batch PCKh 0.1875\n",
      "Trained batch 2333 batch loss 0.570314348 batch mAP 0.567840576 batch PCKh 0.8125\n",
      "Trained batch 2334 batch loss 0.563128293 batch mAP 0.584838867 batch PCKh 0.5\n",
      "Trained batch 2335 batch loss 0.694052577 batch mAP 0.558990479 batch PCKh 0.6875\n",
      "Trained batch 2336 batch loss 0.591383219 batch mAP 0.581695557 batch PCKh 0.8125\n",
      "Trained batch 2337 batch loss 0.49201262 batch mAP 0.594207764 batch PCKh 0.875\n",
      "Trained batch 2338 batch loss 0.52615869 batch mAP 0.544281 batch PCKh 0.75\n",
      "Trained batch 2339 batch loss 0.57188642 batch mAP 0.594421387 batch PCKh 0.6875\n",
      "Trained batch 2340 batch loss 0.586345792 batch mAP 0.598999 batch PCKh 0.625\n",
      "Trained batch 2341 batch loss 0.582346916 batch mAP 0.572143555 batch PCKh 0.875\n",
      "Trained batch 2342 batch loss 0.523121238 batch mAP 0.570159912 batch PCKh 0.375\n",
      "Trained batch 2343 batch loss 0.520123422 batch mAP 0.555389404 batch PCKh 0.6875\n",
      "Trained batch 2344 batch loss 0.523049 batch mAP 0.579772949 batch PCKh 0.6875\n",
      "Trained batch 2345 batch loss 0.487296641 batch mAP 0.507751465 batch PCKh 0.4375\n",
      "Trained batch 2346 batch loss 0.459643 batch mAP 0.554199219 batch PCKh 0.5625\n",
      "Trained batch 2347 batch loss 0.43921864 batch mAP 0.571777344 batch PCKh 0.4375\n",
      "Trained batch 2348 batch loss 0.587106287 batch mAP 0.582061768 batch PCKh 0.3125\n",
      "Trained batch 2349 batch loss 0.559518576 batch mAP 0.543762207 batch PCKh 0.625\n",
      "Trained batch 2350 batch loss 0.477618843 batch mAP 0.658172607 batch PCKh 0.5625\n",
      "Trained batch 2351 batch loss 0.582975149 batch mAP 0.54095459 batch PCKh 0.375\n",
      "Trained batch 2352 batch loss 0.480050862 batch mAP 0.62298584 batch PCKh 0.3125\n",
      "Trained batch 2353 batch loss 0.562499523 batch mAP 0.55770874 batch PCKh 0.75\n",
      "Trained batch 2354 batch loss 0.513017118 batch mAP 0.524200439 batch PCKh 0.4375\n",
      "Trained batch 2355 batch loss 0.569404483 batch mAP 0.575256348 batch PCKh 0.875\n",
      "Trained batch 2356 batch loss 0.465550423 batch mAP 0.555236816 batch PCKh 0.8125\n",
      "Trained batch 2357 batch loss 0.493471086 batch mAP 0.556396484 batch PCKh 0.4375\n",
      "Trained batch 2358 batch loss 0.441453695 batch mAP 0.515411377 batch PCKh 0.4375\n",
      "Trained batch 2359 batch loss 0.583926737 batch mAP 0.521026611 batch PCKh 0.5\n",
      "Trained batch 2360 batch loss 0.535818458 batch mAP 0.536071777 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2361 batch loss 0.48012 batch mAP 0.687866211 batch PCKh 0.4375\n",
      "Trained batch 2362 batch loss 0.467407256 batch mAP 0.658416748 batch PCKh 0.375\n",
      "Trained batch 2363 batch loss 0.452103972 batch mAP 0.697692871 batch PCKh 0.625\n",
      "Trained batch 2364 batch loss 0.495739698 batch mAP 0.677703857 batch PCKh 0.6875\n",
      "Trained batch 2365 batch loss 0.421429485 batch mAP 0.687225342 batch PCKh 0.625\n",
      "Trained batch 2366 batch loss 0.447894692 batch mAP 0.675689697 batch PCKh 0.5\n",
      "Trained batch 2367 batch loss 0.398195207 batch mAP 0.629119873 batch PCKh 0.3125\n",
      "Trained batch 2368 batch loss 0.529478073 batch mAP 0.609130859 batch PCKh 0.75\n",
      "Trained batch 2369 batch loss 0.50708878 batch mAP 0.637329102 batch PCKh 0.6875\n",
      "Trained batch 2370 batch loss 0.507172 batch mAP 0.562072754 batch PCKh 0.375\n",
      "Trained batch 2371 batch loss 0.569177091 batch mAP 0.579528809 batch PCKh 0.375\n",
      "Trained batch 2372 batch loss 0.508039415 batch mAP 0.581787109 batch PCKh 0.625\n",
      "Trained batch 2373 batch loss 0.523072124 batch mAP 0.59185791 batch PCKh 0.0625\n",
      "Trained batch 2374 batch loss 0.459998876 batch mAP 0.560546875 batch PCKh 0.5625\n",
      "Trained batch 2375 batch loss 0.47462377 batch mAP 0.586456299 batch PCKh 0.875\n",
      "Trained batch 2376 batch loss 0.453200966 batch mAP 0.618591309 batch PCKh 0.875\n",
      "Trained batch 2377 batch loss 0.407142937 batch mAP 0.669616699 batch PCKh 0.8125\n",
      "Trained batch 2378 batch loss 0.332104683 batch mAP 0.72869873 batch PCKh 0.5625\n",
      "Trained batch 2379 batch loss 0.383582503 batch mAP 0.733123779 batch PCKh 0.75\n",
      "Trained batch 2380 batch loss 0.443205655 batch mAP 0.661682129 batch PCKh 0.5625\n",
      "Trained batch 2381 batch loss 0.437374473 batch mAP 0.673431396 batch PCKh 0.8125\n",
      "Trained batch 2382 batch loss 0.338653088 batch mAP 0.738342285 batch PCKh 0.625\n",
      "Trained batch 2383 batch loss 0.35890615 batch mAP 0.700866699 batch PCKh 0.6875\n",
      "Trained batch 2384 batch loss 0.414829165 batch mAP 0.698425293 batch PCKh 0.8125\n",
      "Trained batch 2385 batch loss 0.426935881 batch mAP 0.647949219 batch PCKh 0.6875\n",
      "Trained batch 2386 batch loss 0.509486198 batch mAP 0.634979248 batch PCKh 0.3125\n",
      "Trained batch 2387 batch loss 0.467822284 batch mAP 0.705535889 batch PCKh 0.5\n",
      "Trained batch 2388 batch loss 0.528396 batch mAP 0.635559082 batch PCKh 0.5625\n",
      "Trained batch 2389 batch loss 0.531821609 batch mAP 0.662323 batch PCKh 0.3125\n",
      "Trained batch 2390 batch loss 0.56395179 batch mAP 0.631561279 batch PCKh 0.375\n",
      "Trained batch 2391 batch loss 0.59869647 batch mAP 0.66986084 batch PCKh 0.375\n",
      "Trained batch 2392 batch loss 0.595894814 batch mAP 0.680328369 batch PCKh 0.3125\n",
      "Trained batch 2393 batch loss 0.462961674 batch mAP 0.741333 batch PCKh 0.375\n",
      "Trained batch 2394 batch loss 0.467057377 batch mAP 0.723510742 batch PCKh 0.25\n",
      "Trained batch 2395 batch loss 0.4807688 batch mAP 0.711914062 batch PCKh 0.3125\n",
      "Trained batch 2396 batch loss 0.464191377 batch mAP 0.709136963 batch PCKh 0.5625\n",
      "Trained batch 2397 batch loss 0.535179436 batch mAP 0.655975342 batch PCKh 0.4375\n",
      "Trained batch 2398 batch loss 0.454838902 batch mAP 0.700317383 batch PCKh 0.4375\n",
      "Trained batch 2399 batch loss 0.506362438 batch mAP 0.73739624 batch PCKh 0.375\n",
      "Trained batch 2400 batch loss 0.505264759 batch mAP 0.681640625 batch PCKh 0.6875\n",
      "Trained batch 2401 batch loss 0.435983419 batch mAP 0.656494141 batch PCKh 0.625\n",
      "Trained batch 2402 batch loss 0.505188644 batch mAP 0.550201416 batch PCKh 0.5\n",
      "Trained batch 2403 batch loss 0.535272479 batch mAP 0.568115234 batch PCKh 0.875\n",
      "Trained batch 2404 batch loss 0.463362515 batch mAP 0.571502686 batch PCKh 0.75\n",
      "Trained batch 2405 batch loss 0.496828824 batch mAP 0.545776367 batch PCKh 0.75\n",
      "Trained batch 2406 batch loss 0.451604486 batch mAP 0.580322266 batch PCKh 0.375\n",
      "Trained batch 2407 batch loss 0.450049877 batch mAP 0.601318359 batch PCKh 0.75\n",
      "Trained batch 2408 batch loss 0.500205278 batch mAP 0.559875488 batch PCKh 0.375\n",
      "Trained batch 2409 batch loss 0.447381616 batch mAP 0.628875732 batch PCKh 0.5625\n",
      "Trained batch 2410 batch loss 0.479902029 batch mAP 0.59085083 batch PCKh 0.5625\n",
      "Trained batch 2411 batch loss 0.466306269 batch mAP 0.585418701 batch PCKh 0.75\n",
      "Trained batch 2412 batch loss 0.479341388 batch mAP 0.656677246 batch PCKh 0.875\n",
      "Trained batch 2413 batch loss 0.411360979 batch mAP 0.683349609 batch PCKh 0.5\n",
      "Trained batch 2414 batch loss 0.502213776 batch mAP 0.682800293 batch PCKh 0.875\n",
      "Trained batch 2415 batch loss 0.494693041 batch mAP 0.634429932 batch PCKh 0.6875\n",
      "Trained batch 2416 batch loss 0.525077 batch mAP 0.614624 batch PCKh 0.1875\n",
      "Trained batch 2417 batch loss 0.427342385 batch mAP 0.616272 batch PCKh 0.625\n",
      "Trained batch 2418 batch loss 0.511860967 batch mAP 0.577972412 batch PCKh 0.5625\n",
      "Trained batch 2419 batch loss 0.448253512 batch mAP 0.575775146 batch PCKh 0.5625\n",
      "Trained batch 2420 batch loss 0.508852482 batch mAP 0.620513916 batch PCKh 0.3125\n",
      "Trained batch 2421 batch loss 0.548300564 batch mAP 0.604095459 batch PCKh 0.4375\n",
      "Trained batch 2422 batch loss 0.499962479 batch mAP 0.624023438 batch PCKh 0.3125\n",
      "Trained batch 2423 batch loss 0.480665267 batch mAP 0.671264648 batch PCKh 0.1875\n",
      "Trained batch 2424 batch loss 0.446658701 batch mAP 0.681945801 batch PCKh 0.375\n",
      "Trained batch 2425 batch loss 0.451212585 batch mAP 0.624389648 batch PCKh 0.3125\n",
      "Trained batch 2426 batch loss 0.513005733 batch mAP 0.622436523 batch PCKh 0.25\n",
      "Trained batch 2427 batch loss 0.541220725 batch mAP 0.542877197 batch PCKh 0.5625\n",
      "Trained batch 2428 batch loss 0.553936362 batch mAP 0.640258789 batch PCKh 0.75\n",
      "Trained batch 2429 batch loss 0.551252365 batch mAP 0.588287354 batch PCKh 0.5625\n",
      "Trained batch 2430 batch loss 0.62728858 batch mAP 0.561523438 batch PCKh 0.75\n",
      "Trained batch 2431 batch loss 0.480838448 batch mAP 0.645019531 batch PCKh 0.1875\n",
      "Trained batch 2432 batch loss 0.413662 batch mAP 0.697601318 batch PCKh 0.1875\n",
      "Trained batch 2433 batch loss 0.521297753 batch mAP 0.633300781 batch PCKh 0.5\n",
      "Trained batch 2434 batch loss 0.591721892 batch mAP 0.554229736 batch PCKh 0.25\n",
      "Trained batch 2435 batch loss 0.564782679 batch mAP 0.556671143 batch PCKh 0.25\n",
      "Trained batch 2436 batch loss 0.499453723 batch mAP 0.554412842 batch PCKh 0.3125\n",
      "Trained batch 2437 batch loss 0.566787839 batch mAP 0.527008057 batch PCKh 0.5625\n",
      "Trained batch 2438 batch loss 0.597247601 batch mAP 0.565704346 batch PCKh 0.5\n",
      "Trained batch 2439 batch loss 0.644247532 batch mAP 0.569458 batch PCKh 0.8125\n",
      "Trained batch 2440 batch loss 0.625282347 batch mAP 0.567688 batch PCKh 0.6875\n",
      "Trained batch 2441 batch loss 0.584275484 batch mAP 0.50994873 batch PCKh 0.6875\n",
      "Trained batch 2442 batch loss 0.642807841 batch mAP 0.552185059 batch PCKh 0\n",
      "Trained batch 2443 batch loss 0.523156404 batch mAP 0.585998535 batch PCKh 0.8125\n",
      "Trained batch 2444 batch loss 0.438891411 batch mAP 0.55682373 batch PCKh 0.6875\n",
      "Trained batch 2445 batch loss 0.47860074 batch mAP 0.579528809 batch PCKh 0.6875\n",
      "Trained batch 2446 batch loss 0.505842626 batch mAP 0.567993164 batch PCKh 0.8125\n",
      "Trained batch 2447 batch loss 0.562620878 batch mAP 0.591095 batch PCKh 0.6875\n",
      "Trained batch 2448 batch loss 0.482263207 batch mAP 0.596282959 batch PCKh 0.6875\n",
      "Trained batch 2449 batch loss 0.514775872 batch mAP 0.522064209 batch PCKh 0.6875\n",
      "Trained batch 2450 batch loss 0.523205817 batch mAP 0.575500488 batch PCKh 0.5\n",
      "Trained batch 2451 batch loss 0.553007603 batch mAP 0.604248047 batch PCKh 0.1875\n",
      "Trained batch 2452 batch loss 0.525273919 batch mAP 0.67956543 batch PCKh 0.375\n",
      "Trained batch 2453 batch loss 0.550189495 batch mAP 0.637481689 batch PCKh 0.375\n",
      "Trained batch 2454 batch loss 0.538870931 batch mAP 0.594543457 batch PCKh 0.5\n",
      "Trained batch 2455 batch loss 0.634074569 batch mAP 0.59185791 batch PCKh 0.5625\n",
      "Trained batch 2456 batch loss 0.534947336 batch mAP 0.624725342 batch PCKh 0.4375\n",
      "Trained batch 2457 batch loss 0.617840052 batch mAP 0.605407715 batch PCKh 0.375\n",
      "Trained batch 2458 batch loss 0.562489092 batch mAP 0.67791748 batch PCKh 0.5\n",
      "Trained batch 2459 batch loss 0.511671066 batch mAP 0.680023193 batch PCKh 0.4375\n",
      "Trained batch 2460 batch loss 0.6268 batch mAP 0.61920166 batch PCKh 0.4375\n",
      "Trained batch 2461 batch loss 0.621349275 batch mAP 0.549835205 batch PCKh 0.375\n",
      "Trained batch 2462 batch loss 0.587714732 batch mAP 0.62399292 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2463 batch loss 0.598156929 batch mAP 0.620391846 batch PCKh 0.4375\n",
      "Trained batch 2464 batch loss 0.641262114 batch mAP 0.642486572 batch PCKh 0.8125\n",
      "Trained batch 2465 batch loss 0.592324436 batch mAP 0.622039795 batch PCKh 0.6875\n",
      "Trained batch 2466 batch loss 0.556026876 batch mAP 0.570861816 batch PCKh 0\n",
      "Trained batch 2467 batch loss 0.567294478 batch mAP 0.569122314 batch PCKh 0.5625\n",
      "Trained batch 2468 batch loss 0.567791641 batch mAP 0.610595703 batch PCKh 0.75\n",
      "Trained batch 2469 batch loss 0.5290398 batch mAP 0.641784668 batch PCKh 0.6875\n",
      "Trained batch 2470 batch loss 0.60106343 batch mAP 0.579559326 batch PCKh 0.3125\n",
      "Trained batch 2471 batch loss 0.538205 batch mAP 0.571990967 batch PCKh 0.75\n",
      "Trained batch 2472 batch loss 0.560355186 batch mAP 0.538696289 batch PCKh 0.8125\n",
      "Trained batch 2473 batch loss 0.583203912 batch mAP 0.559021 batch PCKh 0.125\n",
      "Trained batch 2474 batch loss 0.544277072 batch mAP 0.590179443 batch PCKh 0.4375\n",
      "Trained batch 2475 batch loss 0.479415625 batch mAP 0.605560303 batch PCKh 0.375\n",
      "Trained batch 2476 batch loss 0.613361597 batch mAP 0.582550049 batch PCKh 0.625\n",
      "Trained batch 2477 batch loss 0.518883348 batch mAP 0.559509277 batch PCKh 0.5\n",
      "Trained batch 2478 batch loss 0.599334061 batch mAP 0.604034424 batch PCKh 0.6875\n",
      "Trained batch 2479 batch loss 0.563234031 batch mAP 0.529418945 batch PCKh 0.4375\n",
      "Trained batch 2480 batch loss 0.491462827 batch mAP 0.61026 batch PCKh 0.5625\n",
      "Trained batch 2481 batch loss 0.425620735 batch mAP 0.597045898 batch PCKh 0.75\n",
      "Trained batch 2482 batch loss 0.56465137 batch mAP 0.555084229 batch PCKh 0.4375\n",
      "Trained batch 2483 batch loss 0.589567542 batch mAP 0.504974365 batch PCKh 0.5\n",
      "Trained batch 2484 batch loss 0.492379159 batch mAP 0.5440979 batch PCKh 0.6875\n",
      "Trained batch 2485 batch loss 0.508298576 batch mAP 0.51373291 batch PCKh 0.8125\n",
      "Trained batch 2486 batch loss 0.502249897 batch mAP 0.610931396 batch PCKh 0.5625\n",
      "Trained batch 2487 batch loss 0.582865477 batch mAP 0.545684814 batch PCKh 0.5625\n",
      "Trained batch 2488 batch loss 0.540861964 batch mAP 0.607208252 batch PCKh 0.4375\n",
      "Trained batch 2489 batch loss 0.637181461 batch mAP 0.610870361 batch PCKh 0.375\n",
      "Trained batch 2490 batch loss 0.650321841 batch mAP 0.611541748 batch PCKh 0.4375\n",
      "Trained batch 2491 batch loss 0.485987663 batch mAP 0.636352539 batch PCKh 0.1875\n",
      "Trained batch 2492 batch loss 0.353617102 batch mAP 0.667663574 batch PCKh 0.5625\n",
      "Trained batch 2493 batch loss 0.398443699 batch mAP 0.630218506 batch PCKh 0.6875\n",
      "Trained batch 2494 batch loss 0.405980349 batch mAP 0.621002197 batch PCKh 0.5\n",
      "Trained batch 2495 batch loss 0.531304717 batch mAP 0.600402832 batch PCKh 0.5625\n",
      "Trained batch 2496 batch loss 0.419647813 batch mAP 0.642974854 batch PCKh 0.1875\n",
      "Trained batch 2497 batch loss 0.501601 batch mAP 0.594940186 batch PCKh 0.6875\n",
      "Trained batch 2498 batch loss 0.52376 batch mAP 0.617828369 batch PCKh 0.375\n",
      "Trained batch 2499 batch loss 0.544514894 batch mAP 0.653381348 batch PCKh 0.75\n",
      "Trained batch 2500 batch loss 0.501895547 batch mAP 0.654968262 batch PCKh 0.625\n",
      "Trained batch 2501 batch loss 0.519290447 batch mAP 0.669952393 batch PCKh 0.125\n",
      "Trained batch 2502 batch loss 0.485727 batch mAP 0.631713867 batch PCKh 0.625\n",
      "Trained batch 2503 batch loss 0.468311042 batch mAP 0.649658203 batch PCKh 0.6875\n",
      "Trained batch 2504 batch loss 0.530217588 batch mAP 0.6456604 batch PCKh 0.75\n",
      "Trained batch 2505 batch loss 0.583501875 batch mAP 0.587921143 batch PCKh 0.75\n",
      "Trained batch 2506 batch loss 0.508969605 batch mAP 0.567871094 batch PCKh 0.6875\n",
      "Trained batch 2507 batch loss 0.609954596 batch mAP 0.560516357 batch PCKh 0.25\n",
      "Trained batch 2508 batch loss 0.615409493 batch mAP 0.557220459 batch PCKh 0.3125\n",
      "Trained batch 2509 batch loss 0.504179955 batch mAP 0.538909912 batch PCKh 0.125\n",
      "Trained batch 2510 batch loss 0.501788318 batch mAP 0.529327393 batch PCKh 0.875\n",
      "Trained batch 2511 batch loss 0.495497674 batch mAP 0.571807861 batch PCKh 0.75\n",
      "Trained batch 2512 batch loss 0.604135811 batch mAP 0.521698 batch PCKh 0.3125\n",
      "Trained batch 2513 batch loss 0.608633518 batch mAP 0.605011 batch PCKh 0.75\n",
      "Trained batch 2514 batch loss 0.61534059 batch mAP 0.566192627 batch PCKh 0.5\n",
      "Trained batch 2515 batch loss 0.630089879 batch mAP 0.603912354 batch PCKh 0.375\n",
      "Trained batch 2516 batch loss 0.56751585 batch mAP 0.611084 batch PCKh 0.5\n",
      "Trained batch 2517 batch loss 0.454542696 batch mAP 0.693389893 batch PCKh 0.375\n",
      "Trained batch 2518 batch loss 0.413651168 batch mAP 0.681976318 batch PCKh 0.4375\n",
      "Trained batch 2519 batch loss 0.439367652 batch mAP 0.70916748 batch PCKh 0.4375\n",
      "Trained batch 2520 batch loss 0.453698814 batch mAP 0.67376709 batch PCKh 0.5\n",
      "Trained batch 2521 batch loss 0.491316497 batch mAP 0.633850098 batch PCKh 0.5625\n",
      "Trained batch 2522 batch loss 0.497402966 batch mAP 0.60333252 batch PCKh 0.75\n",
      "Trained batch 2523 batch loss 0.471095681 batch mAP 0.602020264 batch PCKh 0.625\n",
      "Trained batch 2524 batch loss 0.483529121 batch mAP 0.636322 batch PCKh 0.4375\n",
      "Trained batch 2525 batch loss 0.495358258 batch mAP 0.61920166 batch PCKh 0.4375\n",
      "Trained batch 2526 batch loss 0.456713676 batch mAP 0.63269043 batch PCKh 0.3125\n",
      "Trained batch 2527 batch loss 0.528960526 batch mAP 0.626922607 batch PCKh 0.6875\n",
      "Trained batch 2528 batch loss 0.547420621 batch mAP 0.593933105 batch PCKh 0.5\n",
      "Trained batch 2529 batch loss 0.527847052 batch mAP 0.551849365 batch PCKh 0.5\n",
      "Trained batch 2530 batch loss 0.530653954 batch mAP 0.53024292 batch PCKh 0.8125\n",
      "Trained batch 2531 batch loss 0.579721332 batch mAP 0.54989624 batch PCKh 0.8125\n",
      "Trained batch 2532 batch loss 0.565205 batch mAP 0.54119873 batch PCKh 0.125\n",
      "Trained batch 2533 batch loss 0.562038779 batch mAP 0.541442871 batch PCKh 0.375\n",
      "Trained batch 2534 batch loss 0.405520111 batch mAP 0.703186035 batch PCKh 0.625\n",
      "Trained batch 2535 batch loss 0.48432374 batch mAP 0.667877197 batch PCKh 0.4375\n",
      "Trained batch 2536 batch loss 0.570758581 batch mAP 0.600402832 batch PCKh 0.625\n",
      "Trained batch 2537 batch loss 0.543150485 batch mAP 0.606231689 batch PCKh 0.625\n",
      "Trained batch 2538 batch loss 0.445781827 batch mAP 0.667022705 batch PCKh 0.5625\n",
      "Trained batch 2539 batch loss 0.528 batch mAP 0.65914917 batch PCKh 0.375\n",
      "Trained batch 2540 batch loss 0.513580322 batch mAP 0.690856934 batch PCKh 0.625\n",
      "Trained batch 2541 batch loss 0.496311843 batch mAP 0.683685303 batch PCKh 0.625\n",
      "Trained batch 2542 batch loss 0.542897344 batch mAP 0.607574463 batch PCKh 0.5625\n",
      "Trained batch 2543 batch loss 0.578382969 batch mAP 0.607940674 batch PCKh 0.5\n",
      "Trained batch 2544 batch loss 0.585117698 batch mAP 0.627349854 batch PCKh 0.5\n",
      "Trained batch 2545 batch loss 0.533419 batch mAP 0.623352051 batch PCKh 0.5625\n",
      "Trained batch 2546 batch loss 0.526625037 batch mAP 0.608062744 batch PCKh 0.4375\n",
      "Trained batch 2547 batch loss 0.567629099 batch mAP 0.603851318 batch PCKh 0.5\n",
      "Trained batch 2548 batch loss 0.497640789 batch mAP 0.543121338 batch PCKh 0.6875\n",
      "Trained batch 2549 batch loss 0.440045834 batch mAP 0.64263916 batch PCKh 0.625\n",
      "Trained batch 2550 batch loss 0.508262157 batch mAP 0.607391357 batch PCKh 0.375\n",
      "Trained batch 2551 batch loss 0.578221262 batch mAP 0.599243164 batch PCKh 0.3125\n",
      "Trained batch 2552 batch loss 0.574010074 batch mAP 0.586151123 batch PCKh 0.5625\n",
      "Trained batch 2553 batch loss 0.548299611 batch mAP 0.549926758 batch PCKh 0.6875\n",
      "Trained batch 2554 batch loss 0.617236197 batch mAP 0.523468 batch PCKh 0.6875\n",
      "Trained batch 2555 batch loss 0.592081904 batch mAP 0.522979736 batch PCKh 0.625\n",
      "Trained batch 2556 batch loss 0.416768 batch mAP 0.731506348 batch PCKh 0.5\n",
      "Trained batch 2557 batch loss 0.581374884 batch mAP 0.618408203 batch PCKh 0.6875\n",
      "Trained batch 2558 batch loss 0.503865719 batch mAP 0.60345459 batch PCKh 0.4375\n",
      "Trained batch 2559 batch loss 0.419347703 batch mAP 0.674163818 batch PCKh 0.5625\n",
      "Trained batch 2560 batch loss 0.413112164 batch mAP 0.658966064 batch PCKh 0.5625\n",
      "Trained batch 2561 batch loss 0.381196529 batch mAP 0.660583496 batch PCKh 0\n",
      "Trained batch 2562 batch loss 0.360990047 batch mAP 0.625732422 batch PCKh 0.3125\n",
      "Trained batch 2563 batch loss 0.358783245 batch mAP 0.60144043 batch PCKh 0\n",
      "Trained batch 2564 batch loss 0.404636323 batch mAP 0.627685547 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2565 batch loss 0.477340519 batch mAP 0.649597168 batch PCKh 0.1875\n",
      "Trained batch 2566 batch loss 0.672137439 batch mAP 0.623138428 batch PCKh 0.8125\n",
      "Trained batch 2567 batch loss 0.596256793 batch mAP 0.624725342 batch PCKh 0.4375\n",
      "Trained batch 2568 batch loss 0.609974623 batch mAP 0.583496094 batch PCKh 0.4375\n",
      "Trained batch 2569 batch loss 0.646439075 batch mAP 0.60144043 batch PCKh 0.4375\n",
      "Trained batch 2570 batch loss 0.654114604 batch mAP 0.558136 batch PCKh 0.375\n",
      "Trained batch 2571 batch loss 0.635011792 batch mAP 0.571563721 batch PCKh 0.375\n",
      "Trained batch 2572 batch loss 0.449344128 batch mAP 0.609283447 batch PCKh 0.0625\n",
      "Trained batch 2573 batch loss 0.535559118 batch mAP 0.577728271 batch PCKh 0.6875\n",
      "Trained batch 2574 batch loss 0.496461928 batch mAP 0.571167 batch PCKh 0.4375\n",
      "Trained batch 2575 batch loss 0.475731134 batch mAP 0.635376 batch PCKh 0.625\n",
      "Trained batch 2576 batch loss 0.476977795 batch mAP 0.631164551 batch PCKh 0.5625\n",
      "Trained batch 2577 batch loss 0.56748414 batch mAP 0.635925293 batch PCKh 0.4375\n",
      "Trained batch 2578 batch loss 0.556009948 batch mAP 0.634735107 batch PCKh 0.4375\n",
      "Trained batch 2579 batch loss 0.524538636 batch mAP 0.649200439 batch PCKh 0.5625\n",
      "Trained batch 2580 batch loss 0.543631077 batch mAP 0.701507568 batch PCKh 0.375\n",
      "Trained batch 2581 batch loss 0.489522815 batch mAP 0.713378906 batch PCKh 0.5625\n",
      "Trained batch 2582 batch loss 0.594099581 batch mAP 0.62323 batch PCKh 0.4375\n",
      "Trained batch 2583 batch loss 0.518125176 batch mAP 0.676239 batch PCKh 0.5625\n",
      "Trained batch 2584 batch loss 0.474783242 batch mAP 0.699951172 batch PCKh 0.75\n",
      "Trained batch 2585 batch loss 0.523542404 batch mAP 0.639892578 batch PCKh 0.5625\n",
      "Trained batch 2586 batch loss 0.577485204 batch mAP 0.56137085 batch PCKh 0.8125\n",
      "Trained batch 2587 batch loss 0.610330582 batch mAP 0.51574707 batch PCKh 0.5\n",
      "Trained batch 2588 batch loss 0.502435744 batch mAP 0.612518311 batch PCKh 0.6875\n",
      "Trained batch 2589 batch loss 0.419061244 batch mAP 0.636901855 batch PCKh 0.5\n",
      "Trained batch 2590 batch loss 0.483125865 batch mAP 0.593322754 batch PCKh 0.4375\n",
      "Trained batch 2591 batch loss 0.533355 batch mAP 0.60067749 batch PCKh 0.1875\n",
      "Trained batch 2592 batch loss 0.476640731 batch mAP 0.631286621 batch PCKh 0.5625\n",
      "Trained batch 2593 batch loss 0.395041764 batch mAP 0.63659668 batch PCKh 0.4375\n",
      "Trained batch 2594 batch loss 0.464829654 batch mAP 0.603271484 batch PCKh 0.625\n",
      "Trained batch 2595 batch loss 0.462446421 batch mAP 0.666564941 batch PCKh 0.5\n",
      "Trained batch 2596 batch loss 0.528874516 batch mAP 0.690063477 batch PCKh 0.5\n",
      "Trained batch 2597 batch loss 0.48862648 batch mAP 0.7059021 batch PCKh 0.625\n",
      "Trained batch 2598 batch loss 0.597383261 batch mAP 0.64251709 batch PCKh 0.125\n",
      "Trained batch 2599 batch loss 0.491938353 batch mAP 0.626556396 batch PCKh 0.875\n",
      "Trained batch 2600 batch loss 0.449379265 batch mAP 0.69128418 batch PCKh 0.625\n",
      "Trained batch 2601 batch loss 0.386909306 batch mAP 0.66394043 batch PCKh 0.4375\n",
      "Trained batch 2602 batch loss 0.402012944 batch mAP 0.613769531 batch PCKh 0.125\n",
      "Trained batch 2603 batch loss 0.393289268 batch mAP 0.642120361 batch PCKh 0.5\n",
      "Trained batch 2604 batch loss 0.396107346 batch mAP 0.614929199 batch PCKh 0.5\n",
      "Trained batch 2605 batch loss 0.399462104 batch mAP 0.650543213 batch PCKh 0\n",
      "Trained batch 2606 batch loss 0.415899783 batch mAP 0.672851562 batch PCKh 0.3125\n",
      "Trained batch 2607 batch loss 0.457254231 batch mAP 0.670532227 batch PCKh 0.625\n",
      "Trained batch 2608 batch loss 0.575881243 batch mAP 0.600585938 batch PCKh 0.8125\n",
      "Trained batch 2609 batch loss 0.628017664 batch mAP 0.629303 batch PCKh 0.5625\n",
      "Trained batch 2610 batch loss 0.640020788 batch mAP 0.579193115 batch PCKh 0.75\n",
      "Trained batch 2611 batch loss 0.500888467 batch mAP 0.634124756 batch PCKh 0.75\n",
      "Trained batch 2612 batch loss 0.571688712 batch mAP 0.578704834 batch PCKh 0.375\n",
      "Trained batch 2613 batch loss 0.570843875 batch mAP 0.632720947 batch PCKh 0.3125\n",
      "Trained batch 2614 batch loss 0.584130406 batch mAP 0.656158447 batch PCKh 0.875\n",
      "Trained batch 2615 batch loss 0.494790673 batch mAP 0.642913818 batch PCKh 0.75\n",
      "Trained batch 2616 batch loss 0.655172169 batch mAP 0.614898682 batch PCKh 0.0625\n",
      "Trained batch 2617 batch loss 0.606550336 batch mAP 0.639312744 batch PCKh 0.6875\n",
      "Trained batch 2618 batch loss 0.520483375 batch mAP 0.685882568 batch PCKh 0.75\n",
      "Trained batch 2619 batch loss 0.57300055 batch mAP 0.650756836 batch PCKh 0.4375\n",
      "Trained batch 2620 batch loss 0.470617652 batch mAP 0.685028076 batch PCKh 0.5625\n",
      "Trained batch 2621 batch loss 0.481348723 batch mAP 0.657074 batch PCKh 0.75\n",
      "Trained batch 2622 batch loss 0.630987167 batch mAP 0.632202148 batch PCKh 0.25\n",
      "Trained batch 2623 batch loss 0.584565878 batch mAP 0.608337402 batch PCKh 0.25\n",
      "Trained batch 2624 batch loss 0.571656525 batch mAP 0.533752441 batch PCKh 0.1875\n",
      "Trained batch 2625 batch loss 0.469912589 batch mAP 0.670562744 batch PCKh 0.4375\n",
      "Trained batch 2626 batch loss 0.537412882 batch mAP 0.583984375 batch PCKh 0.5625\n",
      "Trained batch 2627 batch loss 0.527817965 batch mAP 0.651397705 batch PCKh 0.1875\n",
      "Trained batch 2628 batch loss 0.448387593 batch mAP 0.633178711 batch PCKh 0.3125\n",
      "Trained batch 2629 batch loss 0.40550822 batch mAP 0.661743164 batch PCKh 0.5625\n",
      "Trained batch 2630 batch loss 0.452861249 batch mAP 0.67175293 batch PCKh 0.4375\n",
      "Trained batch 2631 batch loss 0.393390536 batch mAP 0.67086792 batch PCKh 0.75\n",
      "Trained batch 2632 batch loss 0.413211405 batch mAP 0.621521 batch PCKh 0.1875\n",
      "Trained batch 2633 batch loss 0.39678514 batch mAP 0.586547852 batch PCKh 0.1875\n",
      "Trained batch 2634 batch loss 0.431735963 batch mAP 0.60446167 batch PCKh 0.3125\n",
      "Trained batch 2635 batch loss 0.44147253 batch mAP 0.609741211 batch PCKh 0.3125\n",
      "Trained batch 2636 batch loss 0.498616934 batch mAP 0.634429932 batch PCKh 0.5625\n",
      "Trained batch 2637 batch loss 0.580592513 batch mAP 0.5887146 batch PCKh 0.3125\n",
      "Trained batch 2638 batch loss 0.487421155 batch mAP 0.645385742 batch PCKh 0.5\n",
      "Trained batch 2639 batch loss 0.575247705 batch mAP 0.600524902 batch PCKh 0.5\n",
      "Trained batch 2640 batch loss 0.52108103 batch mAP 0.547149658 batch PCKh 0.6875\n",
      "Trained batch 2641 batch loss 0.513949037 batch mAP 0.60836792 batch PCKh 0.75\n",
      "Trained batch 2642 batch loss 0.576924562 batch mAP 0.557678223 batch PCKh 0.4375\n",
      "Trained batch 2643 batch loss 0.578885913 batch mAP 0.576049805 batch PCKh 0.75\n",
      "Trained batch 2644 batch loss 0.479408473 batch mAP 0.578765869 batch PCKh 0.75\n",
      "Trained batch 2645 batch loss 0.479242 batch mAP 0.614532471 batch PCKh 0.6875\n",
      "Trained batch 2646 batch loss 0.473425716 batch mAP 0.664032 batch PCKh 0.625\n",
      "Trained batch 2647 batch loss 0.426147342 batch mAP 0.611633301 batch PCKh 0\n",
      "Trained batch 2648 batch loss 0.386257052 batch mAP 0.714752197 batch PCKh 0.5625\n",
      "Trained batch 2649 batch loss 0.465700448 batch mAP 0.73059082 batch PCKh 0.625\n",
      "Trained batch 2650 batch loss 0.521891594 batch mAP 0.673187256 batch PCKh 0.5\n",
      "Trained batch 2651 batch loss 0.614357412 batch mAP 0.66116333 batch PCKh 0.5625\n",
      "Trained batch 2652 batch loss 0.547905564 batch mAP 0.617431641 batch PCKh 0.5\n",
      "Trained batch 2653 batch loss 0.538090348 batch mAP 0.66998291 batch PCKh 0.5625\n",
      "Trained batch 2654 batch loss 0.527139187 batch mAP 0.66619873 batch PCKh 0.4375\n",
      "Trained batch 2655 batch loss 0.562759 batch mAP 0.677490234 batch PCKh 0.4375\n",
      "Trained batch 2656 batch loss 0.553671062 batch mAP 0.668548584 batch PCKh 0.5\n",
      "Trained batch 2657 batch loss 0.549958467 batch mAP 0.614837646 batch PCKh 0.5\n",
      "Trained batch 2658 batch loss 0.596890092 batch mAP 0.605712891 batch PCKh 0.5\n",
      "Trained batch 2659 batch loss 0.469235271 batch mAP 0.587799072 batch PCKh 0.625\n",
      "Trained batch 2660 batch loss 0.526359 batch mAP 0.636901855 batch PCKh 0.625\n",
      "Trained batch 2661 batch loss 0.476430446 batch mAP 0.669311523 batch PCKh 0.75\n",
      "Trained batch 2662 batch loss 0.464672565 batch mAP 0.617797852 batch PCKh 0.875\n",
      "Trained batch 2663 batch loss 0.396529496 batch mAP 0.585662842 batch PCKh 0.75\n",
      "Trained batch 2664 batch loss 0.442225218 batch mAP 0.589538574 batch PCKh 0.875\n",
      "Trained batch 2665 batch loss 0.435851276 batch mAP 0.629577637 batch PCKh 0.625\n",
      "Trained batch 2666 batch loss 0.506276786 batch mAP 0.618927 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2667 batch loss 0.558242619 batch mAP 0.63470459 batch PCKh 0.5\n",
      "Trained batch 2668 batch loss 0.544246614 batch mAP 0.624816895 batch PCKh 0.8125\n",
      "Trained batch 2669 batch loss 0.447941244 batch mAP 0.635101318 batch PCKh 0.6875\n",
      "Trained batch 2670 batch loss 0.508003414 batch mAP 0.621307373 batch PCKh 0.625\n",
      "Trained batch 2671 batch loss 0.500273228 batch mAP 0.623077393 batch PCKh 0.625\n",
      "Trained batch 2672 batch loss 0.569366217 batch mAP 0.527160645 batch PCKh 0.3125\n",
      "Trained batch 2673 batch loss 0.600701809 batch mAP 0.561218262 batch PCKh 0.5\n",
      "Trained batch 2674 batch loss 0.553512812 batch mAP 0.587036133 batch PCKh 0.4375\n",
      "Trained batch 2675 batch loss 0.549972415 batch mAP 0.571868896 batch PCKh 0.75\n",
      "Trained batch 2676 batch loss 0.484100759 batch mAP 0.611328125 batch PCKh 0.5625\n",
      "Trained batch 2677 batch loss 0.572115064 batch mAP 0.598480225 batch PCKh 0.75\n",
      "Trained batch 2678 batch loss 0.608354568 batch mAP 0.541412354 batch PCKh 0.6875\n",
      "Trained batch 2679 batch loss 0.52321893 batch mAP 0.557037354 batch PCKh 0.75\n",
      "Trained batch 2680 batch loss 0.580803275 batch mAP 0.479187 batch PCKh 0.4375\n",
      "Trained batch 2681 batch loss 0.590768099 batch mAP 0.548461914 batch PCKh 0.6875\n",
      "Trained batch 2682 batch loss 0.620346785 batch mAP 0.541229248 batch PCKh 0.625\n",
      "Trained batch 2683 batch loss 0.541433573 batch mAP 0.555786133 batch PCKh 0.875\n",
      "Trained batch 2684 batch loss 0.506163955 batch mAP 0.538147 batch PCKh 0.75\n",
      "Trained batch 2685 batch loss 0.582163334 batch mAP 0.567565918 batch PCKh 0.6875\n",
      "Trained batch 2686 batch loss 0.644456565 batch mAP 0.601806641 batch PCKh 0.375\n",
      "Trained batch 2687 batch loss 0.5243783 batch mAP 0.594940186 batch PCKh 0.6875\n",
      "Trained batch 2688 batch loss 0.576653361 batch mAP 0.596679688 batch PCKh 0.625\n",
      "Trained batch 2689 batch loss 0.577261329 batch mAP 0.523590088 batch PCKh 0.625\n",
      "Trained batch 2690 batch loss 0.627769291 batch mAP 0.573822 batch PCKh 0.25\n",
      "Trained batch 2691 batch loss 0.499567 batch mAP 0.524963379 batch PCKh 0.25\n",
      "Trained batch 2692 batch loss 0.604816079 batch mAP 0.519775391 batch PCKh 0.0625\n",
      "Trained batch 2693 batch loss 0.446389735 batch mAP 0.63583374 batch PCKh 0.5625\n",
      "Trained batch 2694 batch loss 0.567475796 batch mAP 0.554321289 batch PCKh 0.75\n",
      "Trained batch 2695 batch loss 0.561397374 batch mAP 0.562194824 batch PCKh 0.75\n",
      "Trained batch 2696 batch loss 0.524558544 batch mAP 0.597839355 batch PCKh 0.625\n",
      "Trained batch 2697 batch loss 0.458418727 batch mAP 0.632080078 batch PCKh 0.5625\n",
      "Trained batch 2698 batch loss 0.51205492 batch mAP 0.564331055 batch PCKh 0.6875\n",
      "Trained batch 2699 batch loss 0.497385681 batch mAP 0.566162109 batch PCKh 0.6875\n",
      "Trained batch 2700 batch loss 0.453562289 batch mAP 0.549133301 batch PCKh 0.375\n",
      "Trained batch 2701 batch loss 0.512087941 batch mAP 0.543609619 batch PCKh 0.625\n",
      "Trained batch 2702 batch loss 0.483493865 batch mAP 0.498077393 batch PCKh 0.3125\n",
      "Trained batch 2703 batch loss 0.412041873 batch mAP 0.57522583 batch PCKh 0.25\n",
      "Trained batch 2704 batch loss 0.540951729 batch mAP 0.594482422 batch PCKh 0.875\n",
      "Trained batch 2705 batch loss 0.569788754 batch mAP 0.546600342 batch PCKh 0.5\n",
      "Trained batch 2706 batch loss 0.531533718 batch mAP 0.569732666 batch PCKh 0.75\n",
      "Trained batch 2707 batch loss 0.517881632 batch mAP 0.676391602 batch PCKh 0.4375\n",
      "Trained batch 2708 batch loss 0.52806747 batch mAP 0.673339844 batch PCKh 0.5625\n",
      "Trained batch 2709 batch loss 0.506996036 batch mAP 0.640625 batch PCKh 0.3125\n",
      "Trained batch 2710 batch loss 0.538859 batch mAP 0.697631836 batch PCKh 0.6875\n",
      "Trained batch 2711 batch loss 0.466334701 batch mAP 0.614563 batch PCKh 0.1875\n",
      "Trained batch 2712 batch loss 0.513067365 batch mAP 0.686889648 batch PCKh 0.5\n",
      "Trained batch 2713 batch loss 0.517380834 batch mAP 0.653930664 batch PCKh 0.5625\n",
      "Trained batch 2714 batch loss 0.476413757 batch mAP 0.664001465 batch PCKh 0.5\n",
      "Trained batch 2715 batch loss 0.514338076 batch mAP 0.709350586 batch PCKh 0.75\n",
      "Trained batch 2716 batch loss 0.58380872 batch mAP 0.710571289 batch PCKh 0.8125\n",
      "Trained batch 2717 batch loss 0.518999875 batch mAP 0.606964111 batch PCKh 0.25\n",
      "Trained batch 2718 batch loss 0.484850794 batch mAP 0.623565674 batch PCKh 0.75\n",
      "Trained batch 2719 batch loss 0.512990892 batch mAP 0.595092773 batch PCKh 0.75\n",
      "Trained batch 2720 batch loss 0.418292791 batch mAP 0.609039307 batch PCKh 0.75\n",
      "Trained batch 2721 batch loss 0.505271912 batch mAP 0.556030273 batch PCKh 0.75\n",
      "Trained batch 2722 batch loss 0.514835715 batch mAP 0.518127441 batch PCKh 0.5625\n",
      "Trained batch 2723 batch loss 0.52083683 batch mAP 0.636383057 batch PCKh 0.4375\n",
      "Trained batch 2724 batch loss 0.48792 batch mAP 0.671295166 batch PCKh 0.8125\n",
      "Trained batch 2725 batch loss 0.508247614 batch mAP 0.655487061 batch PCKh 0.125\n",
      "Trained batch 2726 batch loss 0.481396616 batch mAP 0.695220947 batch PCKh 0.25\n",
      "Trained batch 2727 batch loss 0.501407087 batch mAP 0.663665771 batch PCKh 0.0625\n",
      "Trained batch 2728 batch loss 0.511750042 batch mAP 0.683776855 batch PCKh 0.625\n",
      "Trained batch 2729 batch loss 0.47554183 batch mAP 0.684387207 batch PCKh 0.6875\n",
      "Trained batch 2730 batch loss 0.463561207 batch mAP 0.694274902 batch PCKh 0.375\n",
      "Trained batch 2731 batch loss 0.416298449 batch mAP 0.726654053 batch PCKh 0.375\n",
      "Trained batch 2732 batch loss 0.385733157 batch mAP 0.756866455 batch PCKh 0.375\n",
      "Trained batch 2733 batch loss 0.51302582 batch mAP 0.657745361 batch PCKh 0.25\n",
      "Trained batch 2734 batch loss 0.659522891 batch mAP 0.542144775 batch PCKh 0.4375\n",
      "Trained batch 2735 batch loss 0.594429195 batch mAP 0.564178467 batch PCKh 0.625\n",
      "Trained batch 2736 batch loss 0.615111291 batch mAP 0.608306885 batch PCKh 0.1875\n",
      "Trained batch 2737 batch loss 0.502864778 batch mAP 0.644622803 batch PCKh 0.3125\n",
      "Trained batch 2738 batch loss 0.545769095 batch mAP 0.641876221 batch PCKh 0.3125\n",
      "Trained batch 2739 batch loss 0.532621861 batch mAP 0.603607178 batch PCKh 0.625\n",
      "Trained batch 2740 batch loss 0.53205359 batch mAP 0.580413818 batch PCKh 0.25\n",
      "Trained batch 2741 batch loss 0.39314878 batch mAP 0.575775146 batch PCKh 0.25\n",
      "Trained batch 2742 batch loss 0.509109259 batch mAP 0.579467773 batch PCKh 0.625\n",
      "Trained batch 2743 batch loss 0.547397256 batch mAP 0.581329346 batch PCKh 0.75\n",
      "Trained batch 2744 batch loss 0.580370605 batch mAP 0.638977051 batch PCKh 0.4375\n",
      "Trained batch 2745 batch loss 0.530222297 batch mAP 0.656585693 batch PCKh 0.5625\n",
      "Trained batch 2746 batch loss 0.529293299 batch mAP 0.647827148 batch PCKh 0.75\n",
      "Trained batch 2747 batch loss 0.47926861 batch mAP 0.622528076 batch PCKh 0.5625\n",
      "Trained batch 2748 batch loss 0.479035795 batch mAP 0.636322 batch PCKh 0.75\n",
      "Trained batch 2749 batch loss 0.41700393 batch mAP 0.604156494 batch PCKh 0.5625\n",
      "Trained batch 2750 batch loss 0.38281849 batch mAP 0.608795166 batch PCKh 0.5\n",
      "Trained batch 2751 batch loss 0.433098614 batch mAP 0.598114 batch PCKh 0.75\n",
      "Trained batch 2752 batch loss 0.417669 batch mAP 0.594909668 batch PCKh 0.0625\n",
      "Trained batch 2753 batch loss 0.496422648 batch mAP 0.62979126 batch PCKh 0.6875\n",
      "Trained batch 2754 batch loss 0.484910131 batch mAP 0.625457764 batch PCKh 0.75\n",
      "Trained batch 2755 batch loss 0.535575271 batch mAP 0.644805908 batch PCKh 0.75\n",
      "Trained batch 2756 batch loss 0.49840346 batch mAP 0.616363525 batch PCKh 0.75\n",
      "Trained batch 2757 batch loss 0.445385545 batch mAP 0.610534668 batch PCKh 0.5625\n",
      "Trained batch 2758 batch loss 0.430518329 batch mAP 0.639312744 batch PCKh 0.125\n",
      "Trained batch 2759 batch loss 0.49390918 batch mAP 0.65914917 batch PCKh 0.5625\n",
      "Trained batch 2760 batch loss 0.510512948 batch mAP 0.650604248 batch PCKh 0.6875\n",
      "Trained batch 2761 batch loss 0.485482574 batch mAP 0.608093262 batch PCKh 0.6875\n",
      "Trained batch 2762 batch loss 0.491478801 batch mAP 0.567901611 batch PCKh 0.6875\n",
      "Trained batch 2763 batch loss 0.553696752 batch mAP 0.524383545 batch PCKh 0.5\n",
      "Trained batch 2764 batch loss 0.569843 batch mAP 0.517822266 batch PCKh 0.8125\n",
      "Trained batch 2765 batch loss 0.535403609 batch mAP 0.517150879 batch PCKh 0.375\n",
      "Trained batch 2766 batch loss 0.46393466 batch mAP 0.614044189 batch PCKh 0.875\n",
      "Trained batch 2767 batch loss 0.444829255 batch mAP 0.614227295 batch PCKh 0.625\n",
      "Trained batch 2768 batch loss 0.509303 batch mAP 0.629272461 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2769 batch loss 0.47003755 batch mAP 0.660797119 batch PCKh 0.6875\n",
      "Trained batch 2770 batch loss 0.563941061 batch mAP 0.625427246 batch PCKh 0.5625\n",
      "Trained batch 2771 batch loss 0.631747127 batch mAP 0.576660156 batch PCKh 0.3125\n",
      "Trained batch 2772 batch loss 0.543046 batch mAP 0.613433838 batch PCKh 0.125\n",
      "Trained batch 2773 batch loss 0.560297 batch mAP 0.683319092 batch PCKh 0.3125\n",
      "Trained batch 2774 batch loss 0.530553043 batch mAP 0.643493652 batch PCKh 0.375\n",
      "Trained batch 2775 batch loss 0.542315423 batch mAP 0.585632324 batch PCKh 0.5625\n",
      "Trained batch 2776 batch loss 0.480000138 batch mAP 0.587005615 batch PCKh 0.375\n",
      "Epoch 9 train loss 0.5054996609687805 train mAP 0.6150641441345215 train PCKh\n",
      "Validated batch 1 batch loss 0.488771379 batch mAP 0.623077393 batch PCKh 0.75\n",
      "Validated batch 2 batch loss 0.601066649 batch mAP 0.623352051 batch PCKh 0.375\n",
      "Validated batch 3 batch loss 0.610307038 batch mAP 0.5703125 batch PCKh 0.6875\n",
      "Validated batch 4 batch loss 0.485420167 batch mAP 0.633728 batch PCKh 0.4375\n",
      "Validated batch 5 batch loss 0.590712547 batch mAP 0.511932373 batch PCKh 0.125\n",
      "Validated batch 6 batch loss 0.463181406 batch mAP 0.666137695 batch PCKh 0.3125\n",
      "Validated batch 7 batch loss 0.52565366 batch mAP 0.573425293 batch PCKh 0.1875\n",
      "Validated batch 8 batch loss 0.553306937 batch mAP 0.640258789 batch PCKh 0.875\n",
      "Validated batch 9 batch loss 0.538925648 batch mAP 0.65222168 batch PCKh 0.5625\n",
      "Validated batch 10 batch loss 0.501364172 batch mAP 0.645355225 batch PCKh 0.4375\n",
      "Validated batch 11 batch loss 0.740086079 batch mAP 0.552337646 batch PCKh 0\n",
      "Validated batch 12 batch loss 0.50755918 batch mAP 0.634460449 batch PCKh 0.5625\n",
      "Validated batch 13 batch loss 0.508488417 batch mAP 0.676055908 batch PCKh 0.375\n",
      "Validated batch 14 batch loss 0.609273195 batch mAP 0.539733887 batch PCKh 0.5625\n",
      "Validated batch 15 batch loss 0.396932602 batch mAP 0.644592285 batch PCKh 0.4375\n",
      "Validated batch 16 batch loss 0.473662347 batch mAP 0.683197 batch PCKh 0.4375\n",
      "Validated batch 17 batch loss 0.523792505 batch mAP 0.676452637 batch PCKh 0.8125\n",
      "Validated batch 18 batch loss 0.521923661 batch mAP 0.62512207 batch PCKh 0.4375\n",
      "Validated batch 19 batch loss 0.614258945 batch mAP 0.564178467 batch PCKh 0\n",
      "Validated batch 20 batch loss 0.453298688 batch mAP 0.672546387 batch PCKh 0.5625\n",
      "Validated batch 21 batch loss 0.520431638 batch mAP 0.658416748 batch PCKh 0.4375\n",
      "Validated batch 22 batch loss 0.526854396 batch mAP 0.572601318 batch PCKh 0.375\n",
      "Validated batch 23 batch loss 0.603436768 batch mAP 0.615661621 batch PCKh 0\n",
      "Validated batch 24 batch loss 0.477002382 batch mAP 0.556793213 batch PCKh 0.0625\n",
      "Validated batch 25 batch loss 0.479767144 batch mAP 0.638122559 batch PCKh 0.5\n",
      "Validated batch 26 batch loss 0.604232073 batch mAP 0.54385376 batch PCKh 0.5625\n",
      "Validated batch 27 batch loss 0.518645287 batch mAP 0.676483154 batch PCKh 0.25\n",
      "Validated batch 28 batch loss 0.569919288 batch mAP 0.604492188 batch PCKh 0.875\n",
      "Validated batch 29 batch loss 0.610702217 batch mAP 0.568359375 batch PCKh 0.625\n",
      "Validated batch 30 batch loss 0.613702893 batch mAP 0.547821045 batch PCKh 0.3125\n",
      "Validated batch 31 batch loss 0.599679708 batch mAP 0.586914062 batch PCKh 0.25\n",
      "Validated batch 32 batch loss 0.562503576 batch mAP 0.587493896 batch PCKh 0.6875\n",
      "Validated batch 33 batch loss 0.520610809 batch mAP 0.581115723 batch PCKh 0.875\n",
      "Validated batch 34 batch loss 0.545412421 batch mAP 0.557556152 batch PCKh 0.25\n",
      "Validated batch 35 batch loss 0.66172713 batch mAP 0.534851074 batch PCKh 0\n",
      "Validated batch 36 batch loss 0.560292959 batch mAP 0.651306152 batch PCKh 0.3125\n",
      "Validated batch 37 batch loss 0.437905312 batch mAP 0.699310303 batch PCKh 0.4375\n",
      "Validated batch 38 batch loss 0.49280411 batch mAP 0.590026855 batch PCKh 0.0625\n",
      "Validated batch 39 batch loss 0.429474056 batch mAP 0.635253906 batch PCKh 0.0625\n",
      "Validated batch 40 batch loss 0.63392365 batch mAP 0.497436523 batch PCKh 0.4375\n",
      "Validated batch 41 batch loss 0.500099063 batch mAP 0.64175415 batch PCKh 0.875\n",
      "Validated batch 42 batch loss 0.449020267 batch mAP 0.68637085 batch PCKh 0.5625\n",
      "Validated batch 43 batch loss 0.554489076 batch mAP 0.613922119 batch PCKh 0.4375\n",
      "Validated batch 44 batch loss 0.587881505 batch mAP 0.547332764 batch PCKh 0.625\n",
      "Validated batch 45 batch loss 0.663429081 batch mAP 0.553070068 batch PCKh 0.125\n",
      "Validated batch 46 batch loss 0.614791393 batch mAP 0.534515381 batch PCKh 0.5\n",
      "Validated batch 47 batch loss 0.567689538 batch mAP 0.593811035 batch PCKh 0.8125\n",
      "Validated batch 48 batch loss 0.566322207 batch mAP 0.556854248 batch PCKh 0.3125\n",
      "Validated batch 49 batch loss 0.55149734 batch mAP 0.632232666 batch PCKh 0.6875\n",
      "Validated batch 50 batch loss 0.60528177 batch mAP 0.562011719 batch PCKh 0.8125\n",
      "Validated batch 51 batch loss 0.522141635 batch mAP 0.634155273 batch PCKh 0.75\n",
      "Validated batch 52 batch loss 0.612754464 batch mAP 0.544189453 batch PCKh 0.5625\n",
      "Validated batch 53 batch loss 0.499536067 batch mAP 0.643798828 batch PCKh 0.625\n",
      "Validated batch 54 batch loss 0.613824904 batch mAP 0.565063477 batch PCKh 0.625\n",
      "Validated batch 55 batch loss 0.614349186 batch mAP 0.54800415 batch PCKh 0.25\n",
      "Validated batch 56 batch loss 0.653414249 batch mAP 0.528076172 batch PCKh 0.6875\n",
      "Validated batch 57 batch loss 0.737753391 batch mAP 0.538085938 batch PCKh 0.3125\n",
      "Validated batch 58 batch loss 0.578292072 batch mAP 0.562957764 batch PCKh 0.3125\n",
      "Validated batch 59 batch loss 0.483080387 batch mAP 0.672729492 batch PCKh 0.4375\n",
      "Validated batch 60 batch loss 0.623181224 batch mAP 0.657318115 batch PCKh 0.3125\n",
      "Validated batch 61 batch loss 0.57303 batch mAP 0.56817627 batch PCKh 0.8125\n",
      "Validated batch 62 batch loss 0.564397097 batch mAP 0.604248047 batch PCKh 0.75\n",
      "Validated batch 63 batch loss 0.511963964 batch mAP 0.646606445 batch PCKh 0.25\n",
      "Validated batch 64 batch loss 0.581408501 batch mAP 0.618408203 batch PCKh 0.625\n",
      "Validated batch 65 batch loss 0.590595 batch mAP 0.571563721 batch PCKh 0.1875\n",
      "Validated batch 66 batch loss 0.584476233 batch mAP 0.602783203 batch PCKh 0.875\n",
      "Validated batch 67 batch loss 0.557745218 batch mAP 0.658508301 batch PCKh 0.6875\n",
      "Validated batch 68 batch loss 0.59370935 batch mAP 0.591003418 batch PCKh 0.875\n",
      "Validated batch 69 batch loss 0.51858449 batch mAP 0.615081787 batch PCKh 0.75\n",
      "Validated batch 70 batch loss 0.547116041 batch mAP 0.63961792 batch PCKh 0.625\n",
      "Validated batch 71 batch loss 0.621281743 batch mAP 0.607269287 batch PCKh 0.75\n",
      "Validated batch 72 batch loss 0.716443598 batch mAP 0.626983643 batch PCKh 0.1875\n",
      "Validated batch 73 batch loss 0.624896049 batch mAP 0.652862549 batch PCKh 0.5\n",
      "Validated batch 74 batch loss 0.45812571 batch mAP 0.644592285 batch PCKh 0.5625\n",
      "Validated batch 75 batch loss 0.599271 batch mAP 0.572570801 batch PCKh 0.4375\n",
      "Validated batch 76 batch loss 0.617473602 batch mAP 0.5909729 batch PCKh 0.0625\n",
      "Validated batch 77 batch loss 0.586848795 batch mAP 0.702819824 batch PCKh 0.25\n",
      "Validated batch 78 batch loss 0.561567664 batch mAP 0.663879395 batch PCKh 0.75\n",
      "Validated batch 79 batch loss 0.584569 batch mAP 0.569397 batch PCKh 0.5\n",
      "Validated batch 80 batch loss 0.650384545 batch mAP 0.521514893 batch PCKh 0.75\n",
      "Validated batch 81 batch loss 0.538695157 batch mAP 0.624450684 batch PCKh 0.75\n",
      "Validated batch 82 batch loss 0.446909308 batch mAP 0.647277832 batch PCKh 0.5\n",
      "Validated batch 83 batch loss 0.606461763 batch mAP 0.522796631 batch PCKh 0.75\n",
      "Validated batch 84 batch loss 0.538735628 batch mAP 0.546905518 batch PCKh 0.75\n",
      "Validated batch 85 batch loss 0.582120121 batch mAP 0.666259766 batch PCKh 0.3125\n",
      "Validated batch 86 batch loss 0.512173593 batch mAP 0.65737915 batch PCKh 0.5625\n",
      "Validated batch 87 batch loss 0.595686734 batch mAP 0.616912842 batch PCKh 0.6875\n",
      "Validated batch 88 batch loss 0.575507879 batch mAP 0.594482422 batch PCKh 0.5625\n",
      "Validated batch 89 batch loss 0.503632784 batch mAP 0.634368896 batch PCKh 0.625\n",
      "Validated batch 90 batch loss 0.591098785 batch mAP 0.613952637 batch PCKh 0.3125\n",
      "Validated batch 91 batch loss 0.518855751 batch mAP 0.616943359 batch PCKh 0.5\n",
      "Validated batch 92 batch loss 0.567227364 batch mAP 0.559997559 batch PCKh 0.625\n",
      "Validated batch 93 batch loss 0.634461343 batch mAP 0.583099365 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 94 batch loss 0.519035816 batch mAP 0.533752441 batch PCKh 0.125\n",
      "Validated batch 95 batch loss 0.530295491 batch mAP 0.619537354 batch PCKh 0.375\n",
      "Validated batch 96 batch loss 0.52071631 batch mAP 0.570739746 batch PCKh 0.1875\n",
      "Validated batch 97 batch loss 0.502268136 batch mAP 0.635040283 batch PCKh 0.75\n",
      "Validated batch 98 batch loss 0.604429 batch mAP 0.543548584 batch PCKh 0.5625\n",
      "Validated batch 99 batch loss 0.504108906 batch mAP 0.701293945 batch PCKh 0.375\n",
      "Validated batch 100 batch loss 0.488573194 batch mAP 0.672668457 batch PCKh 0.6875\n",
      "Validated batch 101 batch loss 0.575752616 batch mAP 0.576568604 batch PCKh 0.75\n",
      "Validated batch 102 batch loss 0.53037256 batch mAP 0.63381958 batch PCKh 0.8125\n",
      "Validated batch 103 batch loss 0.54092133 batch mAP 0.625762939 batch PCKh 0.1875\n",
      "Validated batch 104 batch loss 0.610882521 batch mAP 0.510620117 batch PCKh 0.6875\n",
      "Validated batch 105 batch loss 0.541640162 batch mAP 0.623291 batch PCKh 0.875\n",
      "Validated batch 106 batch loss 0.620510638 batch mAP 0.531860352 batch PCKh 0.3125\n",
      "Validated batch 107 batch loss 0.543412089 batch mAP 0.597625732 batch PCKh 0.375\n",
      "Validated batch 108 batch loss 0.641827226 batch mAP 0.521759033 batch PCKh 0.5625\n",
      "Validated batch 109 batch loss 0.563429594 batch mAP 0.521820068 batch PCKh 0.75\n",
      "Validated batch 110 batch loss 0.58718133 batch mAP 0.650543213 batch PCKh 0.375\n",
      "Validated batch 111 batch loss 0.590099692 batch mAP 0.559417725 batch PCKh 0.3125\n",
      "Validated batch 112 batch loss 0.504688442 batch mAP 0.553619385 batch PCKh 0.75\n",
      "Validated batch 113 batch loss 0.614924669 batch mAP 0.593902588 batch PCKh 0.4375\n",
      "Validated batch 114 batch loss 0.587477326 batch mAP 0.598846436 batch PCKh 0.75\n",
      "Validated batch 115 batch loss 0.642779768 batch mAP 0.560119629 batch PCKh 0.6875\n",
      "Validated batch 116 batch loss 0.624251127 batch mAP 0.55859375 batch PCKh 0.5625\n",
      "Validated batch 117 batch loss 0.63014555 batch mAP 0.56362915 batch PCKh 0.875\n",
      "Validated batch 118 batch loss 0.603713095 batch mAP 0.541137695 batch PCKh 0.4375\n",
      "Validated batch 119 batch loss 0.630887687 batch mAP 0.636566162 batch PCKh 0.5\n",
      "Validated batch 120 batch loss 0.582391739 batch mAP 0.594726562 batch PCKh 0.625\n",
      "Validated batch 121 batch loss 0.595743775 batch mAP 0.615844727 batch PCKh 0.6875\n",
      "Validated batch 122 batch loss 0.639143825 batch mAP 0.580108643 batch PCKh 0.5625\n",
      "Validated batch 123 batch loss 0.720115 batch mAP 0.575073242 batch PCKh 0.8125\n",
      "Validated batch 124 batch loss 0.575221658 batch mAP 0.600708 batch PCKh 0.625\n",
      "Validated batch 125 batch loss 0.693070769 batch mAP 0.540100098 batch PCKh 0.625\n",
      "Validated batch 126 batch loss 0.586437583 batch mAP 0.6043396 batch PCKh 0.3125\n",
      "Validated batch 127 batch loss 0.567356467 batch mAP 0.653808594 batch PCKh 0.625\n",
      "Validated batch 128 batch loss 0.51274538 batch mAP 0.61151123 batch PCKh 0.1875\n",
      "Validated batch 129 batch loss 0.551757336 batch mAP 0.630279541 batch PCKh 0.625\n",
      "Validated batch 130 batch loss 0.610880375 batch mAP 0.625335693 batch PCKh 0.4375\n",
      "Validated batch 131 batch loss 0.664589405 batch mAP 0.556976318 batch PCKh 0.1875\n",
      "Validated batch 132 batch loss 0.579772353 batch mAP 0.619476318 batch PCKh 0.4375\n",
      "Validated batch 133 batch loss 0.59735018 batch mAP 0.61517334 batch PCKh 0.75\n",
      "Validated batch 134 batch loss 0.549440622 batch mAP 0.610778809 batch PCKh 0.625\n",
      "Validated batch 135 batch loss 0.524137199 batch mAP 0.641845703 batch PCKh 0.4375\n",
      "Validated batch 136 batch loss 0.614038944 batch mAP 0.583129883 batch PCKh 0.1875\n",
      "Validated batch 137 batch loss 0.585254848 batch mAP 0.548492432 batch PCKh 0.5625\n",
      "Validated batch 138 batch loss 0.548957348 batch mAP 0.586181641 batch PCKh 0.5\n",
      "Validated batch 139 batch loss 0.60928905 batch mAP 0.552154541 batch PCKh 0.4375\n",
      "Validated batch 140 batch loss 0.640857518 batch mAP 0.550201416 batch PCKh 0.5\n",
      "Validated batch 141 batch loss 0.547585785 batch mAP 0.588989258 batch PCKh 0.75\n",
      "Validated batch 142 batch loss 0.56542635 batch mAP 0.670501709 batch PCKh 0.5\n",
      "Validated batch 143 batch loss 0.584214389 batch mAP 0.589294434 batch PCKh 0.625\n",
      "Validated batch 144 batch loss 0.452067465 batch mAP 0.709991455 batch PCKh 0.5625\n",
      "Validated batch 145 batch loss 0.515437961 batch mAP 0.687805176 batch PCKh 0.625\n",
      "Validated batch 146 batch loss 0.624676108 batch mAP 0.656799316 batch PCKh 0.8125\n",
      "Validated batch 147 batch loss 0.502067506 batch mAP 0.630126953 batch PCKh 0.6875\n",
      "Validated batch 148 batch loss 0.54934144 batch mAP 0.689086914 batch PCKh 0.4375\n",
      "Validated batch 149 batch loss 0.610871673 batch mAP 0.567230225 batch PCKh 0.4375\n",
      "Validated batch 150 batch loss 0.603641629 batch mAP 0.674591064 batch PCKh 0.8125\n",
      "Validated batch 151 batch loss 0.567314506 batch mAP 0.678649902 batch PCKh 0.6875\n",
      "Validated batch 152 batch loss 0.59372282 batch mAP 0.595428467 batch PCKh 0.25\n",
      "Validated batch 153 batch loss 0.579259634 batch mAP 0.593933105 batch PCKh 0.5\n",
      "Validated batch 154 batch loss 0.48734346 batch mAP 0.635650635 batch PCKh 0.5\n",
      "Validated batch 155 batch loss 0.551504135 batch mAP 0.646087646 batch PCKh 0.5625\n",
      "Validated batch 156 batch loss 0.584604442 batch mAP 0.662506104 batch PCKh 0.375\n",
      "Validated batch 157 batch loss 0.600732 batch mAP 0.629394531 batch PCKh 0.75\n",
      "Validated batch 158 batch loss 0.566317201 batch mAP 0.635894775 batch PCKh 0.1875\n",
      "Validated batch 159 batch loss 0.619421422 batch mAP 0.65145874 batch PCKh 0.5\n",
      "Validated batch 160 batch loss 0.598954678 batch mAP 0.557281494 batch PCKh 0.1875\n",
      "Validated batch 161 batch loss 0.652087569 batch mAP 0.542785645 batch PCKh 0.875\n",
      "Validated batch 162 batch loss 0.55225116 batch mAP 0.579437256 batch PCKh 0.375\n",
      "Validated batch 163 batch loss 0.595937252 batch mAP 0.566589355 batch PCKh 0.6875\n",
      "Validated batch 164 batch loss 0.543733776 batch mAP 0.557037354 batch PCKh 0.3125\n",
      "Validated batch 165 batch loss 0.514362574 batch mAP 0.59979248 batch PCKh 0.25\n",
      "Validated batch 166 batch loss 0.531439364 batch mAP 0.547546387 batch PCKh 0.625\n",
      "Validated batch 167 batch loss 0.598785162 batch mAP 0.641876221 batch PCKh 0.625\n",
      "Validated batch 168 batch loss 0.492869258 batch mAP 0.646820068 batch PCKh 0.6875\n",
      "Validated batch 169 batch loss 0.634101033 batch mAP 0.537017822 batch PCKh 0\n",
      "Validated batch 170 batch loss 0.559565961 batch mAP 0.605377197 batch PCKh 0.6875\n",
      "Validated batch 171 batch loss 0.56178081 batch mAP 0.634155273 batch PCKh 0.25\n",
      "Validated batch 172 batch loss 0.562874615 batch mAP 0.617523193 batch PCKh 0.4375\n",
      "Validated batch 173 batch loss 0.582259655 batch mAP 0.483917236 batch PCKh 0.25\n",
      "Validated batch 174 batch loss 0.565187573 batch mAP 0.642669678 batch PCKh 0.5\n",
      "Validated batch 175 batch loss 0.598278642 batch mAP 0.635437 batch PCKh 0.5625\n",
      "Validated batch 176 batch loss 0.566998065 batch mAP 0.63192749 batch PCKh 0.5625\n",
      "Validated batch 177 batch loss 0.635645866 batch mAP 0.597717285 batch PCKh 0.1875\n",
      "Validated batch 178 batch loss 0.72950089 batch mAP 0.517456055 batch PCKh 0.25\n",
      "Validated batch 179 batch loss 0.67495966 batch mAP 0.555908203 batch PCKh 0.625\n",
      "Validated batch 180 batch loss 0.545689762 batch mAP 0.694976807 batch PCKh 0.75\n",
      "Validated batch 181 batch loss 0.566101134 batch mAP 0.643676758 batch PCKh 0.625\n",
      "Validated batch 182 batch loss 0.605586171 batch mAP 0.52230835 batch PCKh 0.6875\n",
      "Validated batch 183 batch loss 0.498710841 batch mAP 0.573852539 batch PCKh 0.4375\n",
      "Validated batch 184 batch loss 0.557262897 batch mAP 0.655731201 batch PCKh 0.3125\n",
      "Validated batch 185 batch loss 0.463918447 batch mAP 0.690979 batch PCKh 0.4375\n",
      "Validated batch 186 batch loss 0.568312645 batch mAP 0.638916 batch PCKh 0.875\n",
      "Validated batch 187 batch loss 0.601724148 batch mAP 0.565002441 batch PCKh 0.625\n",
      "Validated batch 188 batch loss 0.576998472 batch mAP 0.617370605 batch PCKh 0.75\n",
      "Validated batch 189 batch loss 0.536756814 batch mAP 0.608001709 batch PCKh 0.0625\n",
      "Validated batch 190 batch loss 0.592524767 batch mAP 0.611999512 batch PCKh 0.5625\n",
      "Validated batch 191 batch loss 0.493090957 batch mAP 0.633544922 batch PCKh 0.4375\n",
      "Validated batch 192 batch loss 0.558376789 batch mAP 0.561553955 batch PCKh 0.625\n",
      "Validated batch 193 batch loss 0.584245801 batch mAP 0.614898682 batch PCKh 0.4375\n",
      "Validated batch 194 batch loss 0.556842804 batch mAP 0.686035156 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 195 batch loss 0.569175243 batch mAP 0.608978271 batch PCKh 0.75\n",
      "Validated batch 196 batch loss 0.638399 batch mAP 0.717559814 batch PCKh 0.9375\n",
      "Validated batch 197 batch loss 0.62255 batch mAP 0.590209961 batch PCKh 0.625\n",
      "Validated batch 198 batch loss 0.622028112 batch mAP 0.574646 batch PCKh 0.125\n",
      "Validated batch 199 batch loss 0.602348745 batch mAP 0.604705811 batch PCKh 0.5\n",
      "Validated batch 200 batch loss 0.620712042 batch mAP 0.521331787 batch PCKh 0.5\n",
      "Validated batch 201 batch loss 0.555705369 batch mAP 0.573974609 batch PCKh 0.4375\n",
      "Validated batch 202 batch loss 0.62764585 batch mAP 0.603485107 batch PCKh 0\n",
      "Validated batch 203 batch loss 0.541466236 batch mAP 0.589233398 batch PCKh 0.6875\n",
      "Validated batch 204 batch loss 0.517736793 batch mAP 0.701477051 batch PCKh 0.5625\n",
      "Validated batch 205 batch loss 0.577066302 batch mAP 0.672699 batch PCKh 0.5625\n",
      "Validated batch 206 batch loss 0.564666569 batch mAP 0.62109375 batch PCKh 0.3125\n",
      "Validated batch 207 batch loss 0.608847737 batch mAP 0.60647583 batch PCKh 0.1875\n",
      "Validated batch 208 batch loss 0.545181513 batch mAP 0.607635498 batch PCKh 0.8125\n",
      "Validated batch 209 batch loss 0.539889157 batch mAP 0.491912842 batch PCKh 0.25\n",
      "Validated batch 210 batch loss 0.617423058 batch mAP 0.544616699 batch PCKh 0.375\n",
      "Validated batch 211 batch loss 0.6396541 batch mAP 0.585113525 batch PCKh 0.625\n",
      "Validated batch 212 batch loss 0.604022145 batch mAP 0.537506104 batch PCKh 0.0625\n",
      "Validated batch 213 batch loss 0.671180606 batch mAP 0.520324707 batch PCKh 0\n",
      "Validated batch 214 batch loss 0.626406431 batch mAP 0.564086914 batch PCKh 0.1875\n",
      "Validated batch 215 batch loss 0.58074218 batch mAP 0.604034424 batch PCKh 0.625\n",
      "Validated batch 216 batch loss 0.623642504 batch mAP 0.600189209 batch PCKh 0.3125\n",
      "Validated batch 217 batch loss 0.558659911 batch mAP 0.672332764 batch PCKh 0.6875\n",
      "Validated batch 218 batch loss 0.732031226 batch mAP 0.431945801 batch PCKh 0\n",
      "Validated batch 219 batch loss 0.508326888 batch mAP 0.527313232 batch PCKh 0.5625\n",
      "Validated batch 220 batch loss 0.514955044 batch mAP 0.619476318 batch PCKh 0.1875\n",
      "Validated batch 221 batch loss 0.576719463 batch mAP 0.582275391 batch PCKh 0.1875\n",
      "Validated batch 222 batch loss 0.55025816 batch mAP 0.573638916 batch PCKh 0.4375\n",
      "Validated batch 223 batch loss 0.570985198 batch mAP 0.628051758 batch PCKh 0.625\n",
      "Validated batch 224 batch loss 0.469992578 batch mAP 0.66519165 batch PCKh 0.5625\n",
      "Validated batch 225 batch loss 0.474410772 batch mAP 0.655273438 batch PCKh 0.6875\n",
      "Validated batch 226 batch loss 0.569369853 batch mAP 0.559295654 batch PCKh 0.625\n",
      "Validated batch 227 batch loss 0.620610833 batch mAP 0.614685059 batch PCKh 0.1875\n",
      "Validated batch 228 batch loss 0.605766058 batch mAP 0.636169434 batch PCKh 0.75\n",
      "Validated batch 229 batch loss 0.521231174 batch mAP 0.592407227 batch PCKh 0.625\n",
      "Validated batch 230 batch loss 0.524642 batch mAP 0.636230469 batch PCKh 0.75\n",
      "Validated batch 231 batch loss 0.598775089 batch mAP 0.560241699 batch PCKh 0.5\n",
      "Validated batch 232 batch loss 0.57458818 batch mAP 0.585479736 batch PCKh 0.1875\n",
      "Validated batch 233 batch loss 0.618164778 batch mAP 0.627716064 batch PCKh 0.5625\n",
      "Validated batch 234 batch loss 0.570605695 batch mAP 0.63470459 batch PCKh 0.25\n",
      "Validated batch 235 batch loss 0.462499917 batch mAP 0.673950195 batch PCKh 0.625\n",
      "Validated batch 236 batch loss 0.647963822 batch mAP 0.620117188 batch PCKh 0.3125\n",
      "Validated batch 237 batch loss 0.573434949 batch mAP 0.546020508 batch PCKh 0.3125\n",
      "Validated batch 238 batch loss 0.540048122 batch mAP 0.547851562 batch PCKh 0.5625\n",
      "Validated batch 239 batch loss 0.538808942 batch mAP 0.554260254 batch PCKh 0.4375\n",
      "Validated batch 240 batch loss 0.593778312 batch mAP 0.633056641 batch PCKh 0.375\n",
      "Validated batch 241 batch loss 0.534753263 batch mAP 0.58026123 batch PCKh 0.5625\n",
      "Validated batch 242 batch loss 0.606943309 batch mAP 0.628875732 batch PCKh 0.875\n",
      "Validated batch 243 batch loss 0.556968927 batch mAP 0.631835938 batch PCKh 0.4375\n",
      "Validated batch 244 batch loss 0.616547465 batch mAP 0.597564697 batch PCKh 0.375\n",
      "Validated batch 245 batch loss 0.491203606 batch mAP 0.627349854 batch PCKh 0.375\n",
      "Validated batch 246 batch loss 0.59784615 batch mAP 0.525238037 batch PCKh 0.625\n",
      "Validated batch 247 batch loss 0.450592 batch mAP 0.640899658 batch PCKh 0.4375\n",
      "Validated batch 248 batch loss 0.625501633 batch mAP 0.601806641 batch PCKh 0.4375\n",
      "Validated batch 249 batch loss 0.726189494 batch mAP 0.576538086 batch PCKh 0.1875\n",
      "Validated batch 250 batch loss 0.55572319 batch mAP 0.65826416 batch PCKh 0.3125\n",
      "Validated batch 251 batch loss 0.599255085 batch mAP 0.651947 batch PCKh 0.4375\n",
      "Validated batch 252 batch loss 0.465046585 batch mAP 0.642669678 batch PCKh 0.1875\n",
      "Validated batch 253 batch loss 0.507313371 batch mAP 0.655822754 batch PCKh 0.75\n",
      "Validated batch 254 batch loss 0.505121768 batch mAP 0.672668457 batch PCKh 0.625\n",
      "Validated batch 255 batch loss 0.540777087 batch mAP 0.469299316 batch PCKh 0.375\n",
      "Validated batch 256 batch loss 0.539227486 batch mAP 0.606903076 batch PCKh 0.3125\n",
      "Validated batch 257 batch loss 0.643599272 batch mAP 0.618225098 batch PCKh 0.8125\n",
      "Validated batch 258 batch loss 0.636887 batch mAP 0.544555664 batch PCKh 0.125\n",
      "Validated batch 259 batch loss 0.529173434 batch mAP 0.493225098 batch PCKh 0.375\n",
      "Validated batch 260 batch loss 0.70623672 batch mAP 0.462280273 batch PCKh 0.3125\n",
      "Validated batch 261 batch loss 0.629162431 batch mAP 0.484313965 batch PCKh 0.625\n",
      "Validated batch 262 batch loss 0.635210872 batch mAP 0.497009277 batch PCKh 0.5625\n",
      "Validated batch 263 batch loss 0.587299466 batch mAP 0.620269775 batch PCKh 0.75\n",
      "Validated batch 264 batch loss 0.570447326 batch mAP 0.610229492 batch PCKh 0.5\n",
      "Validated batch 265 batch loss 0.679068685 batch mAP 0.601837158 batch PCKh 0.125\n",
      "Validated batch 266 batch loss 0.629460394 batch mAP 0.559143066 batch PCKh 0.5\n",
      "Validated batch 267 batch loss 0.58929497 batch mAP 0.562286377 batch PCKh 0\n",
      "Validated batch 268 batch loss 0.58833009 batch mAP 0.684265137 batch PCKh 0.875\n",
      "Validated batch 269 batch loss 0.704860091 batch mAP 0.570861816 batch PCKh 0.0625\n",
      "Validated batch 270 batch loss 0.499720573 batch mAP 0.604705811 batch PCKh 0.5625\n",
      "Validated batch 271 batch loss 0.56132 batch mAP 0.559967041 batch PCKh 0.6875\n",
      "Validated batch 272 batch loss 0.588910341 batch mAP 0.553588867 batch PCKh 0.8125\n",
      "Validated batch 273 batch loss 0.431470692 batch mAP 0.663085938 batch PCKh 0.625\n",
      "Validated batch 274 batch loss 0.42063576 batch mAP 0.617736816 batch PCKh 0.375\n",
      "Validated batch 275 batch loss 0.525197208 batch mAP 0.642700195 batch PCKh 0.5\n",
      "Validated batch 276 batch loss 0.526530087 batch mAP 0.578582764 batch PCKh 0.4375\n",
      "Validated batch 277 batch loss 0.574807644 batch mAP 0.638916 batch PCKh 0.75\n",
      "Validated batch 278 batch loss 0.505457878 batch mAP 0.594848633 batch PCKh 0.5\n",
      "Validated batch 279 batch loss 0.540698051 batch mAP 0.594848633 batch PCKh 0.75\n",
      "Validated batch 280 batch loss 0.519863248 batch mAP 0.634490967 batch PCKh 0.5\n",
      "Validated batch 281 batch loss 0.595076323 batch mAP 0.593170166 batch PCKh 0.75\n",
      "Validated batch 282 batch loss 0.579828143 batch mAP 0.542205811 batch PCKh 0.6875\n",
      "Validated batch 283 batch loss 0.538839638 batch mAP 0.634460449 batch PCKh 0.75\n",
      "Validated batch 284 batch loss 0.499333858 batch mAP 0.651275635 batch PCKh 0.4375\n",
      "Validated batch 285 batch loss 0.585959196 batch mAP 0.655365 batch PCKh 0.75\n",
      "Validated batch 286 batch loss 0.605111718 batch mAP 0.607391357 batch PCKh 0.625\n",
      "Validated batch 287 batch loss 0.681738317 batch mAP 0.531768799 batch PCKh 0.25\n",
      "Validated batch 288 batch loss 0.689216673 batch mAP 0.57208252 batch PCKh 0.75\n",
      "Validated batch 289 batch loss 0.561662555 batch mAP 0.628814697 batch PCKh 0.75\n",
      "Validated batch 290 batch loss 0.47111544 batch mAP 0.634063721 batch PCKh 0.875\n",
      "Validated batch 291 batch loss 0.568712831 batch mAP 0.668273926 batch PCKh 0.1875\n",
      "Validated batch 292 batch loss 0.585003614 batch mAP 0.669677734 batch PCKh 0.5625\n",
      "Validated batch 293 batch loss 0.576823592 batch mAP 0.593994141 batch PCKh 0.3125\n",
      "Validated batch 294 batch loss 0.500235796 batch mAP 0.574279785 batch PCKh 0.1875\n",
      "Validated batch 295 batch loss 0.58439672 batch mAP 0.558105469 batch PCKh 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 296 batch loss 0.593943536 batch mAP 0.579467773 batch PCKh 0.75\n",
      "Validated batch 297 batch loss 0.602247 batch mAP 0.60256958 batch PCKh 0.25\n",
      "Validated batch 298 batch loss 0.588641405 batch mAP 0.641601562 batch PCKh 0.5625\n",
      "Validated batch 299 batch loss 0.520092726 batch mAP 0.647827148 batch PCKh 0.8125\n",
      "Validated batch 300 batch loss 0.552931786 batch mAP 0.535308838 batch PCKh 0.75\n",
      "Validated batch 301 batch loss 0.393225908 batch mAP 0.621063232 batch PCKh 0.5625\n",
      "Validated batch 302 batch loss 0.484263361 batch mAP 0.667114258 batch PCKh 0.5625\n",
      "Validated batch 303 batch loss 0.583340168 batch mAP 0.627288818 batch PCKh 0.875\n",
      "Validated batch 304 batch loss 0.565671504 batch mAP 0.659240723 batch PCKh 0.625\n",
      "Validated batch 305 batch loss 0.524825811 batch mAP 0.668396 batch PCKh 0.625\n",
      "Validated batch 306 batch loss 0.496467143 batch mAP 0.681610107 batch PCKh 0.375\n",
      "Validated batch 307 batch loss 0.563157618 batch mAP 0.626525879 batch PCKh 0.75\n",
      "Validated batch 308 batch loss 0.577322245 batch mAP 0.545532227 batch PCKh 0.5625\n",
      "Validated batch 309 batch loss 0.646788239 batch mAP 0.543487549 batch PCKh 0.125\n",
      "Validated batch 310 batch loss 0.575581789 batch mAP 0.650970459 batch PCKh 0.75\n",
      "Validated batch 311 batch loss 0.505751073 batch mAP 0.703674316 batch PCKh 0.75\n",
      "Validated batch 312 batch loss 0.485711783 batch mAP 0.666626 batch PCKh 0.5625\n",
      "Validated batch 313 batch loss 0.639427841 batch mAP 0.617034912 batch PCKh 0.125\n",
      "Validated batch 314 batch loss 0.56607461 batch mAP 0.642120361 batch PCKh 0.3125\n",
      "Validated batch 315 batch loss 0.605504274 batch mAP 0.577636719 batch PCKh 0.1875\n",
      "Validated batch 316 batch loss 0.673247695 batch mAP 0.586792 batch PCKh 0.125\n",
      "Validated batch 317 batch loss 0.567845225 batch mAP 0.605834961 batch PCKh 0.3125\n",
      "Validated batch 318 batch loss 0.614842892 batch mAP 0.697753906 batch PCKh 0.8125\n",
      "Validated batch 319 batch loss 0.589800596 batch mAP 0.703430176 batch PCKh 0.4375\n",
      "Validated batch 320 batch loss 0.577994168 batch mAP 0.624298096 batch PCKh 0.625\n",
      "Validated batch 321 batch loss 0.526844203 batch mAP 0.572479248 batch PCKh 0.75\n",
      "Validated batch 322 batch loss 0.47689715 batch mAP 0.590026855 batch PCKh 0.1875\n",
      "Validated batch 323 batch loss 0.582739353 batch mAP 0.643554688 batch PCKh 0.6875\n",
      "Validated batch 324 batch loss 0.519573331 batch mAP 0.634765625 batch PCKh 0.375\n",
      "Validated batch 325 batch loss 0.552441537 batch mAP 0.525909424 batch PCKh 0.625\n",
      "Validated batch 326 batch loss 0.50736779 batch mAP 0.634094238 batch PCKh 0.625\n",
      "Validated batch 327 batch loss 0.563837051 batch mAP 0.581451416 batch PCKh 0.25\n",
      "Validated batch 328 batch loss 0.55945915 batch mAP 0.517944336 batch PCKh 0.6875\n",
      "Validated batch 329 batch loss 0.521588445 batch mAP 0.524261475 batch PCKh 0.125\n",
      "Validated batch 330 batch loss 0.550634 batch mAP 0.604675293 batch PCKh 0.5625\n",
      "Validated batch 331 batch loss 0.489915341 batch mAP 0.571807861 batch PCKh 0.8125\n",
      "Validated batch 332 batch loss 0.522354066 batch mAP 0.609161377 batch PCKh 0.75\n",
      "Validated batch 333 batch loss 0.566210091 batch mAP 0.534118652 batch PCKh 0.5625\n",
      "Validated batch 334 batch loss 0.632032752 batch mAP 0.530426 batch PCKh 0.5\n",
      "Validated batch 335 batch loss 0.575011134 batch mAP 0.616333 batch PCKh 0.625\n",
      "Validated batch 336 batch loss 0.508547783 batch mAP 0.598114 batch PCKh 0.5625\n",
      "Validated batch 337 batch loss 0.625276208 batch mAP 0.465484619 batch PCKh 0.125\n",
      "Validated batch 338 batch loss 0.572536826 batch mAP 0.665710449 batch PCKh 0.625\n",
      "Validated batch 339 batch loss 0.626915097 batch mAP 0.52935791 batch PCKh 0.625\n",
      "Validated batch 340 batch loss 0.594384909 batch mAP 0.543518066 batch PCKh 0.625\n",
      "Validated batch 341 batch loss 0.488607883 batch mAP 0.68927 batch PCKh 0.25\n",
      "Validated batch 342 batch loss 0.501191795 batch mAP 0.706085205 batch PCKh 0.375\n",
      "Validated batch 343 batch loss 0.629644632 batch mAP 0.598602295 batch PCKh 0.375\n",
      "Validated batch 344 batch loss 0.56728214 batch mAP 0.553497314 batch PCKh 0.8125\n",
      "Validated batch 345 batch loss 0.565660715 batch mAP 0.632324219 batch PCKh 0.1875\n",
      "Validated batch 346 batch loss 0.606654 batch mAP 0.603851318 batch PCKh 0.25\n",
      "Validated batch 347 batch loss 0.509461164 batch mAP 0.616119385 batch PCKh 0.3125\n",
      "Validated batch 348 batch loss 0.533919156 batch mAP 0.611328125 batch PCKh 0.25\n",
      "Validated batch 349 batch loss 0.636742234 batch mAP 0.581695557 batch PCKh 0.3125\n",
      "Validated batch 350 batch loss 0.505451858 batch mAP 0.613372803 batch PCKh 0.5\n",
      "Validated batch 351 batch loss 0.61168015 batch mAP 0.596008301 batch PCKh 0.5625\n",
      "Validated batch 352 batch loss 0.53834492 batch mAP 0.591308594 batch PCKh 0.3125\n",
      "Validated batch 353 batch loss 0.564892 batch mAP 0.640380859 batch PCKh 0.3125\n",
      "Validated batch 354 batch loss 0.50891459 batch mAP 0.608581543 batch PCKh 0.4375\n",
      "Validated batch 355 batch loss 0.579042435 batch mAP 0.610473633 batch PCKh 0.1875\n",
      "Validated batch 356 batch loss 0.622324467 batch mAP 0.521606445 batch PCKh 0.25\n",
      "Validated batch 357 batch loss 0.605962753 batch mAP 0.58972168 batch PCKh 0.625\n",
      "Validated batch 358 batch loss 0.540212631 batch mAP 0.5859375 batch PCKh 0.1875\n",
      "Validated batch 359 batch loss 0.635146618 batch mAP 0.535003662 batch PCKh 0.3125\n",
      "Validated batch 360 batch loss 0.548649669 batch mAP 0.652587891 batch PCKh 0.75\n",
      "Validated batch 361 batch loss 0.444218516 batch mAP 0.680389404 batch PCKh 0.5625\n",
      "Validated batch 362 batch loss 0.564854443 batch mAP 0.456787109 batch PCKh 0.5625\n",
      "Validated batch 363 batch loss 0.645271778 batch mAP 0.495117188 batch PCKh 0.375\n",
      "Validated batch 364 batch loss 0.570916235 batch mAP 0.605163574 batch PCKh 0.4375\n",
      "Validated batch 365 batch loss 0.553237855 batch mAP 0.591796875 batch PCKh 0.3125\n",
      "Validated batch 366 batch loss 0.535103 batch mAP 0.494873047 batch PCKh 0.625\n",
      "Validated batch 367 batch loss 0.460995913 batch mAP 0.687225342 batch PCKh 0.75\n",
      "Validated batch 368 batch loss 0.570452809 batch mAP 0.570343 batch PCKh 0.625\n",
      "Validated batch 369 batch loss 0.581116796 batch mAP 0.625976562 batch PCKh 0.5625\n",
      "Epoch 9 val loss 0.5682694911956787 val mAP 0.6018450260162354 val PCKh\n",
      "Epoch 9 completed in 770.72 seconds\n",
      "Model /aiffel/aiffel/model_weight/GD08/y_model-epoch-9-loss-0.5683.h5 saved.\n",
      "Start epoch 10 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 0.361596465 batch mAP 0.710327148 batch PCKh 0.375\n",
      "Trained batch 2 batch loss 0.443398237 batch mAP 0.706359863 batch PCKh 0.3125\n",
      "Trained batch 3 batch loss 0.387930483 batch mAP 0.734985352 batch PCKh 0.3125\n",
      "Trained batch 4 batch loss 0.41538471 batch mAP 0.736541748 batch PCKh 0.4375\n",
      "Trained batch 5 batch loss 0.479653895 batch mAP 0.639434814 batch PCKh 0.4375\n",
      "Trained batch 6 batch loss 0.514979 batch mAP 0.641937256 batch PCKh 0.75\n",
      "Trained batch 7 batch loss 0.404946774 batch mAP 0.625366211 batch PCKh 0.6875\n",
      "Trained batch 8 batch loss 0.413400382 batch mAP 0.681121826 batch PCKh 0.5625\n",
      "Trained batch 9 batch loss 0.467514753 batch mAP 0.600616455 batch PCKh 0.6875\n",
      "Trained batch 10 batch loss 0.51321286 batch mAP 0.622406 batch PCKh 0.625\n",
      "Trained batch 11 batch loss 0.486519068 batch mAP 0.665740967 batch PCKh 0.4375\n",
      "Trained batch 12 batch loss 0.474097 batch mAP 0.67565918 batch PCKh 0.1875\n",
      "Trained batch 13 batch loss 0.525379539 batch mAP 0.580108643 batch PCKh 0.5\n",
      "Trained batch 14 batch loss 0.529380679 batch mAP 0.554840088 batch PCKh 0.875\n",
      "Trained batch 15 batch loss 0.51356554 batch mAP 0.570159912 batch PCKh 0.8125\n",
      "Trained batch 16 batch loss 0.451122284 batch mAP 0.589447 batch PCKh 0.6875\n",
      "Trained batch 17 batch loss 0.579337299 batch mAP 0.518188477 batch PCKh 0.3125\n",
      "Trained batch 18 batch loss 0.459291637 batch mAP 0.614135742 batch PCKh 0.25\n",
      "Trained batch 19 batch loss 0.458412319 batch mAP 0.671447754 batch PCKh 0.5\n",
      "Trained batch 20 batch loss 0.46843183 batch mAP 0.639862061 batch PCKh 0.6875\n",
      "Trained batch 21 batch loss 0.474245429 batch mAP 0.645782471 batch PCKh 0.375\n",
      "Trained batch 22 batch loss 0.512384057 batch mAP 0.618347168 batch PCKh 0.75\n",
      "Trained batch 23 batch loss 0.447749197 batch mAP 0.665496826 batch PCKh 0.5625\n",
      "Trained batch 24 batch loss 0.453731716 batch mAP 0.717956543 batch PCKh 0.5\n",
      "Trained batch 25 batch loss 0.45546031 batch mAP 0.680877686 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 26 batch loss 0.521338284 batch mAP 0.629608154 batch PCKh 0.6875\n",
      "Trained batch 27 batch loss 0.526861131 batch mAP 0.649353 batch PCKh 0.5625\n",
      "Trained batch 28 batch loss 0.471261561 batch mAP 0.633392334 batch PCKh 0.375\n",
      "Trained batch 29 batch loss 0.503877163 batch mAP 0.651184082 batch PCKh 0.5625\n",
      "Trained batch 30 batch loss 0.488896251 batch mAP 0.636901855 batch PCKh 0.4375\n",
      "Trained batch 31 batch loss 0.482921869 batch mAP 0.607696533 batch PCKh 0.25\n",
      "Trained batch 32 batch loss 0.397483647 batch mAP 0.669891357 batch PCKh 0.4375\n",
      "Trained batch 33 batch loss 0.550373614 batch mAP 0.580535889 batch PCKh 0.5625\n",
      "Trained batch 34 batch loss 0.381114423 batch mAP 0.638855 batch PCKh 0.6875\n",
      "Trained batch 35 batch loss 0.508742 batch mAP 0.640045166 batch PCKh 0.3125\n",
      "Trained batch 36 batch loss 0.529782772 batch mAP 0.613250732 batch PCKh 0.4375\n",
      "Trained batch 37 batch loss 0.577129126 batch mAP 0.615386963 batch PCKh 0.5\n",
      "Trained batch 38 batch loss 0.538631797 batch mAP 0.546051 batch PCKh 0.6875\n",
      "Trained batch 39 batch loss 0.553005 batch mAP 0.569702148 batch PCKh 0.75\n",
      "Trained batch 40 batch loss 0.458584487 batch mAP 0.678466797 batch PCKh 0.4375\n",
      "Trained batch 41 batch loss 0.428214908 batch mAP 0.726043701 batch PCKh 0.625\n",
      "Trained batch 42 batch loss 0.456746399 batch mAP 0.66192627 batch PCKh 0.875\n",
      "Trained batch 43 batch loss 0.445601881 batch mAP 0.693573 batch PCKh 0.5625\n",
      "Trained batch 44 batch loss 0.436820865 batch mAP 0.697418213 batch PCKh 0.5625\n",
      "Trained batch 45 batch loss 0.412440687 batch mAP 0.698059082 batch PCKh 0.375\n",
      "Trained batch 46 batch loss 0.537256718 batch mAP 0.660064697 batch PCKh 0.5625\n",
      "Trained batch 47 batch loss 0.455464602 batch mAP 0.671722412 batch PCKh 0.5\n",
      "Trained batch 48 batch loss 0.561822474 batch mAP 0.604003906 batch PCKh 0.5\n",
      "Trained batch 49 batch loss 0.499141723 batch mAP 0.601867676 batch PCKh 0.5625\n",
      "Trained batch 50 batch loss 0.517601669 batch mAP 0.64328 batch PCKh 0.875\n",
      "Trained batch 51 batch loss 0.415898144 batch mAP 0.647918701 batch PCKh 0.75\n",
      "Trained batch 52 batch loss 0.451469064 batch mAP 0.732635498 batch PCKh 0.5625\n",
      "Trained batch 53 batch loss 0.710804045 batch mAP 0.59765625 batch PCKh 0\n",
      "Trained batch 54 batch loss 0.643532634 batch mAP 0.618347168 batch PCKh 0.25\n",
      "Trained batch 55 batch loss 0.57230854 batch mAP 0.627532959 batch PCKh 0.5625\n",
      "Trained batch 56 batch loss 0.592797279 batch mAP 0.604797363 batch PCKh 0.6875\n",
      "Trained batch 57 batch loss 0.545896232 batch mAP 0.563171387 batch PCKh 0.5625\n",
      "Trained batch 58 batch loss 0.524539 batch mAP 0.570282 batch PCKh 0.6875\n",
      "Trained batch 59 batch loss 0.519121945 batch mAP 0.589599609 batch PCKh 0.5625\n",
      "Trained batch 60 batch loss 0.412737161 batch mAP 0.649108887 batch PCKh 0.75\n",
      "Trained batch 61 batch loss 0.492708266 batch mAP 0.634674072 batch PCKh 0.625\n",
      "Trained batch 62 batch loss 0.375568897 batch mAP 0.667785645 batch PCKh 0.1875\n",
      "Trained batch 63 batch loss 0.319090933 batch mAP 0.629943848 batch PCKh 0\n",
      "Trained batch 64 batch loss 0.343370259 batch mAP 0.652618408 batch PCKh 0.25\n",
      "Trained batch 65 batch loss 0.458776355 batch mAP 0.625396729 batch PCKh 0.625\n",
      "Trained batch 66 batch loss 0.518615246 batch mAP 0.566436768 batch PCKh 0.75\n",
      "Trained batch 67 batch loss 0.530859113 batch mAP 0.578582764 batch PCKh 0.1875\n",
      "Trained batch 68 batch loss 0.379954278 batch mAP 0.543762207 batch PCKh 0.125\n",
      "Trained batch 69 batch loss 0.273145676 batch mAP 0.609710693 batch PCKh 0.4375\n",
      "Trained batch 70 batch loss 0.300712913 batch mAP 0.655365 batch PCKh 0\n",
      "Trained batch 71 batch loss 0.314477116 batch mAP 0.628051758 batch PCKh 0\n",
      "Trained batch 72 batch loss 0.338882118 batch mAP 0.637695312 batch PCKh 0.375\n",
      "Trained batch 73 batch loss 0.334466159 batch mAP 0.622467041 batch PCKh 0\n",
      "Trained batch 74 batch loss 0.389553 batch mAP 0.586578369 batch PCKh 0.75\n",
      "Trained batch 75 batch loss 0.437444329 batch mAP 0.597106934 batch PCKh 0.625\n",
      "Trained batch 76 batch loss 0.371172547 batch mAP 0.618499756 batch PCKh 0.625\n",
      "Trained batch 77 batch loss 0.483021647 batch mAP 0.61831665 batch PCKh 0.875\n",
      "Trained batch 78 batch loss 0.41756767 batch mAP 0.628051758 batch PCKh 0.5625\n",
      "Trained batch 79 batch loss 0.405624211 batch mAP 0.631713867 batch PCKh 0.8125\n",
      "Trained batch 80 batch loss 0.461859107 batch mAP 0.608581543 batch PCKh 0.5\n",
      "Trained batch 81 batch loss 0.43738693 batch mAP 0.589935303 batch PCKh 0.375\n",
      "Trained batch 82 batch loss 0.370118022 batch mAP 0.692626953 batch PCKh 0.75\n",
      "Trained batch 83 batch loss 0.471137136 batch mAP 0.632385254 batch PCKh 0.5\n",
      "Trained batch 84 batch loss 0.468022 batch mAP 0.659912109 batch PCKh 0.8125\n",
      "Trained batch 85 batch loss 0.534870088 batch mAP 0.626159668 batch PCKh 0.6875\n",
      "Trained batch 86 batch loss 0.405420125 batch mAP 0.672027588 batch PCKh 0.5625\n",
      "Trained batch 87 batch loss 0.514159322 batch mAP 0.68649292 batch PCKh 0.625\n",
      "Trained batch 88 batch loss 0.44346875 batch mAP 0.634277344 batch PCKh 0.625\n",
      "Trained batch 89 batch loss 0.392059326 batch mAP 0.664489746 batch PCKh 0.6875\n",
      "Trained batch 90 batch loss 0.475429893 batch mAP 0.663024902 batch PCKh 0.6875\n",
      "Trained batch 91 batch loss 0.470413655 batch mAP 0.635101318 batch PCKh 0.625\n",
      "Trained batch 92 batch loss 0.532682538 batch mAP 0.679168701 batch PCKh 0.4375\n",
      "Trained batch 93 batch loss 0.441197187 batch mAP 0.655731201 batch PCKh 0.4375\n",
      "Trained batch 94 batch loss 0.486543655 batch mAP 0.583374 batch PCKh 0.25\n",
      "Trained batch 95 batch loss 0.459561706 batch mAP 0.650756836 batch PCKh 0.1875\n",
      "Trained batch 96 batch loss 0.356232047 batch mAP 0.740966797 batch PCKh 0.4375\n",
      "Trained batch 97 batch loss 0.529607236 batch mAP 0.638824463 batch PCKh 0.4375\n",
      "Trained batch 98 batch loss 0.480213642 batch mAP 0.619445801 batch PCKh 0.1875\n",
      "Trained batch 99 batch loss 0.455748677 batch mAP 0.632049561 batch PCKh 0.5\n",
      "Trained batch 100 batch loss 0.447548151 batch mAP 0.617584229 batch PCKh 0.375\n",
      "Trained batch 101 batch loss 0.400996089 batch mAP 0.625 batch PCKh 0.5625\n",
      "Trained batch 102 batch loss 0.420248568 batch mAP 0.638366699 batch PCKh 0.6875\n",
      "Trained batch 103 batch loss 0.435531646 batch mAP 0.653076172 batch PCKh 0.75\n",
      "Trained batch 104 batch loss 0.504913926 batch mAP 0.622345 batch PCKh 0.75\n",
      "Trained batch 105 batch loss 0.406060308 batch mAP 0.654388428 batch PCKh 0.6875\n",
      "Trained batch 106 batch loss 0.494597793 batch mAP 0.662872314 batch PCKh 0.6875\n",
      "Trained batch 107 batch loss 0.531737268 batch mAP 0.647918701 batch PCKh 0.6875\n",
      "Trained batch 108 batch loss 0.504230201 batch mAP 0.676330566 batch PCKh 0.3125\n",
      "Trained batch 109 batch loss 0.518528 batch mAP 0.666473389 batch PCKh 0.5\n",
      "Trained batch 110 batch loss 0.427059323 batch mAP 0.677886963 batch PCKh 0.25\n",
      "Trained batch 111 batch loss 0.424299359 batch mAP 0.638305664 batch PCKh 0.25\n",
      "Trained batch 112 batch loss 0.462788075 batch mAP 0.632995605 batch PCKh 0.5625\n",
      "Trained batch 113 batch loss 0.422626108 batch mAP 0.607116699 batch PCKh 0.5\n",
      "Trained batch 114 batch loss 0.465405792 batch mAP 0.72769165 batch PCKh 0.625\n",
      "Trained batch 115 batch loss 0.505687714 batch mAP 0.652893066 batch PCKh 0.5625\n",
      "Trained batch 116 batch loss 0.390234649 batch mAP 0.69229126 batch PCKh 0.75\n",
      "Trained batch 117 batch loss 0.463929147 batch mAP 0.686615 batch PCKh 0.5\n",
      "Trained batch 118 batch loss 0.425661922 batch mAP 0.651977539 batch PCKh 0.75\n",
      "Trained batch 119 batch loss 0.544069886 batch mAP 0.575195312 batch PCKh 0.25\n",
      "Trained batch 120 batch loss 0.610802 batch mAP 0.603973389 batch PCKh 0.875\n",
      "Trained batch 121 batch loss 0.555078149 batch mAP 0.568817139 batch PCKh 0.5\n",
      "Trained batch 122 batch loss 0.472618222 batch mAP 0.601196289 batch PCKh 0.375\n",
      "Trained batch 123 batch loss 0.403681308 batch mAP 0.648681641 batch PCKh 0.5\n",
      "Trained batch 124 batch loss 0.476548195 batch mAP 0.56362915 batch PCKh 0.3125\n",
      "Trained batch 125 batch loss 0.519874215 batch mAP 0.552490234 batch PCKh 0.125\n",
      "Trained batch 126 batch loss 0.433068633 batch mAP 0.585479736 batch PCKh 0.4375\n",
      "Trained batch 127 batch loss 0.426223 batch mAP 0.619903564 batch PCKh 0.625\n",
      "Trained batch 128 batch loss 0.455780506 batch mAP 0.637695312 batch PCKh 0.6875\n",
      "Trained batch 129 batch loss 0.487239271 batch mAP 0.64276123 batch PCKh 0.4375\n",
      "Trained batch 130 batch loss 0.510494351 batch mAP 0.649505615 batch PCKh 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 131 batch loss 0.374089479 batch mAP 0.713378906 batch PCKh 0.6875\n",
      "Trained batch 132 batch loss 0.470243514 batch mAP 0.517181396 batch PCKh 0.625\n",
      "Trained batch 133 batch loss 0.491015196 batch mAP 0.645507812 batch PCKh 0.5625\n",
      "Trained batch 134 batch loss 0.551551461 batch mAP 0.599487305 batch PCKh 0.125\n",
      "Trained batch 135 batch loss 0.598318219 batch mAP 0.61541748 batch PCKh 0.6875\n",
      "Trained batch 136 batch loss 0.543140173 batch mAP 0.613647461 batch PCKh 0.5625\n",
      "Trained batch 137 batch loss 0.514391959 batch mAP 0.574920654 batch PCKh 0.6875\n",
      "Trained batch 138 batch loss 0.609153867 batch mAP 0.542633057 batch PCKh 0.3125\n",
      "Trained batch 139 batch loss 0.570312202 batch mAP 0.583709717 batch PCKh 0.375\n",
      "Trained batch 140 batch loss 0.571195841 batch mAP 0.636932373 batch PCKh 0.6875\n",
      "Trained batch 141 batch loss 0.506014943 batch mAP 0.67489624 batch PCKh 0.5625\n",
      "Trained batch 142 batch loss 0.46414 batch mAP 0.66607666 batch PCKh 0.5625\n",
      "Trained batch 143 batch loss 0.573867738 batch mAP 0.560638428 batch PCKh 0.375\n",
      "Trained batch 144 batch loss 0.500740647 batch mAP 0.595977783 batch PCKh 0.8125\n",
      "Trained batch 145 batch loss 0.443839818 batch mAP 0.648590088 batch PCKh 0.3125\n",
      "Trained batch 146 batch loss 0.494375944 batch mAP 0.653198242 batch PCKh 0.5625\n",
      "Trained batch 147 batch loss 0.474754065 batch mAP 0.700378418 batch PCKh 0.625\n",
      "Trained batch 148 batch loss 0.474068403 batch mAP 0.614379883 batch PCKh 0.75\n",
      "Trained batch 149 batch loss 0.404903173 batch mAP 0.616607666 batch PCKh 0.5\n",
      "Trained batch 150 batch loss 0.40582791 batch mAP 0.63861084 batch PCKh 0.4375\n",
      "Trained batch 151 batch loss 0.442254752 batch mAP 0.676635742 batch PCKh 0.5\n",
      "Trained batch 152 batch loss 0.507736862 batch mAP 0.704650879 batch PCKh 0.4375\n",
      "Trained batch 153 batch loss 0.542689264 batch mAP 0.661956787 batch PCKh 0.25\n",
      "Trained batch 154 batch loss 0.523319125 batch mAP 0.566101074 batch PCKh 0.375\n",
      "Trained batch 155 batch loss 0.526134 batch mAP 0.617370605 batch PCKh 0.5625\n",
      "Trained batch 156 batch loss 0.514046073 batch mAP 0.597930908 batch PCKh 0.625\n",
      "Trained batch 157 batch loss 0.469894648 batch mAP 0.557495117 batch PCKh 0.75\n",
      "Trained batch 158 batch loss 0.475797445 batch mAP 0.601654053 batch PCKh 0.5625\n",
      "Trained batch 159 batch loss 0.491534591 batch mAP 0.607605 batch PCKh 0.625\n",
      "Trained batch 160 batch loss 0.419833362 batch mAP 0.601196289 batch PCKh 0.1875\n",
      "Trained batch 161 batch loss 0.446318537 batch mAP 0.608917236 batch PCKh 0.5\n",
      "Trained batch 162 batch loss 0.393072248 batch mAP 0.710754395 batch PCKh 0.75\n",
      "Trained batch 163 batch loss 0.393283516 batch mAP 0.681091309 batch PCKh 0.625\n",
      "Trained batch 164 batch loss 0.374612093 batch mAP 0.677215576 batch PCKh 0.3125\n",
      "Trained batch 165 batch loss 0.390914947 batch mAP 0.730163574 batch PCKh 0.375\n",
      "Trained batch 166 batch loss 0.458241642 batch mAP 0.64263916 batch PCKh 0.5625\n",
      "Trained batch 167 batch loss 0.477591276 batch mAP 0.627349854 batch PCKh 0.5625\n",
      "Trained batch 168 batch loss 0.480777413 batch mAP 0.645721436 batch PCKh 0.6875\n",
      "Trained batch 169 batch loss 0.397467494 batch mAP 0.671875 batch PCKh 0.75\n",
      "Trained batch 170 batch loss 0.38836 batch mAP 0.707305908 batch PCKh 0.5625\n",
      "Trained batch 171 batch loss 0.437723845 batch mAP 0.701690674 batch PCKh 0.5625\n",
      "Trained batch 172 batch loss 0.435638458 batch mAP 0.650299072 batch PCKh 0.4375\n",
      "Trained batch 173 batch loss 0.514292598 batch mAP 0.652648926 batch PCKh 0.375\n",
      "Trained batch 174 batch loss 0.435950041 batch mAP 0.703369141 batch PCKh 0.5\n",
      "Trained batch 175 batch loss 0.508480906 batch mAP 0.701660156 batch PCKh 0.6875\n",
      "Trained batch 176 batch loss 0.42443794 batch mAP 0.742919922 batch PCKh 0.625\n",
      "Trained batch 177 batch loss 0.515088916 batch mAP 0.643096924 batch PCKh 0.4375\n",
      "Trained batch 178 batch loss 0.45819208 batch mAP 0.639984131 batch PCKh 0.25\n",
      "Trained batch 179 batch loss 0.464963794 batch mAP 0.659393311 batch PCKh 0.6875\n",
      "Trained batch 180 batch loss 0.525434554 batch mAP 0.628509521 batch PCKh 0.4375\n",
      "Trained batch 181 batch loss 0.531843185 batch mAP 0.656036377 batch PCKh 0.4375\n",
      "Trained batch 182 batch loss 0.523384452 batch mAP 0.573730469 batch PCKh 0.75\n",
      "Trained batch 183 batch loss 0.573906422 batch mAP 0.619049072 batch PCKh 0.3125\n",
      "Trained batch 184 batch loss 0.463133037 batch mAP 0.63470459 batch PCKh 0.5625\n",
      "Trained batch 185 batch loss 0.496646345 batch mAP 0.611236572 batch PCKh 0.6875\n",
      "Trained batch 186 batch loss 0.557353 batch mAP 0.676971436 batch PCKh 0.875\n",
      "Trained batch 187 batch loss 0.508229 batch mAP 0.634674072 batch PCKh 0.8125\n",
      "Trained batch 188 batch loss 0.521056652 batch mAP 0.615661621 batch PCKh 0.375\n",
      "Trained batch 189 batch loss 0.611830533 batch mAP 0.589141846 batch PCKh 0.125\n",
      "Trained batch 190 batch loss 0.485032886 batch mAP 0.631195068 batch PCKh 0.6875\n",
      "Trained batch 191 batch loss 0.491630256 batch mAP 0.645080566 batch PCKh 0.75\n",
      "Trained batch 192 batch loss 0.504970551 batch mAP 0.698272705 batch PCKh 0.625\n",
      "Trained batch 193 batch loss 0.518708 batch mAP 0.628967285 batch PCKh 0.375\n",
      "Trained batch 194 batch loss 0.568808198 batch mAP 0.589294434 batch PCKh 0.375\n",
      "Trained batch 195 batch loss 0.560277283 batch mAP 0.558105469 batch PCKh 0.3125\n",
      "Trained batch 196 batch loss 0.557697 batch mAP 0.613922119 batch PCKh 0.6875\n",
      "Trained batch 197 batch loss 0.475867361 batch mAP 0.636810303 batch PCKh 0.375\n",
      "Trained batch 198 batch loss 0.533640802 batch mAP 0.630554199 batch PCKh 0.6875\n",
      "Trained batch 199 batch loss 0.550104618 batch mAP 0.594665527 batch PCKh 0.125\n",
      "Trained batch 200 batch loss 0.476052254 batch mAP 0.60836792 batch PCKh 0.5\n",
      "Trained batch 201 batch loss 0.495203376 batch mAP 0.573150635 batch PCKh 0.875\n",
      "Trained batch 202 batch loss 0.458187699 batch mAP 0.604064941 batch PCKh 0.8125\n",
      "Trained batch 203 batch loss 0.487753659 batch mAP 0.577270508 batch PCKh 0.6875\n",
      "Trained batch 204 batch loss 0.505744576 batch mAP 0.625610352 batch PCKh 0.75\n",
      "Trained batch 205 batch loss 0.460425496 batch mAP 0.597320557 batch PCKh 0.6875\n",
      "Trained batch 206 batch loss 0.439611346 batch mAP 0.538452148 batch PCKh 0.3125\n",
      "Trained batch 207 batch loss 0.467938185 batch mAP 0.521087646 batch PCKh 0.1875\n",
      "Trained batch 208 batch loss 0.50526315 batch mAP 0.568389893 batch PCKh 0.6875\n",
      "Trained batch 209 batch loss 0.523074746 batch mAP 0.548034668 batch PCKh 0.6875\n",
      "Trained batch 210 batch loss 0.550754964 batch mAP 0.521362305 batch PCKh 0.6875\n",
      "Trained batch 211 batch loss 0.459835529 batch mAP 0.601898193 batch PCKh 0.5625\n",
      "Trained batch 212 batch loss 0.478194386 batch mAP 0.618286133 batch PCKh 0.625\n",
      "Trained batch 213 batch loss 0.581531405 batch mAP 0.603912354 batch PCKh 0.625\n",
      "Trained batch 214 batch loss 0.409981191 batch mAP 0.623138428 batch PCKh 0.5\n",
      "Trained batch 215 batch loss 0.436474472 batch mAP 0.620483398 batch PCKh 0.75\n",
      "Trained batch 216 batch loss 0.453960389 batch mAP 0.644500732 batch PCKh 0.1875\n",
      "Trained batch 217 batch loss 0.497556627 batch mAP 0.601013184 batch PCKh 0.625\n",
      "Trained batch 218 batch loss 0.513463855 batch mAP 0.618164062 batch PCKh 0.625\n",
      "Trained batch 219 batch loss 0.470900238 batch mAP 0.635162354 batch PCKh 0.625\n",
      "Trained batch 220 batch loss 0.452907264 batch mAP 0.604431152 batch PCKh 0.125\n",
      "Trained batch 221 batch loss 0.497702122 batch mAP 0.54901123 batch PCKh 0.0625\n",
      "Trained batch 222 batch loss 0.576628745 batch mAP 0.560150146 batch PCKh 0.3125\n",
      "Trained batch 223 batch loss 0.561426282 batch mAP 0.55871582 batch PCKh 0.25\n",
      "Trained batch 224 batch loss 0.475070953 batch mAP 0.63571167 batch PCKh 0\n",
      "Trained batch 225 batch loss 0.50116545 batch mAP 0.624572754 batch PCKh 0.5625\n",
      "Trained batch 226 batch loss 0.550188065 batch mAP 0.640045166 batch PCKh 0.6875\n",
      "Trained batch 227 batch loss 0.544259548 batch mAP 0.577911377 batch PCKh 0.1875\n",
      "Trained batch 228 batch loss 0.46195215 batch mAP 0.620758057 batch PCKh 0.0625\n",
      "Trained batch 229 batch loss 0.458382845 batch mAP 0.678588867 batch PCKh 0.8125\n",
      "Trained batch 230 batch loss 0.442124426 batch mAP 0.656555176 batch PCKh 0.875\n",
      "Trained batch 231 batch loss 0.409496814 batch mAP 0.617645264 batch PCKh 0.5\n",
      "Trained batch 232 batch loss 0.483870327 batch mAP 0.613067627 batch PCKh 0.75\n",
      "Trained batch 233 batch loss 0.432340711 batch mAP 0.614471436 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 234 batch loss 0.447906017 batch mAP 0.571136475 batch PCKh 0.75\n",
      "Trained batch 235 batch loss 0.415975273 batch mAP 0.566070557 batch PCKh 0.5\n",
      "Trained batch 236 batch loss 0.470819324 batch mAP 0.506164551 batch PCKh 0.8125\n",
      "Trained batch 237 batch loss 0.51222235 batch mAP 0.640869141 batch PCKh 0.6875\n",
      "Trained batch 238 batch loss 0.48686111 batch mAP 0.645782471 batch PCKh 0.5625\n",
      "Trained batch 239 batch loss 0.535395801 batch mAP 0.633544922 batch PCKh 0.375\n",
      "Trained batch 240 batch loss 0.489542961 batch mAP 0.694213867 batch PCKh 0.4375\n",
      "Trained batch 241 batch loss 0.426300377 batch mAP 0.694244385 batch PCKh 0.375\n",
      "Trained batch 242 batch loss 0.339364827 batch mAP 0.635314941 batch PCKh 0.625\n",
      "Trained batch 243 batch loss 0.421926886 batch mAP 0.563903809 batch PCKh 0.4375\n",
      "Trained batch 244 batch loss 0.387999773 batch mAP 0.637176514 batch PCKh 0.625\n",
      "Trained batch 245 batch loss 0.438549697 batch mAP 0.61138916 batch PCKh 0.75\n",
      "Trained batch 246 batch loss 0.510040462 batch mAP 0.621521 batch PCKh 0.5625\n",
      "Trained batch 247 batch loss 0.580752134 batch mAP 0.552063 batch PCKh 0.3125\n",
      "Trained batch 248 batch loss 0.650780916 batch mAP 0.498382568 batch PCKh 0.625\n",
      "Trained batch 249 batch loss 0.515587389 batch mAP 0.607147217 batch PCKh 0.8125\n",
      "Trained batch 250 batch loss 0.409452409 batch mAP 0.601196289 batch PCKh 0.5625\n",
      "Trained batch 251 batch loss 0.468603879 batch mAP 0.594665527 batch PCKh 0.8125\n",
      "Trained batch 252 batch loss 0.483104408 batch mAP 0.59274292 batch PCKh 0.6875\n",
      "Trained batch 253 batch loss 0.377629489 batch mAP 0.561553955 batch PCKh 0.125\n",
      "Trained batch 254 batch loss 0.357450962 batch mAP 0.649475098 batch PCKh 0.6875\n",
      "Trained batch 255 batch loss 0.34502241 batch mAP 0.730529785 batch PCKh 0.5625\n",
      "Trained batch 256 batch loss 0.326538563 batch mAP 0.757415771 batch PCKh 0.75\n",
      "Trained batch 257 batch loss 0.301153779 batch mAP 0.744537354 batch PCKh 0.6875\n",
      "Trained batch 258 batch loss 0.316471875 batch mAP 0.779876709 batch PCKh 0.6875\n",
      "Trained batch 259 batch loss 0.268280804 batch mAP 0.758026123 batch PCKh 0.75\n",
      "Trained batch 260 batch loss 0.444745541 batch mAP 0.71484375 batch PCKh 0.6875\n",
      "Trained batch 261 batch loss 0.414622426 batch mAP 0.687408447 batch PCKh 0.5\n",
      "Trained batch 262 batch loss 0.511827469 batch mAP 0.60949707 batch PCKh 0.4375\n",
      "Trained batch 263 batch loss 0.538881719 batch mAP 0.666503906 batch PCKh 0.8125\n",
      "Trained batch 264 batch loss 0.526780367 batch mAP 0.688995361 batch PCKh 0.6875\n",
      "Trained batch 265 batch loss 0.611203074 batch mAP 0.602325439 batch PCKh 0.5625\n",
      "Trained batch 266 batch loss 0.531566262 batch mAP 0.635803223 batch PCKh 0.75\n",
      "Trained batch 267 batch loss 0.478087664 batch mAP 0.647644043 batch PCKh 0.5625\n",
      "Trained batch 268 batch loss 0.436415 batch mAP 0.624328613 batch PCKh 0.5625\n",
      "Trained batch 269 batch loss 0.446495771 batch mAP 0.593994141 batch PCKh 0.4375\n",
      "Trained batch 270 batch loss 0.472331703 batch mAP 0.659667969 batch PCKh 0.625\n",
      "Trained batch 271 batch loss 0.464363068 batch mAP 0.640563965 batch PCKh 0.5\n",
      "Trained batch 272 batch loss 0.501947522 batch mAP 0.632141113 batch PCKh 0.6875\n",
      "Trained batch 273 batch loss 0.457181126 batch mAP 0.632141113 batch PCKh 0.6875\n",
      "Trained batch 274 batch loss 0.52592 batch mAP 0.562042236 batch PCKh 0.6875\n",
      "Trained batch 275 batch loss 0.575795889 batch mAP 0.563598633 batch PCKh 0.375\n",
      "Trained batch 276 batch loss 0.528817415 batch mAP 0.554321289 batch PCKh 0.6875\n",
      "Trained batch 277 batch loss 0.501588404 batch mAP 0.663818359 batch PCKh 0.6875\n",
      "Trained batch 278 batch loss 0.516269147 batch mAP 0.596313477 batch PCKh 0.625\n",
      "Trained batch 279 batch loss 0.516719162 batch mAP 0.62878418 batch PCKh 0.375\n",
      "Trained batch 280 batch loss 0.536313653 batch mAP 0.622497559 batch PCKh 0.5\n",
      "Trained batch 281 batch loss 0.445183247 batch mAP 0.624084473 batch PCKh 0.875\n",
      "Trained batch 282 batch loss 0.435536325 batch mAP 0.658874512 batch PCKh 0.3125\n",
      "Trained batch 283 batch loss 0.526050091 batch mAP 0.559021 batch PCKh 0.5625\n",
      "Trained batch 284 batch loss 0.553307 batch mAP 0.535125732 batch PCKh 0.75\n",
      "Trained batch 285 batch loss 0.553177 batch mAP 0.594604492 batch PCKh 0.6875\n",
      "Trained batch 286 batch loss 0.553558946 batch mAP 0.636261 batch PCKh 0.875\n",
      "Trained batch 287 batch loss 0.510301888 batch mAP 0.591827393 batch PCKh 0.875\n",
      "Trained batch 288 batch loss 0.505881131 batch mAP 0.606384277 batch PCKh 0.625\n",
      "Trained batch 289 batch loss 0.553025424 batch mAP 0.602203369 batch PCKh 0.625\n",
      "Trained batch 290 batch loss 0.485422015 batch mAP 0.581054688 batch PCKh 0.5625\n",
      "Trained batch 291 batch loss 0.555994749 batch mAP 0.519042969 batch PCKh 0.875\n",
      "Trained batch 292 batch loss 0.481175274 batch mAP 0.578155518 batch PCKh 0.5625\n",
      "Trained batch 293 batch loss 0.491644204 batch mAP 0.574829102 batch PCKh 0.3125\n",
      "Trained batch 294 batch loss 0.51229018 batch mAP 0.527893066 batch PCKh 0.0625\n",
      "Trained batch 295 batch loss 0.437156439 batch mAP 0.562591553 batch PCKh 0.625\n",
      "Trained batch 296 batch loss 0.443841189 batch mAP 0.598388672 batch PCKh 0.25\n",
      "Trained batch 297 batch loss 0.438834429 batch mAP 0.636627197 batch PCKh 0.25\n",
      "Trained batch 298 batch loss 0.517141461 batch mAP 0.590026855 batch PCKh 0.875\n",
      "Trained batch 299 batch loss 0.56056875 batch mAP 0.539672852 batch PCKh 0.5\n",
      "Trained batch 300 batch loss 0.552697122 batch mAP 0.589233398 batch PCKh 0.75\n",
      "Trained batch 301 batch loss 0.480612159 batch mAP 0.609436035 batch PCKh 0.625\n",
      "Trained batch 302 batch loss 0.546760321 batch mAP 0.625854492 batch PCKh 0.6875\n",
      "Trained batch 303 batch loss 0.501666784 batch mAP 0.593566895 batch PCKh 0.75\n",
      "Trained batch 304 batch loss 0.433692813 batch mAP 0.637084961 batch PCKh 0.5\n",
      "Trained batch 305 batch loss 0.363282204 batch mAP 0.68637085 batch PCKh 0.625\n",
      "Trained batch 306 batch loss 0.39252162 batch mAP 0.641876221 batch PCKh 0\n",
      "Trained batch 307 batch loss 0.366469979 batch mAP 0.64151 batch PCKh 0\n",
      "Trained batch 308 batch loss 0.364976883 batch mAP 0.61328125 batch PCKh 0\n",
      "Trained batch 309 batch loss 0.461453468 batch mAP 0.63394165 batch PCKh 0.5\n",
      "Trained batch 310 batch loss 0.478040665 batch mAP 0.597412109 batch PCKh 0.5625\n",
      "Trained batch 311 batch loss 0.490870833 batch mAP 0.6378479 batch PCKh 0.5\n",
      "Trained batch 312 batch loss 0.525592327 batch mAP 0.593597412 batch PCKh 0.3125\n",
      "Trained batch 313 batch loss 0.616593242 batch mAP 0.573059082 batch PCKh 0.625\n",
      "Trained batch 314 batch loss 0.597408473 batch mAP 0.584411621 batch PCKh 0.4375\n",
      "Trained batch 315 batch loss 0.677806199 batch mAP 0.565887451 batch PCKh 0.125\n",
      "Trained batch 316 batch loss 0.503057778 batch mAP 0.642791748 batch PCKh 0.375\n",
      "Trained batch 317 batch loss 0.471247435 batch mAP 0.573059082 batch PCKh 0.375\n",
      "Trained batch 318 batch loss 0.481482565 batch mAP 0.541107178 batch PCKh 0.3125\n",
      "Trained batch 319 batch loss 0.485425979 batch mAP 0.569641113 batch PCKh 0.75\n",
      "Trained batch 320 batch loss 0.463293523 batch mAP 0.634033203 batch PCKh 0.5625\n",
      "Trained batch 321 batch loss 0.469925344 batch mAP 0.679779053 batch PCKh 0.6875\n",
      "Trained batch 322 batch loss 0.469939798 batch mAP 0.655822754 batch PCKh 0.4375\n",
      "Trained batch 323 batch loss 0.51649 batch mAP 0.640045166 batch PCKh 0.75\n",
      "Trained batch 324 batch loss 0.451049566 batch mAP 0.703613281 batch PCKh 0.375\n",
      "Trained batch 325 batch loss 0.515141308 batch mAP 0.664550781 batch PCKh 0.5\n",
      "Trained batch 326 batch loss 0.498824626 batch mAP 0.662750244 batch PCKh 0.5\n",
      "Trained batch 327 batch loss 0.530139089 batch mAP 0.668243408 batch PCKh 0.625\n",
      "Trained batch 328 batch loss 0.454469472 batch mAP 0.717224121 batch PCKh 0.625\n",
      "Trained batch 329 batch loss 0.49245584 batch mAP 0.714508057 batch PCKh 0.75\n",
      "Trained batch 330 batch loss 0.524402678 batch mAP 0.662506104 batch PCKh 0.625\n",
      "Trained batch 331 batch loss 0.518221378 batch mAP 0.630493164 batch PCKh 0.625\n",
      "Trained batch 332 batch loss 0.535894215 batch mAP 0.647064209 batch PCKh 0.5\n",
      "Trained batch 333 batch loss 0.480513841 batch mAP 0.647796631 batch PCKh 0.75\n",
      "Trained batch 334 batch loss 0.438490897 batch mAP 0.633667 batch PCKh 0.75\n",
      "Trained batch 335 batch loss 0.461417407 batch mAP 0.644287109 batch PCKh 0.25\n",
      "Trained batch 336 batch loss 0.457094669 batch mAP 0.661010742 batch PCKh 0.5\n",
      "Trained batch 337 batch loss 0.372451633 batch mAP 0.716522217 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 338 batch loss 0.470656484 batch mAP 0.617584229 batch PCKh 0.5\n",
      "Trained batch 339 batch loss 0.439632893 batch mAP 0.587677 batch PCKh 0.4375\n",
      "Trained batch 340 batch loss 0.481739104 batch mAP 0.693756104 batch PCKh 0.75\n",
      "Trained batch 341 batch loss 0.47398898 batch mAP 0.692321777 batch PCKh 0.8125\n",
      "Trained batch 342 batch loss 0.447162569 batch mAP 0.713287354 batch PCKh 0.3125\n",
      "Trained batch 343 batch loss 0.577267408 batch mAP 0.673492432 batch PCKh 0.25\n",
      "Trained batch 344 batch loss 0.500788152 batch mAP 0.636901855 batch PCKh 0.6875\n",
      "Trained batch 345 batch loss 0.400028288 batch mAP 0.677948 batch PCKh 0.5625\n",
      "Trained batch 346 batch loss 0.356255054 batch mAP 0.585388184 batch PCKh 0\n",
      "Trained batch 347 batch loss 0.405480087 batch mAP 0.635742188 batch PCKh 0.25\n",
      "Trained batch 348 batch loss 0.50553143 batch mAP 0.677429199 batch PCKh 0.5625\n",
      "Trained batch 349 batch loss 0.442971677 batch mAP 0.645233154 batch PCKh 0\n",
      "Trained batch 350 batch loss 0.431402087 batch mAP 0.727203369 batch PCKh 0.6875\n",
      "Trained batch 351 batch loss 0.383000076 batch mAP 0.700805664 batch PCKh 0.5\n",
      "Trained batch 352 batch loss 0.535645127 batch mAP 0.659759521 batch PCKh 0.375\n",
      "Trained batch 353 batch loss 0.498108029 batch mAP 0.677886963 batch PCKh 0.375\n",
      "Trained batch 354 batch loss 0.46844244 batch mAP 0.719635 batch PCKh 0.4375\n",
      "Trained batch 355 batch loss 0.518300831 batch mAP 0.729766846 batch PCKh 0.375\n",
      "Trained batch 356 batch loss 0.5113976 batch mAP 0.692962646 batch PCKh 0.375\n",
      "Trained batch 357 batch loss 0.482832819 batch mAP 0.659454346 batch PCKh 0.3125\n",
      "Trained batch 358 batch loss 0.533987641 batch mAP 0.648101807 batch PCKh 0.5625\n",
      "Trained batch 359 batch loss 0.529286742 batch mAP 0.671478271 batch PCKh 0.3125\n",
      "Trained batch 360 batch loss 0.529420912 batch mAP 0.69921875 batch PCKh 0.8125\n",
      "Trained batch 361 batch loss 0.524400353 batch mAP 0.682739258 batch PCKh 0.875\n",
      "Trained batch 362 batch loss 0.415175408 batch mAP 0.698455811 batch PCKh 0.4375\n",
      "Trained batch 363 batch loss 0.52895093 batch mAP 0.685760498 batch PCKh 0.3125\n",
      "Trained batch 364 batch loss 0.571165323 batch mAP 0.633605957 batch PCKh 0.75\n",
      "Trained batch 365 batch loss 0.533331215 batch mAP 0.645172119 batch PCKh 0.8125\n",
      "Trained batch 366 batch loss 0.533839762 batch mAP 0.623901367 batch PCKh 0.75\n",
      "Trained batch 367 batch loss 0.558876276 batch mAP 0.668304443 batch PCKh 0.4375\n",
      "Trained batch 368 batch loss 0.494967461 batch mAP 0.625885 batch PCKh 0.6875\n",
      "Trained batch 369 batch loss 0.486601114 batch mAP 0.620574951 batch PCKh 0.8125\n",
      "Trained batch 370 batch loss 0.523402572 batch mAP 0.639038086 batch PCKh 0.375\n",
      "Trained batch 371 batch loss 0.475323856 batch mAP 0.576324463 batch PCKh 0.5625\n",
      "Trained batch 372 batch loss 0.441225827 batch mAP 0.633728 batch PCKh 0.5\n",
      "Trained batch 373 batch loss 0.465688556 batch mAP 0.628326416 batch PCKh 0.25\n",
      "Trained batch 374 batch loss 0.398506254 batch mAP 0.670806885 batch PCKh 0.625\n",
      "Trained batch 375 batch loss 0.530872524 batch mAP 0.641418457 batch PCKh 0.6875\n",
      "Trained batch 376 batch loss 0.553245246 batch mAP 0.634155273 batch PCKh 0.5625\n",
      "Trained batch 377 batch loss 0.492032409 batch mAP 0.643310547 batch PCKh 0.8125\n",
      "Trained batch 378 batch loss 0.509471893 batch mAP 0.598876953 batch PCKh 0.375\n",
      "Trained batch 379 batch loss 0.516085088 batch mAP 0.658660889 batch PCKh 0.25\n",
      "Trained batch 380 batch loss 0.539484739 batch mAP 0.597717285 batch PCKh 0.375\n",
      "Trained batch 381 batch loss 0.551608324 batch mAP 0.634490967 batch PCKh 0.3125\n",
      "Trained batch 382 batch loss 0.574122488 batch mAP 0.601745605 batch PCKh 0.6875\n",
      "Trained batch 383 batch loss 0.578328967 batch mAP 0.604126 batch PCKh 0.5625\n",
      "Trained batch 384 batch loss 0.534061551 batch mAP 0.623321533 batch PCKh 0.6875\n",
      "Trained batch 385 batch loss 0.577664137 batch mAP 0.635742188 batch PCKh 0.75\n",
      "Trained batch 386 batch loss 0.487000704 batch mAP 0.604492188 batch PCKh 0.625\n",
      "Trained batch 387 batch loss 0.5457232 batch mAP 0.642089844 batch PCKh 0.8125\n",
      "Trained batch 388 batch loss 0.501816571 batch mAP 0.694061279 batch PCKh 0.3125\n",
      "Trained batch 389 batch loss 0.444427133 batch mAP 0.668182373 batch PCKh 0.8125\n",
      "Trained batch 390 batch loss 0.475760937 batch mAP 0.670471191 batch PCKh 0.1875\n",
      "Trained batch 391 batch loss 0.53798908 batch mAP 0.646575928 batch PCKh 0.5\n",
      "Trained batch 392 batch loss 0.506918311 batch mAP 0.656585693 batch PCKh 0.3125\n",
      "Trained batch 393 batch loss 0.589361906 batch mAP 0.60055542 batch PCKh 0.3125\n",
      "Trained batch 394 batch loss 0.52845633 batch mAP 0.550354 batch PCKh 0.125\n",
      "Trained batch 395 batch loss 0.463804066 batch mAP 0.587860107 batch PCKh 0.75\n",
      "Trained batch 396 batch loss 0.484752059 batch mAP 0.522125244 batch PCKh 0.8125\n",
      "Trained batch 397 batch loss 0.405871779 batch mAP 0.620758057 batch PCKh 0.25\n",
      "Trained batch 398 batch loss 0.395657569 batch mAP 0.670013428 batch PCKh 0.5625\n",
      "Trained batch 399 batch loss 0.407174379 batch mAP 0.58694458 batch PCKh 0.75\n",
      "Trained batch 400 batch loss 0.396722436 batch mAP 0.60269165 batch PCKh 0.3125\n",
      "Trained batch 401 batch loss 0.406796694 batch mAP 0.698822 batch PCKh 0.3125\n",
      "Trained batch 402 batch loss 0.502588451 batch mAP 0.64352417 batch PCKh 0.1875\n",
      "Trained batch 403 batch loss 0.536663532 batch mAP 0.608917236 batch PCKh 0.5\n",
      "Trained batch 404 batch loss 0.658644736 batch mAP 0.578887939 batch PCKh 0.3125\n",
      "Trained batch 405 batch loss 0.542708 batch mAP 0.666870117 batch PCKh 0.25\n",
      "Trained batch 406 batch loss 0.475807339 batch mAP 0.682678223 batch PCKh 0.625\n",
      "Trained batch 407 batch loss 0.422995657 batch mAP 0.679962158 batch PCKh 0.625\n",
      "Trained batch 408 batch loss 0.434676945 batch mAP 0.677398682 batch PCKh 0.75\n",
      "Trained batch 409 batch loss 0.451739907 batch mAP 0.687866211 batch PCKh 0.375\n",
      "Trained batch 410 batch loss 0.404583722 batch mAP 0.730224609 batch PCKh 0.6875\n",
      "Trained batch 411 batch loss 0.530969918 batch mAP 0.634338379 batch PCKh 0.375\n",
      "Trained batch 412 batch loss 0.492368847 batch mAP 0.708587646 batch PCKh 0.4375\n",
      "Trained batch 413 batch loss 0.39426893 batch mAP 0.705383301 batch PCKh 0.5625\n",
      "Trained batch 414 batch loss 0.46550566 batch mAP 0.675842285 batch PCKh 0.8125\n",
      "Trained batch 415 batch loss 0.42147845 batch mAP 0.664825439 batch PCKh 0.4375\n",
      "Trained batch 416 batch loss 0.396834075 batch mAP 0.69430542 batch PCKh 0.4375\n",
      "Trained batch 417 batch loss 0.475607038 batch mAP 0.610626221 batch PCKh 0.875\n",
      "Trained batch 418 batch loss 0.463322878 batch mAP 0.682495117 batch PCKh 0.625\n",
      "Trained batch 419 batch loss 0.540348709 batch mAP 0.702606201 batch PCKh 0.375\n",
      "Trained batch 420 batch loss 0.464499861 batch mAP 0.66583252 batch PCKh 0.1875\n",
      "Trained batch 421 batch loss 0.495962977 batch mAP 0.630371094 batch PCKh 0.625\n",
      "Trained batch 422 batch loss 0.516246736 batch mAP 0.701599121 batch PCKh 0.875\n",
      "Trained batch 423 batch loss 0.477041215 batch mAP 0.67376709 batch PCKh 0.5\n",
      "Trained batch 424 batch loss 0.510475874 batch mAP 0.629241943 batch PCKh 0.8125\n",
      "Trained batch 425 batch loss 0.482637584 batch mAP 0.710540771 batch PCKh 0.5\n",
      "Trained batch 426 batch loss 0.474118143 batch mAP 0.657836914 batch PCKh 0.5\n",
      "Trained batch 427 batch loss 0.444328547 batch mAP 0.655731201 batch PCKh 0.125\n",
      "Trained batch 428 batch loss 0.428843051 batch mAP 0.707519531 batch PCKh 0.375\n",
      "Trained batch 429 batch loss 0.474727243 batch mAP 0.701385498 batch PCKh 0.625\n",
      "Trained batch 430 batch loss 0.450698525 batch mAP 0.71484375 batch PCKh 0.625\n",
      "Trained batch 431 batch loss 0.429963529 batch mAP 0.702392578 batch PCKh 0.25\n",
      "Trained batch 432 batch loss 0.478697628 batch mAP 0.739044189 batch PCKh 0.625\n",
      "Trained batch 433 batch loss 0.469244689 batch mAP 0.719421387 batch PCKh 0.5\n",
      "Trained batch 434 batch loss 0.547509074 batch mAP 0.695251465 batch PCKh 0.625\n",
      "Trained batch 435 batch loss 0.664460957 batch mAP 0.611358643 batch PCKh 0.1875\n",
      "Trained batch 436 batch loss 0.591750085 batch mAP 0.586700439 batch PCKh 0.3125\n",
      "Trained batch 437 batch loss 0.652376175 batch mAP 0.617614746 batch PCKh 0.4375\n",
      "Trained batch 438 batch loss 0.691587448 batch mAP 0.634338379 batch PCKh 0.1875\n",
      "Trained batch 439 batch loss 0.554961324 batch mAP 0.6534729 batch PCKh 0.4375\n",
      "Trained batch 440 batch loss 0.454108804 batch mAP 0.677063 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 441 batch loss 0.380368829 batch mAP 0.650482178 batch PCKh 0.75\n",
      "Trained batch 442 batch loss 0.506439328 batch mAP 0.671875 batch PCKh 0.75\n",
      "Trained batch 443 batch loss 0.495873451 batch mAP 0.670410156 batch PCKh 0.5\n",
      "Trained batch 444 batch loss 0.427911103 batch mAP 0.674255371 batch PCKh 0.625\n",
      "Trained batch 445 batch loss 0.446166188 batch mAP 0.603637695 batch PCKh 0.75\n",
      "Trained batch 446 batch loss 0.569228888 batch mAP 0.641113281 batch PCKh 0.75\n",
      "Trained batch 447 batch loss 0.484588861 batch mAP 0.597991943 batch PCKh 0.5\n",
      "Trained batch 448 batch loss 0.555633068 batch mAP 0.557342529 batch PCKh 0.1875\n",
      "Trained batch 449 batch loss 0.540781736 batch mAP 0.618560791 batch PCKh 0.3125\n",
      "Trained batch 450 batch loss 0.508014321 batch mAP 0.605133057 batch PCKh 0.5625\n",
      "Trained batch 451 batch loss 0.565279365 batch mAP 0.517181396 batch PCKh 0.8125\n",
      "Trained batch 452 batch loss 0.59499526 batch mAP 0.529968262 batch PCKh 0.25\n",
      "Trained batch 453 batch loss 0.523423135 batch mAP 0.512115479 batch PCKh 0\n",
      "Trained batch 454 batch loss 0.629737914 batch mAP 0.572265625 batch PCKh 0.5\n",
      "Trained batch 455 batch loss 0.512451589 batch mAP 0.60055542 batch PCKh 0.3125\n",
      "Trained batch 456 batch loss 0.661292672 batch mAP 0.543518066 batch PCKh 0.75\n",
      "Trained batch 457 batch loss 0.640083551 batch mAP 0.557128906 batch PCKh 0.4375\n",
      "Trained batch 458 batch loss 0.605168939 batch mAP 0.575012207 batch PCKh 0.375\n",
      "Trained batch 459 batch loss 0.576530457 batch mAP 0.596038818 batch PCKh 0.375\n",
      "Trained batch 460 batch loss 0.540557742 batch mAP 0.674194336 batch PCKh 0.625\n",
      "Trained batch 461 batch loss 0.546134 batch mAP 0.654388428 batch PCKh 0.5625\n",
      "Trained batch 462 batch loss 0.56338954 batch mAP 0.63168335 batch PCKh 0.5\n",
      "Trained batch 463 batch loss 0.459996194 batch mAP 0.580627441 batch PCKh 0.5625\n",
      "Trained batch 464 batch loss 0.469263971 batch mAP 0.647033691 batch PCKh 0.3125\n",
      "Trained batch 465 batch loss 0.516454935 batch mAP 0.645355225 batch PCKh 0.75\n",
      "Trained batch 466 batch loss 0.48344332 batch mAP 0.66418457 batch PCKh 0.25\n",
      "Trained batch 467 batch loss 0.577184677 batch mAP 0.648895264 batch PCKh 0.4375\n",
      "Trained batch 468 batch loss 0.487144411 batch mAP 0.605773926 batch PCKh 0.375\n",
      "Trained batch 469 batch loss 0.534293294 batch mAP 0.572845459 batch PCKh 0.4375\n",
      "Trained batch 470 batch loss 0.536380768 batch mAP 0.665649414 batch PCKh 0.75\n",
      "Trained batch 471 batch loss 0.51768744 batch mAP 0.665100098 batch PCKh 0.5625\n",
      "Trained batch 472 batch loss 0.484922647 batch mAP 0.685974121 batch PCKh 0.75\n",
      "Trained batch 473 batch loss 0.48938024 batch mAP 0.688537598 batch PCKh 0.6875\n",
      "Trained batch 474 batch loss 0.418376267 batch mAP 0.708831787 batch PCKh 0.5\n",
      "Trained batch 475 batch loss 0.478341579 batch mAP 0.65725708 batch PCKh 0.75\n",
      "Trained batch 476 batch loss 0.520036221 batch mAP 0.596557617 batch PCKh 0.25\n",
      "Trained batch 477 batch loss 0.513105512 batch mAP 0.626220703 batch PCKh 0.8125\n",
      "Trained batch 478 batch loss 0.510493934 batch mAP 0.602508545 batch PCKh 0.625\n",
      "Trained batch 479 batch loss 0.497840881 batch mAP 0.621154785 batch PCKh 0.6875\n",
      "Trained batch 480 batch loss 0.547933221 batch mAP 0.615875244 batch PCKh 0.8125\n",
      "Trained batch 481 batch loss 0.477155089 batch mAP 0.659576416 batch PCKh 0.5625\n",
      "Trained batch 482 batch loss 0.471692741 batch mAP 0.646606445 batch PCKh 0.75\n",
      "Trained batch 483 batch loss 0.549513936 batch mAP 0.597381592 batch PCKh 0.6875\n",
      "Trained batch 484 batch loss 0.494263232 batch mAP 0.634765625 batch PCKh 0.4375\n",
      "Trained batch 485 batch loss 0.519323587 batch mAP 0.600341797 batch PCKh 0.6875\n",
      "Trained batch 486 batch loss 0.535104394 batch mAP 0.609130859 batch PCKh 0.875\n",
      "Trained batch 487 batch loss 0.465048164 batch mAP 0.637451172 batch PCKh 0.875\n",
      "Trained batch 488 batch loss 0.57289 batch mAP 0.602020264 batch PCKh 0.5625\n",
      "Trained batch 489 batch loss 0.550712049 batch mAP 0.580291748 batch PCKh 0.75\n",
      "Trained batch 490 batch loss 0.564856827 batch mAP 0.570220947 batch PCKh 0.875\n",
      "Trained batch 491 batch loss 0.470050097 batch mAP 0.581970215 batch PCKh 0.875\n",
      "Trained batch 492 batch loss 0.505597591 batch mAP 0.569519043 batch PCKh 0.5\n",
      "Trained batch 493 batch loss 0.506977081 batch mAP 0.5987854 batch PCKh 0.5625\n",
      "Trained batch 494 batch loss 0.431666672 batch mAP 0.658233643 batch PCKh 0.6875\n",
      "Trained batch 495 batch loss 0.450498819 batch mAP 0.613067627 batch PCKh 0.625\n",
      "Trained batch 496 batch loss 0.441857159 batch mAP 0.619506836 batch PCKh 0.5\n",
      "Trained batch 497 batch loss 0.505428851 batch mAP 0.581665039 batch PCKh 0.6875\n",
      "Trained batch 498 batch loss 0.520370603 batch mAP 0.585876465 batch PCKh 0.6875\n",
      "Trained batch 499 batch loss 0.534310758 batch mAP 0.565643311 batch PCKh 0.625\n",
      "Trained batch 500 batch loss 0.524755776 batch mAP 0.617370605 batch PCKh 0.625\n",
      "Trained batch 501 batch loss 0.562656403 batch mAP 0.576873779 batch PCKh 0.6875\n",
      "Trained batch 502 batch loss 0.488108754 batch mAP 0.596405 batch PCKh 0.625\n",
      "Trained batch 503 batch loss 0.480222523 batch mAP 0.587493896 batch PCKh 0.1875\n",
      "Trained batch 504 batch loss 0.485845268 batch mAP 0.599334717 batch PCKh 0.4375\n",
      "Trained batch 505 batch loss 0.492068619 batch mAP 0.671630859 batch PCKh 0.6875\n",
      "Trained batch 506 batch loss 0.496205747 batch mAP 0.634124756 batch PCKh 0.6875\n",
      "Trained batch 507 batch loss 0.447556794 batch mAP 0.668060303 batch PCKh 0.5625\n",
      "Trained batch 508 batch loss 0.47912991 batch mAP 0.645874 batch PCKh 0.625\n",
      "Trained batch 509 batch loss 0.432307839 batch mAP 0.685974121 batch PCKh 0.625\n",
      "Trained batch 510 batch loss 0.559360623 batch mAP 0.577789307 batch PCKh 0.6875\n",
      "Trained batch 511 batch loss 0.501062512 batch mAP 0.612792969 batch PCKh 0.625\n",
      "Trained batch 512 batch loss 0.43611294 batch mAP 0.629974365 batch PCKh 0.6875\n",
      "Trained batch 513 batch loss 0.332263798 batch mAP 0.679840088 batch PCKh 0.6875\n",
      "Trained batch 514 batch loss 0.54756695 batch mAP 0.600219727 batch PCKh 0.8125\n",
      "Trained batch 515 batch loss 0.55978 batch mAP 0.628326416 batch PCKh 0.8125\n",
      "Trained batch 516 batch loss 0.458574772 batch mAP 0.630279541 batch PCKh 0.75\n",
      "Trained batch 517 batch loss 0.574724376 batch mAP 0.579376221 batch PCKh 0.0625\n",
      "Trained batch 518 batch loss 0.537138224 batch mAP 0.588195801 batch PCKh 0.25\n",
      "Trained batch 519 batch loss 0.502462745 batch mAP 0.604370117 batch PCKh 0.25\n",
      "Trained batch 520 batch loss 0.536410332 batch mAP 0.58605957 batch PCKh 0.25\n",
      "Trained batch 521 batch loss 0.457156062 batch mAP 0.638793945 batch PCKh 0\n",
      "Trained batch 522 batch loss 0.521172166 batch mAP 0.575561523 batch PCKh 0\n",
      "Trained batch 523 batch loss 0.556870222 batch mAP 0.541870117 batch PCKh 0.375\n",
      "Trained batch 524 batch loss 0.612710357 batch mAP 0.597595215 batch PCKh 0.25\n",
      "Trained batch 525 batch loss 0.604355097 batch mAP 0.645599365 batch PCKh 0.1875\n",
      "Trained batch 526 batch loss 0.705495954 batch mAP 0.576660156 batch PCKh 0\n",
      "Trained batch 527 batch loss 0.597128749 batch mAP 0.658935547 batch PCKh 0.375\n",
      "Trained batch 528 batch loss 0.556385159 batch mAP 0.665130615 batch PCKh 0.625\n",
      "Trained batch 529 batch loss 0.619838893 batch mAP 0.480255127 batch PCKh 0.5\n",
      "Trained batch 530 batch loss 0.576359391 batch mAP 0.515228271 batch PCKh 0.75\n",
      "Trained batch 531 batch loss 0.551176906 batch mAP 0.453521729 batch PCKh 0.6875\n",
      "Trained batch 532 batch loss 0.572279572 batch mAP 0.378265381 batch PCKh 0.4375\n",
      "Trained batch 533 batch loss 0.558518469 batch mAP 0.470672607 batch PCKh 0.875\n",
      "Trained batch 534 batch loss 0.470013618 batch mAP 0.518127441 batch PCKh 0.375\n",
      "Trained batch 535 batch loss 0.51212275 batch mAP 0.315612793 batch PCKh 0.75\n",
      "Trained batch 536 batch loss 0.484025419 batch mAP 0.505462646 batch PCKh 0.1875\n",
      "Trained batch 537 batch loss 0.588382661 batch mAP 0.442901611 batch PCKh 0.125\n",
      "Trained batch 538 batch loss 0.564722121 batch mAP 0.545288086 batch PCKh 0.5\n",
      "Trained batch 539 batch loss 0.539642572 batch mAP 0.499938965 batch PCKh 0.5\n",
      "Trained batch 540 batch loss 0.527622938 batch mAP 0.605255127 batch PCKh 0.375\n",
      "Trained batch 541 batch loss 0.591616273 batch mAP 0.583770752 batch PCKh 0.6875\n",
      "Trained batch 542 batch loss 0.53200537 batch mAP 0.574646 batch PCKh 0.8125\n",
      "Trained batch 543 batch loss 0.505845 batch mAP 0.509063721 batch PCKh 0.0625\n",
      "Trained batch 544 batch loss 0.489424199 batch mAP 0.544677734 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 545 batch loss 0.534125745 batch mAP 0.540252686 batch PCKh 0.5625\n",
      "Trained batch 546 batch loss 0.517634928 batch mAP 0.533203125 batch PCKh 0.75\n",
      "Trained batch 547 batch loss 0.515884876 batch mAP 0.485595703 batch PCKh 0.6875\n",
      "Trained batch 548 batch loss 0.424432337 batch mAP 0.467803955 batch PCKh 0.5625\n",
      "Trained batch 549 batch loss 0.478818238 batch mAP 0.50982666 batch PCKh 0.125\n",
      "Trained batch 550 batch loss 0.453122735 batch mAP 0.579528809 batch PCKh 0.375\n",
      "Trained batch 551 batch loss 0.445006341 batch mAP 0.584320068 batch PCKh 0.625\n",
      "Trained batch 552 batch loss 0.356445521 batch mAP 0.561859131 batch PCKh 0.75\n",
      "Trained batch 553 batch loss 0.409894109 batch mAP 0.57623291 batch PCKh 0.75\n",
      "Trained batch 554 batch loss 0.366441727 batch mAP 0.610473633 batch PCKh 0.6875\n",
      "Trained batch 555 batch loss 0.383044809 batch mAP 0.629974365 batch PCKh 0.75\n",
      "Trained batch 556 batch loss 0.4893215 batch mAP 0.546875 batch PCKh 0.875\n",
      "Trained batch 557 batch loss 0.608319938 batch mAP 0.500793457 batch PCKh 0.25\n",
      "Trained batch 558 batch loss 0.525037 batch mAP 0.516662598 batch PCKh 0.875\n",
      "Trained batch 559 batch loss 0.458714426 batch mAP 0.524993896 batch PCKh 0.875\n",
      "Trained batch 560 batch loss 0.555158138 batch mAP 0.471191406 batch PCKh 0.8125\n",
      "Trained batch 561 batch loss 0.569233775 batch mAP 0.528717041 batch PCKh 0.5625\n",
      "Trained batch 562 batch loss 0.556920469 batch mAP 0.484466553 batch PCKh 0.5625\n",
      "Trained batch 563 batch loss 0.533746481 batch mAP 0.560516357 batch PCKh 0.5625\n",
      "Trained batch 564 batch loss 0.490260154 batch mAP 0.526123047 batch PCKh 0.6875\n",
      "Trained batch 565 batch loss 0.478288591 batch mAP 0.51776123 batch PCKh 0.875\n",
      "Trained batch 566 batch loss 0.487390757 batch mAP 0.566009521 batch PCKh 0.875\n",
      "Trained batch 567 batch loss 0.465498507 batch mAP 0.551300049 batch PCKh 0.75\n",
      "Trained batch 568 batch loss 0.583128 batch mAP 0.449859619 batch PCKh 0.4375\n",
      "Trained batch 569 batch loss 0.379619598 batch mAP 0.621917725 batch PCKh 0.5625\n",
      "Trained batch 570 batch loss 0.427707195 batch mAP 0.62210083 batch PCKh 0.625\n",
      "Trained batch 571 batch loss 0.480095804 batch mAP 0.663330078 batch PCKh 0.3125\n",
      "Trained batch 572 batch loss 0.39192149 batch mAP 0.661193848 batch PCKh 0.375\n",
      "Trained batch 573 batch loss 0.38191092 batch mAP 0.724243164 batch PCKh 0.5625\n",
      "Trained batch 574 batch loss 0.435037792 batch mAP 0.698669434 batch PCKh 0.4375\n",
      "Trained batch 575 batch loss 0.47233668 batch mAP 0.637786865 batch PCKh 0.25\n",
      "Trained batch 576 batch loss 0.472728342 batch mAP 0.703765869 batch PCKh 0.5\n",
      "Trained batch 577 batch loss 0.404441386 batch mAP 0.722045898 batch PCKh 0.5\n",
      "Trained batch 578 batch loss 0.564880848 batch mAP 0.661956787 batch PCKh 0.4375\n",
      "Trained batch 579 batch loss 0.474472821 batch mAP 0.69732666 batch PCKh 0.4375\n",
      "Trained batch 580 batch loss 0.460689932 batch mAP 0.606140137 batch PCKh 0.5625\n",
      "Trained batch 581 batch loss 0.421812505 batch mAP 0.612518311 batch PCKh 0.5\n",
      "Trained batch 582 batch loss 0.564795732 batch mAP 0.589172363 batch PCKh 0.8125\n",
      "Trained batch 583 batch loss 0.51501435 batch mAP 0.717132568 batch PCKh 0.375\n",
      "Trained batch 584 batch loss 0.579201877 batch mAP 0.685974121 batch PCKh 0.75\n",
      "Trained batch 585 batch loss 0.566163 batch mAP 0.682739258 batch PCKh 0.375\n",
      "Trained batch 586 batch loss 0.561621428 batch mAP 0.673156738 batch PCKh 0.5\n",
      "Trained batch 587 batch loss 0.587592363 batch mAP 0.503540039 batch PCKh 0.125\n",
      "Trained batch 588 batch loss 0.530118167 batch mAP 0.538665771 batch PCKh 0.5625\n",
      "Trained batch 589 batch loss 0.542537808 batch mAP 0.552825928 batch PCKh 0.5625\n",
      "Trained batch 590 batch loss 0.561791897 batch mAP 0.549865723 batch PCKh 0.5625\n",
      "Trained batch 591 batch loss 0.46308291 batch mAP 0.549469 batch PCKh 0.5625\n",
      "Trained batch 592 batch loss 0.421802938 batch mAP 0.547088623 batch PCKh 0.0625\n",
      "Trained batch 593 batch loss 0.441340446 batch mAP 0.708496094 batch PCKh 0.5\n",
      "Trained batch 594 batch loss 0.477761924 batch mAP 0.604309082 batch PCKh 0.4375\n",
      "Trained batch 595 batch loss 0.508679211 batch mAP 0.601501465 batch PCKh 0.75\n",
      "Trained batch 596 batch loss 0.52126646 batch mAP 0.600067139 batch PCKh 0.75\n",
      "Trained batch 597 batch loss 0.550459802 batch mAP 0.608184814 batch PCKh 0.8125\n",
      "Trained batch 598 batch loss 0.488185346 batch mAP 0.618469238 batch PCKh 0.625\n",
      "Trained batch 599 batch loss 0.568343401 batch mAP 0.602905273 batch PCKh 0.5\n",
      "Trained batch 600 batch loss 0.634002864 batch mAP 0.587005615 batch PCKh 0\n",
      "Trained batch 601 batch loss 0.547456622 batch mAP 0.655609131 batch PCKh 0.4375\n",
      "Trained batch 602 batch loss 0.564800441 batch mAP 0.704437256 batch PCKh 0.5\n",
      "Trained batch 603 batch loss 0.557771444 batch mAP 0.708862305 batch PCKh 0.4375\n",
      "Trained batch 604 batch loss 0.569722652 batch mAP 0.606872559 batch PCKh 0.375\n",
      "Trained batch 605 batch loss 0.47841844 batch mAP 0.646606445 batch PCKh 0.75\n",
      "Trained batch 606 batch loss 0.482878208 batch mAP 0.588378906 batch PCKh 0.1875\n",
      "Trained batch 607 batch loss 0.490817428 batch mAP 0.581268311 batch PCKh 0.6875\n",
      "Trained batch 608 batch loss 0.508795738 batch mAP 0.616882324 batch PCKh 0.25\n",
      "Trained batch 609 batch loss 0.494234443 batch mAP 0.611297607 batch PCKh 0.5625\n",
      "Trained batch 610 batch loss 0.51959008 batch mAP 0.607299805 batch PCKh 0.5625\n",
      "Trained batch 611 batch loss 0.498791516 batch mAP 0.59979248 batch PCKh 0.75\n",
      "Trained batch 612 batch loss 0.481697291 batch mAP 0.620910645 batch PCKh 0.4375\n",
      "Trained batch 613 batch loss 0.525830746 batch mAP 0.652130127 batch PCKh 0.5625\n",
      "Trained batch 614 batch loss 0.493316114 batch mAP 0.607543945 batch PCKh 0.125\n",
      "Trained batch 615 batch loss 0.582209706 batch mAP 0.595916748 batch PCKh 0.5625\n",
      "Trained batch 616 batch loss 0.479611367 batch mAP 0.678497314 batch PCKh 0.8125\n",
      "Trained batch 617 batch loss 0.405477345 batch mAP 0.641235352 batch PCKh 0.375\n",
      "Trained batch 618 batch loss 0.525336623 batch mAP 0.627502441 batch PCKh 0.875\n",
      "Trained batch 619 batch loss 0.42796132 batch mAP 0.637512207 batch PCKh 0.6875\n",
      "Trained batch 620 batch loss 0.569315434 batch mAP 0.645996094 batch PCKh 0.375\n",
      "Trained batch 621 batch loss 0.461277246 batch mAP 0.712371826 batch PCKh 0.625\n",
      "Trained batch 622 batch loss 0.46679306 batch mAP 0.711456299 batch PCKh 0.875\n",
      "Trained batch 623 batch loss 0.43836838 batch mAP 0.646026611 batch PCKh 0.6875\n",
      "Trained batch 624 batch loss 0.451278836 batch mAP 0.69934082 batch PCKh 0.75\n",
      "Trained batch 625 batch loss 0.409910381 batch mAP 0.731079102 batch PCKh 0.625\n",
      "Trained batch 626 batch loss 0.378588796 batch mAP 0.764953613 batch PCKh 0.75\n",
      "Trained batch 627 batch loss 0.480026156 batch mAP 0.725647 batch PCKh 0.5625\n",
      "Trained batch 628 batch loss 0.501468599 batch mAP 0.662445068 batch PCKh 0.5625\n",
      "Trained batch 629 batch loss 0.550874829 batch mAP 0.667785645 batch PCKh 0.0625\n",
      "Trained batch 630 batch loss 0.573370039 batch mAP 0.593170166 batch PCKh 0.5\n",
      "Trained batch 631 batch loss 0.496594161 batch mAP 0.648773193 batch PCKh 0.8125\n",
      "Trained batch 632 batch loss 0.504998267 batch mAP 0.622253418 batch PCKh 0.375\n",
      "Trained batch 633 batch loss 0.498796284 batch mAP 0.677063 batch PCKh 0.375\n",
      "Trained batch 634 batch loss 0.542875051 batch mAP 0.654571533 batch PCKh 0.4375\n",
      "Trained batch 635 batch loss 0.480159 batch mAP 0.64743042 batch PCKh 0.625\n",
      "Trained batch 636 batch loss 0.465482414 batch mAP 0.660980225 batch PCKh 0.75\n",
      "Trained batch 637 batch loss 0.533522 batch mAP 0.615722656 batch PCKh 0.5\n",
      "Trained batch 638 batch loss 0.45186305 batch mAP 0.663085938 batch PCKh 0.375\n",
      "Trained batch 639 batch loss 0.55511111 batch mAP 0.572357178 batch PCKh 0.6875\n",
      "Trained batch 640 batch loss 0.479291409 batch mAP 0.631897 batch PCKh 0.625\n",
      "Trained batch 641 batch loss 0.505695105 batch mAP 0.652313232 batch PCKh 0.4375\n",
      "Trained batch 642 batch loss 0.424565762 batch mAP 0.662841797 batch PCKh 0.6875\n",
      "Trained batch 643 batch loss 0.509147286 batch mAP 0.686065674 batch PCKh 0.5\n",
      "Trained batch 644 batch loss 0.430603147 batch mAP 0.701538086 batch PCKh 0.5625\n",
      "Trained batch 645 batch loss 0.447694838 batch mAP 0.652526855 batch PCKh 0.3125\n",
      "Trained batch 646 batch loss 0.427491307 batch mAP 0.651794434 batch PCKh 0.5\n",
      "Trained batch 647 batch loss 0.480744064 batch mAP 0.662445068 batch PCKh 0.75\n",
      "Trained batch 648 batch loss 0.343723476 batch mAP 0.784484863 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 649 batch loss 0.46430552 batch mAP 0.642120361 batch PCKh 0.8125\n",
      "Trained batch 650 batch loss 0.469079852 batch mAP 0.637786865 batch PCKh 0.8125\n",
      "Trained batch 651 batch loss 0.507093787 batch mAP 0.601532 batch PCKh 0.5\n",
      "Trained batch 652 batch loss 0.503522456 batch mAP 0.557739258 batch PCKh 0.25\n",
      "Trained batch 653 batch loss 0.459950566 batch mAP 0.621337891 batch PCKh 0.75\n",
      "Trained batch 654 batch loss 0.363290191 batch mAP 0.643554688 batch PCKh 0.3125\n",
      "Trained batch 655 batch loss 0.49944821 batch mAP 0.59967041 batch PCKh 0.625\n",
      "Trained batch 656 batch loss 0.530557394 batch mAP 0.647827148 batch PCKh 0.375\n",
      "Trained batch 657 batch loss 0.471895635 batch mAP 0.682617188 batch PCKh 0.375\n",
      "Trained batch 658 batch loss 0.542778552 batch mAP 0.6121521 batch PCKh 0.5625\n",
      "Trained batch 659 batch loss 0.492165446 batch mAP 0.627502441 batch PCKh 0.375\n",
      "Trained batch 660 batch loss 0.440585107 batch mAP 0.594635 batch PCKh 0.1875\n",
      "Trained batch 661 batch loss 0.464312494 batch mAP 0.616546631 batch PCKh 0.625\n",
      "Trained batch 662 batch loss 0.387268 batch mAP 0.691925049 batch PCKh 0.375\n",
      "Trained batch 663 batch loss 0.508878529 batch mAP 0.712585449 batch PCKh 0.5\n",
      "Trained batch 664 batch loss 0.460890472 batch mAP 0.733917236 batch PCKh 0.8125\n",
      "Trained batch 665 batch loss 0.460348725 batch mAP 0.678192139 batch PCKh 0.5\n",
      "Trained batch 666 batch loss 0.381961167 batch mAP 0.725494385 batch PCKh 0.625\n",
      "Trained batch 667 batch loss 0.450204521 batch mAP 0.702392578 batch PCKh 0.375\n",
      "Trained batch 668 batch loss 0.445898 batch mAP 0.68359375 batch PCKh 0.4375\n",
      "Trained batch 669 batch loss 0.392920554 batch mAP 0.722076416 batch PCKh 0.5625\n",
      "Trained batch 670 batch loss 0.386960685 batch mAP 0.689880371 batch PCKh 0.6875\n",
      "Trained batch 671 batch loss 0.382588923 batch mAP 0.704559326 batch PCKh 0.75\n",
      "Trained batch 672 batch loss 0.399543852 batch mAP 0.678863525 batch PCKh 0.625\n",
      "Trained batch 673 batch loss 0.391116977 batch mAP 0.67678833 batch PCKh 0.75\n",
      "Trained batch 674 batch loss 0.449597061 batch mAP 0.659759521 batch PCKh 0.75\n",
      "Trained batch 675 batch loss 0.446520686 batch mAP 0.632995605 batch PCKh 0.6875\n",
      "Trained batch 676 batch loss 0.443565071 batch mAP 0.647125244 batch PCKh 0.6875\n",
      "Trained batch 677 batch loss 0.380642593 batch mAP 0.595336914 batch PCKh 0.5\n",
      "Trained batch 678 batch loss 0.405589819 batch mAP 0.69519043 batch PCKh 0.625\n",
      "Trained batch 679 batch loss 0.439979583 batch mAP 0.642944336 batch PCKh 0.375\n",
      "Trained batch 680 batch loss 0.516052365 batch mAP 0.55770874 batch PCKh 0.5625\n",
      "Trained batch 681 batch loss 0.400272071 batch mAP 0.634857178 batch PCKh 0.625\n",
      "Trained batch 682 batch loss 0.477710068 batch mAP 0.62902832 batch PCKh 0.6875\n",
      "Trained batch 683 batch loss 0.427152127 batch mAP 0.688781738 batch PCKh 0.5\n",
      "Trained batch 684 batch loss 0.417481184 batch mAP 0.682922363 batch PCKh 0.4375\n",
      "Trained batch 685 batch loss 0.549789429 batch mAP 0.639923096 batch PCKh 0.1875\n",
      "Trained batch 686 batch loss 0.451224834 batch mAP 0.661834717 batch PCKh 0.5\n",
      "Trained batch 687 batch loss 0.491287589 batch mAP 0.633392334 batch PCKh 0.3125\n",
      "Trained batch 688 batch loss 0.464668036 batch mAP 0.666015625 batch PCKh 0.75\n",
      "Trained batch 689 batch loss 0.562373757 batch mAP 0.670074463 batch PCKh 0.125\n",
      "Trained batch 690 batch loss 0.470059067 batch mAP 0.716095 batch PCKh 0.75\n",
      "Trained batch 691 batch loss 0.541826844 batch mAP 0.688659668 batch PCKh 0.5\n",
      "Trained batch 692 batch loss 0.607996583 batch mAP 0.654968262 batch PCKh 0.5\n",
      "Trained batch 693 batch loss 0.476792514 batch mAP 0.657592773 batch PCKh 0.5625\n",
      "Trained batch 694 batch loss 0.541838169 batch mAP 0.6015625 batch PCKh 0.125\n",
      "Trained batch 695 batch loss 0.609065413 batch mAP 0.589294434 batch PCKh 0.375\n",
      "Trained batch 696 batch loss 0.543772459 batch mAP 0.609283447 batch PCKh 0.1875\n",
      "Trained batch 697 batch loss 0.566857517 batch mAP 0.625671387 batch PCKh 0.375\n",
      "Trained batch 698 batch loss 0.501984239 batch mAP 0.611450195 batch PCKh 0.5\n",
      "Trained batch 699 batch loss 0.473172277 batch mAP 0.609039307 batch PCKh 0\n",
      "Trained batch 700 batch loss 0.428103209 batch mAP 0.656036377 batch PCKh 0.5\n",
      "Trained batch 701 batch loss 0.435826063 batch mAP 0.643493652 batch PCKh 0.625\n",
      "Trained batch 702 batch loss 0.43898347 batch mAP 0.616210938 batch PCKh 0\n",
      "Trained batch 703 batch loss 0.544412255 batch mAP 0.609100342 batch PCKh 0\n",
      "Trained batch 704 batch loss 0.557563066 batch mAP 0.628692627 batch PCKh 0.1875\n",
      "Trained batch 705 batch loss 0.552402496 batch mAP 0.604492188 batch PCKh 0.3125\n",
      "Trained batch 706 batch loss 0.58947295 batch mAP 0.63861084 batch PCKh 0.6875\n",
      "Trained batch 707 batch loss 0.506362379 batch mAP 0.603363037 batch PCKh 0.5625\n",
      "Trained batch 708 batch loss 0.566369176 batch mAP 0.61618042 batch PCKh 0.5625\n",
      "Trained batch 709 batch loss 0.580751 batch mAP 0.650238037 batch PCKh 0.75\n",
      "Trained batch 710 batch loss 0.557695746 batch mAP 0.590576172 batch PCKh 0.5625\n",
      "Trained batch 711 batch loss 0.466707289 batch mAP 0.608642578 batch PCKh 0.0625\n",
      "Trained batch 712 batch loss 0.559352636 batch mAP 0.619018555 batch PCKh 0.0625\n",
      "Trained batch 713 batch loss 0.513302803 batch mAP 0.678222656 batch PCKh 0.3125\n",
      "Trained batch 714 batch loss 0.509659708 batch mAP 0.64050293 batch PCKh 0.5\n",
      "Trained batch 715 batch loss 0.549341321 batch mAP 0.676361084 batch PCKh 0.25\n",
      "Trained batch 716 batch loss 0.48336494 batch mAP 0.63760376 batch PCKh 0.5\n",
      "Trained batch 717 batch loss 0.508211792 batch mAP 0.536438 batch PCKh 0.125\n",
      "Trained batch 718 batch loss 0.517015 batch mAP 0.520904541 batch PCKh 0.625\n",
      "Trained batch 719 batch loss 0.52151382 batch mAP 0.498504639 batch PCKh 0.4375\n",
      "Trained batch 720 batch loss 0.499079168 batch mAP 0.520904541 batch PCKh 0.125\n",
      "Trained batch 721 batch loss 0.484021664 batch mAP 0.538879395 batch PCKh 0.3125\n",
      "Trained batch 722 batch loss 0.465461195 batch mAP 0.602844238 batch PCKh 0.75\n",
      "Trained batch 723 batch loss 0.475412965 batch mAP 0.602783203 batch PCKh 0.4375\n",
      "Trained batch 724 batch loss 0.561172664 batch mAP 0.543670654 batch PCKh 0.625\n",
      "Trained batch 725 batch loss 0.528335273 batch mAP 0.598968506 batch PCKh 0.75\n",
      "Trained batch 726 batch loss 0.513861179 batch mAP 0.617980957 batch PCKh 0.75\n",
      "Trained batch 727 batch loss 0.497930169 batch mAP 0.63293457 batch PCKh 0.6875\n",
      "Trained batch 728 batch loss 0.525185883 batch mAP 0.601318359 batch PCKh 0.75\n",
      "Trained batch 729 batch loss 0.564555109 batch mAP 0.595794678 batch PCKh 0.75\n",
      "Trained batch 730 batch loss 0.470555246 batch mAP 0.636108398 batch PCKh 0.625\n",
      "Trained batch 731 batch loss 0.476570219 batch mAP 0.655914307 batch PCKh 0.6875\n",
      "Trained batch 732 batch loss 0.545302093 batch mAP 0.610107422 batch PCKh 0.4375\n",
      "Trained batch 733 batch loss 0.566225111 batch mAP 0.591064453 batch PCKh 0.375\n",
      "Trained batch 734 batch loss 0.59636867 batch mAP 0.584472656 batch PCKh 0.25\n",
      "Trained batch 735 batch loss 0.544429243 batch mAP 0.615386963 batch PCKh 0.4375\n",
      "Trained batch 736 batch loss 0.574421823 batch mAP 0.62991333 batch PCKh 0.3125\n",
      "Trained batch 737 batch loss 0.539040864 batch mAP 0.657043457 batch PCKh 0.3125\n",
      "Trained batch 738 batch loss 0.602863789 batch mAP 0.584136963 batch PCKh 0.75\n",
      "Trained batch 739 batch loss 0.594569504 batch mAP 0.570129395 batch PCKh 0.1875\n",
      "Trained batch 740 batch loss 0.581273735 batch mAP 0.650024414 batch PCKh 0.4375\n",
      "Trained batch 741 batch loss 0.485107481 batch mAP 0.661071777 batch PCKh 0.5\n",
      "Trained batch 742 batch loss 0.597005188 batch mAP 0.589477539 batch PCKh 0.8125\n",
      "Trained batch 743 batch loss 0.458032131 batch mAP 0.634307861 batch PCKh 0.75\n",
      "Trained batch 744 batch loss 0.504164 batch mAP 0.640136719 batch PCKh 0.5625\n",
      "Trained batch 745 batch loss 0.611261189 batch mAP 0.510650635 batch PCKh 0.0625\n",
      "Trained batch 746 batch loss 0.619155943 batch mAP 0.473114 batch PCKh 0.375\n",
      "Trained batch 747 batch loss 0.551584601 batch mAP 0.569549561 batch PCKh 0.4375\n",
      "Trained batch 748 batch loss 0.602399 batch mAP 0.496673584 batch PCKh 0.8125\n",
      "Trained batch 749 batch loss 0.516808391 batch mAP 0.472625732 batch PCKh 0.125\n",
      "Trained batch 750 batch loss 0.473796725 batch mAP 0.562164307 batch PCKh 0.4375\n",
      "Trained batch 751 batch loss 0.545643747 batch mAP 0.515472412 batch PCKh 0.5\n",
      "Trained batch 752 batch loss 0.508358479 batch mAP 0.568878174 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 753 batch loss 0.472565591 batch mAP 0.567810059 batch PCKh 0.4375\n",
      "Trained batch 754 batch loss 0.49876368 batch mAP 0.63482666 batch PCKh 0.0625\n",
      "Trained batch 755 batch loss 0.523500144 batch mAP 0.634094238 batch PCKh 0.75\n",
      "Trained batch 756 batch loss 0.473282427 batch mAP 0.556365967 batch PCKh 0.3125\n",
      "Trained batch 757 batch loss 0.456783682 batch mAP 0.534729 batch PCKh 0.125\n",
      "Trained batch 758 batch loss 0.602950335 batch mAP 0.534545898 batch PCKh 0.8125\n",
      "Trained batch 759 batch loss 0.508248508 batch mAP 0.62713623 batch PCKh 0.625\n",
      "Trained batch 760 batch loss 0.437233984 batch mAP 0.664367676 batch PCKh 0.875\n",
      "Trained batch 761 batch loss 0.469162583 batch mAP 0.653930664 batch PCKh 0.75\n",
      "Trained batch 762 batch loss 0.576794863 batch mAP 0.596740723 batch PCKh 0.625\n",
      "Trained batch 763 batch loss 0.553674698 batch mAP 0.610107422 batch PCKh 0.5625\n",
      "Trained batch 764 batch loss 0.562496781 batch mAP 0.620422363 batch PCKh 0.5\n",
      "Trained batch 765 batch loss 0.502460182 batch mAP 0.642395 batch PCKh 0.5\n",
      "Trained batch 766 batch loss 0.505323648 batch mAP 0.691650391 batch PCKh 0.75\n",
      "Trained batch 767 batch loss 0.557462037 batch mAP 0.643615723 batch PCKh 0.375\n",
      "Trained batch 768 batch loss 0.564819694 batch mAP 0.629058838 batch PCKh 0.25\n",
      "Trained batch 769 batch loss 0.560592294 batch mAP 0.654937744 batch PCKh 0.5\n",
      "Trained batch 770 batch loss 0.395163715 batch mAP 0.741027832 batch PCKh 0.5\n",
      "Trained batch 771 batch loss 0.409686089 batch mAP 0.695770264 batch PCKh 0.5625\n",
      "Trained batch 772 batch loss 0.485489666 batch mAP 0.694732666 batch PCKh 0.6875\n",
      "Trained batch 773 batch loss 0.415466338 batch mAP 0.619049072 batch PCKh 0.5\n",
      "Trained batch 774 batch loss 0.433263123 batch mAP 0.668548584 batch PCKh 0.4375\n",
      "Trained batch 775 batch loss 0.498279661 batch mAP 0.580993652 batch PCKh 0.5\n",
      "Trained batch 776 batch loss 0.406845391 batch mAP 0.570495605 batch PCKh 0.25\n",
      "Trained batch 777 batch loss 0.617914081 batch mAP 0.536499 batch PCKh 0.6875\n",
      "Trained batch 778 batch loss 0.506226897 batch mAP 0.61505127 batch PCKh 0.25\n",
      "Trained batch 779 batch loss 0.485571504 batch mAP 0.652709961 batch PCKh 0.25\n",
      "Trained batch 780 batch loss 0.492387354 batch mAP 0.694854736 batch PCKh 0.625\n",
      "Trained batch 781 batch loss 0.512310445 batch mAP 0.645599365 batch PCKh 0.6875\n",
      "Trained batch 782 batch loss 0.622278452 batch mAP 0.532440186 batch PCKh 0.375\n",
      "Trained batch 783 batch loss 0.626123309 batch mAP 0.587158203 batch PCKh 0.5\n",
      "Trained batch 784 batch loss 0.590053618 batch mAP 0.554748535 batch PCKh 0.1875\n",
      "Trained batch 785 batch loss 0.501548707 batch mAP 0.655944824 batch PCKh 0.3125\n",
      "Trained batch 786 batch loss 0.574692547 batch mAP 0.631195068 batch PCKh 0.25\n",
      "Trained batch 787 batch loss 0.53441757 batch mAP 0.663238525 batch PCKh 0.125\n",
      "Trained batch 788 batch loss 0.473055452 batch mAP 0.618377686 batch PCKh 0.1875\n",
      "Trained batch 789 batch loss 0.506204545 batch mAP 0.623657227 batch PCKh 0.8125\n",
      "Trained batch 790 batch loss 0.4143686 batch mAP 0.594055176 batch PCKh 0.1875\n",
      "Trained batch 791 batch loss 0.555803597 batch mAP 0.596893311 batch PCKh 0.625\n",
      "Trained batch 792 batch loss 0.515328467 batch mAP 0.654602051 batch PCKh 0.375\n",
      "Trained batch 793 batch loss 0.509535611 batch mAP 0.677764893 batch PCKh 0.875\n",
      "Trained batch 794 batch loss 0.502664149 batch mAP 0.653167725 batch PCKh 0.4375\n",
      "Trained batch 795 batch loss 0.488442659 batch mAP 0.621063232 batch PCKh 0.75\n",
      "Trained batch 796 batch loss 0.41404891 batch mAP 0.658233643 batch PCKh 0.5625\n",
      "Trained batch 797 batch loss 0.42786777 batch mAP 0.592926 batch PCKh 0.25\n",
      "Trained batch 798 batch loss 0.354559392 batch mAP 0.559143066 batch PCKh 0.4375\n",
      "Trained batch 799 batch loss 0.390786469 batch mAP 0.543457031 batch PCKh 0.625\n",
      "Trained batch 800 batch loss 0.412572056 batch mAP 0.576568604 batch PCKh 0.5625\n",
      "Trained batch 801 batch loss 0.449819267 batch mAP 0.605773926 batch PCKh 0.6875\n",
      "Trained batch 802 batch loss 0.524890065 batch mAP 0.592559814 batch PCKh 0.75\n",
      "Trained batch 803 batch loss 0.515598714 batch mAP 0.647857666 batch PCKh 0.5625\n",
      "Trained batch 804 batch loss 0.412691534 batch mAP 0.644073486 batch PCKh 0.75\n",
      "Trained batch 805 batch loss 0.38321507 batch mAP 0.64630127 batch PCKh 0.4375\n",
      "Trained batch 806 batch loss 0.411668718 batch mAP 0.652435303 batch PCKh 0.6875\n",
      "Trained batch 807 batch loss 0.42484951 batch mAP 0.690032959 batch PCKh 0.4375\n",
      "Trained batch 808 batch loss 0.426694393 batch mAP 0.678375244 batch PCKh 0.75\n",
      "Trained batch 809 batch loss 0.471693665 batch mAP 0.608947754 batch PCKh 0.6875\n",
      "Trained batch 810 batch loss 0.464843363 batch mAP 0.591339111 batch PCKh 0.625\n",
      "Trained batch 811 batch loss 0.534044 batch mAP 0.530181885 batch PCKh 0.625\n",
      "Trained batch 812 batch loss 0.523493767 batch mAP 0.540527344 batch PCKh 0.6875\n",
      "Trained batch 813 batch loss 0.517434299 batch mAP 0.540313721 batch PCKh 0.6875\n",
      "Trained batch 814 batch loss 0.423329175 batch mAP 0.638946533 batch PCKh 0.75\n",
      "Trained batch 815 batch loss 0.44130829 batch mAP 0.573455811 batch PCKh 0.625\n",
      "Trained batch 816 batch loss 0.483566374 batch mAP 0.641296387 batch PCKh 0.75\n",
      "Trained batch 817 batch loss 0.448792398 batch mAP 0.656005859 batch PCKh 0.625\n",
      "Trained batch 818 batch loss 0.558372617 batch mAP 0.596740723 batch PCKh 0.4375\n",
      "Trained batch 819 batch loss 0.57740438 batch mAP 0.591461182 batch PCKh 0.6875\n",
      "Trained batch 820 batch loss 0.611772656 batch mAP 0.65322876 batch PCKh 0.3125\n",
      "Trained batch 821 batch loss 0.560985088 batch mAP 0.67453 batch PCKh 0.0625\n",
      "Trained batch 822 batch loss 0.444576621 batch mAP 0.684845 batch PCKh 0.25\n",
      "Trained batch 823 batch loss 0.471189559 batch mAP 0.582183838 batch PCKh 0.3125\n",
      "Trained batch 824 batch loss 0.403770506 batch mAP 0.650482178 batch PCKh 0.25\n",
      "Trained batch 825 batch loss 0.432019681 batch mAP 0.587615967 batch PCKh 0.375\n",
      "Trained batch 826 batch loss 0.486608028 batch mAP 0.534484863 batch PCKh 0.75\n",
      "Trained batch 827 batch loss 0.376206458 batch mAP 0.637268066 batch PCKh 0.625\n",
      "Trained batch 828 batch loss 0.495280445 batch mAP 0.542999268 batch PCKh 0.6875\n",
      "Trained batch 829 batch loss 0.502925873 batch mAP 0.576599121 batch PCKh 0.5625\n",
      "Trained batch 830 batch loss 0.52681452 batch mAP 0.579223633 batch PCKh 0.375\n",
      "Trained batch 831 batch loss 0.561452627 batch mAP 0.526031494 batch PCKh 0.375\n",
      "Trained batch 832 batch loss 0.504322886 batch mAP 0.592285156 batch PCKh 0.625\n",
      "Trained batch 833 batch loss 0.466410458 batch mAP 0.655517578 batch PCKh 0.375\n",
      "Trained batch 834 batch loss 0.529488862 batch mAP 0.580627441 batch PCKh 0.6875\n",
      "Trained batch 835 batch loss 0.524586797 batch mAP 0.56060791 batch PCKh 0.5625\n",
      "Trained batch 836 batch loss 0.516139925 batch mAP 0.60635376 batch PCKh 0.875\n",
      "Trained batch 837 batch loss 0.477443397 batch mAP 0.627227783 batch PCKh 0.75\n",
      "Trained batch 838 batch loss 0.429569 batch mAP 0.601806641 batch PCKh 0.1875\n",
      "Trained batch 839 batch loss 0.442762583 batch mAP 0.590301514 batch PCKh 0.5\n",
      "Trained batch 840 batch loss 0.526809 batch mAP 0.534881592 batch PCKh 0.4375\n",
      "Trained batch 841 batch loss 0.499072552 batch mAP 0.54083252 batch PCKh 0.75\n",
      "Trained batch 842 batch loss 0.513913929 batch mAP 0.621276855 batch PCKh 0.5\n",
      "Trained batch 843 batch loss 0.516047239 batch mAP 0.638549805 batch PCKh 0.375\n",
      "Trained batch 844 batch loss 0.534804702 batch mAP 0.694152832 batch PCKh 0.375\n",
      "Trained batch 845 batch loss 0.40823406 batch mAP 0.725952148 batch PCKh 0.6875\n",
      "Trained batch 846 batch loss 0.447091103 batch mAP 0.680542 batch PCKh 0.6875\n",
      "Trained batch 847 batch loss 0.372923076 batch mAP 0.728179932 batch PCKh 0.625\n",
      "Trained batch 848 batch loss 0.395288676 batch mAP 0.719909668 batch PCKh 0.75\n",
      "Trained batch 849 batch loss 0.369150788 batch mAP 0.642364502 batch PCKh 0.625\n",
      "Trained batch 850 batch loss 0.485895 batch mAP 0.623626709 batch PCKh 0.1875\n",
      "Trained batch 851 batch loss 0.471278787 batch mAP 0.656433105 batch PCKh 0.4375\n",
      "Trained batch 852 batch loss 0.519486427 batch mAP 0.569213867 batch PCKh 0.375\n",
      "Trained batch 853 batch loss 0.470547527 batch mAP 0.582824707 batch PCKh 0.375\n",
      "Trained batch 854 batch loss 0.526921928 batch mAP 0.606842041 batch PCKh 0.625\n",
      "Trained batch 855 batch loss 0.479708552 batch mAP 0.59954834 batch PCKh 0.25\n",
      "Trained batch 856 batch loss 0.489949465 batch mAP 0.612640381 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 857 batch loss 0.409179896 batch mAP 0.622467041 batch PCKh 0.8125\n",
      "Trained batch 858 batch loss 0.417782 batch mAP 0.639953613 batch PCKh 0.875\n",
      "Trained batch 859 batch loss 0.368992656 batch mAP 0.722808838 batch PCKh 0.625\n",
      "Trained batch 860 batch loss 0.430216789 batch mAP 0.701812744 batch PCKh 0.4375\n",
      "Trained batch 861 batch loss 0.357996941 batch mAP 0.713501 batch PCKh 0.6875\n",
      "Trained batch 862 batch loss 0.366876066 batch mAP 0.724121094 batch PCKh 0.875\n",
      "Trained batch 863 batch loss 0.363889605 batch mAP 0.732116699 batch PCKh 0.875\n",
      "Trained batch 864 batch loss 0.414052129 batch mAP 0.719726562 batch PCKh 0.625\n",
      "Trained batch 865 batch loss 0.309856921 batch mAP 0.721466064 batch PCKh 0.75\n",
      "Trained batch 866 batch loss 0.409151405 batch mAP 0.666870117 batch PCKh 0.8125\n",
      "Trained batch 867 batch loss 0.474793911 batch mAP 0.622711182 batch PCKh 0.875\n",
      "Trained batch 868 batch loss 0.478500605 batch mAP 0.637878418 batch PCKh 0.75\n",
      "Trained batch 869 batch loss 0.501320601 batch mAP 0.587921143 batch PCKh 0.125\n",
      "Trained batch 870 batch loss 0.452789187 batch mAP 0.615722656 batch PCKh 0.5625\n",
      "Trained batch 871 batch loss 0.499969244 batch mAP 0.508178711 batch PCKh 0.75\n",
      "Trained batch 872 batch loss 0.478419483 batch mAP 0.587097168 batch PCKh 0.3125\n",
      "Trained batch 873 batch loss 0.49351564 batch mAP 0.614776611 batch PCKh 0.6875\n",
      "Trained batch 874 batch loss 0.514858365 batch mAP 0.618469238 batch PCKh 0.8125\n",
      "Trained batch 875 batch loss 0.460942686 batch mAP 0.636535645 batch PCKh 0.375\n",
      "Trained batch 876 batch loss 0.507552624 batch mAP 0.593841553 batch PCKh 0.75\n",
      "Trained batch 877 batch loss 0.568610549 batch mAP 0.535461426 batch PCKh 0.4375\n",
      "Trained batch 878 batch loss 0.503687501 batch mAP 0.595214844 batch PCKh 0.25\n",
      "Trained batch 879 batch loss 0.465018719 batch mAP 0.531982422 batch PCKh 0.6875\n",
      "Trained batch 880 batch loss 0.519948483 batch mAP 0.544372559 batch PCKh 0.75\n",
      "Trained batch 881 batch loss 0.455508232 batch mAP 0.578399658 batch PCKh 0.75\n",
      "Trained batch 882 batch loss 0.530755639 batch mAP 0.61932373 batch PCKh 0.5\n",
      "Trained batch 883 batch loss 0.450911224 batch mAP 0.581390381 batch PCKh 0.375\n",
      "Trained batch 884 batch loss 0.387208283 batch mAP 0.624969482 batch PCKh 0.6875\n",
      "Trained batch 885 batch loss 0.398140848 batch mAP 0.602600098 batch PCKh 0.3125\n",
      "Trained batch 886 batch loss 0.392390072 batch mAP 0.598571777 batch PCKh 0.625\n",
      "Trained batch 887 batch loss 0.371631354 batch mAP 0.636505127 batch PCKh 0.75\n",
      "Trained batch 888 batch loss 0.356590271 batch mAP 0.647064209 batch PCKh 0.875\n",
      "Trained batch 889 batch loss 0.415778697 batch mAP 0.65045166 batch PCKh 0.6875\n",
      "Trained batch 890 batch loss 0.468691081 batch mAP 0.628997803 batch PCKh 0.875\n",
      "Trained batch 891 batch loss 0.398457557 batch mAP 0.585205078 batch PCKh 0.75\n",
      "Trained batch 892 batch loss 0.413185805 batch mAP 0.640319824 batch PCKh 0.75\n",
      "Trained batch 893 batch loss 0.478513718 batch mAP 0.64541626 batch PCKh 0.6875\n",
      "Trained batch 894 batch loss 0.46808368 batch mAP 0.646118164 batch PCKh 0.5625\n",
      "Trained batch 895 batch loss 0.563729525 batch mAP 0.641937256 batch PCKh 0.875\n",
      "Trained batch 896 batch loss 0.397173822 batch mAP 0.616912842 batch PCKh 0.375\n",
      "Trained batch 897 batch loss 0.482196391 batch mAP 0.659576416 batch PCKh 0.75\n",
      "Trained batch 898 batch loss 0.406623095 batch mAP 0.660430908 batch PCKh 0.75\n",
      "Trained batch 899 batch loss 0.486187935 batch mAP 0.650085449 batch PCKh 0.25\n",
      "Trained batch 900 batch loss 0.450469762 batch mAP 0.671966553 batch PCKh 0.6875\n",
      "Trained batch 901 batch loss 0.38910532 batch mAP 0.711578369 batch PCKh 0.6875\n",
      "Trained batch 902 batch loss 0.443620682 batch mAP 0.657440186 batch PCKh 0.6875\n",
      "Trained batch 903 batch loss 0.402339309 batch mAP 0.663482666 batch PCKh 0.375\n",
      "Trained batch 904 batch loss 0.536067545 batch mAP 0.663696289 batch PCKh 0.6875\n",
      "Trained batch 905 batch loss 0.541196704 batch mAP 0.625671387 batch PCKh 0.5625\n",
      "Trained batch 906 batch loss 0.575211167 batch mAP 0.611053467 batch PCKh 0.75\n",
      "Trained batch 907 batch loss 0.597184777 batch mAP 0.623840332 batch PCKh 0.4375\n",
      "Trained batch 908 batch loss 0.571752429 batch mAP 0.674621582 batch PCKh 0.75\n",
      "Trained batch 909 batch loss 0.505513787 batch mAP 0.666717529 batch PCKh 0.375\n",
      "Trained batch 910 batch loss 0.453977 batch mAP 0.649169922 batch PCKh 0.5625\n",
      "Trained batch 911 batch loss 0.547753096 batch mAP 0.614501953 batch PCKh 0.125\n",
      "Trained batch 912 batch loss 0.445991576 batch mAP 0.674743652 batch PCKh 0.875\n",
      "Trained batch 913 batch loss 0.430105805 batch mAP 0.695739746 batch PCKh 0.75\n",
      "Trained batch 914 batch loss 0.496851444 batch mAP 0.633636475 batch PCKh 0.75\n",
      "Trained batch 915 batch loss 0.616151214 batch mAP 0.544677734 batch PCKh 0.5\n",
      "Trained batch 916 batch loss 0.512293458 batch mAP 0.590545654 batch PCKh 0.75\n",
      "Trained batch 917 batch loss 0.428731471 batch mAP 0.652496338 batch PCKh 0.75\n",
      "Trained batch 918 batch loss 0.502150059 batch mAP 0.628082275 batch PCKh 0.625\n",
      "Trained batch 919 batch loss 0.557276487 batch mAP 0.562316895 batch PCKh 0.125\n",
      "Trained batch 920 batch loss 0.573663 batch mAP 0.57333374 batch PCKh 0.4375\n",
      "Trained batch 921 batch loss 0.580386698 batch mAP 0.599090576 batch PCKh 0.5625\n",
      "Trained batch 922 batch loss 0.53872323 batch mAP 0.537200928 batch PCKh 0.3125\n",
      "Trained batch 923 batch loss 0.617988288 batch mAP 0.585784912 batch PCKh 0.625\n",
      "Trained batch 924 batch loss 0.673664212 batch mAP 0.553466797 batch PCKh 0.4375\n",
      "Trained batch 925 batch loss 0.677019298 batch mAP 0.576629639 batch PCKh 0.625\n",
      "Trained batch 926 batch loss 0.559583366 batch mAP 0.520904541 batch PCKh 0.1875\n",
      "Trained batch 927 batch loss 0.455544293 batch mAP 0.52948 batch PCKh 0.3125\n",
      "Trained batch 928 batch loss 0.439376533 batch mAP 0.517425537 batch PCKh 0.3125\n",
      "Trained batch 929 batch loss 0.389272541 batch mAP 0.602020264 batch PCKh 0.25\n",
      "Trained batch 930 batch loss 0.54099679 batch mAP 0.607818604 batch PCKh 0.3125\n",
      "Trained batch 931 batch loss 0.536832273 batch mAP 0.635742188 batch PCKh 0.1875\n",
      "Trained batch 932 batch loss 0.494180173 batch mAP 0.655761719 batch PCKh 0.625\n",
      "Trained batch 933 batch loss 0.452514619 batch mAP 0.631256104 batch PCKh 0.5\n",
      "Trained batch 934 batch loss 0.431953639 batch mAP 0.655609131 batch PCKh 0\n",
      "Trained batch 935 batch loss 0.472773433 batch mAP 0.642486572 batch PCKh 0.4375\n",
      "Trained batch 936 batch loss 0.388757527 batch mAP 0.731323242 batch PCKh 0.5\n",
      "Trained batch 937 batch loss 0.403957188 batch mAP 0.713134766 batch PCKh 0.625\n",
      "Trained batch 938 batch loss 0.436781615 batch mAP 0.708221436 batch PCKh 0.625\n",
      "Trained batch 939 batch loss 0.401659071 batch mAP 0.742675781 batch PCKh 0.375\n",
      "Trained batch 940 batch loss 0.491403133 batch mAP 0.658691406 batch PCKh 0.375\n",
      "Trained batch 941 batch loss 0.456783 batch mAP 0.664611816 batch PCKh 0.4375\n",
      "Trained batch 942 batch loss 0.379999131 batch mAP 0.749572754 batch PCKh 0.5\n",
      "Trained batch 943 batch loss 0.363650918 batch mAP 0.733978271 batch PCKh 0.4375\n",
      "Trained batch 944 batch loss 0.384256899 batch mAP 0.716156 batch PCKh 0.75\n",
      "Trained batch 945 batch loss 0.38378793 batch mAP 0.73034668 batch PCKh 0.375\n",
      "Trained batch 946 batch loss 0.39142549 batch mAP 0.688446045 batch PCKh 0.5\n",
      "Trained batch 947 batch loss 0.519033968 batch mAP 0.642669678 batch PCKh 0.125\n",
      "Trained batch 948 batch loss 0.486638725 batch mAP 0.65133667 batch PCKh 0.625\n",
      "Trained batch 949 batch loss 0.503233254 batch mAP 0.615539551 batch PCKh 0.4375\n",
      "Trained batch 950 batch loss 0.514555335 batch mAP 0.675323486 batch PCKh 0.25\n",
      "Trained batch 951 batch loss 0.326917827 batch mAP 0.76574707 batch PCKh 0.5\n",
      "Trained batch 952 batch loss 0.441745758 batch mAP 0.66595459 batch PCKh 0.4375\n",
      "Trained batch 953 batch loss 0.460884213 batch mAP 0.668151855 batch PCKh 0.1875\n",
      "Trained batch 954 batch loss 0.432215691 batch mAP 0.667816162 batch PCKh 0.375\n",
      "Trained batch 955 batch loss 0.434788018 batch mAP 0.647796631 batch PCKh 0.5\n",
      "Trained batch 956 batch loss 0.515138626 batch mAP 0.626190186 batch PCKh 0.4375\n",
      "Trained batch 957 batch loss 0.508174181 batch mAP 0.656768799 batch PCKh 0.5625\n",
      "Trained batch 958 batch loss 0.498164117 batch mAP 0.692840576 batch PCKh 0.5\n",
      "Trained batch 959 batch loss 0.474528491 batch mAP 0.727508545 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 960 batch loss 0.528922319 batch mAP 0.670166 batch PCKh 0.3125\n",
      "Trained batch 961 batch loss 0.543489814 batch mAP 0.657928467 batch PCKh 0.375\n",
      "Trained batch 962 batch loss 0.47275722 batch mAP 0.72644043 batch PCKh 0.5\n",
      "Trained batch 963 batch loss 0.525202036 batch mAP 0.687255859 batch PCKh 0.3125\n",
      "Trained batch 964 batch loss 0.624677658 batch mAP 0.571685791 batch PCKh 0.25\n",
      "Trained batch 965 batch loss 0.613327861 batch mAP 0.581695557 batch PCKh 0.125\n",
      "Trained batch 966 batch loss 0.62133193 batch mAP 0.583557129 batch PCKh 0.3125\n",
      "Trained batch 967 batch loss 0.511110723 batch mAP 0.693664551 batch PCKh 0.5\n",
      "Trained batch 968 batch loss 0.527403057 batch mAP 0.648895264 batch PCKh 0.625\n",
      "Trained batch 969 batch loss 0.492966056 batch mAP 0.529266357 batch PCKh 0.125\n",
      "Trained batch 970 batch loss 0.516468465 batch mAP 0.561004639 batch PCKh 0.3125\n",
      "Trained batch 971 batch loss 0.572828352 batch mAP 0.495422363 batch PCKh 0.25\n",
      "Trained batch 972 batch loss 0.616837382 batch mAP 0.443450928 batch PCKh 0.5\n",
      "Trained batch 973 batch loss 0.519656718 batch mAP 0.526641846 batch PCKh 0.5625\n",
      "Trained batch 974 batch loss 0.493905932 batch mAP 0.536346436 batch PCKh 0.5\n",
      "Trained batch 975 batch loss 0.439913958 batch mAP 0.585235596 batch PCKh 0.25\n",
      "Trained batch 976 batch loss 0.497792155 batch mAP 0.603515625 batch PCKh 0.8125\n",
      "Trained batch 977 batch loss 0.557550371 batch mAP 0.513183594 batch PCKh 0.4375\n",
      "Trained batch 978 batch loss 0.504624605 batch mAP 0.585174561 batch PCKh 0.4375\n",
      "Trained batch 979 batch loss 0.508202136 batch mAP 0.607208252 batch PCKh 0.5\n",
      "Trained batch 980 batch loss 0.491953969 batch mAP 0.592803955 batch PCKh 0.625\n",
      "Trained batch 981 batch loss 0.373650759 batch mAP 0.694671631 batch PCKh 0.1875\n",
      "Trained batch 982 batch loss 0.520929217 batch mAP 0.655059814 batch PCKh 0.4375\n",
      "Trained batch 983 batch loss 0.522592783 batch mAP 0.599487305 batch PCKh 0.5625\n",
      "Trained batch 984 batch loss 0.477769583 batch mAP 0.589386 batch PCKh 0.6875\n",
      "Trained batch 985 batch loss 0.499262512 batch mAP 0.683197 batch PCKh 0.6875\n",
      "Trained batch 986 batch loss 0.521559 batch mAP 0.622131348 batch PCKh 0.6875\n",
      "Trained batch 987 batch loss 0.53152442 batch mAP 0.615722656 batch PCKh 0.4375\n",
      "Trained batch 988 batch loss 0.572936296 batch mAP 0.614044189 batch PCKh 0.625\n",
      "Trained batch 989 batch loss 0.478214502 batch mAP 0.680328369 batch PCKh 0.25\n",
      "Trained batch 990 batch loss 0.430388868 batch mAP 0.668212891 batch PCKh 0.375\n",
      "Trained batch 991 batch loss 0.533076 batch mAP 0.703186035 batch PCKh 0.25\n",
      "Trained batch 992 batch loss 0.506906152 batch mAP 0.730987549 batch PCKh 0.4375\n",
      "Trained batch 993 batch loss 0.500828743 batch mAP 0.67086792 batch PCKh 0.8125\n",
      "Trained batch 994 batch loss 0.445100427 batch mAP 0.739074707 batch PCKh 0.5625\n",
      "Trained batch 995 batch loss 0.568809569 batch mAP 0.634033203 batch PCKh 0.5\n",
      "Trained batch 996 batch loss 0.53437531 batch mAP 0.617523193 batch PCKh 0.3125\n",
      "Trained batch 997 batch loss 0.557144046 batch mAP 0.537536621 batch PCKh 0.25\n",
      "Trained batch 998 batch loss 0.506247401 batch mAP 0.560333252 batch PCKh 0.75\n",
      "Trained batch 999 batch loss 0.471527964 batch mAP 0.566925049 batch PCKh 0.3125\n",
      "Trained batch 1000 batch loss 0.506351 batch mAP 0.606414795 batch PCKh 0.5\n",
      "Trained batch 1001 batch loss 0.469206214 batch mAP 0.586578369 batch PCKh 0.6875\n",
      "Trained batch 1002 batch loss 0.477471 batch mAP 0.539032 batch PCKh 0.8125\n",
      "Trained batch 1003 batch loss 0.471769452 batch mAP 0.578216553 batch PCKh 0.6875\n",
      "Trained batch 1004 batch loss 0.46383661 batch mAP 0.545776367 batch PCKh 0.5625\n",
      "Trained batch 1005 batch loss 0.572165966 batch mAP 0.477783203 batch PCKh 0.125\n",
      "Trained batch 1006 batch loss 0.567039609 batch mAP 0.503295898 batch PCKh 0.625\n",
      "Trained batch 1007 batch loss 0.427119 batch mAP 0.561798096 batch PCKh 0.625\n",
      "Trained batch 1008 batch loss 0.364253163 batch mAP 0.55770874 batch PCKh 0.4375\n",
      "Trained batch 1009 batch loss 0.437155455 batch mAP 0.57019043 batch PCKh 0.3125\n",
      "Trained batch 1010 batch loss 0.432365119 batch mAP 0.612579346 batch PCKh 0.75\n",
      "Trained batch 1011 batch loss 0.376638412 batch mAP 0.704925537 batch PCKh 0.6875\n",
      "Trained batch 1012 batch loss 0.474275768 batch mAP 0.687988281 batch PCKh 0.8125\n",
      "Trained batch 1013 batch loss 0.497910708 batch mAP 0.666137695 batch PCKh 0.4375\n",
      "Trained batch 1014 batch loss 0.597629547 batch mAP 0.61895752 batch PCKh 0.5625\n",
      "Trained batch 1015 batch loss 0.511869967 batch mAP 0.644073486 batch PCKh 0.3125\n",
      "Trained batch 1016 batch loss 0.552939892 batch mAP 0.581451416 batch PCKh 0.8125\n",
      "Trained batch 1017 batch loss 0.608863652 batch mAP 0.536315918 batch PCKh 0.0625\n",
      "Trained batch 1018 batch loss 0.568937957 batch mAP 0.567749 batch PCKh 0.625\n",
      "Trained batch 1019 batch loss 0.482246161 batch mAP 0.67453 batch PCKh 0.625\n",
      "Trained batch 1020 batch loss 0.521678746 batch mAP 0.646728516 batch PCKh 0.4375\n",
      "Trained batch 1021 batch loss 0.626141369 batch mAP 0.565612793 batch PCKh 0.375\n",
      "Trained batch 1022 batch loss 0.544645 batch mAP 0.638519287 batch PCKh 0.6875\n",
      "Trained batch 1023 batch loss 0.56198132 batch mAP 0.662750244 batch PCKh 0.625\n",
      "Trained batch 1024 batch loss 0.550637424 batch mAP 0.619262695 batch PCKh 0.3125\n",
      "Trained batch 1025 batch loss 0.519896209 batch mAP 0.648590088 batch PCKh 0.375\n",
      "Trained batch 1026 batch loss 0.523297191 batch mAP 0.582855225 batch PCKh 0.375\n",
      "Trained batch 1027 batch loss 0.50111711 batch mAP 0.601837158 batch PCKh 0.875\n",
      "Trained batch 1028 batch loss 0.525369346 batch mAP 0.592254639 batch PCKh 0.8125\n",
      "Trained batch 1029 batch loss 0.487423778 batch mAP 0.637237549 batch PCKh 0.625\n",
      "Trained batch 1030 batch loss 0.515888274 batch mAP 0.594970703 batch PCKh 0.375\n",
      "Trained batch 1031 batch loss 0.565316379 batch mAP 0.604919434 batch PCKh 0.75\n",
      "Trained batch 1032 batch loss 0.576477408 batch mAP 0.585388184 batch PCKh 0.5625\n",
      "Trained batch 1033 batch loss 0.517900825 batch mAP 0.571167 batch PCKh 0.4375\n",
      "Trained batch 1034 batch loss 0.553929687 batch mAP 0.571563721 batch PCKh 0.6875\n",
      "Trained batch 1035 batch loss 0.591564238 batch mAP 0.628265381 batch PCKh 0.875\n",
      "Trained batch 1036 batch loss 0.535744131 batch mAP 0.542938232 batch PCKh 0.4375\n",
      "Trained batch 1037 batch loss 0.532890677 batch mAP 0.599273682 batch PCKh 0.5625\n",
      "Trained batch 1038 batch loss 0.49426952 batch mAP 0.591003418 batch PCKh 0.625\n",
      "Trained batch 1039 batch loss 0.597597122 batch mAP 0.545257568 batch PCKh 0.375\n",
      "Trained batch 1040 batch loss 0.548638821 batch mAP 0.532592773 batch PCKh 0.25\n",
      "Trained batch 1041 batch loss 0.488335252 batch mAP 0.599700928 batch PCKh 0.5625\n",
      "Trained batch 1042 batch loss 0.527023673 batch mAP 0.528747559 batch PCKh 0.4375\n",
      "Trained batch 1043 batch loss 0.550389528 batch mAP 0.546569824 batch PCKh 0.25\n",
      "Trained batch 1044 batch loss 0.484185487 batch mAP 0.560394287 batch PCKh 0.8125\n",
      "Trained batch 1045 batch loss 0.533072591 batch mAP 0.559753418 batch PCKh 0.125\n",
      "Trained batch 1046 batch loss 0.445333928 batch mAP 0.57119751 batch PCKh 0.625\n",
      "Trained batch 1047 batch loss 0.416881979 batch mAP 0.657562256 batch PCKh 0.4375\n",
      "Trained batch 1048 batch loss 0.395252705 batch mAP 0.604064941 batch PCKh 0.3125\n",
      "Trained batch 1049 batch loss 0.428466171 batch mAP 0.612731934 batch PCKh 0.5\n",
      "Trained batch 1050 batch loss 0.402638167 batch mAP 0.628936768 batch PCKh 0.375\n",
      "Trained batch 1051 batch loss 0.461764574 batch mAP 0.684204102 batch PCKh 0.4375\n",
      "Trained batch 1052 batch loss 0.457149029 batch mAP 0.648284912 batch PCKh 0.375\n",
      "Trained batch 1053 batch loss 0.449990422 batch mAP 0.686157227 batch PCKh 0.375\n",
      "Trained batch 1054 batch loss 0.470347941 batch mAP 0.681884766 batch PCKh 0.4375\n",
      "Trained batch 1055 batch loss 0.407608 batch mAP 0.683929443 batch PCKh 0.75\n",
      "Trained batch 1056 batch loss 0.335262895 batch mAP 0.736084 batch PCKh 0.8125\n",
      "Trained batch 1057 batch loss 0.466872871 batch mAP 0.714263916 batch PCKh 0.875\n",
      "Trained batch 1058 batch loss 0.460700333 batch mAP 0.681945801 batch PCKh 0.625\n",
      "Trained batch 1059 batch loss 0.444429249 batch mAP 0.659484863 batch PCKh 0.3125\n",
      "Trained batch 1060 batch loss 0.42207554 batch mAP 0.649902344 batch PCKh 0.25\n",
      "Trained batch 1061 batch loss 0.493769407 batch mAP 0.656890869 batch PCKh 0.25\n",
      "Trained batch 1062 batch loss 0.481501102 batch mAP 0.652435303 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1063 batch loss 0.46888113 batch mAP 0.671630859 batch PCKh 0.3125\n",
      "Trained batch 1064 batch loss 0.391388357 batch mAP 0.651001 batch PCKh 0.1875\n",
      "Trained batch 1065 batch loss 0.392731458 batch mAP 0.608795166 batch PCKh 0.25\n",
      "Trained batch 1066 batch loss 0.489182413 batch mAP 0.585113525 batch PCKh 0.5\n",
      "Trained batch 1067 batch loss 0.490558505 batch mAP 0.598938 batch PCKh 0.5\n",
      "Trained batch 1068 batch loss 0.4730165 batch mAP 0.672058105 batch PCKh 0.25\n",
      "Trained batch 1069 batch loss 0.518920124 batch mAP 0.645324707 batch PCKh 0.75\n",
      "Trained batch 1070 batch loss 0.436846405 batch mAP 0.675415039 batch PCKh 0.3125\n",
      "Trained batch 1071 batch loss 0.523647428 batch mAP 0.674469 batch PCKh 0.5\n",
      "Trained batch 1072 batch loss 0.491007328 batch mAP 0.617034912 batch PCKh 0.3125\n",
      "Trained batch 1073 batch loss 0.521860063 batch mAP 0.645172119 batch PCKh 0.25\n",
      "Trained batch 1074 batch loss 0.542200804 batch mAP 0.606964111 batch PCKh 0.25\n",
      "Trained batch 1075 batch loss 0.546780586 batch mAP 0.609405518 batch PCKh 0\n",
      "Trained batch 1076 batch loss 0.484598696 batch mAP 0.616577148 batch PCKh 0.5625\n",
      "Trained batch 1077 batch loss 0.440650195 batch mAP 0.586761475 batch PCKh 0.5\n",
      "Trained batch 1078 batch loss 0.467560589 batch mAP 0.538757324 batch PCKh 0.5625\n",
      "Trained batch 1079 batch loss 0.501429915 batch mAP 0.5987854 batch PCKh 0.6875\n",
      "Trained batch 1080 batch loss 0.429393679 batch mAP 0.661773682 batch PCKh 0.75\n",
      "Trained batch 1081 batch loss 0.533861756 batch mAP 0.573974609 batch PCKh 0.4375\n",
      "Trained batch 1082 batch loss 0.458687693 batch mAP 0.628265381 batch PCKh 0.75\n",
      "Trained batch 1083 batch loss 0.453480422 batch mAP 0.669494629 batch PCKh 0.5\n",
      "Trained batch 1084 batch loss 0.394113123 batch mAP 0.65524292 batch PCKh 0.375\n",
      "Trained batch 1085 batch loss 0.479421914 batch mAP 0.619506836 batch PCKh 0.75\n",
      "Trained batch 1086 batch loss 0.49007082 batch mAP 0.66494751 batch PCKh 0.25\n",
      "Trained batch 1087 batch loss 0.503140211 batch mAP 0.648407 batch PCKh 0.1875\n",
      "Trained batch 1088 batch loss 0.524876 batch mAP 0.671630859 batch PCKh 0.3125\n",
      "Trained batch 1089 batch loss 0.474891961 batch mAP 0.720428467 batch PCKh 0.8125\n",
      "Trained batch 1090 batch loss 0.544832647 batch mAP 0.700744629 batch PCKh 0.6875\n",
      "Trained batch 1091 batch loss 0.539162457 batch mAP 0.645233154 batch PCKh 0.625\n",
      "Trained batch 1092 batch loss 0.564119577 batch mAP 0.638702393 batch PCKh 0.6875\n",
      "Trained batch 1093 batch loss 0.520186961 batch mAP 0.683288574 batch PCKh 0.3125\n",
      "Trained batch 1094 batch loss 0.541334033 batch mAP 0.638763428 batch PCKh 0.3125\n",
      "Trained batch 1095 batch loss 0.514130831 batch mAP 0.666717529 batch PCKh 0.625\n",
      "Trained batch 1096 batch loss 0.511218905 batch mAP 0.70123291 batch PCKh 0.375\n",
      "Trained batch 1097 batch loss 0.503891528 batch mAP 0.654968262 batch PCKh 0.375\n",
      "Trained batch 1098 batch loss 0.483035862 batch mAP 0.666473389 batch PCKh 0.5\n",
      "Trained batch 1099 batch loss 0.479770392 batch mAP 0.661560059 batch PCKh 0.4375\n",
      "Trained batch 1100 batch loss 0.523878932 batch mAP 0.621276855 batch PCKh 0.1875\n",
      "Trained batch 1101 batch loss 0.475624323 batch mAP 0.655639648 batch PCKh 0.8125\n",
      "Trained batch 1102 batch loss 0.405455679 batch mAP 0.664764404 batch PCKh 0.5\n",
      "Trained batch 1103 batch loss 0.479310542 batch mAP 0.693206787 batch PCKh 0.875\n",
      "Trained batch 1104 batch loss 0.432716072 batch mAP 0.703430176 batch PCKh 0.5625\n",
      "Trained batch 1105 batch loss 0.428780466 batch mAP 0.651702881 batch PCKh 0.5625\n",
      "Trained batch 1106 batch loss 0.418542176 batch mAP 0.677948 batch PCKh 0.5\n",
      "Trained batch 1107 batch loss 0.516473889 batch mAP 0.675933838 batch PCKh 0.5\n",
      "Trained batch 1108 batch loss 0.458915889 batch mAP 0.700073242 batch PCKh 0.6875\n",
      "Trained batch 1109 batch loss 0.513828695 batch mAP 0.626464844 batch PCKh 0.4375\n",
      "Trained batch 1110 batch loss 0.501179457 batch mAP 0.692443848 batch PCKh 0.5625\n",
      "Trained batch 1111 batch loss 0.456046015 batch mAP 0.625335693 batch PCKh 0.375\n",
      "Trained batch 1112 batch loss 0.380150557 batch mAP 0.714141846 batch PCKh 0.5\n",
      "Trained batch 1113 batch loss 0.428575337 batch mAP 0.680175781 batch PCKh 0.625\n",
      "Trained batch 1114 batch loss 0.438090563 batch mAP 0.695709229 batch PCKh 0.4375\n",
      "Trained batch 1115 batch loss 0.440123022 batch mAP 0.721923828 batch PCKh 0.5625\n",
      "Trained batch 1116 batch loss 0.418266982 batch mAP 0.747955322 batch PCKh 0.5625\n",
      "Trained batch 1117 batch loss 0.410163969 batch mAP 0.654510498 batch PCKh 0.5625\n",
      "Trained batch 1118 batch loss 0.487125397 batch mAP 0.628082275 batch PCKh 0.75\n",
      "Trained batch 1119 batch loss 0.515464187 batch mAP 0.636291504 batch PCKh 0.375\n",
      "Trained batch 1120 batch loss 0.463332027 batch mAP 0.614807129 batch PCKh 0.75\n",
      "Trained batch 1121 batch loss 0.483484119 batch mAP 0.572418213 batch PCKh 0.4375\n",
      "Trained batch 1122 batch loss 0.51055634 batch mAP 0.614471436 batch PCKh 0.5\n",
      "Trained batch 1123 batch loss 0.444204032 batch mAP 0.670349121 batch PCKh 0.375\n",
      "Trained batch 1124 batch loss 0.481071204 batch mAP 0.567932129 batch PCKh 0.625\n",
      "Trained batch 1125 batch loss 0.528066456 batch mAP 0.598571777 batch PCKh 0.625\n",
      "Trained batch 1126 batch loss 0.520080149 batch mAP 0.560974121 batch PCKh 0.6875\n",
      "Trained batch 1127 batch loss 0.535826504 batch mAP 0.567718506 batch PCKh 0.625\n",
      "Trained batch 1128 batch loss 0.465955019 batch mAP 0.656982422 batch PCKh 0.5\n",
      "Trained batch 1129 batch loss 0.471813977 batch mAP 0.639953613 batch PCKh 0.4375\n",
      "Trained batch 1130 batch loss 0.474587917 batch mAP 0.624603271 batch PCKh 0.875\n",
      "Trained batch 1131 batch loss 0.422051847 batch mAP 0.646179199 batch PCKh 0.625\n",
      "Trained batch 1132 batch loss 0.478614569 batch mAP 0.684967041 batch PCKh 0.5625\n",
      "Trained batch 1133 batch loss 0.433603883 batch mAP 0.690795898 batch PCKh 0.625\n",
      "Trained batch 1134 batch loss 0.442854017 batch mAP 0.659515381 batch PCKh 0.8125\n",
      "Trained batch 1135 batch loss 0.478308678 batch mAP 0.683288574 batch PCKh 0.5\n",
      "Trained batch 1136 batch loss 0.486738414 batch mAP 0.690673828 batch PCKh 0.5\n",
      "Trained batch 1137 batch loss 0.51259768 batch mAP 0.675231934 batch PCKh 0.25\n",
      "Trained batch 1138 batch loss 0.512894034 batch mAP 0.676361084 batch PCKh 0.75\n",
      "Trained batch 1139 batch loss 0.46196568 batch mAP 0.619995117 batch PCKh 0.6875\n",
      "Trained batch 1140 batch loss 0.617942929 batch mAP 0.55657959 batch PCKh 0.4375\n",
      "Trained batch 1141 batch loss 0.499468893 batch mAP 0.545379639 batch PCKh 0.4375\n",
      "Trained batch 1142 batch loss 0.556303322 batch mAP 0.583465576 batch PCKh 0.5\n",
      "Trained batch 1143 batch loss 0.581424654 batch mAP 0.601745605 batch PCKh 0.8125\n",
      "Trained batch 1144 batch loss 0.553862453 batch mAP 0.584625244 batch PCKh 0.4375\n",
      "Trained batch 1145 batch loss 0.60591495 batch mAP 0.549346924 batch PCKh 0.8125\n",
      "Trained batch 1146 batch loss 0.527647495 batch mAP 0.57144165 batch PCKh 0.375\n",
      "Trained batch 1147 batch loss 0.539296865 batch mAP 0.651947 batch PCKh 0.6875\n",
      "Trained batch 1148 batch loss 0.51853174 batch mAP 0.633148193 batch PCKh 0.5625\n",
      "Trained batch 1149 batch loss 0.499808222 batch mAP 0.612945557 batch PCKh 0.75\n",
      "Trained batch 1150 batch loss 0.481244355 batch mAP 0.607971191 batch PCKh 0.375\n",
      "Trained batch 1151 batch loss 0.564227104 batch mAP 0.47265625 batch PCKh 0.75\n",
      "Trained batch 1152 batch loss 0.400687695 batch mAP 0.63369751 batch PCKh 0.4375\n",
      "Trained batch 1153 batch loss 0.371101975 batch mAP 0.63104248 batch PCKh 0.5625\n",
      "Trained batch 1154 batch loss 0.488890886 batch mAP 0.560150146 batch PCKh 0.75\n",
      "Trained batch 1155 batch loss 0.396223187 batch mAP 0.563720703 batch PCKh 0\n",
      "Trained batch 1156 batch loss 0.566065609 batch mAP 0.555419922 batch PCKh 0.625\n",
      "Trained batch 1157 batch loss 0.48196137 batch mAP 0.589599609 batch PCKh 0.6875\n",
      "Trained batch 1158 batch loss 0.517775297 batch mAP 0.49331665 batch PCKh 0.6875\n",
      "Trained batch 1159 batch loss 0.570103824 batch mAP 0.564697266 batch PCKh 0.8125\n",
      "Trained batch 1160 batch loss 0.513423681 batch mAP 0.617492676 batch PCKh 0.75\n",
      "Trained batch 1161 batch loss 0.566886067 batch mAP 0.56427 batch PCKh 0.75\n",
      "Trained batch 1162 batch loss 0.516935527 batch mAP 0.565063477 batch PCKh 0.375\n",
      "Trained batch 1163 batch loss 0.471375167 batch mAP 0.605834961 batch PCKh 0.375\n",
      "Trained batch 1164 batch loss 0.480413139 batch mAP 0.566223145 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1165 batch loss 0.465144724 batch mAP 0.576324463 batch PCKh 0.375\n",
      "Trained batch 1166 batch loss 0.447060972 batch mAP 0.58782959 batch PCKh 0.5625\n",
      "Trained batch 1167 batch loss 0.49106425 batch mAP 0.610290527 batch PCKh 0.1875\n",
      "Trained batch 1168 batch loss 0.536886454 batch mAP 0.636261 batch PCKh 0.75\n",
      "Trained batch 1169 batch loss 0.491086721 batch mAP 0.650695801 batch PCKh 0.375\n",
      "Trained batch 1170 batch loss 0.35771066 batch mAP 0.631622314 batch PCKh 0.5625\n",
      "Trained batch 1171 batch loss 0.455977678 batch mAP 0.69519043 batch PCKh 0.75\n",
      "Trained batch 1172 batch loss 0.531033516 batch mAP 0.638092041 batch PCKh 0.3125\n",
      "Trained batch 1173 batch loss 0.490622938 batch mAP 0.648162842 batch PCKh 0.5625\n",
      "Trained batch 1174 batch loss 0.504126728 batch mAP 0.662902832 batch PCKh 0.625\n",
      "Trained batch 1175 batch loss 0.449940771 batch mAP 0.640899658 batch PCKh 0.4375\n",
      "Trained batch 1176 batch loss 0.577825963 batch mAP 0.600402832 batch PCKh 0.4375\n",
      "Trained batch 1177 batch loss 0.542699337 batch mAP 0.621765137 batch PCKh 0.4375\n",
      "Trained batch 1178 batch loss 0.507135689 batch mAP 0.587249756 batch PCKh 0.3125\n",
      "Trained batch 1179 batch loss 0.44326967 batch mAP 0.610473633 batch PCKh 0.75\n",
      "Trained batch 1180 batch loss 0.476040781 batch mAP 0.605072 batch PCKh 0.75\n",
      "Trained batch 1181 batch loss 0.514562964 batch mAP 0.647216797 batch PCKh 0.5625\n",
      "Trained batch 1182 batch loss 0.551639438 batch mAP 0.590301514 batch PCKh 0.5625\n",
      "Trained batch 1183 batch loss 0.503420353 batch mAP 0.632904053 batch PCKh 0.875\n",
      "Trained batch 1184 batch loss 0.471055925 batch mAP 0.647949219 batch PCKh 0.625\n",
      "Trained batch 1185 batch loss 0.406522334 batch mAP 0.680053711 batch PCKh 0.375\n",
      "Trained batch 1186 batch loss 0.418319643 batch mAP 0.616912842 batch PCKh 0.375\n",
      "Trained batch 1187 batch loss 0.411352 batch mAP 0.658599854 batch PCKh 0.5625\n",
      "Trained batch 1188 batch loss 0.467894852 batch mAP 0.592102051 batch PCKh 0.8125\n",
      "Trained batch 1189 batch loss 0.548262715 batch mAP 0.552215576 batch PCKh 0.125\n",
      "Trained batch 1190 batch loss 0.612226486 batch mAP 0.5675354 batch PCKh 0.75\n",
      "Trained batch 1191 batch loss 0.567170084 batch mAP 0.539398193 batch PCKh 0.5625\n",
      "Trained batch 1192 batch loss 0.545966923 batch mAP 0.547973633 batch PCKh 0.6875\n",
      "Trained batch 1193 batch loss 0.550624728 batch mAP 0.582763672 batch PCKh 0.6875\n",
      "Trained batch 1194 batch loss 0.497134358 batch mAP 0.581695557 batch PCKh 0.875\n",
      "Trained batch 1195 batch loss 0.549374938 batch mAP 0.55847168 batch PCKh 0.8125\n",
      "Trained batch 1196 batch loss 0.513250947 batch mAP 0.555664062 batch PCKh 0.125\n",
      "Trained batch 1197 batch loss 0.531205952 batch mAP 0.613128662 batch PCKh 0.625\n",
      "Trained batch 1198 batch loss 0.513621926 batch mAP 0.660217285 batch PCKh 0.5\n",
      "Trained batch 1199 batch loss 0.539762497 batch mAP 0.617584229 batch PCKh 0.875\n",
      "Trained batch 1200 batch loss 0.546636581 batch mAP 0.586181641 batch PCKh 0.375\n",
      "Trained batch 1201 batch loss 0.597851038 batch mAP 0.59979248 batch PCKh 0.5\n",
      "Trained batch 1202 batch loss 0.559593499 batch mAP 0.656280518 batch PCKh 0.4375\n",
      "Trained batch 1203 batch loss 0.438384593 batch mAP 0.695282 batch PCKh 0.6875\n",
      "Trained batch 1204 batch loss 0.430164099 batch mAP 0.635101318 batch PCKh 0.625\n",
      "Trained batch 1205 batch loss 0.512671232 batch mAP 0.635040283 batch PCKh 0.4375\n",
      "Trained batch 1206 batch loss 0.517493784 batch mAP 0.591796875 batch PCKh 0.8125\n",
      "Trained batch 1207 batch loss 0.473265886 batch mAP 0.631530762 batch PCKh 0.5625\n",
      "Trained batch 1208 batch loss 0.530229211 batch mAP 0.59753418 batch PCKh 0.25\n",
      "Trained batch 1209 batch loss 0.495474547 batch mAP 0.618591309 batch PCKh 0.25\n",
      "Trained batch 1210 batch loss 0.371427327 batch mAP 0.696899414 batch PCKh 0.6875\n",
      "Trained batch 1211 batch loss 0.447369277 batch mAP 0.648712158 batch PCKh 0.625\n",
      "Trained batch 1212 batch loss 0.467007548 batch mAP 0.624298096 batch PCKh 0.75\n",
      "Trained batch 1213 batch loss 0.426152676 batch mAP 0.639556885 batch PCKh 0.75\n",
      "Trained batch 1214 batch loss 0.433259368 batch mAP 0.630493164 batch PCKh 0.5625\n",
      "Trained batch 1215 batch loss 0.513156056 batch mAP 0.583129883 batch PCKh 0.125\n",
      "Trained batch 1216 batch loss 0.443713248 batch mAP 0.597381592 batch PCKh 0.375\n",
      "Trained batch 1217 batch loss 0.502575696 batch mAP 0.605072 batch PCKh 0.1875\n",
      "Trained batch 1218 batch loss 0.522018671 batch mAP 0.568634033 batch PCKh 0.125\n",
      "Trained batch 1219 batch loss 0.580988824 batch mAP 0.500183105 batch PCKh 0.625\n",
      "Trained batch 1220 batch loss 0.541653335 batch mAP 0.579834 batch PCKh 0.375\n",
      "Trained batch 1221 batch loss 0.608683228 batch mAP 0.585296631 batch PCKh 0.1875\n",
      "Trained batch 1222 batch loss 0.46688053 batch mAP 0.654388428 batch PCKh 0.25\n",
      "Trained batch 1223 batch loss 0.477588952 batch mAP 0.703155518 batch PCKh 0.0625\n",
      "Trained batch 1224 batch loss 0.391786456 batch mAP 0.693603516 batch PCKh 0.25\n",
      "Trained batch 1225 batch loss 0.36957109 batch mAP 0.737365723 batch PCKh 0.4375\n",
      "Trained batch 1226 batch loss 0.365884662 batch mAP 0.722747803 batch PCKh 0.5625\n",
      "Trained batch 1227 batch loss 0.428643465 batch mAP 0.6746521 batch PCKh 0.25\n",
      "Trained batch 1228 batch loss 0.388960868 batch mAP 0.669799805 batch PCKh 0.5\n",
      "Trained batch 1229 batch loss 0.445530504 batch mAP 0.703338623 batch PCKh 0.3125\n",
      "Trained batch 1230 batch loss 0.481940955 batch mAP 0.722442627 batch PCKh 0.3125\n",
      "Trained batch 1231 batch loss 0.548467457 batch mAP 0.592407227 batch PCKh 0.8125\n",
      "Trained batch 1232 batch loss 0.639824748 batch mAP 0.571960449 batch PCKh 0.3125\n",
      "Trained batch 1233 batch loss 0.485781908 batch mAP 0.67578125 batch PCKh 0.5\n",
      "Trained batch 1234 batch loss 0.611575425 batch mAP 0.570007324 batch PCKh 0.4375\n",
      "Trained batch 1235 batch loss 0.478539824 batch mAP 0.665527344 batch PCKh 0.375\n",
      "Trained batch 1236 batch loss 0.531792641 batch mAP 0.668548584 batch PCKh 0.25\n",
      "Trained batch 1237 batch loss 0.428229272 batch mAP 0.662719727 batch PCKh 0.1875\n",
      "Trained batch 1238 batch loss 0.507059872 batch mAP 0.626647949 batch PCKh 0.3125\n",
      "Trained batch 1239 batch loss 0.478593886 batch mAP 0.685089111 batch PCKh 0.625\n",
      "Trained batch 1240 batch loss 0.463657171 batch mAP 0.706085205 batch PCKh 0.375\n",
      "Trained batch 1241 batch loss 0.419519603 batch mAP 0.67099 batch PCKh 0.125\n",
      "Trained batch 1242 batch loss 0.444382131 batch mAP 0.586700439 batch PCKh 0.625\n",
      "Trained batch 1243 batch loss 0.559732914 batch mAP 0.606628418 batch PCKh 0\n",
      "Trained batch 1244 batch loss 0.438251197 batch mAP 0.624115 batch PCKh 0.3125\n",
      "Trained batch 1245 batch loss 0.491616488 batch mAP 0.589141846 batch PCKh 0.125\n",
      "Trained batch 1246 batch loss 0.571625471 batch mAP 0.647216797 batch PCKh 0.6875\n",
      "Trained batch 1247 batch loss 0.608509958 batch mAP 0.577880859 batch PCKh 0.25\n",
      "Trained batch 1248 batch loss 0.65123105 batch mAP 0.614563 batch PCKh 0.0625\n",
      "Trained batch 1249 batch loss 0.498487115 batch mAP 0.630554199 batch PCKh 0.625\n",
      "Trained batch 1250 batch loss 0.412625402 batch mAP 0.653076172 batch PCKh 0.875\n",
      "Trained batch 1251 batch loss 0.35702613 batch mAP 0.602813721 batch PCKh 0.5625\n",
      "Trained batch 1252 batch loss 0.42857632 batch mAP 0.55847168 batch PCKh 0.4375\n",
      "Trained batch 1253 batch loss 0.433005512 batch mAP 0.577484131 batch PCKh 0.5625\n",
      "Trained batch 1254 batch loss 0.373671889 batch mAP 0.627075195 batch PCKh 0.1875\n",
      "Trained batch 1255 batch loss 0.422755539 batch mAP 0.617584229 batch PCKh 0.6875\n",
      "Trained batch 1256 batch loss 0.418635815 batch mAP 0.562530518 batch PCKh 0.4375\n",
      "Trained batch 1257 batch loss 0.623974621 batch mAP 0.563873291 batch PCKh 0.5\n",
      "Trained batch 1258 batch loss 0.497949302 batch mAP 0.568450928 batch PCKh 0.625\n",
      "Trained batch 1259 batch loss 0.55753684 batch mAP 0.587890625 batch PCKh 0.5625\n",
      "Trained batch 1260 batch loss 0.56551826 batch mAP 0.625274658 batch PCKh 0.625\n",
      "Trained batch 1261 batch loss 0.542449117 batch mAP 0.578979492 batch PCKh 0.3125\n",
      "Trained batch 1262 batch loss 0.537145138 batch mAP 0.633605957 batch PCKh 0.5625\n",
      "Trained batch 1263 batch loss 0.495971501 batch mAP 0.572540283 batch PCKh 0.5625\n",
      "Trained batch 1264 batch loss 0.45424664 batch mAP 0.559692383 batch PCKh 0.5\n",
      "Trained batch 1265 batch loss 0.523535728 batch mAP 0.665252686 batch PCKh 0.875\n",
      "Trained batch 1266 batch loss 0.471104324 batch mAP 0.626495361 batch PCKh 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1267 batch loss 0.603540719 batch mAP 0.64328 batch PCKh 0.3125\n",
      "Trained batch 1268 batch loss 0.530402482 batch mAP 0.672699 batch PCKh 0.875\n",
      "Trained batch 1269 batch loss 0.479407221 batch mAP 0.658752441 batch PCKh 0.75\n",
      "Trained batch 1270 batch loss 0.504941523 batch mAP 0.664794922 batch PCKh 0.25\n",
      "Trained batch 1271 batch loss 0.526821 batch mAP 0.655944824 batch PCKh 0.6875\n",
      "Trained batch 1272 batch loss 0.434782922 batch mAP 0.685882568 batch PCKh 0.375\n",
      "Trained batch 1273 batch loss 0.455144942 batch mAP 0.663391113 batch PCKh 0.4375\n",
      "Trained batch 1274 batch loss 0.475288838 batch mAP 0.627685547 batch PCKh 0.5625\n",
      "Trained batch 1275 batch loss 0.516161859 batch mAP 0.572509766 batch PCKh 0.5625\n",
      "Trained batch 1276 batch loss 0.428747147 batch mAP 0.647155762 batch PCKh 0.5625\n",
      "Trained batch 1277 batch loss 0.562854409 batch mAP 0.573883057 batch PCKh 0.625\n",
      "Trained batch 1278 batch loss 0.45162797 batch mAP 0.621185303 batch PCKh 0.1875\n",
      "Trained batch 1279 batch loss 0.446650416 batch mAP 0.709136963 batch PCKh 0.5\n",
      "Trained batch 1280 batch loss 0.427378207 batch mAP 0.724456787 batch PCKh 0.75\n",
      "Trained batch 1281 batch loss 0.469794303 batch mAP 0.710113525 batch PCKh 0.8125\n",
      "Trained batch 1282 batch loss 0.540694 batch mAP 0.700561523 batch PCKh 0.6875\n",
      "Trained batch 1283 batch loss 0.534982085 batch mAP 0.65246582 batch PCKh 0.5\n",
      "Trained batch 1284 batch loss 0.569641769 batch mAP 0.605377197 batch PCKh 0.8125\n",
      "Trained batch 1285 batch loss 0.523284316 batch mAP 0.573425293 batch PCKh 0.625\n",
      "Trained batch 1286 batch loss 0.589413643 batch mAP 0.580352783 batch PCKh 0.75\n",
      "Trained batch 1287 batch loss 0.534132481 batch mAP 0.620056152 batch PCKh 0.6875\n",
      "Trained batch 1288 batch loss 0.52535212 batch mAP 0.577758789 batch PCKh 0.5\n",
      "Trained batch 1289 batch loss 0.583268166 batch mAP 0.574279785 batch PCKh 0.375\n",
      "Trained batch 1290 batch loss 0.586693883 batch mAP 0.600708 batch PCKh 0.4375\n",
      "Trained batch 1291 batch loss 0.529870629 batch mAP 0.636108398 batch PCKh 0.5625\n",
      "Trained batch 1292 batch loss 0.587376773 batch mAP 0.554016113 batch PCKh 0.8125\n",
      "Trained batch 1293 batch loss 0.435075939 batch mAP 0.559387207 batch PCKh 0.1875\n",
      "Trained batch 1294 batch loss 0.42968756 batch mAP 0.571105957 batch PCKh 0.75\n",
      "Trained batch 1295 batch loss 0.444754243 batch mAP 0.643737793 batch PCKh 0.5625\n",
      "Trained batch 1296 batch loss 0.370296776 batch mAP 0.593688965 batch PCKh 0\n",
      "Trained batch 1297 batch loss 0.365505278 batch mAP 0.626281738 batch PCKh 0.5625\n",
      "Trained batch 1298 batch loss 0.386178017 batch mAP 0.626922607 batch PCKh 0\n",
      "Trained batch 1299 batch loss 0.407018572 batch mAP 0.605011 batch PCKh 0.75\n",
      "Trained batch 1300 batch loss 0.373109579 batch mAP 0.67477417 batch PCKh 0.75\n",
      "Trained batch 1301 batch loss 0.45015949 batch mAP 0.561279297 batch PCKh 0.6875\n",
      "Trained batch 1302 batch loss 0.477576554 batch mAP 0.592437744 batch PCKh 0.3125\n",
      "Trained batch 1303 batch loss 0.505908132 batch mAP 0.579650879 batch PCKh 0.6875\n",
      "Trained batch 1304 batch loss 0.539467156 batch mAP 0.611755371 batch PCKh 0.75\n",
      "Trained batch 1305 batch loss 0.532075763 batch mAP 0.590515137 batch PCKh 0.5625\n",
      "Trained batch 1306 batch loss 0.400246114 batch mAP 0.645935059 batch PCKh 0.75\n",
      "Trained batch 1307 batch loss 0.421932936 batch mAP 0.608337402 batch PCKh 0.625\n",
      "Trained batch 1308 batch loss 0.381440073 batch mAP 0.661865234 batch PCKh 0.75\n",
      "Trained batch 1309 batch loss 0.340893 batch mAP 0.658355713 batch PCKh 0.6875\n",
      "Trained batch 1310 batch loss 0.393785834 batch mAP 0.617431641 batch PCKh 0.75\n",
      "Trained batch 1311 batch loss 0.413738251 batch mAP 0.623138428 batch PCKh 0.625\n",
      "Trained batch 1312 batch loss 0.360675275 batch mAP 0.628143311 batch PCKh 0.75\n",
      "Trained batch 1313 batch loss 0.567519665 batch mAP 0.566345215 batch PCKh 0.6875\n",
      "Trained batch 1314 batch loss 0.529789269 batch mAP 0.564910889 batch PCKh 0.875\n",
      "Trained batch 1315 batch loss 0.578943312 batch mAP 0.529571533 batch PCKh 0.5625\n",
      "Trained batch 1316 batch loss 0.464259028 batch mAP 0.611328125 batch PCKh 0.5625\n",
      "Trained batch 1317 batch loss 0.611360252 batch mAP 0.551696777 batch PCKh 0.125\n",
      "Trained batch 1318 batch loss 0.556552589 batch mAP 0.613616943 batch PCKh 0.1875\n",
      "Trained batch 1319 batch loss 0.506475925 batch mAP 0.634979248 batch PCKh 0.625\n",
      "Trained batch 1320 batch loss 0.531122148 batch mAP 0.624481201 batch PCKh 0.5\n",
      "Trained batch 1321 batch loss 0.37195617 batch mAP 0.709960938 batch PCKh 0.5\n",
      "Trained batch 1322 batch loss 0.444440544 batch mAP 0.676055908 batch PCKh 0.5\n",
      "Trained batch 1323 batch loss 0.427775204 batch mAP 0.692962646 batch PCKh 0.4375\n",
      "Trained batch 1324 batch loss 0.432761699 batch mAP 0.66922 batch PCKh 0.75\n",
      "Trained batch 1325 batch loss 0.525998771 batch mAP 0.617675781 batch PCKh 0.5625\n",
      "Trained batch 1326 batch loss 0.540288806 batch mAP 0.674835205 batch PCKh 0.4375\n",
      "Trained batch 1327 batch loss 0.564055562 batch mAP 0.586242676 batch PCKh 0.875\n",
      "Trained batch 1328 batch loss 0.457812726 batch mAP 0.715332031 batch PCKh 0.8125\n",
      "Trained batch 1329 batch loss 0.574565887 batch mAP 0.616027832 batch PCKh 0.75\n",
      "Trained batch 1330 batch loss 0.529033661 batch mAP 0.663574219 batch PCKh 0.75\n",
      "Trained batch 1331 batch loss 0.534272313 batch mAP 0.614013672 batch PCKh 0.5625\n",
      "Trained batch 1332 batch loss 0.436785102 batch mAP 0.716583252 batch PCKh 0.8125\n",
      "Trained batch 1333 batch loss 0.505817831 batch mAP 0.656280518 batch PCKh 0.8125\n",
      "Trained batch 1334 batch loss 0.571866333 batch mAP 0.607513428 batch PCKh 0.5\n",
      "Trained batch 1335 batch loss 0.596336365 batch mAP 0.544555664 batch PCKh 0.625\n",
      "Trained batch 1336 batch loss 0.564049125 batch mAP 0.519500732 batch PCKh 0.75\n",
      "Trained batch 1337 batch loss 0.530869961 batch mAP 0.619873047 batch PCKh 0.875\n",
      "Trained batch 1338 batch loss 0.484626 batch mAP 0.635925293 batch PCKh 0.875\n",
      "Trained batch 1339 batch loss 0.551845968 batch mAP 0.667724609 batch PCKh 0.75\n",
      "Trained batch 1340 batch loss 0.46103248 batch mAP 0.659210205 batch PCKh 0.75\n",
      "Trained batch 1341 batch loss 0.494296521 batch mAP 0.620025635 batch PCKh 0.75\n",
      "Trained batch 1342 batch loss 0.469045073 batch mAP 0.570861816 batch PCKh 0.5625\n",
      "Trained batch 1343 batch loss 0.403547019 batch mAP 0.653808594 batch PCKh 0.6875\n",
      "Trained batch 1344 batch loss 0.415161967 batch mAP 0.703521729 batch PCKh 0.75\n",
      "Trained batch 1345 batch loss 0.441142023 batch mAP 0.669769287 batch PCKh 0.5\n",
      "Trained batch 1346 batch loss 0.536510706 batch mAP 0.695526123 batch PCKh 0.5\n",
      "Trained batch 1347 batch loss 0.529007673 batch mAP 0.687957764 batch PCKh 0.75\n",
      "Trained batch 1348 batch loss 0.509184837 batch mAP 0.685211182 batch PCKh 0.375\n",
      "Trained batch 1349 batch loss 0.467234731 batch mAP 0.734710693 batch PCKh 0.875\n",
      "Trained batch 1350 batch loss 0.405993909 batch mAP 0.665283203 batch PCKh 0.375\n",
      "Trained batch 1351 batch loss 0.430752754 batch mAP 0.600921631 batch PCKh 0.6875\n",
      "Trained batch 1352 batch loss 0.433903545 batch mAP 0.654144287 batch PCKh 0.25\n",
      "Trained batch 1353 batch loss 0.442173034 batch mAP 0.686798096 batch PCKh 0.375\n",
      "Trained batch 1354 batch loss 0.464384 batch mAP 0.658782959 batch PCKh 0.5\n",
      "Trained batch 1355 batch loss 0.513469815 batch mAP 0.59362793 batch PCKh 0.5\n",
      "Trained batch 1356 batch loss 0.55181 batch mAP 0.62298584 batch PCKh 0.1875\n",
      "Trained batch 1357 batch loss 0.513323843 batch mAP 0.612548828 batch PCKh 0.375\n",
      "Trained batch 1358 batch loss 0.580065846 batch mAP 0.607879639 batch PCKh 0.3125\n",
      "Trained batch 1359 batch loss 0.514858663 batch mAP 0.653411865 batch PCKh 0.3125\n",
      "Trained batch 1360 batch loss 0.515165687 batch mAP 0.587921143 batch PCKh 0.375\n",
      "Trained batch 1361 batch loss 0.553041875 batch mAP 0.585266113 batch PCKh 0.125\n",
      "Trained batch 1362 batch loss 0.578986883 batch mAP 0.567626953 batch PCKh 0.25\n",
      "Trained batch 1363 batch loss 0.592609525 batch mAP 0.529296875 batch PCKh 0.5625\n",
      "Trained batch 1364 batch loss 0.593610227 batch mAP 0.522979736 batch PCKh 0.1875\n",
      "Trained batch 1365 batch loss 0.415932536 batch mAP 0.622833252 batch PCKh 0.5\n",
      "Trained batch 1366 batch loss 0.511166334 batch mAP 0.671661377 batch PCKh 0.4375\n",
      "Trained batch 1367 batch loss 0.52484268 batch mAP 0.66607666 batch PCKh 0.375\n",
      "Trained batch 1368 batch loss 0.432982266 batch mAP 0.64666748 batch PCKh 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1369 batch loss 0.50023973 batch mAP 0.676727295 batch PCKh 0.4375\n",
      "Trained batch 1370 batch loss 0.478705436 batch mAP 0.72265625 batch PCKh 0.625\n",
      "Trained batch 1371 batch loss 0.569588304 batch mAP 0.598968506 batch PCKh 0.125\n",
      "Trained batch 1372 batch loss 0.477381408 batch mAP 0.647735596 batch PCKh 0.5\n",
      "Trained batch 1373 batch loss 0.587706268 batch mAP 0.613525391 batch PCKh 0.5\n",
      "Trained batch 1374 batch loss 0.502357483 batch mAP 0.682830811 batch PCKh 0.1875\n",
      "Trained batch 1375 batch loss 0.417295575 batch mAP 0.630065918 batch PCKh 0.0625\n",
      "Trained batch 1376 batch loss 0.290052474 batch mAP 0.623260498 batch PCKh 0\n",
      "Trained batch 1377 batch loss 0.330257207 batch mAP 0.559295654 batch PCKh 0\n",
      "Trained batch 1378 batch loss 0.387658358 batch mAP 0.588287354 batch PCKh 0\n",
      "Trained batch 1379 batch loss 0.370707333 batch mAP 0.650482178 batch PCKh 0.5\n",
      "Trained batch 1380 batch loss 0.583176911 batch mAP 0.710754395 batch PCKh 0.875\n",
      "Trained batch 1381 batch loss 0.462864339 batch mAP 0.694488525 batch PCKh 0.5\n",
      "Trained batch 1382 batch loss 0.552840292 batch mAP 0.727630615 batch PCKh 0.75\n",
      "Trained batch 1383 batch loss 0.590606093 batch mAP 0.687133789 batch PCKh 0.8125\n",
      "Trained batch 1384 batch loss 0.547761202 batch mAP 0.707641602 batch PCKh 0.5625\n",
      "Trained batch 1385 batch loss 0.565845251 batch mAP 0.639068604 batch PCKh 0.5\n",
      "Trained batch 1386 batch loss 0.562415 batch mAP 0.648284912 batch PCKh 0.5\n",
      "Trained batch 1387 batch loss 0.543418348 batch mAP 0.626525879 batch PCKh 0.625\n",
      "Trained batch 1388 batch loss 0.509553 batch mAP 0.658782959 batch PCKh 0.625\n",
      "Trained batch 1389 batch loss 0.453796059 batch mAP 0.591949463 batch PCKh 0.0625\n",
      "Trained batch 1390 batch loss 0.491542131 batch mAP 0.616424561 batch PCKh 0.25\n",
      "Trained batch 1391 batch loss 0.545015 batch mAP 0.558837891 batch PCKh 0.6875\n",
      "Trained batch 1392 batch loss 0.603429437 batch mAP 0.622436523 batch PCKh 0.6875\n",
      "Trained batch 1393 batch loss 0.576701701 batch mAP 0.609130859 batch PCKh 0.25\n",
      "Trained batch 1394 batch loss 0.511385 batch mAP 0.671386719 batch PCKh 0.6875\n",
      "Trained batch 1395 batch loss 0.478466511 batch mAP 0.66619873 batch PCKh 0.5625\n",
      "Trained batch 1396 batch loss 0.435460865 batch mAP 0.716339111 batch PCKh 0.75\n",
      "Trained batch 1397 batch loss 0.583628178 batch mAP 0.660491943 batch PCKh 0.875\n",
      "Trained batch 1398 batch loss 0.561987042 batch mAP 0.654632568 batch PCKh 0.875\n",
      "Trained batch 1399 batch loss 0.557486176 batch mAP 0.639831543 batch PCKh 0.75\n",
      "Trained batch 1400 batch loss 0.45374918 batch mAP 0.68258667 batch PCKh 0.875\n",
      "Trained batch 1401 batch loss 0.510277927 batch mAP 0.663360596 batch PCKh 0.8125\n",
      "Trained batch 1402 batch loss 0.465813369 batch mAP 0.60534668 batch PCKh 0.5625\n",
      "Trained batch 1403 batch loss 0.528160691 batch mAP 0.625732422 batch PCKh 0.6875\n",
      "Trained batch 1404 batch loss 0.582533598 batch mAP 0.608215332 batch PCKh 0.75\n",
      "Trained batch 1405 batch loss 0.579963624 batch mAP 0.625976562 batch PCKh 0.4375\n",
      "Trained batch 1406 batch loss 0.614600539 batch mAP 0.636016846 batch PCKh 0.4375\n",
      "Trained batch 1407 batch loss 0.552145362 batch mAP 0.598815918 batch PCKh 0.375\n",
      "Trained batch 1408 batch loss 0.625581503 batch mAP 0.609527588 batch PCKh 0.375\n",
      "Trained batch 1409 batch loss 0.513414323 batch mAP 0.615905762 batch PCKh 0.5\n",
      "Trained batch 1410 batch loss 0.448961675 batch mAP 0.626373291 batch PCKh 0.5625\n",
      "Trained batch 1411 batch loss 0.471326262 batch mAP 0.640411377 batch PCKh 0.75\n",
      "Trained batch 1412 batch loss 0.512271225 batch mAP 0.587982178 batch PCKh 0.625\n",
      "Trained batch 1413 batch loss 0.551737487 batch mAP 0.60534668 batch PCKh 0.3125\n",
      "Trained batch 1414 batch loss 0.48949337 batch mAP 0.62387085 batch PCKh 0.75\n",
      "Trained batch 1415 batch loss 0.451247513 batch mAP 0.660736084 batch PCKh 0.375\n",
      "Trained batch 1416 batch loss 0.507025599 batch mAP 0.694366455 batch PCKh 0.5625\n",
      "Trained batch 1417 batch loss 0.504891753 batch mAP 0.675537109 batch PCKh 0.1875\n",
      "Trained batch 1418 batch loss 0.583799362 batch mAP 0.691558838 batch PCKh 0.5\n",
      "Trained batch 1419 batch loss 0.552004457 batch mAP 0.683990479 batch PCKh 0.75\n",
      "Trained batch 1420 batch loss 0.494317204 batch mAP 0.669799805 batch PCKh 0.5\n",
      "Trained batch 1421 batch loss 0.532152414 batch mAP 0.589416504 batch PCKh 0.6875\n",
      "Trained batch 1422 batch loss 0.507633328 batch mAP 0.608612061 batch PCKh 0.8125\n",
      "Trained batch 1423 batch loss 0.510880232 batch mAP 0.59967041 batch PCKh 0.8125\n",
      "Trained batch 1424 batch loss 0.495752275 batch mAP 0.644226074 batch PCKh 0.75\n",
      "Trained batch 1425 batch loss 0.518782 batch mAP 0.603240967 batch PCKh 0.625\n",
      "Trained batch 1426 batch loss 0.473918885 batch mAP 0.5809021 batch PCKh 0.1875\n",
      "Trained batch 1427 batch loss 0.585886896 batch mAP 0.535858154 batch PCKh 0.3125\n",
      "Trained batch 1428 batch loss 0.554276705 batch mAP 0.601989746 batch PCKh 0.4375\n",
      "Trained batch 1429 batch loss 0.411501169 batch mAP 0.751678467 batch PCKh 0.375\n",
      "Trained batch 1430 batch loss 0.562678695 batch mAP 0.614501953 batch PCKh 0.4375\n",
      "Trained batch 1431 batch loss 0.488789499 batch mAP 0.680664062 batch PCKh 0.625\n",
      "Trained batch 1432 batch loss 0.462397188 batch mAP 0.644378662 batch PCKh 0.625\n",
      "Trained batch 1433 batch loss 0.488337934 batch mAP 0.597412109 batch PCKh 0.6875\n",
      "Trained batch 1434 batch loss 0.505278409 batch mAP 0.639160156 batch PCKh 0.75\n",
      "Trained batch 1435 batch loss 0.521227062 batch mAP 0.678772 batch PCKh 0.75\n",
      "Trained batch 1436 batch loss 0.531970263 batch mAP 0.696685791 batch PCKh 0.4375\n",
      "Trained batch 1437 batch loss 0.512187123 batch mAP 0.704101562 batch PCKh 0.4375\n",
      "Trained batch 1438 batch loss 0.453667 batch mAP 0.698822 batch PCKh 0.25\n",
      "Trained batch 1439 batch loss 0.512207806 batch mAP 0.680297852 batch PCKh 0.375\n",
      "Trained batch 1440 batch loss 0.557265401 batch mAP 0.641601562 batch PCKh 0.6875\n",
      "Trained batch 1441 batch loss 0.553084731 batch mAP 0.561004639 batch PCKh 0.5\n",
      "Trained batch 1442 batch loss 0.633845806 batch mAP 0.54953 batch PCKh 0.75\n",
      "Trained batch 1443 batch loss 0.551258385 batch mAP 0.607452393 batch PCKh 0.375\n",
      "Trained batch 1444 batch loss 0.504953444 batch mAP 0.636016846 batch PCKh 0.75\n",
      "Trained batch 1445 batch loss 0.57265079 batch mAP 0.63067627 batch PCKh 0.6875\n",
      "Trained batch 1446 batch loss 0.54529196 batch mAP 0.598938 batch PCKh 0.875\n",
      "Trained batch 1447 batch loss 0.472649455 batch mAP 0.657226562 batch PCKh 0.8125\n",
      "Trained batch 1448 batch loss 0.456267715 batch mAP 0.614501953 batch PCKh 0.8125\n",
      "Trained batch 1449 batch loss 0.494227171 batch mAP 0.628814697 batch PCKh 0.8125\n",
      "Trained batch 1450 batch loss 0.495786458 batch mAP 0.664733887 batch PCKh 0.8125\n",
      "Trained batch 1451 batch loss 0.494284362 batch mAP 0.650604248 batch PCKh 0.4375\n",
      "Trained batch 1452 batch loss 0.499865144 batch mAP 0.666290283 batch PCKh 0.25\n",
      "Trained batch 1453 batch loss 0.569792747 batch mAP 0.660553 batch PCKh 0.25\n",
      "Trained batch 1454 batch loss 0.566231251 batch mAP 0.601165771 batch PCKh 0.5625\n",
      "Trained batch 1455 batch loss 0.586087167 batch mAP 0.624023438 batch PCKh 0.625\n",
      "Trained batch 1456 batch loss 0.569063127 batch mAP 0.624542236 batch PCKh 0.75\n",
      "Trained batch 1457 batch loss 0.63586694 batch mAP 0.559906 batch PCKh 0.0625\n",
      "Trained batch 1458 batch loss 0.611340344 batch mAP 0.53314209 batch PCKh 0.4375\n",
      "Trained batch 1459 batch loss 0.620738506 batch mAP 0.547393799 batch PCKh 0.375\n",
      "Trained batch 1460 batch loss 0.542872667 batch mAP 0.606933594 batch PCKh 0.25\n",
      "Trained batch 1461 batch loss 0.480535567 batch mAP 0.642944336 batch PCKh 0.3125\n",
      "Trained batch 1462 batch loss 0.44458136 batch mAP 0.66897583 batch PCKh 0.75\n",
      "Trained batch 1463 batch loss 0.392811924 batch mAP 0.682800293 batch PCKh 0.4375\n",
      "Trained batch 1464 batch loss 0.600109 batch mAP 0.605651855 batch PCKh 0.3125\n",
      "Trained batch 1465 batch loss 0.507555366 batch mAP 0.674835205 batch PCKh 0.375\n",
      "Trained batch 1466 batch loss 0.43449989 batch mAP 0.681152344 batch PCKh 0.375\n",
      "Trained batch 1467 batch loss 0.481948197 batch mAP 0.623901367 batch PCKh 0.4375\n",
      "Trained batch 1468 batch loss 0.596927047 batch mAP 0.563720703 batch PCKh 0.4375\n",
      "Trained batch 1469 batch loss 0.560452759 batch mAP 0.589752197 batch PCKh 0.25\n",
      "Trained batch 1470 batch loss 0.577580333 batch mAP 0.60357666 batch PCKh 0.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1471 batch loss 0.50353682 batch mAP 0.624908447 batch PCKh 0.375\n",
      "Trained batch 1472 batch loss 0.509190679 batch mAP 0.660736084 batch PCKh 0.375\n",
      "Trained batch 1473 batch loss 0.471425653 batch mAP 0.713653564 batch PCKh 0.375\n",
      "Trained batch 1474 batch loss 0.405482441 batch mAP 0.666564941 batch PCKh 0.5625\n",
      "Trained batch 1475 batch loss 0.559453964 batch mAP 0.616943359 batch PCKh 0.5\n",
      "Trained batch 1476 batch loss 0.577404916 batch mAP 0.502929688 batch PCKh 0.25\n",
      "Trained batch 1477 batch loss 0.366369724 batch mAP 0.684326172 batch PCKh 0.0625\n",
      "Trained batch 1478 batch loss 0.42583859 batch mAP 0.655578613 batch PCKh 0.25\n",
      "Trained batch 1479 batch loss 0.543481 batch mAP 0.561767578 batch PCKh 0.1875\n",
      "Trained batch 1480 batch loss 0.537566543 batch mAP 0.499847412 batch PCKh 0.375\n",
      "Trained batch 1481 batch loss 0.523951 batch mAP 0.527557373 batch PCKh 0.1875\n",
      "Trained batch 1482 batch loss 0.545497775 batch mAP 0.539550781 batch PCKh 0.4375\n",
      "Trained batch 1483 batch loss 0.562204719 batch mAP 0.555542 batch PCKh 0.8125\n",
      "Trained batch 1484 batch loss 0.577851772 batch mAP 0.556060791 batch PCKh 0.125\n",
      "Trained batch 1485 batch loss 0.564268589 batch mAP 0.603607178 batch PCKh 0.6875\n",
      "Trained batch 1486 batch loss 0.521729708 batch mAP 0.535064697 batch PCKh 0.625\n",
      "Trained batch 1487 batch loss 0.630293727 batch mAP 0.537261963 batch PCKh 0.875\n",
      "Trained batch 1488 batch loss 0.52809 batch mAP 0.606506348 batch PCKh 0.1875\n",
      "Trained batch 1489 batch loss 0.447199941 batch mAP 0.670074463 batch PCKh 0.5625\n",
      "Trained batch 1490 batch loss 0.527003288 batch mAP 0.620544434 batch PCKh 0.75\n",
      "Trained batch 1491 batch loss 0.469748467 batch mAP 0.607849121 batch PCKh 0.5\n",
      "Trained batch 1492 batch loss 0.5327245 batch mAP 0.655670166 batch PCKh 0.75\n",
      "Trained batch 1493 batch loss 0.538847804 batch mAP 0.629730225 batch PCKh 0.6875\n",
      "Trained batch 1494 batch loss 0.43388778 batch mAP 0.637878418 batch PCKh 0.625\n",
      "Trained batch 1495 batch loss 0.479644626 batch mAP 0.642150879 batch PCKh 0.8125\n",
      "Trained batch 1496 batch loss 0.480433524 batch mAP 0.658721924 batch PCKh 0.0625\n",
      "Trained batch 1497 batch loss 0.556981444 batch mAP 0.619689941 batch PCKh 0.375\n",
      "Trained batch 1498 batch loss 0.566988 batch mAP 0.631744385 batch PCKh 0.375\n",
      "Trained batch 1499 batch loss 0.525048614 batch mAP 0.59588623 batch PCKh 0.625\n",
      "Trained batch 1500 batch loss 0.521241188 batch mAP 0.618988037 batch PCKh 0.375\n",
      "Trained batch 1501 batch loss 0.576626 batch mAP 0.562927246 batch PCKh 0.5625\n",
      "Trained batch 1502 batch loss 0.539047539 batch mAP 0.618164062 batch PCKh 0.3125\n",
      "Trained batch 1503 batch loss 0.622255683 batch mAP 0.6121521 batch PCKh 0.5625\n",
      "Trained batch 1504 batch loss 0.597330153 batch mAP 0.670501709 batch PCKh 0.3125\n",
      "Trained batch 1505 batch loss 0.53108 batch mAP 0.63949585 batch PCKh 0.3125\n",
      "Trained batch 1506 batch loss 0.538702488 batch mAP 0.636199951 batch PCKh 0.5\n",
      "Trained batch 1507 batch loss 0.644539 batch mAP 0.494384766 batch PCKh 0.625\n",
      "Trained batch 1508 batch loss 0.573624313 batch mAP 0.621429443 batch PCKh 0.5625\n",
      "Trained batch 1509 batch loss 0.582073212 batch mAP 0.61505127 batch PCKh 0.5625\n",
      "Trained batch 1510 batch loss 0.595889449 batch mAP 0.64932251 batch PCKh 0.5625\n",
      "Trained batch 1511 batch loss 0.651441395 batch mAP 0.564331055 batch PCKh 0.8125\n",
      "Trained batch 1512 batch loss 0.524046779 batch mAP 0.567199707 batch PCKh 0.4375\n",
      "Trained batch 1513 batch loss 0.564773262 batch mAP 0.62612915 batch PCKh 0.5625\n",
      "Trained batch 1514 batch loss 0.538877487 batch mAP 0.605987549 batch PCKh 0.625\n",
      "Trained batch 1515 batch loss 0.529371321 batch mAP 0.584869385 batch PCKh 0.625\n",
      "Trained batch 1516 batch loss 0.478572249 batch mAP 0.580291748 batch PCKh 0.6875\n",
      "Trained batch 1517 batch loss 0.484121293 batch mAP 0.562011719 batch PCKh 0.75\n",
      "Trained batch 1518 batch loss 0.535072386 batch mAP 0.572357178 batch PCKh 0\n",
      "Trained batch 1519 batch loss 0.529538929 batch mAP 0.602630615 batch PCKh 0.6875\n",
      "Trained batch 1520 batch loss 0.461230338 batch mAP 0.651977539 batch PCKh 0.375\n",
      "Trained batch 1521 batch loss 0.558861911 batch mAP 0.664520264 batch PCKh 0.4375\n",
      "Trained batch 1522 batch loss 0.533206 batch mAP 0.629119873 batch PCKh 0.8125\n",
      "Trained batch 1523 batch loss 0.527305841 batch mAP 0.612701416 batch PCKh 0.6875\n",
      "Trained batch 1524 batch loss 0.556742609 batch mAP 0.615325928 batch PCKh 0.75\n",
      "Trained batch 1525 batch loss 0.588244379 batch mAP 0.593994141 batch PCKh 0.3125\n",
      "Trained batch 1526 batch loss 0.462704241 batch mAP 0.668243408 batch PCKh 0.75\n",
      "Trained batch 1527 batch loss 0.53596 batch mAP 0.586608887 batch PCKh 0.75\n",
      "Trained batch 1528 batch loss 0.535726726 batch mAP 0.563415527 batch PCKh 0.5\n",
      "Trained batch 1529 batch loss 0.521594763 batch mAP 0.580352783 batch PCKh 0.6875\n",
      "Trained batch 1530 batch loss 0.485977054 batch mAP 0.570098877 batch PCKh 0.6875\n",
      "Trained batch 1531 batch loss 0.593828917 batch mAP 0.589447 batch PCKh 0.75\n",
      "Trained batch 1532 batch loss 0.509111702 batch mAP 0.64654541 batch PCKh 0.75\n",
      "Trained batch 1533 batch loss 0.473388493 batch mAP 0.632324219 batch PCKh 0.75\n",
      "Trained batch 1534 batch loss 0.58551985 batch mAP 0.577362061 batch PCKh 0.75\n",
      "Trained batch 1535 batch loss 0.529534578 batch mAP 0.571136475 batch PCKh 0.75\n",
      "Trained batch 1536 batch loss 0.546060085 batch mAP 0.563110352 batch PCKh 0.1875\n",
      "Trained batch 1537 batch loss 0.562730432 batch mAP 0.60446167 batch PCKh 0.75\n",
      "Trained batch 1538 batch loss 0.47009182 batch mAP 0.593170166 batch PCKh 0\n",
      "Trained batch 1539 batch loss 0.590034187 batch mAP 0.566436768 batch PCKh 0.5\n",
      "Trained batch 1540 batch loss 0.480271399 batch mAP 0.610595703 batch PCKh 0.3125\n",
      "Trained batch 1541 batch loss 0.569017887 batch mAP 0.636322 batch PCKh 0.625\n",
      "Trained batch 1542 batch loss 0.539068937 batch mAP 0.633605957 batch PCKh 0.8125\n",
      "Trained batch 1543 batch loss 0.576916873 batch mAP 0.627075195 batch PCKh 0.25\n",
      "Trained batch 1544 batch loss 0.551715374 batch mAP 0.602355957 batch PCKh 0.6875\n",
      "Trained batch 1545 batch loss 0.560135365 batch mAP 0.634002686 batch PCKh 0.6875\n",
      "Trained batch 1546 batch loss 0.503701687 batch mAP 0.624206543 batch PCKh 0.625\n",
      "Trained batch 1547 batch loss 0.520372689 batch mAP 0.633422852 batch PCKh 0.75\n",
      "Trained batch 1548 batch loss 0.512034118 batch mAP 0.671325684 batch PCKh 0.6875\n",
      "Trained batch 1549 batch loss 0.419190854 batch mAP 0.694763184 batch PCKh 0.5\n",
      "Trained batch 1550 batch loss 0.397887468 batch mAP 0.729492188 batch PCKh 0.4375\n",
      "Trained batch 1551 batch loss 0.402365565 batch mAP 0.720001221 batch PCKh 0.375\n",
      "Trained batch 1552 batch loss 0.419789612 batch mAP 0.746429443 batch PCKh 0.5\n",
      "Trained batch 1553 batch loss 0.521711469 batch mAP 0.672546387 batch PCKh 0.375\n",
      "Trained batch 1554 batch loss 0.450592548 batch mAP 0.727294922 batch PCKh 0.5625\n",
      "Trained batch 1555 batch loss 0.459767 batch mAP 0.681793213 batch PCKh 0.25\n",
      "Trained batch 1556 batch loss 0.562797606 batch mAP 0.639556885 batch PCKh 0.4375\n",
      "Trained batch 1557 batch loss 0.524915934 batch mAP 0.59375 batch PCKh 0.75\n",
      "Trained batch 1558 batch loss 0.517596304 batch mAP 0.574615479 batch PCKh 0.5625\n",
      "Trained batch 1559 batch loss 0.550534785 batch mAP 0.56640625 batch PCKh 0.1875\n",
      "Trained batch 1560 batch loss 0.533931613 batch mAP 0.572113037 batch PCKh 0.375\n",
      "Trained batch 1561 batch loss 0.581290066 batch mAP 0.598632812 batch PCKh 0.3125\n",
      "Trained batch 1562 batch loss 0.577128887 batch mAP 0.631988525 batch PCKh 0.4375\n",
      "Trained batch 1563 batch loss 0.608404398 batch mAP 0.604370117 batch PCKh 0.375\n",
      "Trained batch 1564 batch loss 0.587999225 batch mAP 0.648223877 batch PCKh 0.375\n",
      "Trained batch 1565 batch loss 0.422847331 batch mAP 0.636047363 batch PCKh 0.0625\n",
      "Trained batch 1566 batch loss 0.503627837 batch mAP 0.673584 batch PCKh 0.6875\n",
      "Trained batch 1567 batch loss 0.483732879 batch mAP 0.650695801 batch PCKh 0.375\n",
      "Trained batch 1568 batch loss 0.511436343 batch mAP 0.62399292 batch PCKh 0.5625\n",
      "Trained batch 1569 batch loss 0.563753366 batch mAP 0.661895752 batch PCKh 0.6875\n",
      "Trained batch 1570 batch loss 0.462420285 batch mAP 0.694610596 batch PCKh 0.5625\n",
      "Trained batch 1571 batch loss 0.528831601 batch mAP 0.614074707 batch PCKh 0.3125\n",
      "Trained batch 1572 batch loss 0.500556588 batch mAP 0.631256104 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1573 batch loss 0.471684694 batch mAP 0.620849609 batch PCKh 0.4375\n",
      "Trained batch 1574 batch loss 0.542197347 batch mAP 0.598083496 batch PCKh 0.4375\n",
      "Trained batch 1575 batch loss 0.524881244 batch mAP 0.591644287 batch PCKh 0.125\n",
      "Trained batch 1576 batch loss 0.531665266 batch mAP 0.670776367 batch PCKh 0.8125\n",
      "Trained batch 1577 batch loss 0.486228436 batch mAP 0.662811279 batch PCKh 0.875\n",
      "Trained batch 1578 batch loss 0.514873147 batch mAP 0.66885376 batch PCKh 0.75\n",
      "Trained batch 1579 batch loss 0.590828419 batch mAP 0.58380127 batch PCKh 0.8125\n",
      "Trained batch 1580 batch loss 0.55873239 batch mAP 0.627166748 batch PCKh 0.375\n",
      "Trained batch 1581 batch loss 0.580151916 batch mAP 0.628173828 batch PCKh 0.5625\n",
      "Trained batch 1582 batch loss 0.518440723 batch mAP 0.685974121 batch PCKh 0.5625\n",
      "Trained batch 1583 batch loss 0.639284253 batch mAP 0.587738037 batch PCKh 0.8125\n",
      "Trained batch 1584 batch loss 0.594071209 batch mAP 0.63961792 batch PCKh 0.6875\n",
      "Trained batch 1585 batch loss 0.554903924 batch mAP 0.532287598 batch PCKh 0.6875\n",
      "Trained batch 1586 batch loss 0.512953401 batch mAP 0.546081543 batch PCKh 0.5\n",
      "Trained batch 1587 batch loss 0.537148833 batch mAP 0.57901 batch PCKh 0.75\n",
      "Trained batch 1588 batch loss 0.418106973 batch mAP 0.631897 batch PCKh 0.75\n",
      "Trained batch 1589 batch loss 0.487380266 batch mAP 0.640289307 batch PCKh 0.75\n",
      "Trained batch 1590 batch loss 0.467604488 batch mAP 0.613891602 batch PCKh 0.25\n",
      "Trained batch 1591 batch loss 0.549798429 batch mAP 0.613739 batch PCKh 0.75\n",
      "Trained batch 1592 batch loss 0.544950247 batch mAP 0.584411621 batch PCKh 0.8125\n",
      "Trained batch 1593 batch loss 0.487766623 batch mAP 0.628448486 batch PCKh 0.4375\n",
      "Trained batch 1594 batch loss 0.539499521 batch mAP 0.625152588 batch PCKh 0.6875\n",
      "Trained batch 1595 batch loss 0.544353366 batch mAP 0.624511719 batch PCKh 0.75\n",
      "Trained batch 1596 batch loss 0.528095543 batch mAP 0.617797852 batch PCKh 0.375\n",
      "Trained batch 1597 batch loss 0.514526129 batch mAP 0.624908447 batch PCKh 0.375\n",
      "Trained batch 1598 batch loss 0.532457829 batch mAP 0.569030762 batch PCKh 0.3125\n",
      "Trained batch 1599 batch loss 0.575711966 batch mAP 0.607757568 batch PCKh 0.5625\n",
      "Trained batch 1600 batch loss 0.504742503 batch mAP 0.699005127 batch PCKh 0.6875\n",
      "Trained batch 1601 batch loss 0.519521177 batch mAP 0.66784668 batch PCKh 0.6875\n",
      "Trained batch 1602 batch loss 0.509704947 batch mAP 0.72366333 batch PCKh 0.375\n",
      "Trained batch 1603 batch loss 0.503215075 batch mAP 0.730102539 batch PCKh 0.6875\n",
      "Trained batch 1604 batch loss 0.530959904 batch mAP 0.670196533 batch PCKh 0.5625\n",
      "Trained batch 1605 batch loss 0.536846042 batch mAP 0.670684814 batch PCKh 0.1875\n",
      "Trained batch 1606 batch loss 0.373806596 batch mAP 0.726257324 batch PCKh 0.0625\n",
      "Trained batch 1607 batch loss 0.435875624 batch mAP 0.726532 batch PCKh 0.5\n",
      "Trained batch 1608 batch loss 0.452311695 batch mAP 0.71975708 batch PCKh 0.75\n",
      "Trained batch 1609 batch loss 0.529314339 batch mAP 0.621612549 batch PCKh 0.25\n",
      "Trained batch 1610 batch loss 0.622053266 batch mAP 0.560668945 batch PCKh 0.3125\n",
      "Trained batch 1611 batch loss 0.580299616 batch mAP 0.531707764 batch PCKh 0.625\n",
      "Trained batch 1612 batch loss 0.500867963 batch mAP 0.574432373 batch PCKh 0.625\n",
      "Trained batch 1613 batch loss 0.559102058 batch mAP 0.553741455 batch PCKh 0.5625\n",
      "Trained batch 1614 batch loss 0.508396745 batch mAP 0.579772949 batch PCKh 0.375\n",
      "Trained batch 1615 batch loss 0.562499821 batch mAP 0.515045166 batch PCKh 0.5\n",
      "Trained batch 1616 batch loss 0.549104929 batch mAP 0.551483154 batch PCKh 0.5625\n",
      "Trained batch 1617 batch loss 0.514684618 batch mAP 0.569274902 batch PCKh 0.5\n",
      "Trained batch 1618 batch loss 0.515623868 batch mAP 0.589874268 batch PCKh 0.5625\n",
      "Trained batch 1619 batch loss 0.517592847 batch mAP 0.590057373 batch PCKh 0.75\n",
      "Trained batch 1620 batch loss 0.424359769 batch mAP 0.642730713 batch PCKh 0.75\n",
      "Trained batch 1621 batch loss 0.441852897 batch mAP 0.633392334 batch PCKh 0.75\n",
      "Trained batch 1622 batch loss 0.400399238 batch mAP 0.613250732 batch PCKh 0.875\n",
      "Trained batch 1623 batch loss 0.412946701 batch mAP 0.662536621 batch PCKh 0.75\n",
      "Trained batch 1624 batch loss 0.410158664 batch mAP 0.647674561 batch PCKh 0.5625\n",
      "Trained batch 1625 batch loss 0.453772306 batch mAP 0.624359131 batch PCKh 0.8125\n",
      "Trained batch 1626 batch loss 0.554241598 batch mAP 0.631439209 batch PCKh 0.75\n",
      "Trained batch 1627 batch loss 0.509446144 batch mAP 0.633667 batch PCKh 0.8125\n",
      "Trained batch 1628 batch loss 0.438341379 batch mAP 0.634368896 batch PCKh 0.5625\n",
      "Trained batch 1629 batch loss 0.447262704 batch mAP 0.623291 batch PCKh 0.5\n",
      "Trained batch 1630 batch loss 0.532369912 batch mAP 0.641845703 batch PCKh 0.75\n",
      "Trained batch 1631 batch loss 0.54975 batch mAP 0.568389893 batch PCKh 0.75\n",
      "Trained batch 1632 batch loss 0.638932 batch mAP 0.5340271 batch PCKh 0.5\n",
      "Trained batch 1633 batch loss 0.575765789 batch mAP 0.596710205 batch PCKh 0.75\n",
      "Trained batch 1634 batch loss 0.449795902 batch mAP 0.583221436 batch PCKh 0.625\n",
      "Trained batch 1635 batch loss 0.517023802 batch mAP 0.604095459 batch PCKh 0.1875\n",
      "Trained batch 1636 batch loss 0.494237393 batch mAP 0.589752197 batch PCKh 0.5625\n",
      "Trained batch 1637 batch loss 0.598200321 batch mAP 0.527771 batch PCKh 0.8125\n",
      "Trained batch 1638 batch loss 0.522152066 batch mAP 0.536438 batch PCKh 0.75\n",
      "Trained batch 1639 batch loss 0.54874593 batch mAP 0.51272583 batch PCKh 0.5\n",
      "Trained batch 1640 batch loss 0.59200865 batch mAP 0.550750732 batch PCKh 0.75\n",
      "Trained batch 1641 batch loss 0.628546417 batch mAP 0.579498291 batch PCKh 0.5\n",
      "Trained batch 1642 batch loss 0.498198628 batch mAP 0.555053711 batch PCKh 0.875\n",
      "Trained batch 1643 batch loss 0.522349 batch mAP 0.551361084 batch PCKh 0.5625\n",
      "Trained batch 1644 batch loss 0.559218526 batch mAP 0.552093506 batch PCKh 0.75\n",
      "Trained batch 1645 batch loss 0.630577385 batch mAP 0.581878662 batch PCKh 0.5625\n",
      "Trained batch 1646 batch loss 0.624224842 batch mAP 0.549285889 batch PCKh 0.75\n",
      "Trained batch 1647 batch loss 0.497820795 batch mAP 0.630310059 batch PCKh 0.5625\n",
      "Trained batch 1648 batch loss 0.551706612 batch mAP 0.559692383 batch PCKh 0.5625\n",
      "Trained batch 1649 batch loss 0.490949035 batch mAP 0.631774902 batch PCKh 0.6875\n",
      "Trained batch 1650 batch loss 0.398220837 batch mAP 0.676147461 batch PCKh 0.8125\n",
      "Trained batch 1651 batch loss 0.438005924 batch mAP 0.687530518 batch PCKh 0.9375\n",
      "Trained batch 1652 batch loss 0.523382545 batch mAP 0.626525879 batch PCKh 0.4375\n",
      "Trained batch 1653 batch loss 0.438478917 batch mAP 0.634674072 batch PCKh 0.8125\n",
      "Trained batch 1654 batch loss 0.449005663 batch mAP 0.664123535 batch PCKh 0.75\n",
      "Trained batch 1655 batch loss 0.421240687 batch mAP 0.703552246 batch PCKh 0.625\n",
      "Trained batch 1656 batch loss 0.407930583 batch mAP 0.731750488 batch PCKh 0.5\n",
      "Trained batch 1657 batch loss 0.403915018 batch mAP 0.687133789 batch PCKh 0.625\n",
      "Trained batch 1658 batch loss 0.405872166 batch mAP 0.697937 batch PCKh 0.375\n",
      "Trained batch 1659 batch loss 0.388608575 batch mAP 0.761108398 batch PCKh 0.75\n",
      "Trained batch 1660 batch loss 0.314732254 batch mAP 0.737609863 batch PCKh 0.1875\n",
      "Trained batch 1661 batch loss 0.49698 batch mAP 0.627044678 batch PCKh 0.25\n",
      "Trained batch 1662 batch loss 0.508827 batch mAP 0.632629395 batch PCKh 0.5625\n",
      "Trained batch 1663 batch loss 0.494288117 batch mAP 0.62109375 batch PCKh 0.375\n",
      "Trained batch 1664 batch loss 0.55121696 batch mAP 0.624481201 batch PCKh 0.625\n",
      "Trained batch 1665 batch loss 0.498984486 batch mAP 0.626556396 batch PCKh 0.625\n",
      "Trained batch 1666 batch loss 0.621185184 batch mAP 0.551452637 batch PCKh 0.25\n",
      "Trained batch 1667 batch loss 0.524133205 batch mAP 0.594360352 batch PCKh 0.25\n",
      "Trained batch 1668 batch loss 0.628720582 batch mAP 0.563140869 batch PCKh 0.0625\n",
      "Trained batch 1669 batch loss 0.599597931 batch mAP 0.595977783 batch PCKh 0.3125\n",
      "Trained batch 1670 batch loss 0.563518882 batch mAP 0.625396729 batch PCKh 0.8125\n",
      "Trained batch 1671 batch loss 0.52432549 batch mAP 0.619720459 batch PCKh 0.375\n",
      "Trained batch 1672 batch loss 0.585106 batch mAP 0.639129639 batch PCKh 0.4375\n",
      "Trained batch 1673 batch loss 0.557631552 batch mAP 0.62298584 batch PCKh 0.3125\n",
      "Trained batch 1674 batch loss 0.53605032 batch mAP 0.648620605 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1675 batch loss 0.451793075 batch mAP 0.752410889 batch PCKh 0.625\n",
      "Trained batch 1676 batch loss 0.471349299 batch mAP 0.656677246 batch PCKh 0.625\n",
      "Trained batch 1677 batch loss 0.395029694 batch mAP 0.731262207 batch PCKh 0.75\n",
      "Trained batch 1678 batch loss 0.503838658 batch mAP 0.639099121 batch PCKh 0.625\n",
      "Trained batch 1679 batch loss 0.491647333 batch mAP 0.607788086 batch PCKh 0.25\n",
      "Trained batch 1680 batch loss 0.460027277 batch mAP 0.638549805 batch PCKh 0.1875\n",
      "Trained batch 1681 batch loss 0.575464 batch mAP 0.608917236 batch PCKh 0.375\n",
      "Trained batch 1682 batch loss 0.445704341 batch mAP 0.702423096 batch PCKh 0.6875\n",
      "Trained batch 1683 batch loss 0.498892754 batch mAP 0.610473633 batch PCKh 0.6875\n",
      "Trained batch 1684 batch loss 0.577084124 batch mAP 0.664886475 batch PCKh 0.5625\n",
      "Trained batch 1685 batch loss 0.4730497 batch mAP 0.72177124 batch PCKh 0.625\n",
      "Trained batch 1686 batch loss 0.548172295 batch mAP 0.692901611 batch PCKh 0.625\n",
      "Trained batch 1687 batch loss 0.516707838 batch mAP 0.682495117 batch PCKh 0.875\n",
      "Trained batch 1688 batch loss 0.475672483 batch mAP 0.709228516 batch PCKh 0.75\n",
      "Trained batch 1689 batch loss 0.569662511 batch mAP 0.671356201 batch PCKh 0.5\n",
      "Trained batch 1690 batch loss 0.551698685 batch mAP 0.633575439 batch PCKh 0.75\n",
      "Trained batch 1691 batch loss 0.433168203 batch mAP 0.638244629 batch PCKh 0.5625\n",
      "Trained batch 1692 batch loss 0.488591015 batch mAP 0.625640869 batch PCKh 0.375\n",
      "Trained batch 1693 batch loss 0.482612491 batch mAP 0.619842529 batch PCKh 0.75\n",
      "Trained batch 1694 batch loss 0.499968141 batch mAP 0.605133057 batch PCKh 0.6875\n",
      "Trained batch 1695 batch loss 0.508193851 batch mAP 0.67880249 batch PCKh 0.5\n",
      "Trained batch 1696 batch loss 0.459646255 batch mAP 0.707366943 batch PCKh 0.5\n",
      "Trained batch 1697 batch loss 0.433035225 batch mAP 0.713287354 batch PCKh 0.6875\n",
      "Trained batch 1698 batch loss 0.460821509 batch mAP 0.702484131 batch PCKh 0.5\n",
      "Trained batch 1699 batch loss 0.474596 batch mAP 0.695282 batch PCKh 0.5625\n",
      "Trained batch 1700 batch loss 0.536494493 batch mAP 0.634216309 batch PCKh 0.6875\n",
      "Trained batch 1701 batch loss 0.528130412 batch mAP 0.630859375 batch PCKh 0.5\n",
      "Trained batch 1702 batch loss 0.516551077 batch mAP 0.637817383 batch PCKh 0.375\n",
      "Trained batch 1703 batch loss 0.567965 batch mAP 0.621307373 batch PCKh 0.4375\n",
      "Trained batch 1704 batch loss 0.514065385 batch mAP 0.665557861 batch PCKh 0.75\n",
      "Trained batch 1705 batch loss 0.494178951 batch mAP 0.614013672 batch PCKh 0.625\n",
      "Trained batch 1706 batch loss 0.516699255 batch mAP 0.647613525 batch PCKh 0.75\n",
      "Trained batch 1707 batch loss 0.483197153 batch mAP 0.678070068 batch PCKh 0.6875\n",
      "Trained batch 1708 batch loss 0.497584879 batch mAP 0.639312744 batch PCKh 0.6875\n",
      "Trained batch 1709 batch loss 0.495894253 batch mAP 0.622680664 batch PCKh 0.875\n",
      "Trained batch 1710 batch loss 0.505022287 batch mAP 0.581726074 batch PCKh 0.4375\n",
      "Trained batch 1711 batch loss 0.545636296 batch mAP 0.609832764 batch PCKh 0.5\n",
      "Trained batch 1712 batch loss 0.575509727 batch mAP 0.57913208 batch PCKh 0.4375\n",
      "Trained batch 1713 batch loss 0.558777094 batch mAP 0.581298828 batch PCKh 0.5\n",
      "Trained batch 1714 batch loss 0.481479615 batch mAP 0.642364502 batch PCKh 0.375\n",
      "Trained batch 1715 batch loss 0.436075032 batch mAP 0.617919922 batch PCKh 0.625\n",
      "Trained batch 1716 batch loss 0.479030877 batch mAP 0.57321167 batch PCKh 0.25\n",
      "Trained batch 1717 batch loss 0.436470687 batch mAP 0.636993408 batch PCKh 0.4375\n",
      "Trained batch 1718 batch loss 0.52750361 batch mAP 0.611419678 batch PCKh 0.6875\n",
      "Trained batch 1719 batch loss 0.480145574 batch mAP 0.706787109 batch PCKh 0.25\n",
      "Trained batch 1720 batch loss 0.466311455 batch mAP 0.580383301 batch PCKh 0.75\n",
      "Trained batch 1721 batch loss 0.498802453 batch mAP 0.557800293 batch PCKh 0.5\n",
      "Trained batch 1722 batch loss 0.443355381 batch mAP 0.603088379 batch PCKh 0.375\n",
      "Trained batch 1723 batch loss 0.363796771 batch mAP 0.642089844 batch PCKh 0.6875\n",
      "Trained batch 1724 batch loss 0.441257179 batch mAP 0.602935791 batch PCKh 0.25\n",
      "Trained batch 1725 batch loss 0.548963249 batch mAP 0.584533691 batch PCKh 0.625\n",
      "Trained batch 1726 batch loss 0.428794503 batch mAP 0.58013916 batch PCKh 0.8125\n",
      "Trained batch 1727 batch loss 0.432776064 batch mAP 0.634643555 batch PCKh 0.1875\n",
      "Trained batch 1728 batch loss 0.364897132 batch mAP 0.718536377 batch PCKh 0.625\n",
      "Trained batch 1729 batch loss 0.464667857 batch mAP 0.672271729 batch PCKh 0.5\n",
      "Trained batch 1730 batch loss 0.484641194 batch mAP 0.655792236 batch PCKh 0.5\n",
      "Trained batch 1731 batch loss 0.414459378 batch mAP 0.683136 batch PCKh 0.75\n",
      "Trained batch 1732 batch loss 0.353435576 batch mAP 0.700195312 batch PCKh 0.75\n",
      "Trained batch 1733 batch loss 0.446696371 batch mAP 0.620025635 batch PCKh 0.5625\n",
      "Trained batch 1734 batch loss 0.423381299 batch mAP 0.634216309 batch PCKh 0.75\n",
      "Trained batch 1735 batch loss 0.507932842 batch mAP 0.589477539 batch PCKh 0.625\n",
      "Trained batch 1736 batch loss 0.54944855 batch mAP 0.650238037 batch PCKh 0.6875\n",
      "Trained batch 1737 batch loss 0.508488178 batch mAP 0.602600098 batch PCKh 0.5625\n",
      "Trained batch 1738 batch loss 0.481505394 batch mAP 0.63104248 batch PCKh 0.3125\n",
      "Trained batch 1739 batch loss 0.429837525 batch mAP 0.679657 batch PCKh 0.375\n",
      "Trained batch 1740 batch loss 0.442272604 batch mAP 0.666442871 batch PCKh 0.625\n",
      "Trained batch 1741 batch loss 0.483565301 batch mAP 0.689209 batch PCKh 0.625\n",
      "Trained batch 1742 batch loss 0.521597803 batch mAP 0.617675781 batch PCKh 0.75\n",
      "Trained batch 1743 batch loss 0.512193739 batch mAP 0.642974854 batch PCKh 0.4375\n",
      "Trained batch 1744 batch loss 0.523088813 batch mAP 0.615631104 batch PCKh 0.5625\n",
      "Trained batch 1745 batch loss 0.519136488 batch mAP 0.579772949 batch PCKh 0.75\n",
      "Trained batch 1746 batch loss 0.45073241 batch mAP 0.641296387 batch PCKh 0.375\n",
      "Trained batch 1747 batch loss 0.430386275 batch mAP 0.622406 batch PCKh 0.625\n",
      "Trained batch 1748 batch loss 0.412725508 batch mAP 0.675567627 batch PCKh 0.6875\n",
      "Trained batch 1749 batch loss 0.476625502 batch mAP 0.633514404 batch PCKh 0.625\n",
      "Trained batch 1750 batch loss 0.491849631 batch mAP 0.621734619 batch PCKh 0.75\n",
      "Trained batch 1751 batch loss 0.459519744 batch mAP 0.733764648 batch PCKh 0.625\n",
      "Trained batch 1752 batch loss 0.48883009 batch mAP 0.673278809 batch PCKh 0.75\n",
      "Trained batch 1753 batch loss 0.489865899 batch mAP 0.632019043 batch PCKh 0.6875\n",
      "Trained batch 1754 batch loss 0.491279304 batch mAP 0.614807129 batch PCKh 0.75\n",
      "Trained batch 1755 batch loss 0.555389762 batch mAP 0.560699463 batch PCKh 0.6875\n",
      "Trained batch 1756 batch loss 0.508454263 batch mAP 0.608062744 batch PCKh 0.625\n",
      "Trained batch 1757 batch loss 0.578413 batch mAP 0.557251 batch PCKh 0.75\n",
      "Trained batch 1758 batch loss 0.42149061 batch mAP 0.656799316 batch PCKh 0.375\n",
      "Trained batch 1759 batch loss 0.41369 batch mAP 0.684021 batch PCKh 0.4375\n",
      "Trained batch 1760 batch loss 0.441218108 batch mAP 0.636627197 batch PCKh 0.5\n",
      "Trained batch 1761 batch loss 0.456629336 batch mAP 0.660339355 batch PCKh 0.625\n",
      "Trained batch 1762 batch loss 0.474372327 batch mAP 0.650024414 batch PCKh 0.75\n",
      "Trained batch 1763 batch loss 0.440531909 batch mAP 0.704834 batch PCKh 0.75\n",
      "Trained batch 1764 batch loss 0.49062255 batch mAP 0.658996582 batch PCKh 0.5625\n",
      "Trained batch 1765 batch loss 0.444055974 batch mAP 0.679443359 batch PCKh 0.625\n",
      "Trained batch 1766 batch loss 0.473069936 batch mAP 0.692749 batch PCKh 0.75\n",
      "Trained batch 1767 batch loss 0.504032969 batch mAP 0.645599365 batch PCKh 0.5\n",
      "Trained batch 1768 batch loss 0.49725917 batch mAP 0.622283936 batch PCKh 0.5625\n",
      "Trained batch 1769 batch loss 0.531082749 batch mAP 0.621551514 batch PCKh 0.8125\n",
      "Trained batch 1770 batch loss 0.463648677 batch mAP 0.662719727 batch PCKh 0.5625\n",
      "Trained batch 1771 batch loss 0.43356 batch mAP 0.647003174 batch PCKh 0.5\n",
      "Trained batch 1772 batch loss 0.487493873 batch mAP 0.687072754 batch PCKh 0.625\n",
      "Trained batch 1773 batch loss 0.492262155 batch mAP 0.636047363 batch PCKh 0.75\n",
      "Trained batch 1774 batch loss 0.45853579 batch mAP 0.602905273 batch PCKh 0.5625\n",
      "Trained batch 1775 batch loss 0.418176174 batch mAP 0.706848145 batch PCKh 0.6875\n",
      "Trained batch 1776 batch loss 0.45197165 batch mAP 0.724823 batch PCKh 0.8125\n",
      "Trained batch 1777 batch loss 0.418215156 batch mAP 0.632782 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1778 batch loss 0.469532 batch mAP 0.655670166 batch PCKh 0.5\n",
      "Trained batch 1779 batch loss 0.437581867 batch mAP 0.659088135 batch PCKh 0.25\n",
      "Trained batch 1780 batch loss 0.53823185 batch mAP 0.623657227 batch PCKh 0.5625\n",
      "Trained batch 1781 batch loss 0.519304574 batch mAP 0.66809082 batch PCKh 0.375\n",
      "Trained batch 1782 batch loss 0.565175772 batch mAP 0.660369873 batch PCKh 0.375\n",
      "Trained batch 1783 batch loss 0.566352367 batch mAP 0.607147217 batch PCKh 0.5\n",
      "Trained batch 1784 batch loss 0.561371207 batch mAP 0.630126953 batch PCKh 0.4375\n",
      "Trained batch 1785 batch loss 0.538763165 batch mAP 0.69342041 batch PCKh 0.375\n",
      "Trained batch 1786 batch loss 0.461098611 batch mAP 0.699981689 batch PCKh 0.25\n",
      "Trained batch 1787 batch loss 0.452334642 batch mAP 0.690551758 batch PCKh 0.4375\n",
      "Trained batch 1788 batch loss 0.44666481 batch mAP 0.688415527 batch PCKh 0.625\n",
      "Trained batch 1789 batch loss 0.545767665 batch mAP 0.647522 batch PCKh 0.5\n",
      "Trained batch 1790 batch loss 0.484033 batch mAP 0.713470459 batch PCKh 0.375\n",
      "Trained batch 1791 batch loss 0.494038403 batch mAP 0.6925354 batch PCKh 0.4375\n",
      "Trained batch 1792 batch loss 0.484775 batch mAP 0.691864 batch PCKh 0.375\n",
      "Trained batch 1793 batch loss 0.392560333 batch mAP 0.689209 batch PCKh 0.25\n",
      "Trained batch 1794 batch loss 0.525551796 batch mAP 0.617797852 batch PCKh 0.375\n",
      "Trained batch 1795 batch loss 0.442812264 batch mAP 0.623748779 batch PCKh 0.625\n",
      "Trained batch 1796 batch loss 0.474421442 batch mAP 0.622619629 batch PCKh 0.8125\n",
      "Trained batch 1797 batch loss 0.527412653 batch mAP 0.589874268 batch PCKh 0.75\n",
      "Trained batch 1798 batch loss 0.473772 batch mAP 0.604797363 batch PCKh 0.75\n",
      "Trained batch 1799 batch loss 0.399606764 batch mAP 0.637054443 batch PCKh 0.75\n",
      "Trained batch 1800 batch loss 0.474609613 batch mAP 0.591339111 batch PCKh 0.5\n",
      "Trained batch 1801 batch loss 0.474954456 batch mAP 0.640289307 batch PCKh 0.5625\n",
      "Trained batch 1802 batch loss 0.461207122 batch mAP 0.641479492 batch PCKh 0.5625\n",
      "Trained batch 1803 batch loss 0.481956184 batch mAP 0.598327637 batch PCKh 0.875\n",
      "Trained batch 1804 batch loss 0.471458405 batch mAP 0.604431152 batch PCKh 0.4375\n",
      "Trained batch 1805 batch loss 0.41374734 batch mAP 0.668548584 batch PCKh 0.75\n",
      "Trained batch 1806 batch loss 0.387126505 batch mAP 0.697906494 batch PCKh 0.625\n",
      "Trained batch 1807 batch loss 0.550942659 batch mAP 0.656890869 batch PCKh 0.625\n",
      "Trained batch 1808 batch loss 0.436883926 batch mAP 0.656524658 batch PCKh 0.75\n",
      "Trained batch 1809 batch loss 0.549788594 batch mAP 0.613616943 batch PCKh 0.125\n",
      "Trained batch 1810 batch loss 0.493017256 batch mAP 0.568878174 batch PCKh 0.5\n",
      "Trained batch 1811 batch loss 0.480449438 batch mAP 0.60849 batch PCKh 0.5\n",
      "Trained batch 1812 batch loss 0.446410269 batch mAP 0.606750488 batch PCKh 0.375\n",
      "Trained batch 1813 batch loss 0.413148135 batch mAP 0.650939941 batch PCKh 0.625\n",
      "Trained batch 1814 batch loss 0.511023045 batch mAP 0.596923828 batch PCKh 0.375\n",
      "Trained batch 1815 batch loss 0.494011641 batch mAP 0.62689209 batch PCKh 0.3125\n",
      "Trained batch 1816 batch loss 0.49424997 batch mAP 0.626586914 batch PCKh 0.1875\n",
      "Trained batch 1817 batch loss 0.496889114 batch mAP 0.659881592 batch PCKh 0.3125\n",
      "Trained batch 1818 batch loss 0.412445 batch mAP 0.641479492 batch PCKh 0.3125\n",
      "Trained batch 1819 batch loss 0.526827574 batch mAP 0.562133789 batch PCKh 0.5625\n",
      "Trained batch 1820 batch loss 0.443254471 batch mAP 0.624359131 batch PCKh 0.375\n",
      "Trained batch 1821 batch loss 0.579152226 batch mAP 0.585510254 batch PCKh 0.8125\n",
      "Trained batch 1822 batch loss 0.59143889 batch mAP 0.579986572 batch PCKh 0.3125\n",
      "Trained batch 1823 batch loss 0.539498448 batch mAP 0.606140137 batch PCKh 0.75\n",
      "Trained batch 1824 batch loss 0.533368 batch mAP 0.647583 batch PCKh 0.8125\n",
      "Trained batch 1825 batch loss 0.461498976 batch mAP 0.609100342 batch PCKh 0.4375\n",
      "Trained batch 1826 batch loss 0.478425026 batch mAP 0.652160645 batch PCKh 0.4375\n",
      "Trained batch 1827 batch loss 0.502996802 batch mAP 0.609527588 batch PCKh 0.6875\n",
      "Trained batch 1828 batch loss 0.556887269 batch mAP 0.569885254 batch PCKh 0.6875\n",
      "Trained batch 1829 batch loss 0.478743 batch mAP 0.615539551 batch PCKh 0.3125\n",
      "Trained batch 1830 batch loss 0.528214276 batch mAP 0.514526367 batch PCKh 0.5625\n",
      "Trained batch 1831 batch loss 0.527882 batch mAP 0.548797607 batch PCKh 0.625\n",
      "Trained batch 1832 batch loss 0.454268187 batch mAP 0.59185791 batch PCKh 0.3125\n",
      "Trained batch 1833 batch loss 0.469820261 batch mAP 0.650390625 batch PCKh 0.875\n",
      "Trained batch 1834 batch loss 0.471384615 batch mAP 0.620452881 batch PCKh 0.625\n",
      "Trained batch 1835 batch loss 0.455064744 batch mAP 0.544250488 batch PCKh 0\n",
      "Trained batch 1836 batch loss 0.575739622 batch mAP 0.58505249 batch PCKh 0.125\n",
      "Trained batch 1837 batch loss 0.563881 batch mAP 0.588531494 batch PCKh 0.75\n",
      "Trained batch 1838 batch loss 0.539506674 batch mAP 0.593170166 batch PCKh 0.6875\n",
      "Trained batch 1839 batch loss 0.455839962 batch mAP 0.552368164 batch PCKh 0.5\n",
      "Trained batch 1840 batch loss 0.480938435 batch mAP 0.621002197 batch PCKh 0.1875\n",
      "Trained batch 1841 batch loss 0.505166054 batch mAP 0.581634521 batch PCKh 0.5625\n",
      "Trained batch 1842 batch loss 0.470184207 batch mAP 0.679351807 batch PCKh 0.875\n",
      "Trained batch 1843 batch loss 0.577034235 batch mAP 0.612884521 batch PCKh 0.4375\n",
      "Trained batch 1844 batch loss 0.546868265 batch mAP 0.684997559 batch PCKh 0.5625\n",
      "Trained batch 1845 batch loss 0.509708703 batch mAP 0.70224 batch PCKh 0.6875\n",
      "Trained batch 1846 batch loss 0.517022371 batch mAP 0.646240234 batch PCKh 0.6875\n",
      "Trained batch 1847 batch loss 0.518838406 batch mAP 0.51159668 batch PCKh 0.5\n",
      "Trained batch 1848 batch loss 0.437555045 batch mAP 0.615478516 batch PCKh 0.875\n",
      "Trained batch 1849 batch loss 0.394621491 batch mAP 0.632965088 batch PCKh 0.6875\n",
      "Trained batch 1850 batch loss 0.464223146 batch mAP 0.581634521 batch PCKh 0.3125\n",
      "Trained batch 1851 batch loss 0.471283197 batch mAP 0.578582764 batch PCKh 0.625\n",
      "Trained batch 1852 batch loss 0.567861199 batch mAP 0.496704102 batch PCKh 0.125\n",
      "Trained batch 1853 batch loss 0.471232533 batch mAP 0.536956787 batch PCKh 0.625\n",
      "Trained batch 1854 batch loss 0.48447305 batch mAP 0.586364746 batch PCKh 0.5\n",
      "Trained batch 1855 batch loss 0.590043306 batch mAP 0.593566895 batch PCKh 0.3125\n",
      "Trained batch 1856 batch loss 0.489393979 batch mAP 0.575134277 batch PCKh 0.375\n",
      "Trained batch 1857 batch loss 0.525073409 batch mAP 0.593536377 batch PCKh 0.375\n",
      "Trained batch 1858 batch loss 0.533551931 batch mAP 0.601318359 batch PCKh 0.5625\n",
      "Trained batch 1859 batch loss 0.490355432 batch mAP 0.645019531 batch PCKh 0.625\n",
      "Trained batch 1860 batch loss 0.408697546 batch mAP 0.674407959 batch PCKh 0.6875\n",
      "Trained batch 1861 batch loss 0.353531212 batch mAP 0.663208 batch PCKh 0.75\n",
      "Trained batch 1862 batch loss 0.490794599 batch mAP 0.547790527 batch PCKh 0.3125\n",
      "Trained batch 1863 batch loss 0.514343381 batch mAP 0.557220459 batch PCKh 0.125\n",
      "Trained batch 1864 batch loss 0.532463193 batch mAP 0.561676 batch PCKh 0.25\n",
      "Trained batch 1865 batch loss 0.524405956 batch mAP 0.56854248 batch PCKh 0.25\n",
      "Trained batch 1866 batch loss 0.555507779 batch mAP 0.568511963 batch PCKh 0.125\n",
      "Trained batch 1867 batch loss 0.586636603 batch mAP 0.559936523 batch PCKh 0.375\n",
      "Trained batch 1868 batch loss 0.585530519 batch mAP 0.540344238 batch PCKh 0.375\n",
      "Trained batch 1869 batch loss 0.486672401 batch mAP 0.591918945 batch PCKh 0.25\n",
      "Trained batch 1870 batch loss 0.479404867 batch mAP 0.631134033 batch PCKh 0.5625\n",
      "Trained batch 1871 batch loss 0.504229426 batch mAP 0.634094238 batch PCKh 0.25\n",
      "Trained batch 1872 batch loss 0.53780818 batch mAP 0.643219 batch PCKh 0.375\n",
      "Trained batch 1873 batch loss 0.523341656 batch mAP 0.613708496 batch PCKh 0.4375\n",
      "Trained batch 1874 batch loss 0.647567749 batch mAP 0.578613281 batch PCKh 0.3125\n",
      "Trained batch 1875 batch loss 0.443019509 batch mAP 0.5418396 batch PCKh 0.3125\n",
      "Trained batch 1876 batch loss 0.36412847 batch mAP 0.538299561 batch PCKh 0.1875\n",
      "Trained batch 1877 batch loss 0.493717194 batch mAP 0.511779785 batch PCKh 0.25\n",
      "Trained batch 1878 batch loss 0.566058338 batch mAP 0.488739 batch PCKh 0.125\n",
      "Trained batch 1879 batch loss 0.539112628 batch mAP 0.511047363 batch PCKh 0.625\n",
      "Trained batch 1880 batch loss 0.657944441 batch mAP 0.47869873 batch PCKh 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1881 batch loss 0.562294602 batch mAP 0.540496826 batch PCKh 0.625\n",
      "Trained batch 1882 batch loss 0.578939557 batch mAP 0.553588867 batch PCKh 0.4375\n",
      "Trained batch 1883 batch loss 0.52857691 batch mAP 0.685852051 batch PCKh 0.5\n",
      "Trained batch 1884 batch loss 0.425189078 batch mAP 0.690795898 batch PCKh 0.4375\n",
      "Trained batch 1885 batch loss 0.419556767 batch mAP 0.695831299 batch PCKh 0.4375\n",
      "Trained batch 1886 batch loss 0.455148131 batch mAP 0.733886719 batch PCKh 0.4375\n",
      "Trained batch 1887 batch loss 0.41689381 batch mAP 0.702972412 batch PCKh 0.75\n",
      "Trained batch 1888 batch loss 0.453802 batch mAP 0.6534729 batch PCKh 0.5\n",
      "Trained batch 1889 batch loss 0.432688206 batch mAP 0.671661377 batch PCKh 0.625\n",
      "Trained batch 1890 batch loss 0.431153744 batch mAP 0.664215088 batch PCKh 0.625\n",
      "Trained batch 1891 batch loss 0.452044 batch mAP 0.659057617 batch PCKh 0.5625\n",
      "Trained batch 1892 batch loss 0.452267319 batch mAP 0.677032471 batch PCKh 0.625\n",
      "Trained batch 1893 batch loss 0.408048391 batch mAP 0.744140625 batch PCKh 0.5\n",
      "Trained batch 1894 batch loss 0.436866343 batch mAP 0.695800781 batch PCKh 0.25\n",
      "Trained batch 1895 batch loss 0.449557275 batch mAP 0.634765625 batch PCKh 0.3125\n",
      "Trained batch 1896 batch loss 0.484410763 batch mAP 0.634765625 batch PCKh 0.5625\n",
      "Trained batch 1897 batch loss 0.426698565 batch mAP 0.639953613 batch PCKh 0.6875\n",
      "Trained batch 1898 batch loss 0.46305 batch mAP 0.635009766 batch PCKh 0.875\n",
      "Trained batch 1899 batch loss 0.533290625 batch mAP 0.589050293 batch PCKh 0.875\n",
      "Trained batch 1900 batch loss 0.45694381 batch mAP 0.662109375 batch PCKh 0.75\n",
      "Trained batch 1901 batch loss 0.446926653 batch mAP 0.682525635 batch PCKh 0.75\n",
      "Trained batch 1902 batch loss 0.48884213 batch mAP 0.589660645 batch PCKh 0.75\n",
      "Trained batch 1903 batch loss 0.593878865 batch mAP 0.551849365 batch PCKh 0.875\n",
      "Trained batch 1904 batch loss 0.540624321 batch mAP 0.597473145 batch PCKh 0.875\n",
      "Trained batch 1905 batch loss 0.474065602 batch mAP 0.609588623 batch PCKh 0.8125\n",
      "Trained batch 1906 batch loss 0.549242854 batch mAP 0.589111328 batch PCKh 0.5625\n",
      "Trained batch 1907 batch loss 0.499206483 batch mAP 0.577667236 batch PCKh 0.5\n",
      "Trained batch 1908 batch loss 0.477482378 batch mAP 0.544525146 batch PCKh 0.5\n",
      "Trained batch 1909 batch loss 0.604433417 batch mAP 0.526794434 batch PCKh 0.5\n",
      "Trained batch 1910 batch loss 0.559317708 batch mAP 0.546661377 batch PCKh 0.5\n",
      "Trained batch 1911 batch loss 0.590547919 batch mAP 0.553710938 batch PCKh 0.3125\n",
      "Trained batch 1912 batch loss 0.545089781 batch mAP 0.589386 batch PCKh 0.125\n",
      "Trained batch 1913 batch loss 0.482209414 batch mAP 0.663604736 batch PCKh 0.875\n",
      "Trained batch 1914 batch loss 0.572690308 batch mAP 0.575531 batch PCKh 0.625\n",
      "Trained batch 1915 batch loss 0.528843164 batch mAP 0.613922119 batch PCKh 0.625\n",
      "Trained batch 1916 batch loss 0.484273195 batch mAP 0.582519531 batch PCKh 0.3125\n",
      "Trained batch 1917 batch loss 0.439983428 batch mAP 0.560913086 batch PCKh 0.5625\n",
      "Trained batch 1918 batch loss 0.499158323 batch mAP 0.547210693 batch PCKh 0.1875\n",
      "Trained batch 1919 batch loss 0.478475213 batch mAP 0.549255371 batch PCKh 0.875\n",
      "Trained batch 1920 batch loss 0.42772153 batch mAP 0.518096924 batch PCKh 0.3125\n",
      "Trained batch 1921 batch loss 0.508901417 batch mAP 0.570220947 batch PCKh 0.625\n",
      "Trained batch 1922 batch loss 0.411755681 batch mAP 0.534301758 batch PCKh 0.5625\n",
      "Trained batch 1923 batch loss 0.50053525 batch mAP 0.575683594 batch PCKh 0.5\n",
      "Trained batch 1924 batch loss 0.551481724 batch mAP 0.599121094 batch PCKh 0.75\n",
      "Trained batch 1925 batch loss 0.592673182 batch mAP 0.592956543 batch PCKh 0.4375\n",
      "Trained batch 1926 batch loss 0.453109026 batch mAP 0.675933838 batch PCKh 0.75\n",
      "Trained batch 1927 batch loss 0.557395279 batch mAP 0.650115967 batch PCKh 0.5625\n",
      "Trained batch 1928 batch loss 0.472300768 batch mAP 0.658905 batch PCKh 0.4375\n",
      "Trained batch 1929 batch loss 0.477464676 batch mAP 0.708282471 batch PCKh 0.8125\n",
      "Trained batch 1930 batch loss 0.467305213 batch mAP 0.64276123 batch PCKh 0.125\n",
      "Trained batch 1931 batch loss 0.552924514 batch mAP 0.669189453 batch PCKh 0.75\n",
      "Trained batch 1932 batch loss 0.485259086 batch mAP 0.72869873 batch PCKh 0.625\n",
      "Trained batch 1933 batch loss 0.452970207 batch mAP 0.710479736 batch PCKh 0.6875\n",
      "Trained batch 1934 batch loss 0.549234331 batch mAP 0.711975098 batch PCKh 0.5625\n",
      "Trained batch 1935 batch loss 0.473806798 batch mAP 0.707214355 batch PCKh 0.5625\n",
      "Trained batch 1936 batch loss 0.544399917 batch mAP 0.648071289 batch PCKh 0.5625\n",
      "Trained batch 1937 batch loss 0.517258525 batch mAP 0.594512939 batch PCKh 0.75\n",
      "Trained batch 1938 batch loss 0.468505144 batch mAP 0.622192383 batch PCKh 0.75\n",
      "Trained batch 1939 batch loss 0.454644978 batch mAP 0.621826172 batch PCKh 0.6875\n",
      "Trained batch 1940 batch loss 0.458536685 batch mAP 0.586792 batch PCKh 0.6875\n",
      "Trained batch 1941 batch loss 0.523931086 batch mAP 0.558258057 batch PCKh 0.75\n",
      "Trained batch 1942 batch loss 0.515982091 batch mAP 0.660461426 batch PCKh 0.5\n",
      "Trained batch 1943 batch loss 0.457855 batch mAP 0.611633301 batch PCKh 0.625\n",
      "Trained batch 1944 batch loss 0.472702742 batch mAP 0.661102295 batch PCKh 0.25\n",
      "Trained batch 1945 batch loss 0.524469316 batch mAP 0.690673828 batch PCKh 0.375\n",
      "Trained batch 1946 batch loss 0.515488267 batch mAP 0.690948486 batch PCKh 0.625\n",
      "Trained batch 1947 batch loss 0.446549624 batch mAP 0.70526123 batch PCKh 0.5\n",
      "Trained batch 1948 batch loss 0.457479 batch mAP 0.711517334 batch PCKh 0.3125\n",
      "Trained batch 1949 batch loss 0.48273918 batch mAP 0.725982666 batch PCKh 0.375\n",
      "Trained batch 1950 batch loss 0.420229882 batch mAP 0.721008301 batch PCKh 0.4375\n",
      "Trained batch 1951 batch loss 0.398446769 batch mAP 0.753479 batch PCKh 0.4375\n",
      "Trained batch 1952 batch loss 0.526332378 batch mAP 0.65524292 batch PCKh 0.125\n",
      "Trained batch 1953 batch loss 0.499645144 batch mAP 0.657012939 batch PCKh 0.75\n",
      "Trained batch 1954 batch loss 0.483385623 batch mAP 0.650665283 batch PCKh 0.6875\n",
      "Trained batch 1955 batch loss 0.437176496 batch mAP 0.685699463 batch PCKh 0.875\n",
      "Trained batch 1956 batch loss 0.515483379 batch mAP 0.654815674 batch PCKh 0.5\n",
      "Trained batch 1957 batch loss 0.495128572 batch mAP 0.630218506 batch PCKh 0.875\n",
      "Trained batch 1958 batch loss 0.483287394 batch mAP 0.628967285 batch PCKh 0.75\n",
      "Trained batch 1959 batch loss 0.467499018 batch mAP 0.673614502 batch PCKh 0.5625\n",
      "Trained batch 1960 batch loss 0.508280158 batch mAP 0.681793213 batch PCKh 0.3125\n",
      "Trained batch 1961 batch loss 0.430342495 batch mAP 0.712585449 batch PCKh 0.6875\n",
      "Trained batch 1962 batch loss 0.384634435 batch mAP 0.738433838 batch PCKh 0.6875\n",
      "Trained batch 1963 batch loss 0.454574406 batch mAP 0.64074707 batch PCKh 0.75\n",
      "Trained batch 1964 batch loss 0.472985923 batch mAP 0.694244385 batch PCKh 0.75\n",
      "Trained batch 1965 batch loss 0.473476708 batch mAP 0.650543213 batch PCKh 0.4375\n",
      "Trained batch 1966 batch loss 0.496393621 batch mAP 0.674346924 batch PCKh 0.8125\n",
      "Trained batch 1967 batch loss 0.472465456 batch mAP 0.674743652 batch PCKh 0.8125\n",
      "Trained batch 1968 batch loss 0.468360066 batch mAP 0.692169189 batch PCKh 0.3125\n",
      "Trained batch 1969 batch loss 0.428459287 batch mAP 0.69732666 batch PCKh 0.5\n",
      "Trained batch 1970 batch loss 0.415356517 batch mAP 0.696777344 batch PCKh 0.3125\n",
      "Trained batch 1971 batch loss 0.429095894 batch mAP 0.715698242 batch PCKh 0.5\n",
      "Trained batch 1972 batch loss 0.423175931 batch mAP 0.768249512 batch PCKh 0.5625\n",
      "Trained batch 1973 batch loss 0.419790804 batch mAP 0.675018311 batch PCKh 0.375\n",
      "Trained batch 1974 batch loss 0.444544077 batch mAP 0.708190918 batch PCKh 0.4375\n",
      "Trained batch 1975 batch loss 0.390678585 batch mAP 0.716339111 batch PCKh 0.4375\n",
      "Trained batch 1976 batch loss 0.394292355 batch mAP 0.703430176 batch PCKh 0.6875\n",
      "Trained batch 1977 batch loss 0.436804533 batch mAP 0.709838867 batch PCKh 0.625\n",
      "Trained batch 1978 batch loss 0.487258613 batch mAP 0.6534729 batch PCKh 0.125\n",
      "Trained batch 1979 batch loss 0.503493309 batch mAP 0.665771484 batch PCKh 0.4375\n",
      "Trained batch 1980 batch loss 0.58606565 batch mAP 0.539581299 batch PCKh 0.25\n",
      "Trained batch 1981 batch loss 0.508370876 batch mAP 0.620239258 batch PCKh 0.4375\n",
      "Trained batch 1982 batch loss 0.50785625 batch mAP 0.596496582 batch PCKh 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1983 batch loss 0.44784683 batch mAP 0.618927 batch PCKh 0.125\n",
      "Trained batch 1984 batch loss 0.440040827 batch mAP 0.600158691 batch PCKh 0\n",
      "Trained batch 1985 batch loss 0.508571744 batch mAP 0.557403564 batch PCKh 0\n",
      "Trained batch 1986 batch loss 0.485113442 batch mAP 0.55960083 batch PCKh 0.25\n",
      "Trained batch 1987 batch loss 0.444671422 batch mAP 0.600738525 batch PCKh 0.5625\n",
      "Trained batch 1988 batch loss 0.514534831 batch mAP 0.570129395 batch PCKh 0.3125\n",
      "Trained batch 1989 batch loss 0.377008766 batch mAP 0.678741455 batch PCKh 0.75\n",
      "Trained batch 1990 batch loss 0.480262965 batch mAP 0.671600342 batch PCKh 0.4375\n",
      "Trained batch 1991 batch loss 0.459847629 batch mAP 0.612640381 batch PCKh 0.5\n",
      "Trained batch 1992 batch loss 0.439171731 batch mAP 0.607269287 batch PCKh 0.5\n",
      "Trained batch 1993 batch loss 0.52906847 batch mAP 0.609558105 batch PCKh 0.3125\n",
      "Trained batch 1994 batch loss 0.521718562 batch mAP 0.601196289 batch PCKh 0.5625\n",
      "Trained batch 1995 batch loss 0.491584808 batch mAP 0.626251221 batch PCKh 0.75\n",
      "Trained batch 1996 batch loss 0.654786766 batch mAP 0.585388184 batch PCKh 0.6875\n",
      "Trained batch 1997 batch loss 0.5105865 batch mAP 0.666503906 batch PCKh 0.375\n",
      "Trained batch 1998 batch loss 0.512355566 batch mAP 0.709075928 batch PCKh 0.75\n",
      "Trained batch 1999 batch loss 0.517229 batch mAP 0.637420654 batch PCKh 0.5\n",
      "Trained batch 2000 batch loss 0.470033079 batch mAP 0.698944092 batch PCKh 0.625\n",
      "Trained batch 2001 batch loss 0.456790715 batch mAP 0.72277832 batch PCKh 0.4375\n",
      "Trained batch 2002 batch loss 0.477495879 batch mAP 0.750732422 batch PCKh 0.5625\n",
      "Trained batch 2003 batch loss 0.540704191 batch mAP 0.639312744 batch PCKh 0.625\n",
      "Trained batch 2004 batch loss 0.520029306 batch mAP 0.618377686 batch PCKh 0.6875\n",
      "Trained batch 2005 batch loss 0.480319083 batch mAP 0.637817383 batch PCKh 0.75\n",
      "Trained batch 2006 batch loss 0.49575454 batch mAP 0.643157959 batch PCKh 0.6875\n",
      "Trained batch 2007 batch loss 0.561678827 batch mAP 0.605407715 batch PCKh 0.75\n",
      "Trained batch 2008 batch loss 0.48004216 batch mAP 0.584991455 batch PCKh 0.6875\n",
      "Trained batch 2009 batch loss 0.510099769 batch mAP 0.612213135 batch PCKh 0.6875\n",
      "Trained batch 2010 batch loss 0.487844169 batch mAP 0.6144104 batch PCKh 0.625\n",
      "Trained batch 2011 batch loss 0.537759602 batch mAP 0.600952148 batch PCKh 0.5625\n",
      "Trained batch 2012 batch loss 0.582633138 batch mAP 0.615264893 batch PCKh 0.5625\n",
      "Trained batch 2013 batch loss 0.506885767 batch mAP 0.600952148 batch PCKh 0.6875\n",
      "Trained batch 2014 batch loss 0.514731586 batch mAP 0.590454102 batch PCKh 0.625\n",
      "Trained batch 2015 batch loss 0.512622535 batch mAP 0.607635498 batch PCKh 0.5\n",
      "Trained batch 2016 batch loss 0.437406659 batch mAP 0.632049561 batch PCKh 0.75\n",
      "Trained batch 2017 batch loss 0.496285945 batch mAP 0.637146 batch PCKh 0.75\n",
      "Trained batch 2018 batch loss 0.498803675 batch mAP 0.617157 batch PCKh 0.5625\n",
      "Trained batch 2019 batch loss 0.408956736 batch mAP 0.624847412 batch PCKh 0.5625\n",
      "Trained batch 2020 batch loss 0.567067623 batch mAP 0.596466064 batch PCKh 0.6875\n",
      "Trained batch 2021 batch loss 0.604821682 batch mAP 0.514160156 batch PCKh 0.6875\n",
      "Trained batch 2022 batch loss 0.513464868 batch mAP 0.621337891 batch PCKh 0.5625\n",
      "Trained batch 2023 batch loss 0.439235568 batch mAP 0.612915039 batch PCKh 0.625\n",
      "Trained batch 2024 batch loss 0.456562638 batch mAP 0.561279297 batch PCKh 0.625\n",
      "Trained batch 2025 batch loss 0.435063779 batch mAP 0.666748047 batch PCKh 0.8125\n",
      "Trained batch 2026 batch loss 0.433335781 batch mAP 0.714874268 batch PCKh 0.625\n",
      "Trained batch 2027 batch loss 0.366266161 batch mAP 0.677612305 batch PCKh 0.6875\n",
      "Trained batch 2028 batch loss 0.40994 batch mAP 0.698730469 batch PCKh 0.6875\n",
      "Trained batch 2029 batch loss 0.418411255 batch mAP 0.687713623 batch PCKh 0.75\n",
      "Trained batch 2030 batch loss 0.430203438 batch mAP 0.699188232 batch PCKh 0.875\n",
      "Trained batch 2031 batch loss 0.443567902 batch mAP 0.693634033 batch PCKh 0.5625\n",
      "Trained batch 2032 batch loss 0.464920044 batch mAP 0.666534424 batch PCKh 0.625\n",
      "Trained batch 2033 batch loss 0.481632292 batch mAP 0.677001953 batch PCKh 0.875\n",
      "Trained batch 2034 batch loss 0.474359214 batch mAP 0.635742188 batch PCKh 0.75\n",
      "Trained batch 2035 batch loss 0.438955963 batch mAP 0.640197754 batch PCKh 0.4375\n",
      "Trained batch 2036 batch loss 0.338051915 batch mAP 0.703918457 batch PCKh 0.375\n",
      "Trained batch 2037 batch loss 0.368492424 batch mAP 0.672912598 batch PCKh 0.6875\n",
      "Trained batch 2038 batch loss 0.45846653 batch mAP 0.662109375 batch PCKh 0.75\n",
      "Trained batch 2039 batch loss 0.463747323 batch mAP 0.643859863 batch PCKh 0.6875\n",
      "Trained batch 2040 batch loss 0.535102725 batch mAP 0.627716064 batch PCKh 0.4375\n",
      "Trained batch 2041 batch loss 0.548239708 batch mAP 0.581573486 batch PCKh 0.8125\n",
      "Trained batch 2042 batch loss 0.541759968 batch mAP 0.643463135 batch PCKh 0.1875\n",
      "Trained batch 2043 batch loss 0.470281541 batch mAP 0.652069092 batch PCKh 0.4375\n",
      "Trained batch 2044 batch loss 0.643680215 batch mAP 0.585083 batch PCKh 0.875\n",
      "Trained batch 2045 batch loss 0.459082901 batch mAP 0.654693604 batch PCKh 0.5\n",
      "Trained batch 2046 batch loss 0.551885605 batch mAP 0.602722168 batch PCKh 0.875\n",
      "Trained batch 2047 batch loss 0.509593427 batch mAP 0.635314941 batch PCKh 0.375\n",
      "Trained batch 2048 batch loss 0.529754579 batch mAP 0.631011963 batch PCKh 0.625\n",
      "Trained batch 2049 batch loss 0.579063535 batch mAP 0.65032959 batch PCKh 0.3125\n",
      "Trained batch 2050 batch loss 0.577861309 batch mAP 0.627044678 batch PCKh 0.6875\n",
      "Trained batch 2051 batch loss 0.491121769 batch mAP 0.66027832 batch PCKh 0.8125\n",
      "Trained batch 2052 batch loss 0.411110431 batch mAP 0.710540771 batch PCKh 0.6875\n",
      "Trained batch 2053 batch loss 0.4662413 batch mAP 0.689025879 batch PCKh 0.5625\n",
      "Trained batch 2054 batch loss 0.525240541 batch mAP 0.610809326 batch PCKh 0.4375\n",
      "Trained batch 2055 batch loss 0.405126691 batch mAP 0.626586914 batch PCKh 0.1875\n",
      "Trained batch 2056 batch loss 0.400119483 batch mAP 0.681549072 batch PCKh 0.4375\n",
      "Trained batch 2057 batch loss 0.426586181 batch mAP 0.629089355 batch PCKh 0.6875\n",
      "Trained batch 2058 batch loss 0.395976216 batch mAP 0.636108398 batch PCKh 0\n",
      "Trained batch 2059 batch loss 0.432436883 batch mAP 0.612609863 batch PCKh 0.75\n",
      "Trained batch 2060 batch loss 0.40378204 batch mAP 0.642822266 batch PCKh 0.5625\n",
      "Trained batch 2061 batch loss 0.379199326 batch mAP 0.626800537 batch PCKh 0.6875\n",
      "Trained batch 2062 batch loss 0.446448952 batch mAP 0.564849854 batch PCKh 0.75\n",
      "Trained batch 2063 batch loss 0.452734917 batch mAP 0.584533691 batch PCKh 0.75\n",
      "Trained batch 2064 batch loss 0.407785177 batch mAP 0.596405 batch PCKh 0.125\n",
      "Trained batch 2065 batch loss 0.378041267 batch mAP 0.640014648 batch PCKh 0.75\n",
      "Trained batch 2066 batch loss 0.337572813 batch mAP 0.660095215 batch PCKh 0.6875\n",
      "Trained batch 2067 batch loss 0.359751284 batch mAP 0.696289062 batch PCKh 0.75\n",
      "Trained batch 2068 batch loss 0.366415918 batch mAP 0.611846924 batch PCKh 0\n",
      "Trained batch 2069 batch loss 0.349494934 batch mAP 0.665130615 batch PCKh 0\n",
      "Trained batch 2070 batch loss 0.307335794 batch mAP 0.692382812 batch PCKh 0\n",
      "Trained batch 2071 batch loss 0.310148418 batch mAP 0.6980896 batch PCKh 0.5625\n",
      "Trained batch 2072 batch loss 0.452811331 batch mAP 0.662445068 batch PCKh 0.5\n",
      "Trained batch 2073 batch loss 0.449194342 batch mAP 0.667572 batch PCKh 0.625\n",
      "Trained batch 2074 batch loss 0.469442934 batch mAP 0.692077637 batch PCKh 0.5625\n",
      "Trained batch 2075 batch loss 0.489505172 batch mAP 0.699951172 batch PCKh 0.5\n",
      "Trained batch 2076 batch loss 0.440509439 batch mAP 0.709564209 batch PCKh 0.75\n",
      "Trained batch 2077 batch loss 0.558248043 batch mAP 0.643920898 batch PCKh 0.4375\n",
      "Trained batch 2078 batch loss 0.436134756 batch mAP 0.703460693 batch PCKh 0.25\n",
      "Trained batch 2079 batch loss 0.472222447 batch mAP 0.688690186 batch PCKh 0.5\n",
      "Trained batch 2080 batch loss 0.383605689 batch mAP 0.717437744 batch PCKh 0.625\n",
      "Trained batch 2081 batch loss 0.508589149 batch mAP 0.690887451 batch PCKh 0.75\n",
      "Trained batch 2082 batch loss 0.446671605 batch mAP 0.681365967 batch PCKh 0.125\n",
      "Trained batch 2083 batch loss 0.359929293 batch mAP 0.668060303 batch PCKh 0\n",
      "Trained batch 2084 batch loss 0.410368 batch mAP 0.67880249 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2085 batch loss 0.424407393 batch mAP 0.663574219 batch PCKh 0.375\n",
      "Trained batch 2086 batch loss 0.468350887 batch mAP 0.6512146 batch PCKh 0.125\n",
      "Trained batch 2087 batch loss 0.426335484 batch mAP 0.707214355 batch PCKh 0.5\n",
      "Trained batch 2088 batch loss 0.410245657 batch mAP 0.704559326 batch PCKh 0.8125\n",
      "Trained batch 2089 batch loss 0.518485904 batch mAP 0.624938965 batch PCKh 0.5\n",
      "Trained batch 2090 batch loss 0.461077631 batch mAP 0.665252686 batch PCKh 0.375\n",
      "Trained batch 2091 batch loss 0.545139 batch mAP 0.677001953 batch PCKh 0.8125\n",
      "Trained batch 2092 batch loss 0.438315898 batch mAP 0.69128418 batch PCKh 0.625\n",
      "Trained batch 2093 batch loss 0.454810202 batch mAP 0.724334717 batch PCKh 0.9375\n",
      "Trained batch 2094 batch loss 0.439386636 batch mAP 0.742523193 batch PCKh 0.625\n",
      "Trained batch 2095 batch loss 0.353270411 batch mAP 0.767974854 batch PCKh 0.6875\n",
      "Trained batch 2096 batch loss 0.427359581 batch mAP 0.729705811 batch PCKh 0.6875\n",
      "Trained batch 2097 batch loss 0.496369839 batch mAP 0.544891357 batch PCKh 0.4375\n",
      "Trained batch 2098 batch loss 0.451103032 batch mAP 0.548309326 batch PCKh 0.0625\n",
      "Trained batch 2099 batch loss 0.533881783 batch mAP 0.472625732 batch PCKh 0.75\n",
      "Trained batch 2100 batch loss 0.497647 batch mAP 0.624938965 batch PCKh 0.5625\n",
      "Trained batch 2101 batch loss 0.456445485 batch mAP 0.67868042 batch PCKh 0.4375\n",
      "Trained batch 2102 batch loss 0.510928631 batch mAP 0.603424072 batch PCKh 0.8125\n",
      "Trained batch 2103 batch loss 0.559019148 batch mAP 0.532806396 batch PCKh 0.3125\n",
      "Trained batch 2104 batch loss 0.360742509 batch mAP 0.624786377 batch PCKh 0.1875\n",
      "Trained batch 2105 batch loss 0.430494726 batch mAP 0.665710449 batch PCKh 0.6875\n",
      "Trained batch 2106 batch loss 0.366942316 batch mAP 0.69418335 batch PCKh 0.625\n",
      "Trained batch 2107 batch loss 0.437879503 batch mAP 0.684967041 batch PCKh 0.625\n",
      "Trained batch 2108 batch loss 0.470266581 batch mAP 0.680023193 batch PCKh 0.875\n",
      "Trained batch 2109 batch loss 0.459144592 batch mAP 0.647003174 batch PCKh 0.625\n",
      "Trained batch 2110 batch loss 0.449342161 batch mAP 0.641845703 batch PCKh 0.4375\n",
      "Trained batch 2111 batch loss 0.466952443 batch mAP 0.591827393 batch PCKh 0.625\n",
      "Trained batch 2112 batch loss 0.486313879 batch mAP 0.661468506 batch PCKh 0.5\n",
      "Trained batch 2113 batch loss 0.478038341 batch mAP 0.606658936 batch PCKh 0.3125\n",
      "Trained batch 2114 batch loss 0.524960756 batch mAP 0.62286377 batch PCKh 0.6875\n",
      "Trained batch 2115 batch loss 0.561382651 batch mAP 0.601074219 batch PCKh 0.625\n",
      "Trained batch 2116 batch loss 0.610872507 batch mAP 0.597167969 batch PCKh 0.5625\n",
      "Trained batch 2117 batch loss 0.563165247 batch mAP 0.594665527 batch PCKh 0.6875\n",
      "Trained batch 2118 batch loss 0.61437732 batch mAP 0.560180664 batch PCKh 0.25\n",
      "Trained batch 2119 batch loss 0.497232974 batch mAP 0.517822266 batch PCKh 0.8125\n",
      "Trained batch 2120 batch loss 0.502851784 batch mAP 0.550811768 batch PCKh 0.6875\n",
      "Trained batch 2121 batch loss 0.416515231 batch mAP 0.646362305 batch PCKh 0.625\n",
      "Trained batch 2122 batch loss 0.546926379 batch mAP 0.568267822 batch PCKh 0.75\n",
      "Trained batch 2123 batch loss 0.423160404 batch mAP 0.571289062 batch PCKh 0.3125\n",
      "Trained batch 2124 batch loss 0.476505518 batch mAP 0.581451416 batch PCKh 0.375\n",
      "Trained batch 2125 batch loss 0.533926725 batch mAP 0.532714844 batch PCKh 0.6875\n",
      "Trained batch 2126 batch loss 0.481716424 batch mAP 0.59387207 batch PCKh 0.75\n",
      "Trained batch 2127 batch loss 0.416499436 batch mAP 0.652374268 batch PCKh 0.6875\n",
      "Trained batch 2128 batch loss 0.446609914 batch mAP 0.647705078 batch PCKh 0.5625\n",
      "Trained batch 2129 batch loss 0.379606813 batch mAP 0.699401855 batch PCKh 0.5\n",
      "Trained batch 2130 batch loss 0.437241703 batch mAP 0.628814697 batch PCKh 0.625\n",
      "Trained batch 2131 batch loss 0.417250335 batch mAP 0.672607422 batch PCKh 0.4375\n",
      "Trained batch 2132 batch loss 0.391088158 batch mAP 0.717376709 batch PCKh 0.75\n",
      "Trained batch 2133 batch loss 0.483267695 batch mAP 0.665985107 batch PCKh 0.625\n",
      "Trained batch 2134 batch loss 0.527792215 batch mAP 0.62121582 batch PCKh 0.75\n",
      "Trained batch 2135 batch loss 0.523276 batch mAP 0.635284424 batch PCKh 0.3125\n",
      "Trained batch 2136 batch loss 0.551103592 batch mAP 0.651306152 batch PCKh 0.875\n",
      "Trained batch 2137 batch loss 0.483171165 batch mAP 0.638977051 batch PCKh 0.5625\n",
      "Trained batch 2138 batch loss 0.547426164 batch mAP 0.597198486 batch PCKh 0.75\n",
      "Trained batch 2139 batch loss 0.416826397 batch mAP 0.639801 batch PCKh 0.3125\n",
      "Trained batch 2140 batch loss 0.512216 batch mAP 0.628173828 batch PCKh 0.625\n",
      "Trained batch 2141 batch loss 0.478399098 batch mAP 0.622619629 batch PCKh 0.6875\n",
      "Trained batch 2142 batch loss 0.441703767 batch mAP 0.596221924 batch PCKh 0.25\n",
      "Trained batch 2143 batch loss 0.384843022 batch mAP 0.644226074 batch PCKh 0.4375\n",
      "Trained batch 2144 batch loss 0.496417195 batch mAP 0.570068359 batch PCKh 0.1875\n",
      "Trained batch 2145 batch loss 0.512158632 batch mAP 0.542907715 batch PCKh 0.5\n",
      "Trained batch 2146 batch loss 0.439254731 batch mAP 0.651001 batch PCKh 0.375\n",
      "Trained batch 2147 batch loss 0.4329741 batch mAP 0.706848145 batch PCKh 0.5\n",
      "Trained batch 2148 batch loss 0.499342203 batch mAP 0.662109375 batch PCKh 0.5625\n",
      "Trained batch 2149 batch loss 0.517052591 batch mAP 0.630279541 batch PCKh 0.3125\n",
      "Trained batch 2150 batch loss 0.535116494 batch mAP 0.630523682 batch PCKh 0.4375\n",
      "Trained batch 2151 batch loss 0.511843443 batch mAP 0.592346191 batch PCKh 0.25\n",
      "Trained batch 2152 batch loss 0.602208257 batch mAP 0.656951904 batch PCKh 0.375\n",
      "Trained batch 2153 batch loss 0.47224319 batch mAP 0.657135 batch PCKh 0.6875\n",
      "Trained batch 2154 batch loss 0.559582889 batch mAP 0.651306152 batch PCKh 0.6875\n",
      "Trained batch 2155 batch loss 0.539280951 batch mAP 0.644012451 batch PCKh 0.6875\n",
      "Trained batch 2156 batch loss 0.489049286 batch mAP 0.644958496 batch PCKh 0.5\n",
      "Trained batch 2157 batch loss 0.419376403 batch mAP 0.669769287 batch PCKh 0.4375\n",
      "Trained batch 2158 batch loss 0.543345213 batch mAP 0.661224365 batch PCKh 0.5\n",
      "Trained batch 2159 batch loss 0.47970891 batch mAP 0.615600586 batch PCKh 0.5625\n",
      "Trained batch 2160 batch loss 0.481960535 batch mAP 0.602111816 batch PCKh 0.25\n",
      "Trained batch 2161 batch loss 0.480176419 batch mAP 0.635742188 batch PCKh 0.125\n",
      "Trained batch 2162 batch loss 0.541333437 batch mAP 0.676147461 batch PCKh 0.75\n",
      "Trained batch 2163 batch loss 0.548972 batch mAP 0.658905 batch PCKh 0.6875\n",
      "Trained batch 2164 batch loss 0.462719589 batch mAP 0.55770874 batch PCKh 0.4375\n",
      "Trained batch 2165 batch loss 0.502451897 batch mAP 0.627105713 batch PCKh 0.375\n",
      "Trained batch 2166 batch loss 0.493314326 batch mAP 0.677215576 batch PCKh 0.5\n",
      "Trained batch 2167 batch loss 0.51667738 batch mAP 0.657348633 batch PCKh 0.8125\n",
      "Trained batch 2168 batch loss 0.500421047 batch mAP 0.654205322 batch PCKh 0.375\n",
      "Trained batch 2169 batch loss 0.466673315 batch mAP 0.686187744 batch PCKh 0.75\n",
      "Trained batch 2170 batch loss 0.3719244 batch mAP 0.622375488 batch PCKh 0.6875\n",
      "Trained batch 2171 batch loss 0.455307275 batch mAP 0.642272949 batch PCKh 0.4375\n",
      "Trained batch 2172 batch loss 0.371859252 batch mAP 0.718811035 batch PCKh 0.5\n",
      "Trained batch 2173 batch loss 0.430208564 batch mAP 0.622558594 batch PCKh 0.75\n",
      "Trained batch 2174 batch loss 0.559640765 batch mAP 0.661651611 batch PCKh 0.5625\n",
      "Trained batch 2175 batch loss 0.554974854 batch mAP 0.596618652 batch PCKh 0.6875\n",
      "Trained batch 2176 batch loss 0.600150645 batch mAP 0.629638672 batch PCKh 0.8125\n",
      "Trained batch 2177 batch loss 0.487905085 batch mAP 0.583251953 batch PCKh 0.75\n",
      "Trained batch 2178 batch loss 0.531994462 batch mAP 0.633911133 batch PCKh 0.75\n",
      "Trained batch 2179 batch loss 0.578192353 batch mAP 0.615844727 batch PCKh 0.5625\n",
      "Trained batch 2180 batch loss 0.513687372 batch mAP 0.638122559 batch PCKh 0.6875\n",
      "Trained batch 2181 batch loss 0.571131289 batch mAP 0.646698 batch PCKh 0.8125\n",
      "Trained batch 2182 batch loss 0.565839112 batch mAP 0.633789062 batch PCKh 0.375\n",
      "Trained batch 2183 batch loss 0.631652236 batch mAP 0.596496582 batch PCKh 0.125\n",
      "Trained batch 2184 batch loss 0.481881887 batch mAP 0.654388428 batch PCKh 0.5625\n",
      "Trained batch 2185 batch loss 0.49122718 batch mAP 0.669677734 batch PCKh 0.5\n",
      "Trained batch 2186 batch loss 0.523085356 batch mAP 0.686187744 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2187 batch loss 0.421404 batch mAP 0.709655762 batch PCKh 0.6875\n",
      "Trained batch 2188 batch loss 0.627510726 batch mAP 0.634368896 batch PCKh 0.5\n",
      "Trained batch 2189 batch loss 0.559751749 batch mAP 0.638000488 batch PCKh 0.1875\n",
      "Trained batch 2190 batch loss 0.571731687 batch mAP 0.570709229 batch PCKh 0.5\n",
      "Trained batch 2191 batch loss 0.556103408 batch mAP 0.584655762 batch PCKh 0.6875\n",
      "Trained batch 2192 batch loss 0.544785261 batch mAP 0.656036377 batch PCKh 0.375\n",
      "Trained batch 2193 batch loss 0.473369807 batch mAP 0.645233154 batch PCKh 0.5\n",
      "Trained batch 2194 batch loss 0.432363749 batch mAP 0.601715088 batch PCKh 0.125\n",
      "Trained batch 2195 batch loss 0.421935588 batch mAP 0.666564941 batch PCKh 0.5625\n",
      "Trained batch 2196 batch loss 0.425082028 batch mAP 0.67300415 batch PCKh 0.5625\n",
      "Trained batch 2197 batch loss 0.443106353 batch mAP 0.650268555 batch PCKh 0.5\n",
      "Trained batch 2198 batch loss 0.38452363 batch mAP 0.638092041 batch PCKh 0.25\n",
      "Trained batch 2199 batch loss 0.419882715 batch mAP 0.600158691 batch PCKh 0.625\n",
      "Trained batch 2200 batch loss 0.426684618 batch mAP 0.636779785 batch PCKh 0.375\n",
      "Trained batch 2201 batch loss 0.424231887 batch mAP 0.680236816 batch PCKh 0.25\n",
      "Trained batch 2202 batch loss 0.468866944 batch mAP 0.654541 batch PCKh 0.25\n",
      "Trained batch 2203 batch loss 0.566844 batch mAP 0.573638916 batch PCKh 0.5\n",
      "Trained batch 2204 batch loss 0.472437292 batch mAP 0.696746826 batch PCKh 0.6875\n",
      "Trained batch 2205 batch loss 0.487068832 batch mAP 0.660553 batch PCKh 0.4375\n",
      "Trained batch 2206 batch loss 0.557658434 batch mAP 0.572509766 batch PCKh 0.25\n",
      "Trained batch 2207 batch loss 0.479569584 batch mAP 0.584106445 batch PCKh 0.75\n",
      "Trained batch 2208 batch loss 0.516865551 batch mAP 0.637115479 batch PCKh 0.75\n",
      "Trained batch 2209 batch loss 0.544535339 batch mAP 0.602142334 batch PCKh 0.75\n",
      "Trained batch 2210 batch loss 0.513097763 batch mAP 0.576080322 batch PCKh 0.6875\n",
      "Trained batch 2211 batch loss 0.4935669 batch mAP 0.648254395 batch PCKh 0.6875\n",
      "Trained batch 2212 batch loss 0.446017474 batch mAP 0.604187 batch PCKh 0.75\n",
      "Trained batch 2213 batch loss 0.473559558 batch mAP 0.605865479 batch PCKh 0.625\n",
      "Trained batch 2214 batch loss 0.572263122 batch mAP 0.541473389 batch PCKh 0.8125\n",
      "Trained batch 2215 batch loss 0.50072 batch mAP 0.579406738 batch PCKh 0.75\n",
      "Trained batch 2216 batch loss 0.579596698 batch mAP 0.610229492 batch PCKh 0.3125\n",
      "Trained batch 2217 batch loss 0.563193321 batch mAP 0.638214111 batch PCKh 0.5625\n",
      "Trained batch 2218 batch loss 0.47885257 batch mAP 0.683166504 batch PCKh 0\n",
      "Trained batch 2219 batch loss 0.417990714 batch mAP 0.737182617 batch PCKh 0.4375\n",
      "Trained batch 2220 batch loss 0.457543075 batch mAP 0.749694824 batch PCKh 0.4375\n",
      "Trained batch 2221 batch loss 0.467536569 batch mAP 0.710571289 batch PCKh 0.6875\n",
      "Trained batch 2222 batch loss 0.501349 batch mAP 0.696350098 batch PCKh 0.625\n",
      "Trained batch 2223 batch loss 0.510323226 batch mAP 0.670501709 batch PCKh 0.5625\n",
      "Trained batch 2224 batch loss 0.444693059 batch mAP 0.672058105 batch PCKh 0.6875\n",
      "Trained batch 2225 batch loss 0.540909648 batch mAP 0.627716064 batch PCKh 0.375\n",
      "Trained batch 2226 batch loss 0.521123052 batch mAP 0.595489502 batch PCKh 0.5625\n",
      "Trained batch 2227 batch loss 0.538808823 batch mAP 0.579071045 batch PCKh 0.625\n",
      "Trained batch 2228 batch loss 0.494225413 batch mAP 0.623962402 batch PCKh 0.8125\n",
      "Trained batch 2229 batch loss 0.544805229 batch mAP 0.593933105 batch PCKh 0.25\n",
      "Trained batch 2230 batch loss 0.450251669 batch mAP 0.65423584 batch PCKh 0.5625\n",
      "Trained batch 2231 batch loss 0.567282677 batch mAP 0.637268066 batch PCKh 0.75\n",
      "Trained batch 2232 batch loss 0.534180641 batch mAP 0.584655762 batch PCKh 0.5\n",
      "Trained batch 2233 batch loss 0.563356638 batch mAP 0.576690674 batch PCKh 0.375\n",
      "Trained batch 2234 batch loss 0.518526316 batch mAP 0.642974854 batch PCKh 0.75\n",
      "Trained batch 2235 batch loss 0.582200408 batch mAP 0.639892578 batch PCKh 0.6875\n",
      "Trained batch 2236 batch loss 0.514963746 batch mAP 0.623809814 batch PCKh 0.0625\n",
      "Trained batch 2237 batch loss 0.541833758 batch mAP 0.605438232 batch PCKh 0.75\n",
      "Trained batch 2238 batch loss 0.55949986 batch mAP 0.591552734 batch PCKh 0.5625\n",
      "Trained batch 2239 batch loss 0.541729867 batch mAP 0.579376221 batch PCKh 0.5\n",
      "Trained batch 2240 batch loss 0.575282454 batch mAP 0.566162109 batch PCKh 0.5625\n",
      "Trained batch 2241 batch loss 0.667595625 batch mAP 0.559112549 batch PCKh 0.1875\n",
      "Trained batch 2242 batch loss 0.581038952 batch mAP 0.531433105 batch PCKh 0.125\n",
      "Trained batch 2243 batch loss 0.403906524 batch mAP 0.597351074 batch PCKh 0.1875\n",
      "Trained batch 2244 batch loss 0.493220687 batch mAP 0.580993652 batch PCKh 0.4375\n",
      "Trained batch 2245 batch loss 0.40224117 batch mAP 0.682952881 batch PCKh 0.4375\n",
      "Trained batch 2246 batch loss 0.494041502 batch mAP 0.609741211 batch PCKh 0.1875\n",
      "Trained batch 2247 batch loss 0.464647442 batch mAP 0.645446777 batch PCKh 0.375\n",
      "Trained batch 2248 batch loss 0.514070094 batch mAP 0.617553711 batch PCKh 0.375\n",
      "Trained batch 2249 batch loss 0.482965529 batch mAP 0.63067627 batch PCKh 0.5\n",
      "Trained batch 2250 batch loss 0.390177101 batch mAP 0.696594238 batch PCKh 0.1875\n",
      "Trained batch 2251 batch loss 0.418086052 batch mAP 0.668518066 batch PCKh 0.5\n",
      "Trained batch 2252 batch loss 0.460096538 batch mAP 0.633148193 batch PCKh 0.5625\n",
      "Trained batch 2253 batch loss 0.583123565 batch mAP 0.629180908 batch PCKh 0.8125\n",
      "Trained batch 2254 batch loss 0.461239427 batch mAP 0.648010254 batch PCKh 0.625\n",
      "Trained batch 2255 batch loss 0.502678871 batch mAP 0.643371582 batch PCKh 0.6875\n",
      "Trained batch 2256 batch loss 0.538072 batch mAP 0.634368896 batch PCKh 0.625\n",
      "Trained batch 2257 batch loss 0.501166284 batch mAP 0.642028809 batch PCKh 0.625\n",
      "Trained batch 2258 batch loss 0.449318618 batch mAP 0.658660889 batch PCKh 0.1875\n",
      "Trained batch 2259 batch loss 0.509011865 batch mAP 0.625366211 batch PCKh 0.5625\n",
      "Trained batch 2260 batch loss 0.467518091 batch mAP 0.658111572 batch PCKh 0.1875\n",
      "Trained batch 2261 batch loss 0.39751476 batch mAP 0.653320312 batch PCKh 0.5\n",
      "Trained batch 2262 batch loss 0.471766025 batch mAP 0.665649414 batch PCKh 0.5\n",
      "Trained batch 2263 batch loss 0.46808964 batch mAP 0.67086792 batch PCKh 0.6875\n",
      "Trained batch 2264 batch loss 0.435548663 batch mAP 0.582824707 batch PCKh 0.5\n",
      "Trained batch 2265 batch loss 0.564705729 batch mAP 0.606536865 batch PCKh 0.4375\n",
      "Trained batch 2266 batch loss 0.524474263 batch mAP 0.645935059 batch PCKh 0.375\n",
      "Trained batch 2267 batch loss 0.425237834 batch mAP 0.703399658 batch PCKh 0.4375\n",
      "Trained batch 2268 batch loss 0.603034377 batch mAP 0.589111328 batch PCKh 0\n",
      "Trained batch 2269 batch loss 0.633859336 batch mAP 0.559143066 batch PCKh 0\n",
      "Trained batch 2270 batch loss 0.627644777 batch mAP 0.564056396 batch PCKh 0.375\n",
      "Trained batch 2271 batch loss 0.431938767 batch mAP 0.659179688 batch PCKh 0.75\n",
      "Trained batch 2272 batch loss 0.521485 batch mAP 0.606933594 batch PCKh 0.125\n",
      "Trained batch 2273 batch loss 0.552558959 batch mAP 0.545715332 batch PCKh 0.625\n",
      "Trained batch 2274 batch loss 0.450063378 batch mAP 0.630432129 batch PCKh 0.75\n",
      "Trained batch 2275 batch loss 0.497757763 batch mAP 0.537811279 batch PCKh 0.75\n",
      "Trained batch 2276 batch loss 0.498098791 batch mAP 0.588531494 batch PCKh 0.75\n",
      "Trained batch 2277 batch loss 0.555157542 batch mAP 0.632324219 batch PCKh 0.875\n",
      "Trained batch 2278 batch loss 0.543661833 batch mAP 0.578796387 batch PCKh 0.8125\n",
      "Trained batch 2279 batch loss 0.510281 batch mAP 0.641204834 batch PCKh 0.5\n",
      "Trained batch 2280 batch loss 0.494344771 batch mAP 0.634521484 batch PCKh 0.625\n",
      "Trained batch 2281 batch loss 0.488179743 batch mAP 0.599731445 batch PCKh 0.75\n",
      "Trained batch 2282 batch loss 0.512778163 batch mAP 0.658691406 batch PCKh 0.75\n",
      "Trained batch 2283 batch loss 0.501659334 batch mAP 0.67666626 batch PCKh 0.875\n",
      "Trained batch 2284 batch loss 0.486699939 batch mAP 0.698883057 batch PCKh 0.5\n",
      "Trained batch 2285 batch loss 0.4337354 batch mAP 0.717559814 batch PCKh 0.75\n",
      "Trained batch 2286 batch loss 0.469304621 batch mAP 0.718139648 batch PCKh 0.375\n",
      "Trained batch 2287 batch loss 0.441944838 batch mAP 0.70602417 batch PCKh 0.375\n",
      "Trained batch 2288 batch loss 0.485660166 batch mAP 0.655181885 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2289 batch loss 0.520987272 batch mAP 0.666137695 batch PCKh 0.0625\n",
      "Trained batch 2290 batch loss 0.504982054 batch mAP 0.670227051 batch PCKh 0.8125\n",
      "Trained batch 2291 batch loss 0.476304054 batch mAP 0.692077637 batch PCKh 0.5625\n",
      "Trained batch 2292 batch loss 0.436444163 batch mAP 0.672699 batch PCKh 0.4375\n",
      "Trained batch 2293 batch loss 0.469518602 batch mAP 0.609191895 batch PCKh 0.75\n",
      "Trained batch 2294 batch loss 0.474389553 batch mAP 0.607757568 batch PCKh 0.625\n",
      "Trained batch 2295 batch loss 0.405788779 batch mAP 0.627258301 batch PCKh 0.6875\n",
      "Trained batch 2296 batch loss 0.484477639 batch mAP 0.601745605 batch PCKh 0.5\n",
      "Trained batch 2297 batch loss 0.507707655 batch mAP 0.610748291 batch PCKh 0.75\n",
      "Trained batch 2298 batch loss 0.35873127 batch mAP 0.675476074 batch PCKh 0.75\n",
      "Trained batch 2299 batch loss 0.525955737 batch mAP 0.660919189 batch PCKh 0.5625\n",
      "Trained batch 2300 batch loss 0.555096924 batch mAP 0.613220215 batch PCKh 0.4375\n",
      "Trained batch 2301 batch loss 0.462673694 batch mAP 0.651123047 batch PCKh 0.375\n",
      "Trained batch 2302 batch loss 0.535558641 batch mAP 0.659942627 batch PCKh 0.4375\n",
      "Trained batch 2303 batch loss 0.511792421 batch mAP 0.732666 batch PCKh 0.4375\n",
      "Trained batch 2304 batch loss 0.529767156 batch mAP 0.70324707 batch PCKh 0.625\n",
      "Trained batch 2305 batch loss 0.584314466 batch mAP 0.593841553 batch PCKh 0.5625\n",
      "Trained batch 2306 batch loss 0.546472132 batch mAP 0.625793457 batch PCKh 0.4375\n",
      "Trained batch 2307 batch loss 0.523000598 batch mAP 0.619842529 batch PCKh 0.3125\n",
      "Trained batch 2308 batch loss 0.57677561 batch mAP 0.56539917 batch PCKh 0.6875\n",
      "Trained batch 2309 batch loss 0.504151881 batch mAP 0.543396 batch PCKh 0.625\n",
      "Trained batch 2310 batch loss 0.549706 batch mAP 0.512237549 batch PCKh 0.8125\n",
      "Trained batch 2311 batch loss 0.632260382 batch mAP 0.473358154 batch PCKh 0.625\n",
      "Trained batch 2312 batch loss 0.574906468 batch mAP 0.294708252 batch PCKh 0.5\n",
      "Trained batch 2313 batch loss 0.603583395 batch mAP 0.398712158 batch PCKh 0.5\n",
      "Trained batch 2314 batch loss 0.611644387 batch mAP 0.412414551 batch PCKh 0.625\n",
      "Trained batch 2315 batch loss 0.4652493 batch mAP 0.482635498 batch PCKh 0.5625\n",
      "Trained batch 2316 batch loss 0.463498443 batch mAP 0.610717773 batch PCKh 0.6875\n",
      "Trained batch 2317 batch loss 0.584280372 batch mAP 0.523162842 batch PCKh 0.5\n",
      "Trained batch 2318 batch loss 0.547963381 batch mAP 0.516815186 batch PCKh 0.6875\n",
      "Trained batch 2319 batch loss 0.532316744 batch mAP 0.443450928 batch PCKh 0\n",
      "Trained batch 2320 batch loss 0.534770906 batch mAP 0.504486084 batch PCKh 0.1875\n",
      "Trained batch 2321 batch loss 0.436127126 batch mAP 0.562591553 batch PCKh 0.375\n",
      "Trained batch 2322 batch loss 0.504123688 batch mAP 0.445678711 batch PCKh 0.4375\n",
      "Trained batch 2323 batch loss 0.601943672 batch mAP 0.519378662 batch PCKh 0.8125\n",
      "Trained batch 2324 batch loss 0.507183313 batch mAP 0.491790771 batch PCKh 0.5625\n",
      "Trained batch 2325 batch loss 0.530249715 batch mAP 0.525543213 batch PCKh 0.75\n",
      "Trained batch 2326 batch loss 0.467886627 batch mAP 0.566741943 batch PCKh 0.1875\n",
      "Trained batch 2327 batch loss 0.460450262 batch mAP 0.581390381 batch PCKh 0.75\n",
      "Trained batch 2328 batch loss 0.558824539 batch mAP 0.529907227 batch PCKh 0.6875\n",
      "Trained batch 2329 batch loss 0.469033122 batch mAP 0.598236084 batch PCKh 0.8125\n",
      "Trained batch 2330 batch loss 0.543384314 batch mAP 0.594451904 batch PCKh 0.75\n",
      "Trained batch 2331 batch loss 0.626516461 batch mAP 0.516326904 batch PCKh 0.375\n",
      "Trained batch 2332 batch loss 0.576445699 batch mAP 0.502655 batch PCKh 0.4375\n",
      "Trained batch 2333 batch loss 0.594434857 batch mAP 0.53515625 batch PCKh 0.5\n",
      "Trained batch 2334 batch loss 0.587472677 batch mAP 0.63092041 batch PCKh 0.75\n",
      "Trained batch 2335 batch loss 0.518064082 batch mAP 0.608428955 batch PCKh 0.8125\n",
      "Trained batch 2336 batch loss 0.578140318 batch mAP 0.515991211 batch PCKh 0.5\n",
      "Trained batch 2337 batch loss 0.593139231 batch mAP 0.538787842 batch PCKh 0.0625\n",
      "Trained batch 2338 batch loss 0.523448944 batch mAP 0.624633789 batch PCKh 0.4375\n",
      "Trained batch 2339 batch loss 0.574118495 batch mAP 0.512146 batch PCKh 0.75\n",
      "Trained batch 2340 batch loss 0.595930874 batch mAP 0.496459961 batch PCKh 0.1875\n",
      "Trained batch 2341 batch loss 0.492847502 batch mAP 0.571472168 batch PCKh 0.75\n",
      "Trained batch 2342 batch loss 0.51134932 batch mAP 0.508117676 batch PCKh 0.5625\n",
      "Trained batch 2343 batch loss 0.57819128 batch mAP 0.509674072 batch PCKh 0.5\n",
      "Trained batch 2344 batch loss 0.584805 batch mAP 0.565917969 batch PCKh 0.8125\n",
      "Trained batch 2345 batch loss 0.53795737 batch mAP 0.571075439 batch PCKh 0.3125\n",
      "Trained batch 2346 batch loss 0.583036423 batch mAP 0.54208374 batch PCKh 0.3125\n",
      "Trained batch 2347 batch loss 0.586636066 batch mAP 0.568817139 batch PCKh 0.5625\n",
      "Trained batch 2348 batch loss 0.500392079 batch mAP 0.597198486 batch PCKh 0.8125\n",
      "Trained batch 2349 batch loss 0.578645 batch mAP 0.577270508 batch PCKh 0.625\n",
      "Trained batch 2350 batch loss 0.523529828 batch mAP 0.649261475 batch PCKh 0.3125\n",
      "Trained batch 2351 batch loss 0.450353444 batch mAP 0.587615967 batch PCKh 0.1875\n",
      "Trained batch 2352 batch loss 0.530955 batch mAP 0.569488525 batch PCKh 0.25\n",
      "Trained batch 2353 batch loss 0.523667157 batch mAP 0.563873291 batch PCKh 0.75\n",
      "Trained batch 2354 batch loss 0.52147156 batch mAP 0.574859619 batch PCKh 0.75\n",
      "Trained batch 2355 batch loss 0.478912234 batch mAP 0.653381348 batch PCKh 0.375\n",
      "Trained batch 2356 batch loss 0.572084367 batch mAP 0.620666504 batch PCKh 0.5\n",
      "Trained batch 2357 batch loss 0.421920478 batch mAP 0.695251465 batch PCKh 0.6875\n",
      "Trained batch 2358 batch loss 0.532135129 batch mAP 0.675842285 batch PCKh 0.3125\n",
      "Trained batch 2359 batch loss 0.429540575 batch mAP 0.701202393 batch PCKh 0.5\n",
      "Trained batch 2360 batch loss 0.50628686 batch mAP 0.626037598 batch PCKh 0.3125\n",
      "Trained batch 2361 batch loss 0.560392737 batch mAP 0.65737915 batch PCKh 0.75\n",
      "Trained batch 2362 batch loss 0.578148127 batch mAP 0.587402344 batch PCKh 0.5\n",
      "Trained batch 2363 batch loss 0.484028339 batch mAP 0.631622314 batch PCKh 0.4375\n",
      "Trained batch 2364 batch loss 0.52269429 batch mAP 0.606292725 batch PCKh 0.75\n",
      "Trained batch 2365 batch loss 0.540058732 batch mAP 0.594787598 batch PCKh 0.625\n",
      "Trained batch 2366 batch loss 0.516296387 batch mAP 0.583862305 batch PCKh 0.6875\n",
      "Trained batch 2367 batch loss 0.519959033 batch mAP 0.664215088 batch PCKh 0.25\n",
      "Trained batch 2368 batch loss 0.550690174 batch mAP 0.639862061 batch PCKh 0.3125\n",
      "Trained batch 2369 batch loss 0.455505371 batch mAP 0.665344238 batch PCKh 0.25\n",
      "Trained batch 2370 batch loss 0.486194909 batch mAP 0.625488281 batch PCKh 0.5\n",
      "Trained batch 2371 batch loss 0.379911542 batch mAP 0.716918945 batch PCKh 0.5\n",
      "Trained batch 2372 batch loss 0.435728073 batch mAP 0.696899414 batch PCKh 0.5625\n",
      "Trained batch 2373 batch loss 0.422732949 batch mAP 0.723480225 batch PCKh 0.8125\n",
      "Trained batch 2374 batch loss 0.507985 batch mAP 0.644134521 batch PCKh 0.4375\n",
      "Trained batch 2375 batch loss 0.525701523 batch mAP 0.673736572 batch PCKh 0.6875\n",
      "Trained batch 2376 batch loss 0.411553621 batch mAP 0.743927 batch PCKh 0.875\n",
      "Trained batch 2377 batch loss 0.469433188 batch mAP 0.731567383 batch PCKh 0.6875\n",
      "Trained batch 2378 batch loss 0.436401755 batch mAP 0.746032715 batch PCKh 0.6875\n",
      "Trained batch 2379 batch loss 0.410029 batch mAP 0.748474121 batch PCKh 0.3125\n",
      "Trained batch 2380 batch loss 0.441704452 batch mAP 0.745849609 batch PCKh 0.5\n",
      "Trained batch 2381 batch loss 0.486049384 batch mAP 0.708099365 batch PCKh 0.9375\n",
      "Trained batch 2382 batch loss 0.454659879 batch mAP 0.718109131 batch PCKh 0.625\n",
      "Trained batch 2383 batch loss 0.41419661 batch mAP 0.735748291 batch PCKh 0.5625\n",
      "Trained batch 2384 batch loss 0.546752691 batch mAP 0.661438 batch PCKh 0.8125\n",
      "Trained batch 2385 batch loss 0.525988162 batch mAP 0.614593506 batch PCKh 0.875\n",
      "Trained batch 2386 batch loss 0.444293976 batch mAP 0.704711914 batch PCKh 0.875\n",
      "Trained batch 2387 batch loss 0.494587719 batch mAP 0.653564453 batch PCKh 0.3125\n",
      "Trained batch 2388 batch loss 0.45939222 batch mAP 0.681793213 batch PCKh 0.6875\n",
      "Trained batch 2389 batch loss 0.517615139 batch mAP 0.640991211 batch PCKh 0.1875\n",
      "Trained batch 2390 batch loss 0.435548455 batch mAP 0.731689453 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2391 batch loss 0.403898686 batch mAP 0.720397949 batch PCKh 0.3125\n",
      "Trained batch 2392 batch loss 0.446847051 batch mAP 0.675598145 batch PCKh 0.5625\n",
      "Trained batch 2393 batch loss 0.425197899 batch mAP 0.683654785 batch PCKh 0.3125\n",
      "Trained batch 2394 batch loss 0.4501715 batch mAP 0.689666748 batch PCKh 0.5625\n",
      "Trained batch 2395 batch loss 0.445846081 batch mAP 0.734710693 batch PCKh 0.5625\n",
      "Trained batch 2396 batch loss 0.525465 batch mAP 0.699127197 batch PCKh 0.5625\n",
      "Trained batch 2397 batch loss 0.42287752 batch mAP 0.76184082 batch PCKh 0.375\n",
      "Trained batch 2398 batch loss 0.3765966 batch mAP 0.737091064 batch PCKh 0.625\n",
      "Trained batch 2399 batch loss 0.444340348 batch mAP 0.678527832 batch PCKh 0.375\n",
      "Trained batch 2400 batch loss 0.474133849 batch mAP 0.620880127 batch PCKh 0.5625\n",
      "Trained batch 2401 batch loss 0.411011249 batch mAP 0.691436768 batch PCKh 0.875\n",
      "Trained batch 2402 batch loss 0.508806169 batch mAP 0.674285889 batch PCKh 0.8125\n",
      "Trained batch 2403 batch loss 0.437038153 batch mAP 0.683258057 batch PCKh 0.8125\n",
      "Trained batch 2404 batch loss 0.442338526 batch mAP 0.648162842 batch PCKh 0.5\n",
      "Trained batch 2405 batch loss 0.432445526 batch mAP 0.707489 batch PCKh 0.5\n",
      "Trained batch 2406 batch loss 0.392411709 batch mAP 0.676178 batch PCKh 0.375\n",
      "Trained batch 2407 batch loss 0.381021649 batch mAP 0.687805176 batch PCKh 0.25\n",
      "Trained batch 2408 batch loss 0.48246032 batch mAP 0.669342041 batch PCKh 0.4375\n",
      "Trained batch 2409 batch loss 0.44397375 batch mAP 0.693389893 batch PCKh 0.875\n",
      "Trained batch 2410 batch loss 0.442319334 batch mAP 0.664337158 batch PCKh 0.125\n",
      "Trained batch 2411 batch loss 0.422244608 batch mAP 0.595092773 batch PCKh 0.1875\n",
      "Trained batch 2412 batch loss 0.487248182 batch mAP 0.613616943 batch PCKh 0.875\n",
      "Trained batch 2413 batch loss 0.63165009 batch mAP 0.516845703 batch PCKh 0.3125\n",
      "Trained batch 2414 batch loss 0.628246665 batch mAP 0.517028809 batch PCKh 0\n",
      "Trained batch 2415 batch loss 0.552604318 batch mAP 0.612060547 batch PCKh 0.3125\n",
      "Trained batch 2416 batch loss 0.591103911 batch mAP 0.580749512 batch PCKh 0.0625\n",
      "Trained batch 2417 batch loss 0.489439368 batch mAP 0.701324463 batch PCKh 0.3125\n",
      "Trained batch 2418 batch loss 0.379409939 batch mAP 0.758148193 batch PCKh 0.4375\n",
      "Trained batch 2419 batch loss 0.548533797 batch mAP 0.626647949 batch PCKh 0.375\n",
      "Trained batch 2420 batch loss 0.548475683 batch mAP 0.585266113 batch PCKh 0.75\n",
      "Trained batch 2421 batch loss 0.585860252 batch mAP 0.64755249 batch PCKh 0.3125\n",
      "Trained batch 2422 batch loss 0.552708 batch mAP 0.616851807 batch PCKh 0.3125\n",
      "Trained batch 2423 batch loss 0.486425638 batch mAP 0.691223145 batch PCKh 0.375\n",
      "Trained batch 2424 batch loss 0.381798327 batch mAP 0.7237854 batch PCKh 0.75\n",
      "Trained batch 2425 batch loss 0.541276 batch mAP 0.620666504 batch PCKh 0.1875\n",
      "Trained batch 2426 batch loss 0.501675 batch mAP 0.696563721 batch PCKh 0.625\n",
      "Trained batch 2427 batch loss 0.481348127 batch mAP 0.660308838 batch PCKh 0.8125\n",
      "Trained batch 2428 batch loss 0.447895914 batch mAP 0.666290283 batch PCKh 0.875\n",
      "Trained batch 2429 batch loss 0.542361557 batch mAP 0.65927124 batch PCKh 0.875\n",
      "Trained batch 2430 batch loss 0.545353651 batch mAP 0.653076172 batch PCKh 0.5625\n",
      "Trained batch 2431 batch loss 0.402968347 batch mAP 0.688964844 batch PCKh 0.3125\n",
      "Trained batch 2432 batch loss 0.472285926 batch mAP 0.736602783 batch PCKh 0.625\n",
      "Trained batch 2433 batch loss 0.442739606 batch mAP 0.692047119 batch PCKh 0.5\n",
      "Trained batch 2434 batch loss 0.533339739 batch mAP 0.690124512 batch PCKh 0.625\n",
      "Trained batch 2435 batch loss 0.523264706 batch mAP 0.707244873 batch PCKh 0.75\n",
      "Trained batch 2436 batch loss 0.509130716 batch mAP 0.730865479 batch PCKh 0.25\n",
      "Trained batch 2437 batch loss 0.469828 batch mAP 0.73651123 batch PCKh 0.4375\n",
      "Trained batch 2438 batch loss 0.508646727 batch mAP 0.651031494 batch PCKh 0.25\n",
      "Trained batch 2439 batch loss 0.402654439 batch mAP 0.675231934 batch PCKh 0.5\n",
      "Trained batch 2440 batch loss 0.507019579 batch mAP 0.627594 batch PCKh 0.5625\n",
      "Trained batch 2441 batch loss 0.545530736 batch mAP 0.562713623 batch PCKh 0.5\n",
      "Trained batch 2442 batch loss 0.530854225 batch mAP 0.635864258 batch PCKh 0.875\n",
      "Trained batch 2443 batch loss 0.492183298 batch mAP 0.698303223 batch PCKh 0.75\n",
      "Trained batch 2444 batch loss 0.413880378 batch mAP 0.705657959 batch PCKh 0.75\n",
      "Trained batch 2445 batch loss 0.387932 batch mAP 0.721893311 batch PCKh 0.375\n",
      "Trained batch 2446 batch loss 0.485979795 batch mAP 0.645324707 batch PCKh 0.3125\n",
      "Trained batch 2447 batch loss 0.553856313 batch mAP 0.565826416 batch PCKh 0.3125\n",
      "Trained batch 2448 batch loss 0.48652336 batch mAP 0.595977783 batch PCKh 0.375\n",
      "Trained batch 2449 batch loss 0.457733274 batch mAP 0.532531738 batch PCKh 0.375\n",
      "Trained batch 2450 batch loss 0.442965925 batch mAP 0.626037598 batch PCKh 0.5\n",
      "Trained batch 2451 batch loss 0.439750612 batch mAP 0.67401123 batch PCKh 0.125\n",
      "Trained batch 2452 batch loss 0.46152842 batch mAP 0.625610352 batch PCKh 0.0625\n",
      "Trained batch 2453 batch loss 0.509345412 batch mAP 0.651916504 batch PCKh 0.375\n",
      "Trained batch 2454 batch loss 0.442236066 batch mAP 0.573608398 batch PCKh 0.5625\n",
      "Trained batch 2455 batch loss 0.490343332 batch mAP 0.629455566 batch PCKh 0.0625\n",
      "Trained batch 2456 batch loss 0.5419541 batch mAP 0.627807617 batch PCKh 0.1875\n",
      "Trained batch 2457 batch loss 0.450004458 batch mAP 0.667053223 batch PCKh 0.5625\n",
      "Trained batch 2458 batch loss 0.567689896 batch mAP 0.633331299 batch PCKh 0.5625\n",
      "Trained batch 2459 batch loss 0.511271119 batch mAP 0.675537109 batch PCKh 0.4375\n",
      "Trained batch 2460 batch loss 0.52344507 batch mAP 0.685119629 batch PCKh 0.5\n",
      "Trained batch 2461 batch loss 0.583074927 batch mAP 0.700775146 batch PCKh 0.5625\n",
      "Trained batch 2462 batch loss 0.504078567 batch mAP 0.672363281 batch PCKh 0.3125\n",
      "Trained batch 2463 batch loss 0.482929409 batch mAP 0.687133789 batch PCKh 0.75\n",
      "Trained batch 2464 batch loss 0.535444856 batch mAP 0.714782715 batch PCKh 0.375\n",
      "Trained batch 2465 batch loss 0.563424528 batch mAP 0.669891357 batch PCKh 0.3125\n",
      "Trained batch 2466 batch loss 0.564315856 batch mAP 0.609619141 batch PCKh 0.25\n",
      "Trained batch 2467 batch loss 0.487702638 batch mAP 0.702880859 batch PCKh 0.625\n",
      "Trained batch 2468 batch loss 0.580986261 batch mAP 0.5809021 batch PCKh 0.3125\n",
      "Trained batch 2469 batch loss 0.539081216 batch mAP 0.66192627 batch PCKh 0.4375\n",
      "Trained batch 2470 batch loss 0.50391829 batch mAP 0.690643311 batch PCKh 0.3125\n",
      "Trained batch 2471 batch loss 0.54101634 batch mAP 0.680175781 batch PCKh 0.3125\n",
      "Trained batch 2472 batch loss 0.510061562 batch mAP 0.669616699 batch PCKh 0.5\n",
      "Trained batch 2473 batch loss 0.507366 batch mAP 0.661315918 batch PCKh 0.8125\n",
      "Trained batch 2474 batch loss 0.483722717 batch mAP 0.617004395 batch PCKh 0.75\n",
      "Trained batch 2475 batch loss 0.486852467 batch mAP 0.655883789 batch PCKh 0.6875\n",
      "Trained batch 2476 batch loss 0.51589191 batch mAP 0.697570801 batch PCKh 0.4375\n",
      "Trained batch 2477 batch loss 0.46979028 batch mAP 0.688781738 batch PCKh 0.5625\n",
      "Trained batch 2478 batch loss 0.527322054 batch mAP 0.658660889 batch PCKh 0.4375\n",
      "Trained batch 2479 batch loss 0.430938631 batch mAP 0.698791504 batch PCKh 0.3125\n",
      "Trained batch 2480 batch loss 0.504196525 batch mAP 0.633850098 batch PCKh 0.6875\n",
      "Trained batch 2481 batch loss 0.502385437 batch mAP 0.676300049 batch PCKh 0.6875\n",
      "Trained batch 2482 batch loss 0.478332311 batch mAP 0.693328857 batch PCKh 0.75\n",
      "Trained batch 2483 batch loss 0.428356588 batch mAP 0.733642578 batch PCKh 0.5\n",
      "Trained batch 2484 batch loss 0.444707692 batch mAP 0.673400879 batch PCKh 0.4375\n",
      "Trained batch 2485 batch loss 0.573843241 batch mAP 0.585632324 batch PCKh 0.25\n",
      "Trained batch 2486 batch loss 0.489155114 batch mAP 0.617706299 batch PCKh 0\n",
      "Trained batch 2487 batch loss 0.435995698 batch mAP 0.636169434 batch PCKh 0.3125\n",
      "Trained batch 2488 batch loss 0.418486059 batch mAP 0.548034668 batch PCKh 0.375\n",
      "Trained batch 2489 batch loss 0.460115522 batch mAP 0.501220703 batch PCKh 0.25\n",
      "Trained batch 2490 batch loss 0.505354285 batch mAP 0.522155762 batch PCKh 0.3125\n",
      "Trained batch 2491 batch loss 0.421857119 batch mAP 0.598968506 batch PCKh 0.4375\n",
      "Trained batch 2492 batch loss 0.260700107 batch mAP 0.627502441 batch PCKh 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2493 batch loss 0.38633129 batch mAP 0.583496094 batch PCKh 0\n",
      "Trained batch 2494 batch loss 0.447197497 batch mAP 0.663879395 batch PCKh 0.375\n",
      "Trained batch 2495 batch loss 0.4736633 batch mAP 0.637573242 batch PCKh 0.3125\n",
      "Trained batch 2496 batch loss 0.627653 batch mAP 0.545379639 batch PCKh 0.0625\n",
      "Trained batch 2497 batch loss 0.491737664 batch mAP 0.581420898 batch PCKh 0.5\n",
      "Trained batch 2498 batch loss 0.453302 batch mAP 0.606872559 batch PCKh 0.75\n",
      "Trained batch 2499 batch loss 0.458346248 batch mAP 0.58984375 batch PCKh 0.75\n",
      "Trained batch 2500 batch loss 0.474879742 batch mAP 0.553009033 batch PCKh 0\n",
      "Trained batch 2501 batch loss 0.507233143 batch mAP 0.510376 batch PCKh 0.1875\n",
      "Trained batch 2502 batch loss 0.487831324 batch mAP 0.544769287 batch PCKh 0.75\n",
      "Trained batch 2503 batch loss 0.457179785 batch mAP 0.543762207 batch PCKh 0.75\n",
      "Trained batch 2504 batch loss 0.477793574 batch mAP 0.617218 batch PCKh 0.6875\n",
      "Trained batch 2505 batch loss 0.425218344 batch mAP 0.69720459 batch PCKh 0.6875\n",
      "Trained batch 2506 batch loss 0.395656645 batch mAP 0.585998535 batch PCKh 0.25\n",
      "Trained batch 2507 batch loss 0.458776414 batch mAP 0.599517822 batch PCKh 0.25\n",
      "Trained batch 2508 batch loss 0.370236576 batch mAP 0.640869141 batch PCKh 0.75\n",
      "Trained batch 2509 batch loss 0.448959231 batch mAP 0.632110596 batch PCKh 0.6875\n",
      "Trained batch 2510 batch loss 0.448884964 batch mAP 0.605834961 batch PCKh 0.5625\n",
      "Trained batch 2511 batch loss 0.425121784 batch mAP 0.606536865 batch PCKh 0.25\n",
      "Trained batch 2512 batch loss 0.455495954 batch mAP 0.605011 batch PCKh 0.1875\n",
      "Trained batch 2513 batch loss 0.465388536 batch mAP 0.523620605 batch PCKh 0.75\n",
      "Trained batch 2514 batch loss 0.554240346 batch mAP 0.562103271 batch PCKh 0.625\n",
      "Trained batch 2515 batch loss 0.550204635 batch mAP 0.571350098 batch PCKh 0\n",
      "Trained batch 2516 batch loss 0.48054859 batch mAP 0.636901855 batch PCKh 0.75\n",
      "Trained batch 2517 batch loss 0.527968049 batch mAP 0.624755859 batch PCKh 0.5625\n",
      "Trained batch 2518 batch loss 0.479782104 batch mAP 0.653137207 batch PCKh 0.625\n",
      "Trained batch 2519 batch loss 0.54272306 batch mAP 0.606689453 batch PCKh 0.8125\n",
      "Trained batch 2520 batch loss 0.481530249 batch mAP 0.624298096 batch PCKh 0.875\n",
      "Trained batch 2521 batch loss 0.543258667 batch mAP 0.631317139 batch PCKh 0.875\n",
      "Trained batch 2522 batch loss 0.570686281 batch mAP 0.609008789 batch PCKh 0.25\n",
      "Trained batch 2523 batch loss 0.615912318 batch mAP 0.597442627 batch PCKh 0.375\n",
      "Trained batch 2524 batch loss 0.430502176 batch mAP 0.657226562 batch PCKh 0.3125\n",
      "Trained batch 2525 batch loss 0.480880886 batch mAP 0.687744141 batch PCKh 0.5625\n",
      "Trained batch 2526 batch loss 0.541275501 batch mAP 0.684356689 batch PCKh 0.375\n",
      "Trained batch 2527 batch loss 0.526078582 batch mAP 0.647247314 batch PCKh 0.125\n",
      "Trained batch 2528 batch loss 0.52002126 batch mAP 0.633209229 batch PCKh 0.1875\n",
      "Trained batch 2529 batch loss 0.465271473 batch mAP 0.677368164 batch PCKh 0.875\n",
      "Trained batch 2530 batch loss 0.494428575 batch mAP 0.650878906 batch PCKh 0.875\n",
      "Trained batch 2531 batch loss 0.403399169 batch mAP 0.686157227 batch PCKh 0.6875\n",
      "Trained batch 2532 batch loss 0.459755063 batch mAP 0.645080566 batch PCKh 0.625\n",
      "Trained batch 2533 batch loss 0.409003466 batch mAP 0.733734131 batch PCKh 0.5\n",
      "Trained batch 2534 batch loss 0.538992405 batch mAP 0.681152344 batch PCKh 0.4375\n",
      "Trained batch 2535 batch loss 0.583271682 batch mAP 0.595977783 batch PCKh 0.375\n",
      "Trained batch 2536 batch loss 0.575292289 batch mAP 0.573547363 batch PCKh 0.5625\n",
      "Trained batch 2537 batch loss 0.508596718 batch mAP 0.615936279 batch PCKh 0.5\n",
      "Trained batch 2538 batch loss 0.54002142 batch mAP 0.599761963 batch PCKh 0.5\n",
      "Trained batch 2539 batch loss 0.521388292 batch mAP 0.695037842 batch PCKh 0.25\n",
      "Trained batch 2540 batch loss 0.475652218 batch mAP 0.698852539 batch PCKh 0.3125\n",
      "Trained batch 2541 batch loss 0.458972961 batch mAP 0.73550415 batch PCKh 0.375\n",
      "Trained batch 2542 batch loss 0.49621284 batch mAP 0.712219238 batch PCKh 0.4375\n",
      "Trained batch 2543 batch loss 0.483637452 batch mAP 0.666870117 batch PCKh 0.4375\n",
      "Trained batch 2544 batch loss 0.594722509 batch mAP 0.625488281 batch PCKh 0.375\n",
      "Trained batch 2545 batch loss 0.546729743 batch mAP 0.697845459 batch PCKh 0.5\n",
      "Trained batch 2546 batch loss 0.498876691 batch mAP 0.695922852 batch PCKh 0.5\n",
      "Trained batch 2547 batch loss 0.46005109 batch mAP 0.6300354 batch PCKh 0.4375\n",
      "Trained batch 2548 batch loss 0.563958526 batch mAP 0.588439941 batch PCKh 0.4375\n",
      "Trained batch 2549 batch loss 0.576595902 batch mAP 0.592926 batch PCKh 0.875\n",
      "Trained batch 2550 batch loss 0.598174453 batch mAP 0.54107666 batch PCKh 0.875\n",
      "Trained batch 2551 batch loss 0.564721823 batch mAP 0.471740723 batch PCKh 0.1875\n",
      "Trained batch 2552 batch loss 0.616101503 batch mAP 0.510467529 batch PCKh 0.625\n",
      "Trained batch 2553 batch loss 0.541718841 batch mAP 0.592956543 batch PCKh 0.8125\n",
      "Trained batch 2554 batch loss 0.507237077 batch mAP 0.6144104 batch PCKh 0.625\n",
      "Trained batch 2555 batch loss 0.454220712 batch mAP 0.602661133 batch PCKh 0.75\n",
      "Trained batch 2556 batch loss 0.412725151 batch mAP 0.580963135 batch PCKh 0.3125\n",
      "Trained batch 2557 batch loss 0.401237547 batch mAP 0.577606201 batch PCKh 0\n",
      "Trained batch 2558 batch loss 0.3971681 batch mAP 0.595184326 batch PCKh 0.3125\n",
      "Trained batch 2559 batch loss 0.433712959 batch mAP 0.580078125 batch PCKh 0.375\n",
      "Trained batch 2560 batch loss 0.433066368 batch mAP 0.624664307 batch PCKh 0.5625\n",
      "Trained batch 2561 batch loss 0.408818573 batch mAP 0.700012207 batch PCKh 0.4375\n",
      "Trained batch 2562 batch loss 0.47633028 batch mAP 0.668182373 batch PCKh 0.625\n",
      "Trained batch 2563 batch loss 0.517188787 batch mAP 0.638031 batch PCKh 0.6875\n",
      "Trained batch 2564 batch loss 0.606502712 batch mAP 0.579467773 batch PCKh 0.6875\n",
      "Trained batch 2565 batch loss 0.605390549 batch mAP 0.570678711 batch PCKh 0.25\n",
      "Trained batch 2566 batch loss 0.539609969 batch mAP 0.62512207 batch PCKh 0.125\n",
      "Trained batch 2567 batch loss 0.475385666 batch mAP 0.68145752 batch PCKh 0.4375\n",
      "Trained batch 2568 batch loss 0.502296209 batch mAP 0.67868042 batch PCKh 0.5\n",
      "Trained batch 2569 batch loss 0.548478603 batch mAP 0.653564453 batch PCKh 0.6875\n",
      "Trained batch 2570 batch loss 0.507737458 batch mAP 0.660705566 batch PCKh 0.625\n",
      "Trained batch 2571 batch loss 0.506232738 batch mAP 0.573486328 batch PCKh 0.625\n",
      "Trained batch 2572 batch loss 0.418724 batch mAP 0.719055176 batch PCKh 0.3125\n",
      "Trained batch 2573 batch loss 0.501700819 batch mAP 0.622406 batch PCKh 0.875\n",
      "Trained batch 2574 batch loss 0.529412568 batch mAP 0.589324951 batch PCKh 0.5\n",
      "Trained batch 2575 batch loss 0.543768764 batch mAP 0.53704834 batch PCKh 0.5\n",
      "Trained batch 2576 batch loss 0.350144506 batch mAP 0.731048584 batch PCKh 0.1875\n",
      "Trained batch 2577 batch loss 0.461996913 batch mAP 0.696777344 batch PCKh 0.5625\n",
      "Trained batch 2578 batch loss 0.52171874 batch mAP 0.67401123 batch PCKh 0.4375\n",
      "Trained batch 2579 batch loss 0.53091228 batch mAP 0.631897 batch PCKh 0.5625\n",
      "Trained batch 2580 batch loss 0.413521081 batch mAP 0.664917 batch PCKh 0.75\n",
      "Trained batch 2581 batch loss 0.43155688 batch mAP 0.656951904 batch PCKh 0.625\n",
      "Trained batch 2582 batch loss 0.404188275 batch mAP 0.639282227 batch PCKh 0.375\n",
      "Trained batch 2583 batch loss 0.480457574 batch mAP 0.640838623 batch PCKh 0.25\n",
      "Trained batch 2584 batch loss 0.403892279 batch mAP 0.652099609 batch PCKh 0.125\n",
      "Trained batch 2585 batch loss 0.399160743 batch mAP 0.701629639 batch PCKh 0.125\n",
      "Trained batch 2586 batch loss 0.349970192 batch mAP 0.733520508 batch PCKh 0.5\n",
      "Trained batch 2587 batch loss 0.380498409 batch mAP 0.712921143 batch PCKh 0.5\n",
      "Trained batch 2588 batch loss 0.364964426 batch mAP 0.739715576 batch PCKh 0.5\n",
      "Trained batch 2589 batch loss 0.448847294 batch mAP 0.732910156 batch PCKh 0.75\n",
      "Trained batch 2590 batch loss 0.463139683 batch mAP 0.654876709 batch PCKh 0.5625\n",
      "Trained batch 2591 batch loss 0.613875449 batch mAP 0.62109375 batch PCKh 0.125\n",
      "Trained batch 2592 batch loss 0.607689202 batch mAP 0.561981201 batch PCKh 0.0625\n",
      "Trained batch 2593 batch loss 0.668132544 batch mAP 0.556884766 batch PCKh 0.6875\n",
      "Trained batch 2594 batch loss 0.546919823 batch mAP 0.600097656 batch PCKh 0.625\n",
      "Trained batch 2595 batch loss 0.633992076 batch mAP 0.55657959 batch PCKh 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2596 batch loss 0.529669285 batch mAP 0.602630615 batch PCKh 0.875\n",
      "Trained batch 2597 batch loss 0.531174779 batch mAP 0.615905762 batch PCKh 0.6875\n",
      "Trained batch 2598 batch loss 0.520672679 batch mAP 0.577148438 batch PCKh 0.75\n",
      "Trained batch 2599 batch loss 0.536433458 batch mAP 0.616516113 batch PCKh 0.75\n",
      "Trained batch 2600 batch loss 0.575250387 batch mAP 0.588134766 batch PCKh 0.75\n",
      "Trained batch 2601 batch loss 0.612004399 batch mAP 0.579345703 batch PCKh 0.875\n",
      "Trained batch 2602 batch loss 0.630300164 batch mAP 0.554077148 batch PCKh 0.75\n",
      "Trained batch 2603 batch loss 0.516168416 batch mAP 0.572479248 batch PCKh 0.0625\n",
      "Trained batch 2604 batch loss 0.533565 batch mAP 0.55380249 batch PCKh 0.3125\n",
      "Trained batch 2605 batch loss 0.479116052 batch mAP 0.608581543 batch PCKh 0.4375\n",
      "Trained batch 2606 batch loss 0.545408785 batch mAP 0.590118408 batch PCKh 0.6875\n",
      "Trained batch 2607 batch loss 0.516697049 batch mAP 0.567077637 batch PCKh 0.75\n",
      "Trained batch 2608 batch loss 0.553141534 batch mAP 0.576385498 batch PCKh 0.4375\n",
      "Trained batch 2609 batch loss 0.599207699 batch mAP 0.567749 batch PCKh 0.1875\n",
      "Trained batch 2610 batch loss 0.591625631 batch mAP 0.558868408 batch PCKh 0.3125\n",
      "Trained batch 2611 batch loss 0.450578809 batch mAP 0.623474121 batch PCKh 0.875\n",
      "Trained batch 2612 batch loss 0.52097404 batch mAP 0.587219238 batch PCKh 0.6875\n",
      "Trained batch 2613 batch loss 0.506401062 batch mAP 0.542633057 batch PCKh 0.5625\n",
      "Trained batch 2614 batch loss 0.53843832 batch mAP 0.544647217 batch PCKh 0.875\n",
      "Trained batch 2615 batch loss 0.546500564 batch mAP 0.540100098 batch PCKh 0.75\n",
      "Trained batch 2616 batch loss 0.500880122 batch mAP 0.604309082 batch PCKh 0.375\n",
      "Trained batch 2617 batch loss 0.500815153 batch mAP 0.584259033 batch PCKh 0.25\n",
      "Trained batch 2618 batch loss 0.520280421 batch mAP 0.652832031 batch PCKh 0.6875\n",
      "Trained batch 2619 batch loss 0.631895661 batch mAP 0.646392822 batch PCKh 0.375\n",
      "Trained batch 2620 batch loss 0.568876565 batch mAP 0.61505127 batch PCKh 0.1875\n",
      "Trained batch 2621 batch loss 0.606541097 batch mAP 0.567901611 batch PCKh 0.4375\n",
      "Trained batch 2622 batch loss 0.367794096 batch mAP 0.631652832 batch PCKh 0.6875\n",
      "Trained batch 2623 batch loss 0.343644559 batch mAP 0.63092041 batch PCKh 0.6875\n",
      "Trained batch 2624 batch loss 0.414407402 batch mAP 0.580749512 batch PCKh 0.625\n",
      "Trained batch 2625 batch loss 0.40364027 batch mAP 0.601379395 batch PCKh 0.4375\n",
      "Trained batch 2626 batch loss 0.541419208 batch mAP 0.551422119 batch PCKh 0.1875\n",
      "Trained batch 2627 batch loss 0.528531849 batch mAP 0.595855713 batch PCKh 0.75\n",
      "Trained batch 2628 batch loss 0.520738959 batch mAP 0.602355957 batch PCKh 0.25\n",
      "Trained batch 2629 batch loss 0.493146628 batch mAP 0.649017334 batch PCKh 0.5625\n",
      "Trained batch 2630 batch loss 0.505921 batch mAP 0.642181396 batch PCKh 0.4375\n",
      "Trained batch 2631 batch loss 0.438057959 batch mAP 0.642730713 batch PCKh 0.5625\n",
      "Trained batch 2632 batch loss 0.496497631 batch mAP 0.640960693 batch PCKh 0.5\n",
      "Trained batch 2633 batch loss 0.543531835 batch mAP 0.593078613 batch PCKh 0.625\n",
      "Trained batch 2634 batch loss 0.512481689 batch mAP 0.629608154 batch PCKh 0.5\n",
      "Trained batch 2635 batch loss 0.562143862 batch mAP 0.609680176 batch PCKh 0.5625\n",
      "Trained batch 2636 batch loss 0.483226866 batch mAP 0.609375 batch PCKh 0.5625\n",
      "Trained batch 2637 batch loss 0.540961802 batch mAP 0.587493896 batch PCKh 0.8125\n",
      "Trained batch 2638 batch loss 0.569163203 batch mAP 0.606994629 batch PCKh 0.5\n",
      "Trained batch 2639 batch loss 0.579201937 batch mAP 0.533782959 batch PCKh 0.3125\n",
      "Trained batch 2640 batch loss 0.453073025 batch mAP 0.592803955 batch PCKh 0.5625\n",
      "Trained batch 2641 batch loss 0.527099 batch mAP 0.589599609 batch PCKh 0.8125\n",
      "Trained batch 2642 batch loss 0.565877199 batch mAP 0.540313721 batch PCKh 0.4375\n",
      "Trained batch 2643 batch loss 0.608250082 batch mAP 0.617095947 batch PCKh 0.625\n",
      "Trained batch 2644 batch loss 0.57068634 batch mAP 0.627960205 batch PCKh 0.5625\n",
      "Trained batch 2645 batch loss 0.62820518 batch mAP 0.614715576 batch PCKh 0.1875\n",
      "Trained batch 2646 batch loss 0.537590265 batch mAP 0.616455078 batch PCKh 0.4375\n",
      "Trained batch 2647 batch loss 0.631492257 batch mAP 0.598571777 batch PCKh 0.125\n",
      "Trained batch 2648 batch loss 0.581896782 batch mAP 0.59954834 batch PCKh 0.3125\n",
      "Trained batch 2649 batch loss 0.539353669 batch mAP 0.631286621 batch PCKh 0.5\n",
      "Trained batch 2650 batch loss 0.601912439 batch mAP 0.530914307 batch PCKh 0.4375\n",
      "Trained batch 2651 batch loss 0.481430143 batch mAP 0.581298828 batch PCKh 0.4375\n",
      "Trained batch 2652 batch loss 0.391899824 batch mAP 0.659606934 batch PCKh 0.3125\n",
      "Trained batch 2653 batch loss 0.391032279 batch mAP 0.639404297 batch PCKh 0.125\n",
      "Trained batch 2654 batch loss 0.389879942 batch mAP 0.666748047 batch PCKh 0.4375\n",
      "Trained batch 2655 batch loss 0.434738398 batch mAP 0.690429688 batch PCKh 0.5\n",
      "Trained batch 2656 batch loss 0.562893212 batch mAP 0.651397705 batch PCKh 0.8125\n",
      "Trained batch 2657 batch loss 0.459100336 batch mAP 0.704193115 batch PCKh 0.5625\n",
      "Trained batch 2658 batch loss 0.408501506 batch mAP 0.727844238 batch PCKh 0.875\n",
      "Trained batch 2659 batch loss 0.468498 batch mAP 0.691619873 batch PCKh 0.5625\n",
      "Trained batch 2660 batch loss 0.473078489 batch mAP 0.638031 batch PCKh 0.25\n",
      "Trained batch 2661 batch loss 0.484740555 batch mAP 0.626800537 batch PCKh 0.375\n",
      "Trained batch 2662 batch loss 0.436146677 batch mAP 0.613708496 batch PCKh 0.4375\n",
      "Trained batch 2663 batch loss 0.540263295 batch mAP 0.596282959 batch PCKh 0.3125\n",
      "Trained batch 2664 batch loss 0.505389035 batch mAP 0.578369141 batch PCKh 0.1875\n",
      "Trained batch 2665 batch loss 0.425434321 batch mAP 0.598815918 batch PCKh 0\n",
      "Trained batch 2666 batch loss 0.492201924 batch mAP 0.647003174 batch PCKh 0.5625\n",
      "Trained batch 2667 batch loss 0.419417024 batch mAP 0.721038818 batch PCKh 0.3125\n",
      "Trained batch 2668 batch loss 0.525984168 batch mAP 0.676574707 batch PCKh 0.75\n",
      "Trained batch 2669 batch loss 0.524656177 batch mAP 0.615386963 batch PCKh 0.4375\n",
      "Trained batch 2670 batch loss 0.540606 batch mAP 0.61151123 batch PCKh 0.6875\n",
      "Trained batch 2671 batch loss 0.519773483 batch mAP 0.613067627 batch PCKh 0.5\n",
      "Trained batch 2672 batch loss 0.527538717 batch mAP 0.644104 batch PCKh 0.625\n",
      "Trained batch 2673 batch loss 0.567628741 batch mAP 0.586975098 batch PCKh 0.375\n",
      "Trained batch 2674 batch loss 0.624459 batch mAP 0.557525635 batch PCKh 0.4375\n",
      "Trained batch 2675 batch loss 0.619585335 batch mAP 0.600524902 batch PCKh 0.3125\n",
      "Trained batch 2676 batch loss 0.651320219 batch mAP 0.563781738 batch PCKh 0.1875\n",
      "Trained batch 2677 batch loss 0.59205091 batch mAP 0.523284912 batch PCKh 0.375\n",
      "Trained batch 2678 batch loss 0.546310484 batch mAP 0.560668945 batch PCKh 0.125\n",
      "Trained batch 2679 batch loss 0.497326791 batch mAP 0.53012085 batch PCKh 0.3125\n",
      "Trained batch 2680 batch loss 0.4855223 batch mAP 0.465362549 batch PCKh 0.0625\n",
      "Trained batch 2681 batch loss 0.422225803 batch mAP 0.516449 batch PCKh 0.125\n",
      "Trained batch 2682 batch loss 0.406897128 batch mAP 0.494628906 batch PCKh 0.125\n",
      "Trained batch 2683 batch loss 0.506477714 batch mAP 0.565979 batch PCKh 0.75\n",
      "Trained batch 2684 batch loss 0.585932732 batch mAP 0.566772461 batch PCKh 0.25\n",
      "Trained batch 2685 batch loss 0.541466653 batch mAP 0.5675354 batch PCKh 0.4375\n",
      "Trained batch 2686 batch loss 0.612611413 batch mAP 0.534820557 batch PCKh 0.4375\n",
      "Trained batch 2687 batch loss 0.50165391 batch mAP 0.537963867 batch PCKh 0\n",
      "Trained batch 2688 batch loss 0.571583152 batch mAP 0.529296875 batch PCKh 0.5\n",
      "Trained batch 2689 batch loss 0.50230372 batch mAP 0.626678467 batch PCKh 0.5625\n",
      "Trained batch 2690 batch loss 0.478382528 batch mAP 0.580932617 batch PCKh 0.625\n",
      "Trained batch 2691 batch loss 0.494438052 batch mAP 0.581787109 batch PCKh 0.8125\n",
      "Trained batch 2692 batch loss 0.40517047 batch mAP 0.599029541 batch PCKh 0.6875\n",
      "Trained batch 2693 batch loss 0.415058047 batch mAP 0.586547852 batch PCKh 0.6875\n",
      "Trained batch 2694 batch loss 0.471204519 batch mAP 0.672363281 batch PCKh 0.5625\n",
      "Trained batch 2695 batch loss 0.492423296 batch mAP 0.690338135 batch PCKh 0.8125\n",
      "Trained batch 2696 batch loss 0.523158193 batch mAP 0.62323 batch PCKh 0.5\n",
      "Trained batch 2697 batch loss 0.520078897 batch mAP 0.693939209 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2698 batch loss 0.544677734 batch mAP 0.611053467 batch PCKh 0.625\n",
      "Trained batch 2699 batch loss 0.501297355 batch mAP 0.650695801 batch PCKh 0.375\n",
      "Trained batch 2700 batch loss 0.501001894 batch mAP 0.666870117 batch PCKh 0.625\n",
      "Trained batch 2701 batch loss 0.540483057 batch mAP 0.639312744 batch PCKh 0.3125\n",
      "Trained batch 2702 batch loss 0.521899164 batch mAP 0.628997803 batch PCKh 0\n",
      "Trained batch 2703 batch loss 0.503659248 batch mAP 0.601074219 batch PCKh 0.5625\n",
      "Trained batch 2704 batch loss 0.626562238 batch mAP 0.592224121 batch PCKh 0.4375\n",
      "Trained batch 2705 batch loss 0.584922791 batch mAP 0.510925293 batch PCKh 0\n",
      "Trained batch 2706 batch loss 0.605641127 batch mAP 0.580657959 batch PCKh 0.25\n",
      "Trained batch 2707 batch loss 0.634212852 batch mAP 0.569458 batch PCKh 0.125\n",
      "Trained batch 2708 batch loss 0.619696736 batch mAP 0.507080078 batch PCKh 0\n",
      "Trained batch 2709 batch loss 0.598822594 batch mAP 0.489624023 batch PCKh 0.125\n",
      "Trained batch 2710 batch loss 0.560626686 batch mAP 0.475189209 batch PCKh 0.3125\n",
      "Trained batch 2711 batch loss 0.561382055 batch mAP 0.560241699 batch PCKh 0.25\n",
      "Trained batch 2712 batch loss 0.503601074 batch mAP 0.621612549 batch PCKh 0.5625\n",
      "Trained batch 2713 batch loss 0.447971344 batch mAP 0.651123047 batch PCKh 0.5625\n",
      "Trained batch 2714 batch loss 0.515684843 batch mAP 0.631774902 batch PCKh 0.3125\n",
      "Trained batch 2715 batch loss 0.58702141 batch mAP 0.523773193 batch PCKh 0.75\n",
      "Trained batch 2716 batch loss 0.603209257 batch mAP 0.530700684 batch PCKh 0.375\n",
      "Trained batch 2717 batch loss 0.580371499 batch mAP 0.548156738 batch PCKh 0.5\n",
      "Trained batch 2718 batch loss 0.58917284 batch mAP 0.574676514 batch PCKh 0.5\n",
      "Trained batch 2719 batch loss 0.420173 batch mAP 0.726776123 batch PCKh 0.625\n",
      "Trained batch 2720 batch loss 0.49641192 batch mAP 0.684875488 batch PCKh 0.5\n",
      "Trained batch 2721 batch loss 0.552873611 batch mAP 0.718200684 batch PCKh 0.5\n",
      "Trained batch 2722 batch loss 0.466218114 batch mAP 0.699584961 batch PCKh 0.625\n",
      "Trained batch 2723 batch loss 0.466478229 batch mAP 0.675018311 batch PCKh 0.875\n",
      "Trained batch 2724 batch loss 0.479845613 batch mAP 0.674560547 batch PCKh 0.5\n",
      "Trained batch 2725 batch loss 0.460588515 batch mAP 0.658447266 batch PCKh 0.5625\n",
      "Trained batch 2726 batch loss 0.491724968 batch mAP 0.69241333 batch PCKh 0.3125\n",
      "Trained batch 2727 batch loss 0.378216207 batch mAP 0.713317871 batch PCKh 0.625\n",
      "Trained batch 2728 batch loss 0.491259396 batch mAP 0.706817627 batch PCKh 0.5625\n",
      "Trained batch 2729 batch loss 0.472230256 batch mAP 0.713531494 batch PCKh 0.5625\n",
      "Trained batch 2730 batch loss 0.44082576 batch mAP 0.70916748 batch PCKh 0.5\n",
      "Trained batch 2731 batch loss 0.473921657 batch mAP 0.691711426 batch PCKh 0.3125\n",
      "Trained batch 2732 batch loss 0.53694874 batch mAP 0.666564941 batch PCKh 0.4375\n",
      "Trained batch 2733 batch loss 0.528085232 batch mAP 0.65914917 batch PCKh 0.3125\n",
      "Trained batch 2734 batch loss 0.480433315 batch mAP 0.727020264 batch PCKh 0.75\n",
      "Trained batch 2735 batch loss 0.372078478 batch mAP 0.703155518 batch PCKh 0.875\n",
      "Trained batch 2736 batch loss 0.468241096 batch mAP 0.61907959 batch PCKh 0.1875\n",
      "Trained batch 2737 batch loss 0.452441454 batch mAP 0.672180176 batch PCKh 0.75\n",
      "Trained batch 2738 batch loss 0.493810087 batch mAP 0.703460693 batch PCKh 0.625\n",
      "Trained batch 2739 batch loss 0.423525244 batch mAP 0.68258667 batch PCKh 0.625\n",
      "Trained batch 2740 batch loss 0.43391794 batch mAP 0.679046631 batch PCKh 0.6875\n",
      "Trained batch 2741 batch loss 0.505557239 batch mAP 0.643707275 batch PCKh 0.375\n",
      "Trained batch 2742 batch loss 0.518472195 batch mAP 0.666229248 batch PCKh 0.4375\n",
      "Trained batch 2743 batch loss 0.496921659 batch mAP 0.616363525 batch PCKh 0.6875\n",
      "Trained batch 2744 batch loss 0.518818855 batch mAP 0.671112061 batch PCKh 0.4375\n",
      "Trained batch 2745 batch loss 0.550692558 batch mAP 0.670166 batch PCKh 0.5\n",
      "Trained batch 2746 batch loss 0.46686703 batch mAP 0.678527832 batch PCKh 0.625\n",
      "Trained batch 2747 batch loss 0.490385205 batch mAP 0.62902832 batch PCKh 0.4375\n",
      "Trained batch 2748 batch loss 0.496380478 batch mAP 0.618804932 batch PCKh 0.6875\n",
      "Trained batch 2749 batch loss 0.526687622 batch mAP 0.590820312 batch PCKh 0.375\n",
      "Trained batch 2750 batch loss 0.608701706 batch mAP 0.554046631 batch PCKh 0.1875\n",
      "Trained batch 2751 batch loss 0.527442575 batch mAP 0.623138428 batch PCKh 0.3125\n",
      "Trained batch 2752 batch loss 0.559260964 batch mAP 0.592193604 batch PCKh 0.625\n",
      "Trained batch 2753 batch loss 0.518834233 batch mAP 0.627716064 batch PCKh 0.5\n",
      "Trained batch 2754 batch loss 0.500805616 batch mAP 0.586090088 batch PCKh 0.6875\n",
      "Trained batch 2755 batch loss 0.561523736 batch mAP 0.540802 batch PCKh 0.6875\n",
      "Trained batch 2756 batch loss 0.4868958 batch mAP 0.576721191 batch PCKh 0.5625\n",
      "Trained batch 2757 batch loss 0.551800668 batch mAP 0.573028564 batch PCKh 0.5625\n",
      "Trained batch 2758 batch loss 0.486396849 batch mAP 0.618804932 batch PCKh 0.5625\n",
      "Trained batch 2759 batch loss 0.397088 batch mAP 0.654205322 batch PCKh 0.625\n",
      "Trained batch 2760 batch loss 0.496997625 batch mAP 0.623626709 batch PCKh 0.375\n",
      "Trained batch 2761 batch loss 0.447590888 batch mAP 0.628356934 batch PCKh 0.25\n",
      "Trained batch 2762 batch loss 0.563824654 batch mAP 0.607513428 batch PCKh 0.1875\n",
      "Trained batch 2763 batch loss 0.53233254 batch mAP 0.645324707 batch PCKh 0.25\n",
      "Trained batch 2764 batch loss 0.520021796 batch mAP 0.63067627 batch PCKh 0.75\n",
      "Trained batch 2765 batch loss 0.504585564 batch mAP 0.668701172 batch PCKh 0.1875\n",
      "Trained batch 2766 batch loss 0.580053627 batch mAP 0.608154297 batch PCKh 0.25\n",
      "Trained batch 2767 batch loss 0.653968215 batch mAP 0.648010254 batch PCKh 0.375\n",
      "Trained batch 2768 batch loss 0.565831184 batch mAP 0.656768799 batch PCKh 0.4375\n",
      "Trained batch 2769 batch loss 0.479828238 batch mAP 0.639038086 batch PCKh 0.8125\n",
      "Trained batch 2770 batch loss 0.59901315 batch mAP 0.567169189 batch PCKh 0.75\n",
      "Trained batch 2771 batch loss 0.567213535 batch mAP 0.555389404 batch PCKh 0.4375\n",
      "Trained batch 2772 batch loss 0.370147228 batch mAP 0.594543457 batch PCKh 0.375\n",
      "Trained batch 2773 batch loss 0.435816944 batch mAP 0.611206055 batch PCKh 0.4375\n",
      "Trained batch 2774 batch loss 0.482460946 batch mAP 0.618133545 batch PCKh 0.5625\n",
      "Trained batch 2775 batch loss 0.521383 batch mAP 0.639831543 batch PCKh 0.75\n",
      "Trained batch 2776 batch loss 0.501736045 batch mAP 0.632019043 batch PCKh 0.75\n",
      "Epoch 10 train loss 0.494452565908432 train mAP 0.62715083360672 train PCKh\n",
      "Validated batch 1 batch loss 0.534553051 batch mAP 0.649536133 batch PCKh 0.4375\n",
      "Validated batch 2 batch loss 0.706650853 batch mAP 0.632354736 batch PCKh 0.625\n",
      "Validated batch 3 batch loss 0.537074566 batch mAP 0.678894043 batch PCKh 0.6875\n",
      "Validated batch 4 batch loss 0.555340827 batch mAP 0.632415771 batch PCKh 0.625\n",
      "Validated batch 5 batch loss 0.488586068 batch mAP 0.726623535 batch PCKh 0.5625\n",
      "Validated batch 6 batch loss 0.614430368 batch mAP 0.546112061 batch PCKh 0.1875\n",
      "Validated batch 7 batch loss 0.52777493 batch mAP 0.619110107 batch PCKh 0.3125\n",
      "Validated batch 8 batch loss 0.583478808 batch mAP 0.511322 batch PCKh 0.5625\n",
      "Validated batch 9 batch loss 0.580900669 batch mAP 0.653106689 batch PCKh 0.75\n",
      "Validated batch 10 batch loss 0.518577099 batch mAP 0.65423584 batch PCKh 0.125\n",
      "Validated batch 11 batch loss 0.637896419 batch mAP 0.590148926 batch PCKh 0.75\n",
      "Validated batch 12 batch loss 0.547629416 batch mAP 0.664215088 batch PCKh 0.5625\n",
      "Validated batch 13 batch loss 0.642022729 batch mAP 0.63583374 batch PCKh 0.375\n",
      "Validated batch 14 batch loss 0.501730323 batch mAP 0.64364624 batch PCKh 0.25\n",
      "Validated batch 15 batch loss 0.547448337 batch mAP 0.598693848 batch PCKh 0.5625\n",
      "Validated batch 16 batch loss 0.514452815 batch mAP 0.652496338 batch PCKh 0.6875\n",
      "Validated batch 17 batch loss 0.545494556 batch mAP 0.613220215 batch PCKh 0.4375\n",
      "Validated batch 18 batch loss 0.752618313 batch mAP 0.631073 batch PCKh 0.25\n",
      "Validated batch 19 batch loss 0.611063778 batch mAP 0.651519775 batch PCKh 0.3125\n",
      "Validated batch 20 batch loss 0.56957078 batch mAP 0.673706055 batch PCKh 0.375\n",
      "Validated batch 21 batch loss 0.542318702 batch mAP 0.6690979 batch PCKh 0.25\n",
      "Validated batch 22 batch loss 0.520999193 batch mAP 0.628753662 batch PCKh 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 23 batch loss 0.46924153 batch mAP 0.682800293 batch PCKh 0\n",
      "Validated batch 24 batch loss 0.557167888 batch mAP 0.572784424 batch PCKh 0.75\n",
      "Validated batch 25 batch loss 0.540567219 batch mAP 0.608825684 batch PCKh 0.3125\n",
      "Validated batch 26 batch loss 0.653243661 batch mAP 0.592956543 batch PCKh 0.8125\n",
      "Validated batch 27 batch loss 0.642111182 batch mAP 0.597625732 batch PCKh 0.625\n",
      "Validated batch 28 batch loss 0.561341763 batch mAP 0.509857178 batch PCKh 0.125\n",
      "Validated batch 29 batch loss 0.661573648 batch mAP 0.518249512 batch PCKh 0.25\n",
      "Validated batch 30 batch loss 0.631986737 batch mAP 0.489074707 batch PCKh 0.6875\n",
      "Validated batch 31 batch loss 0.657265186 batch mAP 0.491668701 batch PCKh 0.75\n",
      "Validated batch 32 batch loss 0.618684411 batch mAP 0.583740234 batch PCKh 0.75\n",
      "Validated batch 33 batch loss 0.540176392 batch mAP 0.621032715 batch PCKh 0.6875\n",
      "Validated batch 34 batch loss 0.666856527 batch mAP 0.629150391 batch PCKh 0.1875\n",
      "Validated batch 35 batch loss 0.702961564 batch mAP 0.647064209 batch PCKh 0.75\n",
      "Validated batch 36 batch loss 0.555916369 batch mAP 0.566101074 batch PCKh 0.3125\n",
      "Validated batch 37 batch loss 0.56605196 batch mAP 0.604187 batch PCKh 0.5625\n",
      "Validated batch 38 batch loss 0.652886748 batch mAP 0.653076172 batch PCKh 0.625\n",
      "Validated batch 39 batch loss 0.608971894 batch mAP 0.571990967 batch PCKh 0.5625\n",
      "Validated batch 40 batch loss 0.508322895 batch mAP 0.558898926 batch PCKh 0.3125\n",
      "Validated batch 41 batch loss 0.631774724 batch mAP 0.630889893 batch PCKh 0.6875\n",
      "Validated batch 42 batch loss 0.542423666 batch mAP 0.637817383 batch PCKh 0.6875\n",
      "Validated batch 43 batch loss 0.408641279 batch mAP 0.664764404 batch PCKh 0.25\n",
      "Validated batch 44 batch loss 0.490301281 batch mAP 0.620330811 batch PCKh 0.5625\n",
      "Validated batch 45 batch loss 0.52073735 batch mAP 0.599639893 batch PCKh 0.625\n",
      "Validated batch 46 batch loss 0.585401952 batch mAP 0.671539307 batch PCKh 0.4375\n",
      "Validated batch 47 batch loss 0.572325349 batch mAP 0.663360596 batch PCKh 0.5625\n",
      "Validated batch 48 batch loss 0.605522275 batch mAP 0.644470215 batch PCKh 0.875\n",
      "Validated batch 49 batch loss 0.563014328 batch mAP 0.582397461 batch PCKh 0.6875\n",
      "Validated batch 50 batch loss 0.605069458 batch mAP 0.640411377 batch PCKh 0.8125\n",
      "Validated batch 51 batch loss 0.602947712 batch mAP 0.618774414 batch PCKh 0.5625\n",
      "Validated batch 52 batch loss 0.504586816 batch mAP 0.654266357 batch PCKh 0.25\n",
      "Validated batch 53 batch loss 0.551268339 batch mAP 0.605773926 batch PCKh 0.625\n",
      "Validated batch 54 batch loss 0.553589284 batch mAP 0.680786133 batch PCKh 0.625\n",
      "Validated batch 55 batch loss 0.574406326 batch mAP 0.652496338 batch PCKh 0.4375\n",
      "Validated batch 56 batch loss 0.600057483 batch mAP 0.65435791 batch PCKh 0.25\n",
      "Validated batch 57 batch loss 0.576423645 batch mAP 0.656860352 batch PCKh 0.875\n",
      "Validated batch 58 batch loss 0.645944595 batch mAP 0.738647461 batch PCKh 0.5625\n",
      "Validated batch 59 batch loss 0.600332737 batch mAP 0.569397 batch PCKh 0.6875\n",
      "Validated batch 60 batch loss 0.661319196 batch mAP 0.620147705 batch PCKh 0.3125\n",
      "Validated batch 61 batch loss 0.607904434 batch mAP 0.540710449 batch PCKh 0.1875\n",
      "Validated batch 62 batch loss 0.585382223 batch mAP 0.642547607 batch PCKh 0.75\n",
      "Validated batch 63 batch loss 0.660542 batch mAP 0.550964355 batch PCKh 0.0625\n",
      "Validated batch 64 batch loss 0.543699622 batch mAP 0.644683838 batch PCKh 0.375\n",
      "Validated batch 65 batch loss 0.53060323 batch mAP 0.648162842 batch PCKh 0.75\n",
      "Validated batch 66 batch loss 0.527792931 batch mAP 0.702148438 batch PCKh 0.5625\n",
      "Validated batch 67 batch loss 0.574307919 batch mAP 0.708984375 batch PCKh 0.625\n",
      "Validated batch 68 batch loss 0.618824124 batch mAP 0.630218506 batch PCKh 0.5\n",
      "Validated batch 69 batch loss 0.557670414 batch mAP 0.641082764 batch PCKh 0.25\n",
      "Validated batch 70 batch loss 0.576968074 batch mAP 0.481079102 batch PCKh 0.5\n",
      "Validated batch 71 batch loss 0.564590394 batch mAP 0.534332275 batch PCKh 0.1875\n",
      "Validated batch 72 batch loss 0.58827424 batch mAP 0.612487793 batch PCKh 0.6875\n",
      "Validated batch 73 batch loss 0.611647725 batch mAP 0.623382568 batch PCKh 0.625\n",
      "Validated batch 74 batch loss 0.647538662 batch mAP 0.580566406 batch PCKh 0.3125\n",
      "Validated batch 75 batch loss 0.710285187 batch mAP 0.514556885 batch PCKh 0\n",
      "Validated batch 76 batch loss 0.594181776 batch mAP 0.603790283 batch PCKh 0.5625\n",
      "Validated batch 77 batch loss 0.555642903 batch mAP 0.605926514 batch PCKh 0.5625\n",
      "Validated batch 78 batch loss 0.558944285 batch mAP 0.637451172 batch PCKh 0.5\n",
      "Validated batch 79 batch loss 0.5975281 batch mAP 0.581390381 batch PCKh 0.5625\n",
      "Validated batch 80 batch loss 0.701678634 batch mAP 0.49395752 batch PCKh 0.0625\n",
      "Validated batch 81 batch loss 0.496713698 batch mAP 0.576843262 batch PCKh 0.5625\n",
      "Validated batch 82 batch loss 0.545171 batch mAP 0.653381348 batch PCKh 0.1875\n",
      "Validated batch 83 batch loss 0.568755388 batch mAP 0.607055664 batch PCKh 0.25\n",
      "Validated batch 84 batch loss 0.705091417 batch mAP 0.557891846 batch PCKh 0.5625\n",
      "Validated batch 85 batch loss 0.415166438 batch mAP 0.730499268 batch PCKh 0.5625\n",
      "Validated batch 86 batch loss 0.481951773 batch mAP 0.708679199 batch PCKh 0.5\n",
      "Validated batch 87 batch loss 0.49017486 batch mAP 0.617645264 batch PCKh 0.75\n",
      "Validated batch 88 batch loss 0.614774585 batch mAP 0.582305908 batch PCKh 0.625\n",
      "Validated batch 89 batch loss 0.587203 batch mAP 0.701965332 batch PCKh 0.5\n",
      "Validated batch 90 batch loss 0.616464496 batch mAP 0.588684082 batch PCKh 0.8125\n",
      "Validated batch 91 batch loss 0.452738166 batch mAP 0.662414551 batch PCKh 0.75\n",
      "Validated batch 92 batch loss 0.531976 batch mAP 0.660919189 batch PCKh 0.8125\n",
      "Validated batch 93 batch loss 0.535907269 batch mAP 0.664581299 batch PCKh 0.3125\n",
      "Validated batch 94 batch loss 0.586975396 batch mAP 0.533660889 batch PCKh 0.5\n",
      "Validated batch 95 batch loss 0.520701885 batch mAP 0.615020752 batch PCKh 0.5\n",
      "Validated batch 96 batch loss 0.542669415 batch mAP 0.637817383 batch PCKh 0.4375\n",
      "Validated batch 97 batch loss 0.510128379 batch mAP 0.589233398 batch PCKh 0.6875\n",
      "Validated batch 98 batch loss 0.567755461 batch mAP 0.530517578 batch PCKh 0.125\n",
      "Validated batch 99 batch loss 0.564976454 batch mAP 0.603271484 batch PCKh 0.6875\n",
      "Validated batch 100 batch loss 0.446923107 batch mAP 0.589630127 batch PCKh 0.6875\n",
      "Validated batch 101 batch loss 0.570556104 batch mAP 0.60333252 batch PCKh 0.8125\n",
      "Validated batch 102 batch loss 0.546190917 batch mAP 0.632080078 batch PCKh 0.6875\n",
      "Validated batch 103 batch loss 0.585391164 batch mAP 0.566436768 batch PCKh 0.375\n",
      "Validated batch 104 batch loss 0.582631826 batch mAP 0.608398438 batch PCKh 0.375\n",
      "Validated batch 105 batch loss 0.551478744 batch mAP 0.622680664 batch PCKh 0.6875\n",
      "Validated batch 106 batch loss 0.603212297 batch mAP 0.534454346 batch PCKh 0.125\n",
      "Validated batch 107 batch loss 0.53577733 batch mAP 0.633117676 batch PCKh 0.4375\n",
      "Validated batch 108 batch loss 0.559750676 batch mAP 0.636383057 batch PCKh 0.625\n",
      "Validated batch 109 batch loss 0.625218213 batch mAP 0.540710449 batch PCKh 0.5625\n",
      "Validated batch 110 batch loss 0.502237856 batch mAP 0.647522 batch PCKh 0.5625\n",
      "Validated batch 111 batch loss 0.52039516 batch mAP 0.714386 batch PCKh 0.875\n",
      "Validated batch 112 batch loss 0.549741864 batch mAP 0.664825439 batch PCKh 0.25\n",
      "Validated batch 113 batch loss 0.672195673 batch mAP 0.589202881 batch PCKh 0.5\n",
      "Validated batch 114 batch loss 0.462056398 batch mAP 0.684112549 batch PCKh 0.4375\n",
      "Validated batch 115 batch loss 0.70116663 batch mAP 0.554657 batch PCKh 0.0625\n",
      "Validated batch 116 batch loss 0.530835867 batch mAP 0.606292725 batch PCKh 0.375\n",
      "Validated batch 117 batch loss 0.490508169 batch mAP 0.686553955 batch PCKh 0.1875\n",
      "Validated batch 118 batch loss 0.577710152 batch mAP 0.621307373 batch PCKh 0.6875\n",
      "Validated batch 119 batch loss 0.537735581 batch mAP 0.605957031 batch PCKh 0.5625\n",
      "Validated batch 120 batch loss 0.537384689 batch mAP 0.654724121 batch PCKh 0\n",
      "Validated batch 121 batch loss 0.601486385 batch mAP 0.598053 batch PCKh 0.8125\n",
      "Validated batch 122 batch loss 0.535136 batch mAP 0.621337891 batch PCKh 0.5\n",
      "Validated batch 123 batch loss 0.481993467 batch mAP 0.681854248 batch PCKh 0.625\n",
      "Validated batch 124 batch loss 0.571164489 batch mAP 0.628601074 batch PCKh 0.1875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 125 batch loss 0.656378031 batch mAP 0.542327881 batch PCKh 0.25\n",
      "Validated batch 126 batch loss 0.656817317 batch mAP 0.581848145 batch PCKh 0.5\n",
      "Validated batch 127 batch loss 0.51163578 batch mAP 0.615631104 batch PCKh 0.3125\n",
      "Validated batch 128 batch loss 0.677316189 batch mAP 0.618438721 batch PCKh 0.5625\n",
      "Validated batch 129 batch loss 0.564174771 batch mAP 0.632507324 batch PCKh 0.6875\n",
      "Validated batch 130 batch loss 0.399668902 batch mAP 0.71005249 batch PCKh 0.3125\n",
      "Validated batch 131 batch loss 0.579747558 batch mAP 0.576263428 batch PCKh 0.875\n",
      "Validated batch 132 batch loss 0.619092882 batch mAP 0.484832764 batch PCKh 0.1875\n",
      "Validated batch 133 batch loss 0.600125313 batch mAP 0.604858398 batch PCKh 0.4375\n",
      "Validated batch 134 batch loss 0.554989457 batch mAP 0.610473633 batch PCKh 0.25\n",
      "Validated batch 135 batch loss 0.566447139 batch mAP 0.561462402 batch PCKh 0.1875\n",
      "Validated batch 136 batch loss 0.418139 batch mAP 0.69543457 batch PCKh 0.6875\n",
      "Validated batch 137 batch loss 0.577308893 batch mAP 0.617492676 batch PCKh 0.1875\n",
      "Validated batch 138 batch loss 0.522542834 batch mAP 0.622070312 batch PCKh 0.625\n",
      "Validated batch 139 batch loss 0.587632895 batch mAP 0.604431152 batch PCKh 0.5\n",
      "Validated batch 140 batch loss 0.531079769 batch mAP 0.55480957 batch PCKh 0.0625\n",
      "Validated batch 141 batch loss 0.517605722 batch mAP 0.618682861 batch PCKh 0.375\n",
      "Validated batch 142 batch loss 0.542303324 batch mAP 0.595916748 batch PCKh 0.375\n",
      "Validated batch 143 batch loss 0.506534219 batch mAP 0.676696777 batch PCKh 0.75\n",
      "Validated batch 144 batch loss 0.587767899 batch mAP 0.564483643 batch PCKh 0.5625\n",
      "Validated batch 145 batch loss 0.505106032 batch mAP 0.719390869 batch PCKh 0.625\n",
      "Validated batch 146 batch loss 0.512064219 batch mAP 0.698272705 batch PCKh 0.6875\n",
      "Validated batch 147 batch loss 0.55934757 batch mAP 0.602386475 batch PCKh 0.75\n",
      "Validated batch 148 batch loss 0.494815052 batch mAP 0.631561279 batch PCKh 0.75\n",
      "Validated batch 149 batch loss 0.537576556 batch mAP 0.647277832 batch PCKh 0.25\n",
      "Validated batch 150 batch loss 0.643923044 batch mAP 0.54422 batch PCKh 0.8125\n",
      "Validated batch 151 batch loss 0.518638849 batch mAP 0.629394531 batch PCKh 0.5\n",
      "Validated batch 152 batch loss 0.618256211 batch mAP 0.560516357 batch PCKh 0.25\n",
      "Validated batch 153 batch loss 0.554439664 batch mAP 0.623504639 batch PCKh 0.375\n",
      "Validated batch 154 batch loss 0.654347897 batch mAP 0.541931152 batch PCKh 0.375\n",
      "Validated batch 155 batch loss 0.570471525 batch mAP 0.538085938 batch PCKh 0.625\n",
      "Validated batch 156 batch loss 0.564586937 batch mAP 0.67288208 batch PCKh 0.6875\n",
      "Validated batch 157 batch loss 0.596091747 batch mAP 0.567810059 batch PCKh 0.375\n",
      "Validated batch 158 batch loss 0.518102646 batch mAP 0.553253174 batch PCKh 0.5625\n",
      "Validated batch 159 batch loss 0.596534312 batch mAP 0.634246826 batch PCKh 0.4375\n",
      "Validated batch 160 batch loss 0.625849247 batch mAP 0.604736328 batch PCKh 0.75\n",
      "Validated batch 161 batch loss 0.676866293 batch mAP 0.590148926 batch PCKh 0.75\n",
      "Validated batch 162 batch loss 0.659095645 batch mAP 0.5703125 batch PCKh 0.5\n",
      "Validated batch 163 batch loss 0.651226521 batch mAP 0.594787598 batch PCKh 0.6875\n",
      "Validated batch 164 batch loss 0.600187659 batch mAP 0.504180908 batch PCKh 0.5625\n",
      "Validated batch 165 batch loss 0.629413 batch mAP 0.652954102 batch PCKh 0.5625\n",
      "Validated batch 166 batch loss 0.611990571 batch mAP 0.633361816 batch PCKh 0.5\n",
      "Validated batch 167 batch loss 0.623090565 batch mAP 0.650238037 batch PCKh 0.6875\n",
      "Validated batch 168 batch loss 0.640818059 batch mAP 0.625061035 batch PCKh 0.6875\n",
      "Validated batch 169 batch loss 0.674980223 batch mAP 0.582336426 batch PCKh 0.625\n",
      "Validated batch 170 batch loss 0.616875768 batch mAP 0.642181396 batch PCKh 0.6875\n",
      "Validated batch 171 batch loss 0.6812675 batch mAP 0.54876709 batch PCKh 0.6875\n",
      "Validated batch 172 batch loss 0.569492042 batch mAP 0.66809082 batch PCKh 0.1875\n",
      "Validated batch 173 batch loss 0.611330271 batch mAP 0.675445557 batch PCKh 0.75\n",
      "Validated batch 174 batch loss 0.509390116 batch mAP 0.598022461 batch PCKh 0.1875\n",
      "Validated batch 175 batch loss 0.556702852 batch mAP 0.642608643 batch PCKh 0.625\n",
      "Validated batch 176 batch loss 0.592116058 batch mAP 0.657928467 batch PCKh 0.5625\n",
      "Validated batch 177 batch loss 0.64451 batch mAP 0.585144043 batch PCKh 0.3125\n",
      "Validated batch 178 batch loss 0.59280169 batch mAP 0.675628662 batch PCKh 0.5\n",
      "Validated batch 179 batch loss 0.642436743 batch mAP 0.615509033 batch PCKh 0.6875\n",
      "Validated batch 180 batch loss 0.546677351 batch mAP 0.605316162 batch PCKh 0.75\n",
      "Validated batch 181 batch loss 0.534056246 batch mAP 0.7215271 batch PCKh 0.4375\n",
      "Validated batch 182 batch loss 0.597155094 batch mAP 0.614013672 batch PCKh 0.5\n",
      "Validated batch 183 batch loss 0.630928576 batch mAP 0.577209473 batch PCKh 0.375\n",
      "Validated batch 184 batch loss 0.545325398 batch mAP 0.607574463 batch PCKh 0.625\n",
      "Validated batch 185 batch loss 0.608487904 batch mAP 0.614471436 batch PCKh 0.75\n",
      "Validated batch 186 batch loss 0.462110579 batch mAP 0.627441406 batch PCKh 0.5625\n",
      "Validated batch 187 batch loss 0.510256648 batch mAP 0.630981445 batch PCKh 0.75\n",
      "Validated batch 188 batch loss 0.526332438 batch mAP 0.62677 batch PCKh 0.75\n",
      "Validated batch 189 batch loss 0.635793209 batch mAP 0.596466064 batch PCKh 0.5\n",
      "Validated batch 190 batch loss 0.568234444 batch mAP 0.579223633 batch PCKh 0.6875\n",
      "Validated batch 191 batch loss 0.592578173 batch mAP 0.660736084 batch PCKh 0.1875\n",
      "Validated batch 192 batch loss 0.471350431 batch mAP 0.676696777 batch PCKh 0.5\n",
      "Validated batch 193 batch loss 0.558176637 batch mAP 0.631286621 batch PCKh 0.625\n",
      "Validated batch 194 batch loss 0.613751411 batch mAP 0.61239624 batch PCKh 0.1875\n",
      "Validated batch 195 batch loss 0.711137056 batch mAP 0.565368652 batch PCKh 0.375\n",
      "Validated batch 196 batch loss 0.607303858 batch mAP 0.652191162 batch PCKh 0.6875\n",
      "Validated batch 197 batch loss 0.521552682 batch mAP 0.662261963 batch PCKh 0.875\n",
      "Validated batch 198 batch loss 0.457835257 batch mAP 0.668670654 batch PCKh 0.875\n",
      "Validated batch 199 batch loss 0.539915442 batch mAP 0.692657471 batch PCKh 0.25\n",
      "Validated batch 200 batch loss 0.556228876 batch mAP 0.68548584 batch PCKh 0.5\n",
      "Validated batch 201 batch loss 0.602147102 batch mAP 0.599273682 batch PCKh 0.375\n",
      "Validated batch 202 batch loss 0.469202191 batch mAP 0.591461182 batch PCKh 0.25\n",
      "Validated batch 203 batch loss 0.592556238 batch mAP 0.537200928 batch PCKh 0.75\n",
      "Validated batch 204 batch loss 0.579018831 batch mAP 0.610321045 batch PCKh 0.625\n",
      "Validated batch 205 batch loss 0.614991307 batch mAP 0.649169922 batch PCKh 0.25\n",
      "Validated batch 206 batch loss 0.535544693 batch mAP 0.640686035 batch PCKh 0.625\n",
      "Validated batch 207 batch loss 0.510479569 batch mAP 0.680877686 batch PCKh 0.5625\n",
      "Validated batch 208 batch loss 0.49860245 batch mAP 0.586090088 batch PCKh 0.75\n",
      "Validated batch 209 batch loss 0.386399388 batch mAP 0.654632568 batch PCKh 0.5625\n",
      "Validated batch 210 batch loss 0.5396083 batch mAP 0.670715332 batch PCKh 0.25\n",
      "Validated batch 211 batch loss 0.540097177 batch mAP 0.675415039 batch PCKh 0.8125\n",
      "Validated batch 212 batch loss 0.502647 batch mAP 0.653900146 batch PCKh 0.75\n",
      "Validated batch 213 batch loss 0.548223 batch mAP 0.692260742 batch PCKh 0.5\n",
      "Validated batch 214 batch loss 0.472371101 batch mAP 0.67880249 batch PCKh 0.5\n",
      "Validated batch 215 batch loss 0.555347741 batch mAP 0.64050293 batch PCKh 0.8125\n",
      "Validated batch 216 batch loss 0.552745938 batch mAP 0.557373047 batch PCKh 0.625\n",
      "Validated batch 217 batch loss 0.611137 batch mAP 0.613739 batch PCKh 0.125\n",
      "Validated batch 218 batch loss 0.544151902 batch mAP 0.693054199 batch PCKh 0.875\n",
      "Validated batch 219 batch loss 0.497902662 batch mAP 0.710540771 batch PCKh 0.5625\n",
      "Validated batch 220 batch loss 0.458958149 batch mAP 0.673278809 batch PCKh 0.8125\n",
      "Validated batch 221 batch loss 0.614602566 batch mAP 0.677246094 batch PCKh 0.3125\n",
      "Validated batch 222 batch loss 0.572284043 batch mAP 0.658172607 batch PCKh 0.3125\n",
      "Validated batch 223 batch loss 0.630434513 batch mAP 0.580993652 batch PCKh 0.0625\n",
      "Validated batch 224 batch loss 0.61087209 batch mAP 0.663848877 batch PCKh 0.375\n",
      "Validated batch 225 batch loss 0.597288966 batch mAP 0.644775391 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 226 batch loss 0.575675607 batch mAP 0.733154297 batch PCKh 0.75\n",
      "Validated batch 227 batch loss 0.566537142 batch mAP 0.691772461 batch PCKh 0.4375\n",
      "Validated batch 228 batch loss 0.582539797 batch mAP 0.604309082 batch PCKh 0.75\n",
      "Validated batch 229 batch loss 0.51166 batch mAP 0.58291626 batch PCKh 0.75\n",
      "Validated batch 230 batch loss 0.471235454 batch mAP 0.591400146 batch PCKh 0.375\n",
      "Validated batch 231 batch loss 0.594514 batch mAP 0.643341064 batch PCKh 0.75\n",
      "Validated batch 232 batch loss 0.645219505 batch mAP 0.564758301 batch PCKh 0.375\n",
      "Validated batch 233 batch loss 0.542709112 batch mAP 0.625518799 batch PCKh 0.5\n",
      "Validated batch 234 batch loss 0.587705314 batch mAP 0.647216797 batch PCKh 0.625\n",
      "Validated batch 235 batch loss 0.594421148 batch mAP 0.624267578 batch PCKh 0.6875\n",
      "Validated batch 236 batch loss 0.482376337 batch mAP 0.703704834 batch PCKh 0.6875\n",
      "Validated batch 237 batch loss 0.569672406 batch mAP 0.682067871 batch PCKh 0.5\n",
      "Validated batch 238 batch loss 0.597403169 batch mAP 0.724609375 batch PCKh 0.75\n",
      "Validated batch 239 batch loss 0.545960069 batch mAP 0.670257568 batch PCKh 0.875\n",
      "Validated batch 240 batch loss 0.509552956 batch mAP 0.700653076 batch PCKh 0.625\n",
      "Validated batch 241 batch loss 0.578605175 batch mAP 0.59387207 batch PCKh 0.375\n",
      "Validated batch 242 batch loss 0.599541724 batch mAP 0.710174561 batch PCKh 0.8125\n",
      "Validated batch 243 batch loss 0.613804698 batch mAP 0.645355225 batch PCKh 0.6875\n",
      "Validated batch 244 batch loss 0.575543702 batch mAP 0.627868652 batch PCKh 0.6875\n",
      "Validated batch 245 batch loss 0.60067904 batch mAP 0.672271729 batch PCKh 0.25\n",
      "Validated batch 246 batch loss 0.503815651 batch mAP 0.602508545 batch PCKh 0.5625\n",
      "Validated batch 247 batch loss 0.537843943 batch mAP 0.633972168 batch PCKh 0.625\n",
      "Validated batch 248 batch loss 0.564046443 batch mAP 0.690795898 batch PCKh 0.3125\n",
      "Validated batch 249 batch loss 0.592128396 batch mAP 0.668396 batch PCKh 0.75\n",
      "Validated batch 250 batch loss 0.570982933 batch mAP 0.632537842 batch PCKh 0.4375\n",
      "Validated batch 251 batch loss 0.621955693 batch mAP 0.710449219 batch PCKh 0.5625\n",
      "Validated batch 252 batch loss 0.60490334 batch mAP 0.625549316 batch PCKh 0.1875\n",
      "Validated batch 253 batch loss 0.624904633 batch mAP 0.553466797 batch PCKh 0.6875\n",
      "Validated batch 254 batch loss 0.570649922 batch mAP 0.584747314 batch PCKh 0.6875\n",
      "Validated batch 255 batch loss 0.5710361 batch mAP 0.617675781 batch PCKh 0.5\n",
      "Validated batch 256 batch loss 0.58142978 batch mAP 0.60849 batch PCKh 0.4375\n",
      "Validated batch 257 batch loss 0.485891342 batch mAP 0.588867188 batch PCKh 0\n",
      "Validated batch 258 batch loss 0.535324812 batch mAP 0.572814941 batch PCKh 0.75\n",
      "Validated batch 259 batch loss 0.616868675 batch mAP 0.608184814 batch PCKh 0.1875\n",
      "Validated batch 260 batch loss 0.446026772 batch mAP 0.688781738 batch PCKh 0.3125\n",
      "Validated batch 261 batch loss 0.574094474 batch mAP 0.568969727 batch PCKh 0.75\n",
      "Validated batch 262 batch loss 0.550491 batch mAP 0.633880615 batch PCKh 0.6875\n",
      "Validated batch 263 batch loss 0.485241652 batch mAP 0.6668396 batch PCKh 0.3125\n",
      "Validated batch 264 batch loss 0.635849237 batch mAP 0.5987854 batch PCKh 0.5\n",
      "Validated batch 265 batch loss 0.535574913 batch mAP 0.571838379 batch PCKh 0.125\n",
      "Validated batch 266 batch loss 0.579547405 batch mAP 0.637695312 batch PCKh 0.625\n",
      "Validated batch 267 batch loss 0.575325966 batch mAP 0.658294678 batch PCKh 0.5\n",
      "Validated batch 268 batch loss 0.569794536 batch mAP 0.652008057 batch PCKh 0.3125\n",
      "Validated batch 269 batch loss 0.613976657 batch mAP 0.647064209 batch PCKh 0.5\n",
      "Validated batch 270 batch loss 0.692006171 batch mAP 0.544952393 batch PCKh 0.1875\n",
      "Validated batch 271 batch loss 0.698207378 batch mAP 0.599273682 batch PCKh 0.25\n",
      "Validated batch 272 batch loss 0.574558794 batch mAP 0.674591064 batch PCKh 0.6875\n",
      "Validated batch 273 batch loss 0.512105107 batch mAP 0.647857666 batch PCKh 0.6875\n",
      "Validated batch 274 batch loss 0.63088727 batch mAP 0.57925415 batch PCKh 0.625\n",
      "Validated batch 275 batch loss 0.526498199 batch mAP 0.581298828 batch PCKh 0.5\n",
      "Validated batch 276 batch loss 0.516960859 batch mAP 0.662780762 batch PCKh 0.125\n",
      "Validated batch 277 batch loss 0.456840754 batch mAP 0.698730469 batch PCKh 0.6875\n",
      "Validated batch 278 batch loss 0.551114559 batch mAP 0.636993408 batch PCKh 0.625\n",
      "Validated batch 279 batch loss 0.606707 batch mAP 0.632629395 batch PCKh 0.0625\n",
      "Validated batch 280 batch loss 0.547868133 batch mAP 0.607208252 batch PCKh 0.5625\n",
      "Validated batch 281 batch loss 0.54896456 batch mAP 0.566894531 batch PCKh 0.5\n",
      "Validated batch 282 batch loss 0.546559 batch mAP 0.555969238 batch PCKh 0.125\n",
      "Validated batch 283 batch loss 0.472679079 batch mAP 0.669525146 batch PCKh 0.25\n",
      "Validated batch 284 batch loss 0.54376173 batch mAP 0.618103 batch PCKh 0\n",
      "Validated batch 285 batch loss 0.551376224 batch mAP 0.686798096 batch PCKh 0.9375\n",
      "Validated batch 286 batch loss 0.531387806 batch mAP 0.65322876 batch PCKh 0.25\n",
      "Validated batch 287 batch loss 0.566010773 batch mAP 0.660980225 batch PCKh 0.5\n",
      "Validated batch 288 batch loss 0.737401366 batch mAP 0.583343506 batch PCKh 0\n",
      "Validated batch 289 batch loss 0.514977813 batch mAP 0.659057617 batch PCKh 0.625\n",
      "Validated batch 290 batch loss 0.487153471 batch mAP 0.646942139 batch PCKh 0.5625\n",
      "Validated batch 291 batch loss 0.634700179 batch mAP 0.626586914 batch PCKh 0.625\n",
      "Validated batch 292 batch loss 0.443879724 batch mAP 0.673370361 batch PCKh 0.4375\n",
      "Validated batch 293 batch loss 0.483232379 batch mAP 0.715423584 batch PCKh 0.5625\n",
      "Validated batch 294 batch loss 0.557642221 batch mAP 0.665313721 batch PCKh 0.6875\n",
      "Validated batch 295 batch loss 0.528409481 batch mAP 0.710174561 batch PCKh 0.1875\n",
      "Validated batch 296 batch loss 0.577342153 batch mAP 0.586029053 batch PCKh 0.25\n",
      "Validated batch 297 batch loss 0.466017783 batch mAP 0.715179443 batch PCKh 0.5\n",
      "Validated batch 298 batch loss 0.480093598 batch mAP 0.684295654 batch PCKh 0.625\n",
      "Validated batch 299 batch loss 0.585773468 batch mAP 0.576019287 batch PCKh 0.1875\n",
      "Validated batch 300 batch loss 0.572156787 batch mAP 0.610534668 batch PCKh 0\n",
      "Validated batch 301 batch loss 0.471701831 batch mAP 0.573272705 batch PCKh 0.0625\n",
      "Validated batch 302 batch loss 0.50915885 batch mAP 0.650909424 batch PCKh 0.5625\n",
      "Validated batch 303 batch loss 0.619009614 batch mAP 0.607452393 batch PCKh 0.5625\n",
      "Validated batch 304 batch loss 0.512416482 batch mAP 0.661499 batch PCKh 0.25\n",
      "Validated batch 305 batch loss 0.618911266 batch mAP 0.621887207 batch PCKh 0.875\n",
      "Validated batch 306 batch loss 0.658640385 batch mAP 0.603027344 batch PCKh 0.8125\n",
      "Validated batch 307 batch loss 0.62842232 batch mAP 0.595001221 batch PCKh 0.1875\n",
      "Validated batch 308 batch loss 0.611480117 batch mAP 0.629821777 batch PCKh 0.375\n",
      "Validated batch 309 batch loss 0.514777362 batch mAP 0.660217285 batch PCKh 0.5625\n",
      "Validated batch 310 batch loss 0.540008187 batch mAP 0.605377197 batch PCKh 0.75\n",
      "Validated batch 311 batch loss 0.594436407 batch mAP 0.609924316 batch PCKh 0.3125\n",
      "Validated batch 312 batch loss 0.661123872 batch mAP 0.559417725 batch PCKh 0\n",
      "Validated batch 313 batch loss 0.496320605 batch mAP 0.691741943 batch PCKh 0.3125\n",
      "Validated batch 314 batch loss 0.436118841 batch mAP 0.716125488 batch PCKh 0.5\n",
      "Validated batch 315 batch loss 0.516223311 batch mAP 0.539703369 batch PCKh 0\n",
      "Validated batch 316 batch loss 0.462665886 batch mAP 0.691345215 batch PCKh 0.375\n",
      "Validated batch 317 batch loss 0.581960559 batch mAP 0.574493408 batch PCKh 0.4375\n",
      "Validated batch 318 batch loss 0.457677752 batch mAP 0.713012695 batch PCKh 0.75\n",
      "Validated batch 319 batch loss 0.463713586 batch mAP 0.699646 batch PCKh 0.5\n",
      "Validated batch 320 batch loss 0.582879901 batch mAP 0.634765625 batch PCKh 0.3125\n",
      "Validated batch 321 batch loss 0.591752 batch mAP 0.586486816 batch PCKh 0.625\n",
      "Validated batch 322 batch loss 0.621335506 batch mAP 0.586639404 batch PCKh 0.625\n",
      "Validated batch 323 batch loss 0.624488235 batch mAP 0.551818848 batch PCKh 0.3125\n",
      "Validated batch 324 batch loss 0.576020718 batch mAP 0.635681152 batch PCKh 0.8125\n",
      "Validated batch 325 batch loss 0.61159873 batch mAP 0.544708252 batch PCKh 0.625\n",
      "Validated batch 326 batch loss 0.553182125 batch mAP 0.659698486 batch PCKh 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 327 batch loss 0.56966269 batch mAP 0.575073242 batch PCKh 0.5625\n",
      "Validated batch 328 batch loss 0.573444128 batch mAP 0.645355225 batch PCKh 0.6875\n",
      "Validated batch 329 batch loss 0.581770837 batch mAP 0.624359131 batch PCKh 0.8125\n",
      "Validated batch 330 batch loss 0.572425723 batch mAP 0.626861572 batch PCKh 0.1875\n",
      "Validated batch 331 batch loss 0.586453855 batch mAP 0.567138672 batch PCKh 0.4375\n",
      "Validated batch 332 batch loss 0.657190919 batch mAP 0.58972168 batch PCKh 0.375\n",
      "Validated batch 333 batch loss 0.68029207 batch mAP 0.539825439 batch PCKh 0.5\n",
      "Validated batch 334 batch loss 0.694463849 batch mAP 0.590515137 batch PCKh 0.75\n",
      "Validated batch 335 batch loss 0.594773054 batch mAP 0.642395 batch PCKh 0\n",
      "Validated batch 336 batch loss 0.543430209 batch mAP 0.65802 batch PCKh 0.5625\n",
      "Validated batch 337 batch loss 0.571395874 batch mAP 0.683654785 batch PCKh 0.8125\n",
      "Validated batch 338 batch loss 0.60750252 batch mAP 0.56652832 batch PCKh 0.875\n",
      "Validated batch 339 batch loss 0.562757075 batch mAP 0.687408447 batch PCKh 0.5\n",
      "Validated batch 340 batch loss 0.513447344 batch mAP 0.692199707 batch PCKh 0.5625\n",
      "Validated batch 341 batch loss 0.63949281 batch mAP 0.618774414 batch PCKh 0.3125\n",
      "Validated batch 342 batch loss 0.590049 batch mAP 0.596069336 batch PCKh 0.125\n",
      "Validated batch 343 batch loss 0.608288765 batch mAP 0.654632568 batch PCKh 0.75\n",
      "Validated batch 344 batch loss 0.552801669 batch mAP 0.6612854 batch PCKh 0.875\n",
      "Validated batch 345 batch loss 0.591715157 batch mAP 0.603363037 batch PCKh 0.75\n",
      "Validated batch 346 batch loss 0.559999585 batch mAP 0.649719238 batch PCKh 0.75\n",
      "Validated batch 347 batch loss 0.545900464 batch mAP 0.630706787 batch PCKh 0.75\n",
      "Validated batch 348 batch loss 0.642960429 batch mAP 0.639923096 batch PCKh 0.5625\n",
      "Validated batch 349 batch loss 0.740291059 batch mAP 0.653015137 batch PCKh 0.375\n",
      "Validated batch 350 batch loss 0.540766537 batch mAP 0.707305908 batch PCKh 0.5\n",
      "Validated batch 351 batch loss 0.526927412 batch mAP 0.602233887 batch PCKh 0.5625\n",
      "Validated batch 352 batch loss 0.639239788 batch mAP 0.632019043 batch PCKh 0.4375\n",
      "Validated batch 353 batch loss 0.599803507 batch mAP 0.639709473 batch PCKh 0\n",
      "Validated batch 354 batch loss 0.65063 batch mAP 0.718597412 batch PCKh 0.25\n",
      "Validated batch 355 batch loss 0.586058438 batch mAP 0.68182373 batch PCKh 0.8125\n",
      "Validated batch 356 batch loss 0.597808957 batch mAP 0.573761 batch PCKh 0.4375\n",
      "Validated batch 357 batch loss 0.63186729 batch mAP 0.579711914 batch PCKh 0.8125\n",
      "Validated batch 358 batch loss 0.505103469 batch mAP 0.635803223 batch PCKh 0.5625\n",
      "Validated batch 359 batch loss 0.501655281 batch mAP 0.667236328 batch PCKh 0.75\n",
      "Validated batch 360 batch loss 0.651663184 batch mAP 0.50668335 batch PCKh 0.75\n",
      "Validated batch 361 batch loss 0.600554824 batch mAP 0.583282471 batch PCKh 0.75\n",
      "Validated batch 362 batch loss 0.603962123 batch mAP 0.694061279 batch PCKh 0.625\n",
      "Validated batch 363 batch loss 0.504572213 batch mAP 0.700897217 batch PCKh 0.4375\n",
      "Validated batch 364 batch loss 0.606516063 batch mAP 0.639160156 batch PCKh 0.6875\n",
      "Validated batch 365 batch loss 0.585921884 batch mAP 0.60836792 batch PCKh 0.625\n",
      "Validated batch 366 batch loss 0.500546038 batch mAP 0.652679443 batch PCKh 0.1875\n",
      "Validated batch 367 batch loss 0.584264398 batch mAP 0.639465332 batch PCKh 0.875\n",
      "Validated batch 368 batch loss 0.545514405 batch mAP 0.63861084 batch PCKh 0.0625\n",
      "Validated batch 369 batch loss 0.654816628 batch mAP 0.550415039 batch PCKh 0.6875\n",
      "Epoch 10 val loss 0.569962203502655 val mAP 0.6247891783714294 val PCKh\n",
      "Epoch 10 completed in 769.49 seconds\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file, history = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e788a58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(history['train_loss'], 'r')\n",
    "plt.plot(history['val_loss'], 'y')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "356cfe53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(history['train_map'], 'r')\n",
    "plt.plot(history['val_map'], 'y')\n",
    "plt.title('Model mAP')\n",
    "plt.ylabel('mAP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7b1ea21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(history['train_pckh'], 'r')\n",
    "plt.plot(history['val_pckh'], 'y')\n",
    "plt.title('Model PCKh')\n",
    "plt.ylabel('PCKh')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03881ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history/history_sp_10.json', 'w') as f:\n",
    "    pd.DataFrame(history).to_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a009668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
