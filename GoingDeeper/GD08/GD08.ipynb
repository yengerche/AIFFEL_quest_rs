{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfbdb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "import ray\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/node_data/GD08/mpii'\n",
    "MODEL_PATH = os.getenv('HOME') + '/aiffel/model_weight/GD08'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3eb6d6",
   "metadata": {},
   "source": [
    "### Label 변환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3adc2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610cffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efa2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc78b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a63950",
   "metadata": {},
   "source": [
    "### 모델 구축 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444af794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae601bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f68405e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b2a384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c1a9818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def compute_mAP(self, labels, outputs, thr=0.5):\n",
    "        mAP = 0.0\n",
    "        map_batch = tf.zeros([self.global_batch_size], tf.float32)\n",
    "        \n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        outputs = tf.cast(outputs, dtype=tf.float32)\n",
    "            \n",
    "        for output in outputs:\n",
    "            distances = tf.reduce_mean(tf.sqrt(tf.square(labels - output)), axis=(1, 2))\n",
    "            correct_predictions = tf.cast(distances >= thr, tf.float32)\n",
    "            precision = tf.reduce_mean(correct_predictions, axis=-1)\n",
    "            map_batch += precision\n",
    "        map_batch = map_batch/len(outputs)\n",
    "        mAP = tf.reduce_mean(map_batch)\n",
    "        return mAP\n",
    "    \n",
    "    def compute_PCKh(self, labels, outputs, thr=0.5):\n",
    "        labels = tf.cast(labels, dtype=tf.float32)\n",
    "        outputs = tf.cast(outputs, dtype=tf.float32)\n",
    "        \n",
    "        def get_coords(heatmap):\n",
    "            if len(heatmap.shape) <= 3:\n",
    "                heatmap = tf.expand_dims(heatmap, axis=-1)\n",
    "                \n",
    "            height = heatmap.shape[1]\n",
    "            widht = heatmap.shape[2]\n",
    "            classes = heatmap.shape[3]\n",
    "            \n",
    "            flattened_heatmap = tf.reshape(tensor=heatmap, shape=[self.global_batch_size, -1, classes])\n",
    "            max_coords = tf.argmax(flattened_heatmap, axis=1)\n",
    "            \n",
    "            y = max_coords // height\n",
    "            x = max_coords % widht\n",
    "            \n",
    "            return tf.stack([x, y], axis=-2)\n",
    "        \n",
    "        label_0_coords = tf.cast(tf.squeeze(get_coords(labels[..., 0]), axis=-1), tf.float32)\n",
    "        label_1_coords = tf.cast(tf.squeeze(get_coords(labels[..., 1]), axis=-1), tf.float32)\n",
    "        head_size = tf.sqrt(tf.reduce_sum(tf.square(label_0_coords - label_1_coords), axis=-1))\n",
    "        \n",
    "        predictions = []\n",
    "        for class_idx in range(labels.shape[-1]):\n",
    "            prediction = tf.zeros([self.global_batch_size], tf.float32)\n",
    "            for output in outputs:\n",
    "                label_coords = tf.cast(tf.squeeze(get_coords(labels[..., class_idx])), tf.float32)\n",
    "                output_coords = tf.cast(tf.squeeze(get_coords(output[..., class_idx])), tf.float32)\n",
    "                \n",
    "                dist = tf.sqrt(tf.reduce_sum(tf.square(label_coords - output_coords), axis=-1))\n",
    "                correct_prediction = tf.cast(dist <= thr * head_size, tf.float32)\n",
    "                prediction += correct_prediction\n",
    "            prediction = prediction/len(outputs)\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "        predictions = tf.stack(predictions, axis=-1)\n",
    "        PCKh = tf.reduce_mean(predictions)\n",
    "        return PCKh\n",
    "        \n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "            \n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        mAP = self.compute_mAP(labels, outputs)\n",
    "        PCKh = self.compute_PCKh(labels, outputs)\n",
    "        \n",
    "        return loss, mAP, PCKh\n",
    "    \n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        \n",
    "        mAP = self.compute_mAP(labels, outputs)\n",
    "        PCKh = self.compute_PCKh(labels, outputs)\n",
    "        \n",
    "        return loss, mAP, PCKh\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            total_map = 0.0\n",
    "            total_pckh = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss, per_replica_map, per_replica_pckh  = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                \n",
    "                num_train_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                batch_map = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_map, axis=None)\n",
    "                batch_pckh = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_pckh, axis=None)\n",
    "                tf.print('Trained batch', num_train_batches,\n",
    "                         'batch loss', batch_loss,\n",
    "                         'batch mAP', batch_map,\n",
    "                         'batch PCKh', batch_pckh)\n",
    "                \n",
    "                total_loss += batch_loss\n",
    "                total_map += batch_map\n",
    "                total_pckh += batch_pckh\n",
    "                \n",
    "            return total_loss, total_map, total_pckh, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            total_map = 0.0\n",
    "            total_pckh = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss, per_replica_map, per_replica_pckh = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                \n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                batch_map = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_map, axis=None)\n",
    "                batch_pckh = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_pckh, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches,\n",
    "                         'batch loss', batch_loss,\n",
    "                         'batch mAP', batch_map,\n",
    "                         'batch PCKh', batch_pckh)\n",
    "                \n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                    total_map += batch_map\n",
    "                    total_pckh += batch_pckh\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "                    \n",
    "            return total_loss, total_map, total_pckh, num_val_batches\n",
    "        \n",
    "        history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"train_map\": [],\n",
    "            \"val_map\": [],\n",
    "            \"train_pckh\": [],\n",
    "            \"val_pckh\": [],\n",
    "            \"epoch_time\": []}\n",
    "        \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            start_time = time.time()\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "            \n",
    "            train_total_loss, train_total_map, train_total_pckh, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            train_map = train_total_map / num_train_batches\n",
    "            train_pckh = train_total_pckh / num_train_batches\n",
    "            print('Epoch {} train loss {} train mAP {} train PCKh {}'.format(epoch, train_loss, train_map, train_pckh))\n",
    "            \n",
    "            val_total_loss, val_total_map, val_total_pckh, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            val_map = val_total_map / num_val_batches\n",
    "            val_pckh = val_total_pckh / num_val_batches\n",
    "            print('Epoch {} val loss {} val mAP {} val PCKh {}'.format(epoch, val_loss, val_map, val_pckh))\n",
    "            \n",
    "            end_time = time.time()\n",
    "            epoch_duration = end_time - start_time\n",
    "            print(f'Epoch {epoch} completed in {epoch_duration:.2f} seconds')\n",
    "            \n",
    "            history[\"train_loss\"].append(train_loss.numpy())\n",
    "            history[\"val_loss\"].append(val_loss.numpy())\n",
    "            history[\"train_map\"].append(train_map.numpy())\n",
    "            history[\"val_map\"].append(val_map.numpy())\n",
    "            history[\"train_pckh\"].append(train_pckh.numpy())\n",
    "            history[\"val_pckh\"].append(val_pckh.numpy())\n",
    "            history[\"epoch_time\"].append(epoch_duration)\n",
    "            \n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "            \n",
    "        return self.best_model, history\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/i_model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c4d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2708f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd278e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 4.7368145 batch mAP 0.873046875 batch PCKh 0.033203125\n",
      "Trained batch 2 batch loss 5.12859917 batch mAP 0.8046875 batch PCKh 0.021484375\n",
      "Trained batch 3 batch loss 4.71795177 batch mAP 0.69140625 batch PCKh 0.048828125\n",
      "Trained batch 4 batch loss 4.24303865 batch mAP 0.701171875 batch PCKh 0.068359375\n",
      "Trained batch 5 batch loss 4.71073151 batch mAP 0.72265625 batch PCKh 0.103515625\n",
      "Trained batch 6 batch loss 4.80445242 batch mAP 0.791015625 batch PCKh 0.033203125\n",
      "Trained batch 7 batch loss 4.88720226 batch mAP 0.80078125 batch PCKh 0.0390625\n",
      "Trained batch 8 batch loss 4.62131119 batch mAP 0.716796875 batch PCKh 0.0859375\n",
      "Trained batch 9 batch loss 4.78496122 batch mAP 0.625 batch PCKh 0.0625\n",
      "Trained batch 10 batch loss 4.07065535 batch mAP 0.63671875 batch PCKh 0.03125\n",
      "Trained batch 11 batch loss 3.25651121 batch mAP 0.62890625 batch PCKh 0.001953125\n",
      "Trained batch 12 batch loss 3.77517891 batch mAP 0.837890625 batch PCKh 0.0078125\n",
      "Trained batch 13 batch loss 4.40590858 batch mAP 0.68359375 batch PCKh 0.046875\n",
      "Trained batch 14 batch loss 4.04168224 batch mAP 0.798828125 batch PCKh 0.021484375\n",
      "Trained batch 15 batch loss 4.38742542 batch mAP 0.787109375 batch PCKh 0.265625\n",
      "Trained batch 16 batch loss 4.51733971 batch mAP 0.61328125 batch PCKh 0.07421875\n",
      "Trained batch 17 batch loss 4.43928719 batch mAP 0.689453125 batch PCKh 0.0625\n",
      "Trained batch 18 batch loss 4.29958391 batch mAP 0.787109375 batch PCKh 0.083984375\n",
      "Trained batch 19 batch loss 4.31412506 batch mAP 0.7421875 batch PCKh 0.07421875\n",
      "Trained batch 20 batch loss 4.2463026 batch mAP 0.794921875 batch PCKh 0.060546875\n",
      "Trained batch 21 batch loss 4.13830757 batch mAP 0.71484375 batch PCKh 0.12109375\n",
      "Trained batch 22 batch loss 4.15463781 batch mAP 0.796875 batch PCKh 0.140625\n",
      "Trained batch 23 batch loss 4.24312401 batch mAP 0.78125 batch PCKh 0.0546875\n",
      "Trained batch 24 batch loss 4.14500475 batch mAP 0.87109375 batch PCKh 0.06640625\n",
      "Trained batch 25 batch loss 4.14667034 batch mAP 0.849609375 batch PCKh 0.048828125\n",
      "Trained batch 26 batch loss 4.01868153 batch mAP 0.861328125 batch PCKh 0.072265625\n",
      "Trained batch 27 batch loss 4.02964687 batch mAP 0.873046875 batch PCKh 0.095703125\n",
      "Trained batch 28 batch loss 3.98613358 batch mAP 0.90625 batch PCKh 0.109375\n",
      "Trained batch 29 batch loss 3.77246213 batch mAP 0.900390625 batch PCKh 0.115234375\n",
      "Trained batch 30 batch loss 3.52736139 batch mAP 0.833984375 batch PCKh 0.142578125\n",
      "Trained batch 31 batch loss 3.93746233 batch mAP 0.888671875 batch PCKh 0.091796875\n",
      "Trained batch 32 batch loss 3.77881312 batch mAP 0.8828125 batch PCKh 0.2421875\n",
      "Trained batch 33 batch loss 4.01433754 batch mAP 0.77734375 batch PCKh 0.158203125\n",
      "Trained batch 34 batch loss 3.68636036 batch mAP 0.896484375 batch PCKh 0.671875\n",
      "Trained batch 35 batch loss 3.68405342 batch mAP 0.8515625 batch PCKh 0.322265625\n",
      "Trained batch 36 batch loss 3.87922573 batch mAP 0.779296875 batch PCKh 0.1484375\n",
      "Trained batch 37 batch loss 3.80556202 batch mAP 0.849609375 batch PCKh 0.376953125\n",
      "Trained batch 38 batch loss 3.61624885 batch mAP 0.8515625 batch PCKh 0.373046875\n",
      "Trained batch 39 batch loss 3.70367765 batch mAP 0.83203125 batch PCKh 0.294921875\n",
      "Trained batch 40 batch loss 3.69411564 batch mAP 0.796875 batch PCKh 0.431640625\n",
      "Trained batch 41 batch loss 3.46390867 batch mAP 0.818359375 batch PCKh 0.103515625\n",
      "Trained batch 42 batch loss 3.45031214 batch mAP 0.884765625 batch PCKh 0.1953125\n",
      "Trained batch 43 batch loss 3.50174713 batch mAP 0.861328125 batch PCKh 0.12890625\n",
      "Trained batch 44 batch loss 3.53559351 batch mAP 0.865234375 batch PCKh 0.173828125\n",
      "Trained batch 45 batch loss 3.54626179 batch mAP 0.861328125 batch PCKh 0.21875\n",
      "Trained batch 46 batch loss 3.88478422 batch mAP 0.896484375 batch PCKh 0.18359375\n",
      "Trained batch 47 batch loss 3.87526369 batch mAP 0.87109375 batch PCKh 0.115234375\n",
      "Trained batch 48 batch loss 3.62061262 batch mAP 0.892578125 batch PCKh 0.095703125\n",
      "Trained batch 49 batch loss 3.35834455 batch mAP 0.912109375 batch PCKh 0.173828125\n",
      "Trained batch 50 batch loss 3.37651658 batch mAP 0.890625 batch PCKh 0.4609375\n",
      "Trained batch 51 batch loss 3.54407382 batch mAP 0.916015625 batch PCKh 0.4140625\n",
      "Trained batch 52 batch loss 3.81554604 batch mAP 0.85546875 batch PCKh 0.17578125\n",
      "Trained batch 53 batch loss 3.75464511 batch mAP 0.826171875 batch PCKh 0.22265625\n",
      "Trained batch 54 batch loss 3.61640263 batch mAP 0.837890625 batch PCKh 0.306640625\n",
      "Trained batch 55 batch loss 3.35114384 batch mAP 0.86328125 batch PCKh 0.140625\n",
      "Trained batch 56 batch loss 3.69732428 batch mAP 0.8984375 batch PCKh 0.19921875\n",
      "Trained batch 57 batch loss 3.39898658 batch mAP 0.869140625 batch PCKh 0.22265625\n",
      "Trained batch 58 batch loss 3.71185255 batch mAP 0.849609375 batch PCKh 0.216796875\n",
      "Trained batch 59 batch loss 3.6242981 batch mAP 0.810546875 batch PCKh 0.236328125\n",
      "Trained batch 60 batch loss 3.23383713 batch mAP 0.876953125 batch PCKh 0.19140625\n",
      "Trained batch 61 batch loss 3.40178871 batch mAP 0.87890625 batch PCKh 0.341796875\n",
      "Trained batch 62 batch loss 3.45934534 batch mAP 0.90625 batch PCKh 0.068359375\n",
      "Trained batch 63 batch loss 3.10579801 batch mAP 0.865234375 batch PCKh 0.080078125\n",
      "Trained batch 64 batch loss 3.57765627 batch mAP 0.837890625 batch PCKh 0.1171875\n",
      "Trained batch 65 batch loss 3.7372272 batch mAP 0.869140625 batch PCKh 0.14453125\n",
      "Trained batch 66 batch loss 3.64582777 batch mAP 0.79296875 batch PCKh 0.27734375\n",
      "Trained batch 67 batch loss 3.61815929 batch mAP 0.880859375 batch PCKh 0.314453125\n",
      "Trained batch 68 batch loss 3.74860835 batch mAP 0.837890625 batch PCKh 0.1171875\n",
      "Trained batch 69 batch loss 3.76727557 batch mAP 0.736328125 batch PCKh 0.185546875\n",
      "Trained batch 70 batch loss 3.6558342 batch mAP 0.759765625 batch PCKh 0.21484375\n",
      "Trained batch 71 batch loss 3.84674096 batch mAP 0.896484375 batch PCKh 0.166015625\n",
      "Trained batch 72 batch loss 3.75490332 batch mAP 0.765625 batch PCKh 0.23828125\n",
      "Trained batch 73 batch loss 3.70656061 batch mAP 0.923828125 batch PCKh 0.17578125\n",
      "Trained batch 74 batch loss 3.61390781 batch mAP 0.89453125 batch PCKh 0.361328125\n",
      "Trained batch 75 batch loss 3.79463863 batch mAP 0.767578125 batch PCKh 0.115234375\n",
      "Trained batch 76 batch loss 3.52116251 batch mAP 0.83203125 batch PCKh 0.33203125\n",
      "Trained batch 77 batch loss 3.37487221 batch mAP 0.814453125 batch PCKh 0.07421875\n",
      "Trained batch 78 batch loss 3.40034771 batch mAP 0.859375 batch PCKh 0.15234375\n",
      "Trained batch 79 batch loss 3.31156111 batch mAP 0.828125 batch PCKh 0.177734375\n",
      "Trained batch 80 batch loss 2.91510963 batch mAP 0.892578125 batch PCKh 0.154296875\n",
      "Trained batch 81 batch loss 3.11988473 batch mAP 0.82421875 batch PCKh 0.216796875\n",
      "Trained batch 82 batch loss 2.74330807 batch mAP 0.8515625 batch PCKh 0.00390625\n",
      "Trained batch 83 batch loss 2.93398237 batch mAP 0.923828125 batch PCKh 0.09375\n",
      "Trained batch 84 batch loss 3.01660633 batch mAP 0.91796875 batch PCKh 0.162109375\n",
      "Trained batch 85 batch loss 2.98686028 batch mAP 0.908203125 batch PCKh 0.287109375\n",
      "Trained batch 86 batch loss 2.93644094 batch mAP 0.880859375 batch PCKh 0.248046875\n",
      "Trained batch 87 batch loss 3.45491695 batch mAP 0.876953125 batch PCKh 0.166015625\n",
      "Trained batch 88 batch loss 3.6534276 batch mAP 0.837890625 batch PCKh 0.125\n",
      "Trained batch 89 batch loss 3.70368481 batch mAP 0.802734375 batch PCKh 0.19921875\n",
      "Trained batch 90 batch loss 3.43737817 batch mAP 0.87109375 batch PCKh 0.439453125\n",
      "Trained batch 91 batch loss 3.43980598 batch mAP 0.783203125 batch PCKh 0.33203125\n",
      "Trained batch 92 batch loss 3.4843781 batch mAP 0.845703125 batch PCKh 0.3359375\n",
      "Trained batch 93 batch loss 3.35774326 batch mAP 0.7265625 batch PCKh 0.337890625\n",
      "Trained batch 94 batch loss 3.42737961 batch mAP 0.79296875 batch PCKh 0.296875\n",
      "Trained batch 95 batch loss 3.47694468 batch mAP 0.853515625 batch PCKh 0.14453125\n",
      "Trained batch 96 batch loss 3.58142972 batch mAP 0.849609375 batch PCKh 0.201171875\n",
      "Trained batch 97 batch loss 3.27348232 batch mAP 0.79296875 batch PCKh 0.34765625\n",
      "Trained batch 98 batch loss 3.14315391 batch mAP 0.841796875 batch PCKh 0.357421875\n",
      "Trained batch 99 batch loss 3.02946472 batch mAP 0.857421875 batch PCKh 0.509765625\n",
      "Trained batch 100 batch loss 3.24812078 batch mAP 0.802734375 batch PCKh 0.30078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 101 batch loss 3.51722503 batch mAP 0.8515625 batch PCKh 0.29296875\n",
      "Trained batch 102 batch loss 3.33230734 batch mAP 0.7578125 batch PCKh 0.34765625\n",
      "Trained batch 103 batch loss 3.42952394 batch mAP 0.638671875 batch PCKh 0.15625\n",
      "Trained batch 104 batch loss 3.35359669 batch mAP 0.77734375 batch PCKh 0.212890625\n",
      "Trained batch 105 batch loss 3.61653471 batch mAP 0.84765625 batch PCKh 0.25\n",
      "Trained batch 106 batch loss 3.56384754 batch mAP 0.892578125 batch PCKh 0.125\n",
      "Trained batch 107 batch loss 3.58249021 batch mAP 0.8359375 batch PCKh 0.1796875\n",
      "Trained batch 108 batch loss 3.25851107 batch mAP 0.76171875 batch PCKh 0.16796875\n",
      "Trained batch 109 batch loss 3.62344 batch mAP 0.759765625 batch PCKh 0.193359375\n",
      "Trained batch 110 batch loss 3.59533882 batch mAP 0.857421875 batch PCKh 0.166015625\n",
      "Trained batch 111 batch loss 3.36576676 batch mAP 0.828125 batch PCKh 0.26953125\n",
      "Trained batch 112 batch loss 3.30402255 batch mAP 0.748046875 batch PCKh 0.244140625\n",
      "Trained batch 113 batch loss 3.37411499 batch mAP 0.8046875 batch PCKh 0.154296875\n",
      "Trained batch 114 batch loss 3.67530394 batch mAP 0.763671875 batch PCKh 0.134765625\n",
      "Trained batch 115 batch loss 3.44442916 batch mAP 0.7734375 batch PCKh 0.140625\n",
      "Trained batch 116 batch loss 3.40909934 batch mAP 0.724609375 batch PCKh 0.171875\n",
      "Trained batch 117 batch loss 3.28251743 batch mAP 0.734375 batch PCKh 0.125\n",
      "Trained batch 118 batch loss 3.38375926 batch mAP 0.671875 batch PCKh 0.361328125\n",
      "Trained batch 119 batch loss 2.86736226 batch mAP 0.78125 batch PCKh 0.025390625\n",
      "Trained batch 120 batch loss 3.37166762 batch mAP 0.798828125 batch PCKh 0.103515625\n",
      "Trained batch 121 batch loss 3.55289793 batch mAP 0.453125 batch PCKh 0.11328125\n",
      "Trained batch 122 batch loss 3.49732614 batch mAP 0.783203125 batch PCKh 0.19140625\n",
      "Trained batch 123 batch loss 3.27132082 batch mAP 0.81640625 batch PCKh 0.169921875\n",
      "Trained batch 124 batch loss 3.49758029 batch mAP 0.74609375 batch PCKh 0.17578125\n",
      "Trained batch 125 batch loss 3.47352076 batch mAP 0.865234375 batch PCKh 0.19921875\n",
      "Trained batch 126 batch loss 3.38392568 batch mAP 0.82421875 batch PCKh 0.1953125\n",
      "Trained batch 127 batch loss 3.57963538 batch mAP 0.85546875 batch PCKh 0.1484375\n",
      "Trained batch 128 batch loss 3.58327866 batch mAP 0.7421875 batch PCKh 0.1796875\n",
      "Trained batch 129 batch loss 3.622293 batch mAP 0.833984375 batch PCKh 0.283203125\n",
      "Trained batch 130 batch loss 3.61178422 batch mAP 0.78125 batch PCKh 0.193359375\n",
      "Trained batch 131 batch loss 3.26135802 batch mAP 0.7421875 batch PCKh 0.296875\n",
      "Trained batch 132 batch loss 3.30003428 batch mAP 0.822265625 batch PCKh 0.240234375\n",
      "Trained batch 133 batch loss 3.45219326 batch mAP 0.859375 batch PCKh 0.232421875\n",
      "Trained batch 134 batch loss 3.46461463 batch mAP 0.873046875 batch PCKh 0.3203125\n",
      "Trained batch 135 batch loss 3.59238887 batch mAP 0.779296875 batch PCKh 0.1640625\n",
      "Trained batch 136 batch loss 3.20610189 batch mAP 0.81640625 batch PCKh 0.35546875\n",
      "Trained batch 137 batch loss 3.71900368 batch mAP 0.763671875 batch PCKh 0.125\n",
      "Trained batch 138 batch loss 3.73563147 batch mAP 0.79296875 batch PCKh 0.119140625\n",
      "Trained batch 139 batch loss 3.73091793 batch mAP 0.91796875 batch PCKh 0.13671875\n",
      "Trained batch 140 batch loss 3.64360595 batch mAP 0.8828125 batch PCKh 0.1015625\n",
      "Trained batch 141 batch loss 3.62450504 batch mAP 0.90625 batch PCKh 0.171875\n",
      "Trained batch 142 batch loss 3.63894796 batch mAP 0.6328125 batch PCKh 0.24609375\n",
      "Trained batch 143 batch loss 3.50704145 batch mAP 0.72265625 batch PCKh 0.185546875\n",
      "Trained batch 144 batch loss 3.42376494 batch mAP 0.859375 batch PCKh 0.123046875\n",
      "Trained batch 145 batch loss 3.5192697 batch mAP 0.83203125 batch PCKh 0.28125\n",
      "Trained batch 146 batch loss 3.57225227 batch mAP 0.947265625 batch PCKh 0.09765625\n",
      "Trained batch 147 batch loss 3.36064768 batch mAP 0.955078125 batch PCKh 0.14453125\n",
      "Trained batch 148 batch loss 3.1123085 batch mAP 0.943359375 batch PCKh 0.44140625\n",
      "Trained batch 149 batch loss 2.96007609 batch mAP 0.75390625 batch PCKh 0.12109375\n",
      "Trained batch 150 batch loss 2.96211076 batch mAP 0.85546875 batch PCKh 0.1796875\n",
      "Trained batch 151 batch loss 3.68785286 batch mAP 0.875 batch PCKh 0.083984375\n",
      "Trained batch 152 batch loss 3.48714638 batch mAP 0.90234375 batch PCKh 0.16796875\n",
      "Trained batch 153 batch loss 3.53190112 batch mAP 0.859375 batch PCKh 0.115234375\n",
      "Trained batch 154 batch loss 3.5785594 batch mAP 0.8203125 batch PCKh 0.130859375\n",
      "Trained batch 155 batch loss 3.49313354 batch mAP 0.748046875 batch PCKh 0.2578125\n",
      "Trained batch 156 batch loss 3.42176938 batch mAP 0.8984375 batch PCKh 0.23046875\n",
      "Trained batch 157 batch loss 3.14532614 batch mAP 0.951171875 batch PCKh 0.162109375\n",
      "Trained batch 158 batch loss 3.23722696 batch mAP 0.83984375 batch PCKh 0.341796875\n",
      "Trained batch 159 batch loss 3.08550119 batch mAP 0.8515625 batch PCKh 0.166015625\n",
      "Trained batch 160 batch loss 3.29379439 batch mAP 0.865234375 batch PCKh 0.234375\n",
      "Trained batch 161 batch loss 3.3381958 batch mAP 0.849609375 batch PCKh 0.32421875\n",
      "Trained batch 162 batch loss 3.42225289 batch mAP 0.78515625 batch PCKh 0.1484375\n",
      "Trained batch 163 batch loss 3.55662 batch mAP 0.90234375 batch PCKh 0.244140625\n",
      "Trained batch 164 batch loss 3.50375915 batch mAP 0.859375 batch PCKh 0.154296875\n",
      "Trained batch 165 batch loss 3.42546797 batch mAP 0.85546875 batch PCKh 0.232421875\n",
      "Trained batch 166 batch loss 2.66618013 batch mAP 0.91796875 batch PCKh 0.25390625\n",
      "Trained batch 167 batch loss 3.20006657 batch mAP 0.875 batch PCKh 0.439453125\n",
      "Trained batch 168 batch loss 3.60888386 batch mAP 0.916015625 batch PCKh 0.30078125\n",
      "Trained batch 169 batch loss 3.20864081 batch mAP 0.83984375 batch PCKh 0.388671875\n",
      "Trained batch 170 batch loss 3.28924704 batch mAP 0.828125 batch PCKh 0.234375\n",
      "Trained batch 171 batch loss 3.32990146 batch mAP 0.828125 batch PCKh 0.2578125\n",
      "Trained batch 172 batch loss 3.4787693 batch mAP 0.763671875 batch PCKh 0.1953125\n",
      "Trained batch 173 batch loss 3.47555351 batch mAP 0.873046875 batch PCKh 0.26953125\n",
      "Trained batch 174 batch loss 3.52186966 batch mAP 0.759765625 batch PCKh 0.259765625\n",
      "Trained batch 175 batch loss 3.4887414 batch mAP 0.900390625 batch PCKh 0.265625\n",
      "Trained batch 176 batch loss 3.48575592 batch mAP 0.66015625 batch PCKh 0.248046875\n",
      "Trained batch 177 batch loss 3.45095205 batch mAP 0.734375 batch PCKh 0.216796875\n",
      "Trained batch 178 batch loss 3.62019587 batch mAP 0.84375 batch PCKh 0.205078125\n",
      "Trained batch 179 batch loss 3.46305323 batch mAP 0.818359375 batch PCKh 0.279296875\n",
      "Trained batch 180 batch loss 3.60705614 batch mAP 0.78515625 batch PCKh 0.125\n",
      "Trained batch 181 batch loss 3.52322435 batch mAP 0.771484375 batch PCKh 0.056640625\n",
      "Trained batch 182 batch loss 3.66711807 batch mAP 0.73828125 batch PCKh 0.119140625\n",
      "Trained batch 183 batch loss 3.55081439 batch mAP 0.677734375 batch PCKh 0.232421875\n",
      "Trained batch 184 batch loss 3.44883394 batch mAP 0.81640625 batch PCKh 0.248046875\n",
      "Trained batch 185 batch loss 3.56127429 batch mAP 0.8125 batch PCKh 0.279296875\n",
      "Trained batch 186 batch loss 3.27928185 batch mAP 0.880859375 batch PCKh 0.265625\n",
      "Trained batch 187 batch loss 3.41006207 batch mAP 0.87109375 batch PCKh 0.236328125\n",
      "Trained batch 188 batch loss 3.32313347 batch mAP 0.890625 batch PCKh 0.17578125\n",
      "Trained batch 189 batch loss 3.14293623 batch mAP 0.857421875 batch PCKh 0.171875\n",
      "Trained batch 190 batch loss 3.40220213 batch mAP 0.900390625 batch PCKh 0.349609375\n",
      "Trained batch 191 batch loss 3.44646502 batch mAP 0.75390625 batch PCKh 0.24609375\n",
      "Trained batch 192 batch loss 3.55850124 batch mAP 0.810546875 batch PCKh 0.240234375\n",
      "Trained batch 193 batch loss 3.50900555 batch mAP 0.669921875 batch PCKh 0.20703125\n",
      "Trained batch 194 batch loss 3.56784439 batch mAP 0.83203125 batch PCKh 0.234375\n",
      "Trained batch 195 batch loss 3.38177466 batch mAP 0.724609375 batch PCKh 0.40234375\n",
      "Trained batch 196 batch loss 3.39429808 batch mAP 0.7578125 batch PCKh 0.232421875\n",
      "Trained batch 197 batch loss 3.4595952 batch mAP 0.65625 batch PCKh 0.205078125\n",
      "Trained batch 198 batch loss 3.30715466 batch mAP 0.84375 batch PCKh 0.224609375\n",
      "Trained batch 199 batch loss 3.27375746 batch mAP 0.873046875 batch PCKh 0.21484375\n",
      "Trained batch 200 batch loss 3.48066759 batch mAP 0.66015625 batch PCKh 0.330078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 201 batch loss 3.48237729 batch mAP 0.798828125 batch PCKh 0.294921875\n",
      "Trained batch 202 batch loss 3.43884134 batch mAP 0.787109375 batch PCKh 0.25\n",
      "Trained batch 203 batch loss 3.31623721 batch mAP 0.771484375 batch PCKh 0.2578125\n",
      "Trained batch 204 batch loss 3.48128843 batch mAP 0.646484375 batch PCKh 0.287109375\n",
      "Trained batch 205 batch loss 3.53599358 batch mAP 0.658203125 batch PCKh 0.259765625\n",
      "Trained batch 206 batch loss 3.54750395 batch mAP 0.62109375 batch PCKh 0.30078125\n",
      "Trained batch 207 batch loss 3.68402433 batch mAP 0.587890625 batch PCKh 0.19140625\n",
      "Trained batch 208 batch loss 3.60266161 batch mAP 0.712890625 batch PCKh 0.1875\n",
      "Trained batch 209 batch loss 3.33690143 batch mAP 0.70703125 batch PCKh 0.4140625\n",
      "Trained batch 210 batch loss 3.56668329 batch mAP 0.740234375 batch PCKh 0.3125\n",
      "Trained batch 211 batch loss 3.47925138 batch mAP 0.796875 batch PCKh 0.4453125\n",
      "Trained batch 212 batch loss 3.10858 batch mAP 0.775390625 batch PCKh 0.41015625\n",
      "Trained batch 213 batch loss 3.22817564 batch mAP 0.828125 batch PCKh 0.45703125\n",
      "Trained batch 214 batch loss 3.63216209 batch mAP 0.865234375 batch PCKh 0.310546875\n",
      "Trained batch 215 batch loss 3.56171131 batch mAP 0.869140625 batch PCKh 0.259765625\n",
      "Trained batch 216 batch loss 3.69539309 batch mAP 0.822265625 batch PCKh 0.1875\n",
      "Trained batch 217 batch loss 3.1831336 batch mAP 0.810546875 batch PCKh 0.099609375\n",
      "Trained batch 218 batch loss 3.04398251 batch mAP 0.763671875 batch PCKh 0.03515625\n",
      "Trained batch 219 batch loss 3.0903542 batch mAP 0.861328125 batch PCKh 0.095703125\n",
      "Trained batch 220 batch loss 2.91418242 batch mAP 0.783203125 batch PCKh 0.080078125\n",
      "Trained batch 221 batch loss 3.18277144 batch mAP 0.84765625 batch PCKh 0.177734375\n",
      "Trained batch 222 batch loss 3.3962822 batch mAP 0.783203125 batch PCKh 0.2734375\n",
      "Trained batch 223 batch loss 3.62993932 batch mAP 0.748046875 batch PCKh 0.126953125\n",
      "Trained batch 224 batch loss 3.29789495 batch mAP 0.66015625 batch PCKh 0.205078125\n",
      "Trained batch 225 batch loss 3.34449434 batch mAP 0.794921875 batch PCKh 0.23046875\n",
      "Trained batch 226 batch loss 3.27880406 batch mAP 0.912109375 batch PCKh 0.1171875\n",
      "Trained batch 227 batch loss 3.24168921 batch mAP 0.869140625 batch PCKh 0.220703125\n",
      "Trained batch 228 batch loss 3.46226573 batch mAP 0.705078125 batch PCKh 0.3515625\n",
      "Trained batch 229 batch loss 3.39651537 batch mAP 0.8671875 batch PCKh 0.35546875\n",
      "Trained batch 230 batch loss 3.34847498 batch mAP 0.88671875 batch PCKh 0.248046875\n",
      "Trained batch 231 batch loss 3.43255377 batch mAP 0.833984375 batch PCKh 0.2109375\n",
      "Trained batch 232 batch loss 3.52363014 batch mAP 0.896484375 batch PCKh 0.166015625\n",
      "Trained batch 233 batch loss 3.40311575 batch mAP 0.87109375 batch PCKh 0.34375\n",
      "Trained batch 234 batch loss 3.48164582 batch mAP 0.814453125 batch PCKh 0.255859375\n",
      "Trained batch 235 batch loss 3.5766561 batch mAP 0.689453125 batch PCKh 0.234375\n",
      "Trained batch 236 batch loss 3.63607097 batch mAP 0.765625 batch PCKh 0.138671875\n",
      "Trained batch 237 batch loss 3.32510138 batch mAP 0.69921875 batch PCKh 0.1875\n",
      "Trained batch 238 batch loss 3.28155422 batch mAP 0.705078125 batch PCKh 0.20703125\n",
      "Trained batch 239 batch loss 3.41729712 batch mAP 0.783203125 batch PCKh 0.3359375\n",
      "Trained batch 240 batch loss 3.3175993 batch mAP 0.76953125 batch PCKh 0.130859375\n",
      "Trained batch 241 batch loss 3.32697749 batch mAP 0.662109375 batch PCKh 0.251953125\n",
      "Trained batch 242 batch loss 3.40329409 batch mAP 0.6640625 batch PCKh 0.228515625\n",
      "Trained batch 243 batch loss 3.32560062 batch mAP 0.486328125 batch PCKh 0.1953125\n",
      "Trained batch 244 batch loss 2.97184753 batch mAP 0.62109375 batch PCKh 0.193359375\n",
      "Trained batch 245 batch loss 3.34753823 batch mAP 0.65234375 batch PCKh 0.26171875\n",
      "Trained batch 246 batch loss 3.15079832 batch mAP 0.5234375 batch PCKh 0.265625\n",
      "Trained batch 247 batch loss 3.4060576 batch mAP 0.72265625 batch PCKh 0.259765625\n",
      "Trained batch 248 batch loss 3.35343671 batch mAP 0.65234375 batch PCKh 0.060546875\n",
      "Trained batch 249 batch loss 3.48444271 batch mAP 0.728515625 batch PCKh 0.302734375\n",
      "Trained batch 250 batch loss 3.62501383 batch mAP 0.66796875 batch PCKh 0.072265625\n",
      "Trained batch 251 batch loss 3.3597331 batch mAP 0.654296875 batch PCKh 0.45703125\n",
      "Trained batch 252 batch loss 3.32954764 batch mAP 0.650390625 batch PCKh 0.36328125\n",
      "Trained batch 253 batch loss 3.00134611 batch mAP 0.6640625 batch PCKh 0.189453125\n",
      "Trained batch 254 batch loss 3.10139704 batch mAP 0.607421875 batch PCKh 0.46484375\n",
      "Trained batch 255 batch loss 3.22844076 batch mAP 0.697265625 batch PCKh 0.423828125\n",
      "Trained batch 256 batch loss 3.01793957 batch mAP 0.7265625 batch PCKh 0.158203125\n",
      "Trained batch 257 batch loss 3.32883954 batch mAP 0.794921875 batch PCKh 0.2421875\n",
      "Trained batch 258 batch loss 3.05168247 batch mAP 0.7265625 batch PCKh 0.388671875\n",
      "Trained batch 259 batch loss 3.06465816 batch mAP 0.634765625 batch PCKh 0.447265625\n",
      "Trained batch 260 batch loss 3.37737656 batch mAP 0.736328125 batch PCKh 0.09765625\n",
      "Trained batch 261 batch loss 3.27271581 batch mAP 0.619140625 batch PCKh 0.421875\n",
      "Trained batch 262 batch loss 3.45195818 batch mAP 0.611328125 batch PCKh 0.3359375\n",
      "Trained batch 263 batch loss 3.38377237 batch mAP 0.642578125 batch PCKh 0.294921875\n",
      "Trained batch 264 batch loss 3.32712746 batch mAP 0.806640625 batch PCKh 0.2890625\n",
      "Trained batch 265 batch loss 3.46935654 batch mAP 0.775390625 batch PCKh 0.23828125\n",
      "Trained batch 266 batch loss 3.08103132 batch mAP 0.7734375 batch PCKh 0.2109375\n",
      "Trained batch 267 batch loss 3.34728527 batch mAP 0.7734375 batch PCKh 0.400390625\n",
      "Trained batch 268 batch loss 3.39838815 batch mAP 0.765625 batch PCKh 0.40234375\n",
      "Trained batch 269 batch loss 3.31225777 batch mAP 0.732421875 batch PCKh 0.302734375\n",
      "Trained batch 270 batch loss 3.21273851 batch mAP 0.6796875 batch PCKh 0.318359375\n",
      "Trained batch 271 batch loss 3.38175154 batch mAP 0.681640625 batch PCKh 0.373046875\n",
      "Trained batch 272 batch loss 3.14123058 batch mAP 0.67578125 batch PCKh 0.275390625\n",
      "Trained batch 273 batch loss 3.32765865 batch mAP 0.681640625 batch PCKh 0.15234375\n",
      "Trained batch 274 batch loss 3.29559374 batch mAP 0.654296875 batch PCKh 0.22265625\n",
      "Trained batch 275 batch loss 3.16103387 batch mAP 0.74609375 batch PCKh 0.2421875\n",
      "Trained batch 276 batch loss 2.97546053 batch mAP 0.74609375 batch PCKh 0.044921875\n",
      "Trained batch 277 batch loss 2.53976059 batch mAP 0.724609375 batch PCKh 0.203125\n",
      "Trained batch 278 batch loss 2.81238 batch mAP 0.615234375 batch PCKh 0.185546875\n",
      "Trained batch 279 batch loss 2.6088109 batch mAP 0.736328125 batch PCKh 0.091796875\n",
      "Trained batch 280 batch loss 2.41362309 batch mAP 0.66015625 batch PCKh 0.001953125\n",
      "Trained batch 281 batch loss 2.53802514 batch mAP 0.654296875 batch PCKh 0.029296875\n",
      "Trained batch 282 batch loss 2.85942507 batch mAP 0.701171875 batch PCKh 0.091796875\n",
      "Trained batch 283 batch loss 3.55484295 batch mAP 0.66796875 batch PCKh 0.224609375\n",
      "Trained batch 284 batch loss 3.45255566 batch mAP 0.689453125 batch PCKh 0.20703125\n",
      "Trained batch 285 batch loss 3.51311445 batch mAP 0.67578125 batch PCKh 0.26953125\n",
      "Trained batch 286 batch loss 3.52174187 batch mAP 0.642578125 batch PCKh 0.189453125\n",
      "Trained batch 287 batch loss 3.39120817 batch mAP 0.642578125 batch PCKh 0.154296875\n",
      "Trained batch 288 batch loss 3.63737535 batch mAP 0.59765625 batch PCKh 0.32421875\n",
      "Trained batch 289 batch loss 2.87270784 batch mAP 0.71875 batch PCKh 0.294921875\n",
      "Trained batch 290 batch loss 3.36738253 batch mAP 0.6796875 batch PCKh 0.404296875\n",
      "Trained batch 291 batch loss 3.19376087 batch mAP 0.677734375 batch PCKh 0.365234375\n",
      "Trained batch 292 batch loss 3.42453074 batch mAP 0.720703125 batch PCKh 0.345703125\n",
      "Trained batch 293 batch loss 3.19617963 batch mAP 0.681640625 batch PCKh 0.1796875\n",
      "Trained batch 294 batch loss 3.12858129 batch mAP 0.765625 batch PCKh 0.244140625\n",
      "Trained batch 295 batch loss 2.91104078 batch mAP 0.759765625 batch PCKh 0.04296875\n",
      "Trained batch 296 batch loss 2.93555331 batch mAP 0.744140625 batch PCKh 0.1796875\n",
      "Trained batch 297 batch loss 3.13187122 batch mAP 0.73046875 batch PCKh 0.2109375\n",
      "Trained batch 298 batch loss 3.12689877 batch mAP 0.73828125 batch PCKh 0.388671875\n",
      "Trained batch 299 batch loss 3.18419409 batch mAP 0.728515625 batch PCKh 0.32421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 300 batch loss 3.10391331 batch mAP 0.775390625 batch PCKh 0.0703125\n",
      "Trained batch 301 batch loss 3.02259159 batch mAP 0.70703125 batch PCKh 0.162109375\n",
      "Trained batch 302 batch loss 3.32822514 batch mAP 0.70703125 batch PCKh 0.263671875\n",
      "Trained batch 303 batch loss 2.98173094 batch mAP 0.671875 batch PCKh 0.40625\n",
      "Trained batch 304 batch loss 3.3049264 batch mAP 0.69921875 batch PCKh 0.21484375\n",
      "Trained batch 305 batch loss 3.38961029 batch mAP 0.705078125 batch PCKh 0.34765625\n",
      "Trained batch 306 batch loss 3.28078413 batch mAP 0.640625 batch PCKh 0.158203125\n",
      "Trained batch 307 batch loss 3.05787945 batch mAP 0.666015625 batch PCKh 0.185546875\n",
      "Trained batch 308 batch loss 2.85110235 batch mAP 0.619140625 batch PCKh 0.3125\n",
      "Trained batch 309 batch loss 3.04820108 batch mAP 0.66015625 batch PCKh 0.11328125\n",
      "Trained batch 310 batch loss 3.26289916 batch mAP 0.642578125 batch PCKh 0.109375\n",
      "Trained batch 311 batch loss 3.33559418 batch mAP 0.65234375 batch PCKh 0.189453125\n",
      "Trained batch 312 batch loss 3.21153259 batch mAP 0.62109375 batch PCKh 0.2734375\n",
      "Trained batch 313 batch loss 3.08058786 batch mAP 0.55859375 batch PCKh 0.205078125\n",
      "Trained batch 314 batch loss 2.83562684 batch mAP 0.6015625 batch PCKh 0.1484375\n",
      "Trained batch 315 batch loss 2.8512845 batch mAP 0.671875 batch PCKh 0.337890625\n",
      "Trained batch 316 batch loss 2.91622686 batch mAP 0.70703125 batch PCKh 0.3359375\n",
      "Trained batch 317 batch loss 3.08486342 batch mAP 0.740234375 batch PCKh 0.763671875\n",
      "Trained batch 318 batch loss 2.65773201 batch mAP 0.62109375 batch PCKh 0.3203125\n",
      "Trained batch 319 batch loss 3.14991832 batch mAP 0.654296875 batch PCKh 0.455078125\n",
      "Trained batch 320 batch loss 2.79037952 batch mAP 0.66796875 batch PCKh 0.431640625\n",
      "Trained batch 321 batch loss 3.12631321 batch mAP 0.712890625 batch PCKh 0.498046875\n",
      "Trained batch 322 batch loss 3.21419406 batch mAP 0.677734375 batch PCKh 0.4921875\n",
      "Trained batch 323 batch loss 3.56089926 batch mAP 0.55859375 batch PCKh 0.26171875\n",
      "Trained batch 324 batch loss 3.38515449 batch mAP 0.646484375 batch PCKh 0.34375\n",
      "Trained batch 325 batch loss 3.34317422 batch mAP 0.4765625 batch PCKh 0.39453125\n",
      "Trained batch 326 batch loss 3.32113647 batch mAP 0.5859375 batch PCKh 0.318359375\n",
      "Trained batch 327 batch loss 3.37397361 batch mAP 0.669921875 batch PCKh 0.310546875\n",
      "Trained batch 328 batch loss 3.22728753 batch mAP 0.61328125 batch PCKh 0.208984375\n",
      "Trained batch 329 batch loss 3.07597446 batch mAP 0.67578125 batch PCKh 0.52734375\n",
      "Trained batch 330 batch loss 2.86332 batch mAP 0.67578125 batch PCKh 0.24609375\n",
      "Trained batch 331 batch loss 2.90010405 batch mAP 0.669921875 batch PCKh 0.169921875\n",
      "Trained batch 332 batch loss 3.12410975 batch mAP 0.638671875 batch PCKh 0.578125\n",
      "Trained batch 333 batch loss 3.25377846 batch mAP 0.640625 batch PCKh 0.1953125\n",
      "Trained batch 334 batch loss 3.44399738 batch mAP 0.646484375 batch PCKh 0.4609375\n",
      "Trained batch 335 batch loss 3.34820676 batch mAP 0.66015625 batch PCKh 0.388671875\n",
      "Trained batch 336 batch loss 3.16048 batch mAP 0.6640625 batch PCKh 0.30859375\n",
      "Trained batch 337 batch loss 3.11765575 batch mAP 0.62109375 batch PCKh 0.380859375\n",
      "Trained batch 338 batch loss 3.37610674 batch mAP 0.53515625 batch PCKh 0.365234375\n",
      "Trained batch 339 batch loss 3.1525948 batch mAP 0.54296875 batch PCKh 0.14453125\n",
      "Trained batch 340 batch loss 3.1890831 batch mAP 0.6171875 batch PCKh 0.33984375\n",
      "Trained batch 341 batch loss 3.21024776 batch mAP 0.6015625 batch PCKh 0.498046875\n",
      "Trained batch 342 batch loss 3.20921707 batch mAP 0.537109375 batch PCKh 0.392578125\n",
      "Trained batch 343 batch loss 3.26444674 batch mAP 0.626953125 batch PCKh 0.423828125\n",
      "Trained batch 344 batch loss 3.18390155 batch mAP 0.48046875 batch PCKh 0.55078125\n",
      "Trained batch 345 batch loss 3.24683285 batch mAP 0.57421875 batch PCKh 0.44140625\n",
      "Trained batch 346 batch loss 3.31608224 batch mAP 0.529296875 batch PCKh 0.349609375\n",
      "Trained batch 347 batch loss 3.38077211 batch mAP 0.595703125 batch PCKh 0.37890625\n",
      "Trained batch 348 batch loss 2.99884129 batch mAP 0.5078125 batch PCKh 0.333984375\n",
      "Trained batch 349 batch loss 3.17753315 batch mAP 0.470703125 batch PCKh 0.34765625\n",
      "Trained batch 350 batch loss 3.17905855 batch mAP 0.408203125 batch PCKh 0.296875\n",
      "Trained batch 351 batch loss 3.08865237 batch mAP 0.552734375 batch PCKh 0.28125\n",
      "Trained batch 352 batch loss 2.95906353 batch mAP 0.552734375 batch PCKh 0.234375\n",
      "Trained batch 353 batch loss 3.17450333 batch mAP 0.525390625 batch PCKh 0.3203125\n",
      "Trained batch 354 batch loss 3.23991871 batch mAP 0.419921875 batch PCKh 0.373046875\n",
      "Trained batch 355 batch loss 3.44114351 batch mAP 0.478515625 batch PCKh 0.28515625\n",
      "Trained batch 356 batch loss 2.84816384 batch mAP 0.60546875 batch PCKh 0.4609375\n",
      "Trained batch 357 batch loss 3.19740391 batch mAP 0.515625 batch PCKh 0.439453125\n",
      "Trained batch 358 batch loss 2.97520161 batch mAP 0.4765625 batch PCKh 0.34765625\n",
      "Trained batch 359 batch loss 3.08356953 batch mAP 0.591796875 batch PCKh 0.375\n",
      "Trained batch 360 batch loss 3.12526226 batch mAP 0.59375 batch PCKh 0.3359375\n",
      "Trained batch 361 batch loss 3.13242722 batch mAP 0.48046875 batch PCKh 0.31640625\n",
      "Trained batch 362 batch loss 2.99940372 batch mAP 0.578125 batch PCKh 0.48046875\n",
      "Trained batch 363 batch loss 3.30538416 batch mAP 0.541015625 batch PCKh 0.41796875\n",
      "Trained batch 364 batch loss 3.2297473 batch mAP 0.5625 batch PCKh 0.4140625\n",
      "Trained batch 365 batch loss 3.08812046 batch mAP 0.603515625 batch PCKh 0.6328125\n",
      "Trained batch 366 batch loss 3.03690767 batch mAP 0.615234375 batch PCKh 0.291015625\n",
      "Trained batch 367 batch loss 3.49799013 batch mAP 0.640625 batch PCKh 0.234375\n",
      "Trained batch 368 batch loss 3.37801647 batch mAP 0.669921875 batch PCKh 0.189453125\n",
      "Trained batch 369 batch loss 3.20498919 batch mAP 0.56640625 batch PCKh 0.33203125\n",
      "Trained batch 370 batch loss 3.26953506 batch mAP 0.609375 batch PCKh 0.396484375\n",
      "Trained batch 371 batch loss 2.99038076 batch mAP 0.64453125 batch PCKh 0.17578125\n",
      "Trained batch 372 batch loss 2.94089246 batch mAP 0.71875 batch PCKh 0.3046875\n",
      "Trained batch 373 batch loss 3.22115541 batch mAP 0.625 batch PCKh 0.220703125\n",
      "Trained batch 374 batch loss 3.45999956 batch mAP 0.5234375 batch PCKh 0.35546875\n",
      "Trained batch 375 batch loss 3.20581198 batch mAP 0.525390625 batch PCKh 0.30859375\n",
      "Trained batch 376 batch loss 3.34282494 batch mAP 0.62890625 batch PCKh 0.42578125\n",
      "Trained batch 377 batch loss 3.40678859 batch mAP 0.49609375 batch PCKh 0.396484375\n",
      "Trained batch 378 batch loss 3.31137085 batch mAP 0.650390625 batch PCKh 0.4296875\n",
      "Trained batch 379 batch loss 3.33437037 batch mAP 0.541015625 batch PCKh 0.423828125\n",
      "Trained batch 380 batch loss 3.35989761 batch mAP 0.568359375 batch PCKh 0.357421875\n",
      "Trained batch 381 batch loss 3.3083365 batch mAP 0.5078125 batch PCKh 0.408203125\n",
      "Trained batch 382 batch loss 3.33771014 batch mAP 0.56640625 batch PCKh 0.376953125\n",
      "Trained batch 383 batch loss 2.93070102 batch mAP 0.619140625 batch PCKh 0.123046875\n",
      "Trained batch 384 batch loss 3.00599861 batch mAP 0.53125 batch PCKh 0.40625\n",
      "Trained batch 385 batch loss 3.26643372 batch mAP 0.572265625 batch PCKh 0.38671875\n",
      "Trained batch 386 batch loss 3.11526299 batch mAP 0.578125 batch PCKh 0.412109375\n",
      "Trained batch 387 batch loss 3.22416306 batch mAP 0.466796875 batch PCKh 0.275390625\n",
      "Trained batch 388 batch loss 3.16374826 batch mAP 0.505859375 batch PCKh 0.431640625\n",
      "Trained batch 389 batch loss 3.23933363 batch mAP 0.509765625 batch PCKh 0.3359375\n",
      "Trained batch 390 batch loss 2.66880202 batch mAP 0.41015625 batch PCKh 0.0078125\n",
      "Trained batch 391 batch loss 3.0779264 batch mAP 0.462890625 batch PCKh 0.251953125\n",
      "Trained batch 392 batch loss 3.298877 batch mAP 0.4453125 batch PCKh 0.384765625\n",
      "Trained batch 393 batch loss 3.27020979 batch mAP 0.453125 batch PCKh 0.46484375\n",
      "Trained batch 394 batch loss 3.14572573 batch mAP 0.333984375 batch PCKh 0.376953125\n",
      "Trained batch 395 batch loss 3.29290962 batch mAP 0.501953125 batch PCKh 0.345703125\n",
      "Trained batch 396 batch loss 3.3144033 batch mAP 0.474609375 batch PCKh 0.3984375\n",
      "Trained batch 397 batch loss 2.86113405 batch mAP 0.546875 batch PCKh 0.166015625\n",
      "Trained batch 398 batch loss 2.84280801 batch mAP 0.484375 batch PCKh 0.23828125\n",
      "Trained batch 399 batch loss 2.70288372 batch mAP 0.53125 batch PCKh 0.30078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 400 batch loss 2.67718196 batch mAP 0.48828125 batch PCKh 0.064453125\n",
      "Trained batch 401 batch loss 2.95400047 batch mAP 0.5 batch PCKh 0.61328125\n",
      "Trained batch 402 batch loss 3.17362642 batch mAP 0.5078125 batch PCKh 0.451171875\n",
      "Trained batch 403 batch loss 3.10206747 batch mAP 0.48828125 batch PCKh 0.466796875\n",
      "Trained batch 404 batch loss 3.18290329 batch mAP 0.4296875 batch PCKh 0.232421875\n",
      "Trained batch 405 batch loss 2.99012685 batch mAP 0.455078125 batch PCKh 0.181640625\n",
      "Trained batch 406 batch loss 3.43612981 batch mAP 0.455078125 batch PCKh 0.17578125\n",
      "Trained batch 407 batch loss 3.36605763 batch mAP 0.5 batch PCKh 0.384765625\n",
      "Trained batch 408 batch loss 3.18544102 batch mAP 0.462890625 batch PCKh 0.384765625\n",
      "Trained batch 409 batch loss 3.33027244 batch mAP 0.5234375 batch PCKh 0.208984375\n",
      "Trained batch 410 batch loss 3.44693017 batch mAP 0.53515625 batch PCKh 0.248046875\n",
      "Trained batch 411 batch loss 3.08433485 batch mAP 0.580078125 batch PCKh 0.427734375\n",
      "Trained batch 412 batch loss 3.28344703 batch mAP 0.564453125 batch PCKh 0.37109375\n",
      "Trained batch 413 batch loss 3.29399204 batch mAP 0.623046875 batch PCKh 0.349609375\n",
      "Trained batch 414 batch loss 3.42940712 batch mAP 0.59375 batch PCKh 0.291015625\n",
      "Trained batch 415 batch loss 3.30429506 batch mAP 0.623046875 batch PCKh 0.201171875\n",
      "Trained batch 416 batch loss 3.2693119 batch mAP 0.66015625 batch PCKh 0.486328125\n",
      "Trained batch 417 batch loss 3.3800683 batch mAP 0.669921875 batch PCKh 0.28125\n",
      "Trained batch 418 batch loss 3.3382256 batch mAP 0.640625 batch PCKh 0.23046875\n",
      "Trained batch 419 batch loss 3.10728359 batch mAP 0.55078125 batch PCKh 0.48046875\n",
      "Trained batch 420 batch loss 3.16335487 batch mAP 0.70703125 batch PCKh 0.33984375\n",
      "Trained batch 421 batch loss 3.30397701 batch mAP 0.6015625 batch PCKh 0.287109375\n",
      "Trained batch 422 batch loss 3.24992323 batch mAP 0.44140625 batch PCKh 0.3203125\n",
      "Trained batch 423 batch loss 3.21459031 batch mAP 0.509765625 batch PCKh 0.3046875\n",
      "Trained batch 424 batch loss 3.14665937 batch mAP 0.5859375 batch PCKh 0.251953125\n",
      "Trained batch 425 batch loss 3.16439128 batch mAP 0.654296875 batch PCKh 0.291015625\n",
      "Trained batch 426 batch loss 2.98798656 batch mAP 0.466796875 batch PCKh 0.328125\n",
      "Trained batch 427 batch loss 2.97638178 batch mAP 0.501953125 batch PCKh 0.365234375\n",
      "Trained batch 428 batch loss 3.19330144 batch mAP 0.556640625 batch PCKh 0.4375\n",
      "Trained batch 429 batch loss 3.12996078 batch mAP 0.568359375 batch PCKh 0.3125\n",
      "Trained batch 430 batch loss 3.22927499 batch mAP 0.5 batch PCKh 0.412109375\n",
      "Trained batch 431 batch loss 2.95833635 batch mAP 0.478515625 batch PCKh 0.27734375\n",
      "Trained batch 432 batch loss 2.9546237 batch mAP 0.515625 batch PCKh 0.23046875\n",
      "Trained batch 433 batch loss 3.1576395 batch mAP 0.546875 batch PCKh 0.396484375\n",
      "Trained batch 434 batch loss 2.88897657 batch mAP 0.5078125 batch PCKh 0.0625\n",
      "Trained batch 435 batch loss 3.10961246 batch mAP 0.447265625 batch PCKh 0.431640625\n",
      "Trained batch 436 batch loss 3.28510237 batch mAP 0.57421875 batch PCKh 0.248046875\n",
      "Trained batch 437 batch loss 3.13043833 batch mAP 0.462890625 batch PCKh 0.42578125\n",
      "Trained batch 438 batch loss 3.34068751 batch mAP 0.640625 batch PCKh 0.341796875\n",
      "Trained batch 439 batch loss 3.27896714 batch mAP 0.439453125 batch PCKh 0.421875\n",
      "Trained batch 440 batch loss 3.39848709 batch mAP 0.501953125 batch PCKh 0.337890625\n",
      "Trained batch 441 batch loss 2.96131 batch mAP 0.662109375 batch PCKh 0.3359375\n",
      "Trained batch 442 batch loss 3.08294344 batch mAP 0.4609375 batch PCKh 0.408203125\n",
      "Trained batch 443 batch loss 3.07365608 batch mAP 0.484375 batch PCKh 0.171875\n",
      "Trained batch 444 batch loss 2.7919 batch mAP 0.599609375 batch PCKh 0.142578125\n",
      "Trained batch 445 batch loss 2.62578797 batch mAP 0.61328125 batch PCKh 0.115234375\n",
      "Trained batch 446 batch loss 2.74747 batch mAP 0.56640625 batch PCKh 0.21875\n",
      "Trained batch 447 batch loss 3.02454805 batch mAP 0.548828125 batch PCKh 0.37890625\n",
      "Trained batch 448 batch loss 3.32292843 batch mAP 0.51953125 batch PCKh 0.48828125\n",
      "Trained batch 449 batch loss 3.44243336 batch mAP 0.517578125 batch PCKh 0.44140625\n",
      "Trained batch 450 batch loss 3.43001986 batch mAP 0.421875 batch PCKh 0.373046875\n",
      "Trained batch 451 batch loss 3.41349387 batch mAP 0.50390625 batch PCKh 0.328125\n",
      "Trained batch 452 batch loss 3.27363873 batch mAP 0.390625 batch PCKh 0.490234375\n",
      "Trained batch 453 batch loss 3.31114626 batch mAP 0.49609375 batch PCKh 0.302734375\n",
      "Trained batch 454 batch loss 3.592664 batch mAP 0.423828125 batch PCKh 0.267578125\n",
      "Trained batch 455 batch loss 3.29756498 batch mAP 0.46875 batch PCKh 0.4375\n",
      "Trained batch 456 batch loss 3.3915658 batch mAP 0.541015625 batch PCKh 0.26171875\n",
      "Trained batch 457 batch loss 3.48034692 batch mAP 0.53515625 batch PCKh 0.318359375\n",
      "Trained batch 458 batch loss 3.41204929 batch mAP 0.509765625 batch PCKh 0.267578125\n",
      "Trained batch 459 batch loss 3.3106246 batch mAP 0.501953125 batch PCKh 0.373046875\n",
      "Trained batch 460 batch loss 3.36510515 batch mAP 0.5 batch PCKh 0.3046875\n",
      "Trained batch 461 batch loss 3.27696681 batch mAP 0.51953125 batch PCKh 0.439453125\n",
      "Trained batch 462 batch loss 3.32988906 batch mAP 0.470703125 batch PCKh 0.3671875\n",
      "Trained batch 463 batch loss 3.35291481 batch mAP 0.533203125 batch PCKh 0.380859375\n",
      "Trained batch 464 batch loss 3.07245946 batch mAP 0.544921875 batch PCKh 0.431640625\n",
      "Trained batch 465 batch loss 3.23920536 batch mAP 0.564453125 batch PCKh 0.49609375\n",
      "Trained batch 466 batch loss 3.25719213 batch mAP 0.42578125 batch PCKh 0.287109375\n",
      "Trained batch 467 batch loss 3.37712836 batch mAP 0.5703125 batch PCKh 0.162109375\n",
      "Trained batch 468 batch loss 3.43771744 batch mAP 0.498046875 batch PCKh 0.345703125\n",
      "Trained batch 469 batch loss 3.10834169 batch mAP 0.439453125 batch PCKh 0.51953125\n",
      "Trained batch 470 batch loss 3.19479823 batch mAP 0.486328125 batch PCKh 0.271484375\n",
      "Trained batch 471 batch loss 3.35857511 batch mAP 0.490234375 batch PCKh 0.390625\n",
      "Trained batch 472 batch loss 3.41618347 batch mAP 0.537109375 batch PCKh 0.42578125\n",
      "Trained batch 473 batch loss 3.26437664 batch mAP 0.580078125 batch PCKh 0.30859375\n",
      "Trained batch 474 batch loss 3.28781557 batch mAP 0.421875 batch PCKh 0.3984375\n",
      "Trained batch 475 batch loss 2.98769045 batch mAP 0.54296875 batch PCKh 0.255859375\n",
      "Trained batch 476 batch loss 3.36313295 batch mAP 0.57421875 batch PCKh 0.302734375\n",
      "Trained batch 477 batch loss 3.04352975 batch mAP 0.455078125 batch PCKh 0.25390625\n",
      "Trained batch 478 batch loss 3.32875371 batch mAP 0.48828125 batch PCKh 0.3515625\n",
      "Trained batch 479 batch loss 3.30533814 batch mAP 0.4296875 batch PCKh 0.275390625\n",
      "Trained batch 480 batch loss 3.5627563 batch mAP 0.38671875 batch PCKh 0.19140625\n",
      "Trained batch 481 batch loss 3.61792588 batch mAP 0.4609375 batch PCKh 0.189453125\n",
      "Trained batch 482 batch loss 3.43606615 batch mAP 0.609375 batch PCKh 0.29296875\n",
      "Trained batch 483 batch loss 3.28339624 batch mAP 0.451171875 batch PCKh 0.43359375\n",
      "Trained batch 484 batch loss 3.41677618 batch mAP 0.5078125 batch PCKh 0.30859375\n",
      "Trained batch 485 batch loss 3.09530926 batch mAP 0.552734375 batch PCKh 0.283203125\n",
      "Trained batch 486 batch loss 2.73107195 batch mAP 0.529296875 batch PCKh 0.02734375\n",
      "Trained batch 487 batch loss 3.37318897 batch mAP 0.474609375 batch PCKh 0.31640625\n",
      "Trained batch 488 batch loss 3.06856966 batch mAP 0.44140625 batch PCKh 0.4609375\n",
      "Trained batch 489 batch loss 3.05399132 batch mAP 0.4296875 batch PCKh 0.326171875\n",
      "Trained batch 490 batch loss 3.14641571 batch mAP 0.46875 batch PCKh 0.513671875\n",
      "Trained batch 491 batch loss 3.2984004 batch mAP 0.533203125 batch PCKh 0.365234375\n",
      "Trained batch 492 batch loss 3.14963484 batch mAP 0.400390625 batch PCKh 0.34375\n",
      "Trained batch 493 batch loss 3.24902 batch mAP 0.412109375 batch PCKh 0.498046875\n",
      "Trained batch 494 batch loss 3.23181891 batch mAP 0.53125 batch PCKh 0.41015625\n",
      "Trained batch 495 batch loss 3.20936012 batch mAP 0.44921875 batch PCKh 0.36328125\n",
      "Trained batch 496 batch loss 3.31272984 batch mAP 0.462890625 batch PCKh 0.330078125\n",
      "Trained batch 497 batch loss 3.3287282 batch mAP 0.525390625 batch PCKh 0.400390625\n",
      "Trained batch 498 batch loss 3.33336115 batch mAP 0.4375 batch PCKh 0.26953125\n",
      "Trained batch 499 batch loss 2.81822205 batch mAP 0.44921875 batch PCKh 0.08984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 500 batch loss 3.06452703 batch mAP 0.455078125 batch PCKh 0.34765625\n",
      "Trained batch 501 batch loss 3.05973721 batch mAP 0.4765625 batch PCKh 0.349609375\n",
      "Trained batch 502 batch loss 3.24196863 batch mAP 0.552734375 batch PCKh 0.4140625\n",
      "Trained batch 503 batch loss 3.25462294 batch mAP 0.470703125 batch PCKh 0.283203125\n",
      "Trained batch 504 batch loss 3.33473301 batch mAP 0.46875 batch PCKh 0.37109375\n",
      "Trained batch 505 batch loss 3.39186239 batch mAP 0.50390625 batch PCKh 0.29296875\n",
      "Trained batch 506 batch loss 3.28611541 batch mAP 0.482421875 batch PCKh 0.34375\n",
      "Trained batch 507 batch loss 3.2916131 batch mAP 0.48046875 batch PCKh 0.462890625\n",
      "Trained batch 508 batch loss 3.30429053 batch mAP 0.451171875 batch PCKh 0.38671875\n",
      "Trained batch 509 batch loss 3.28151369 batch mAP 0.498046875 batch PCKh 0.4296875\n",
      "Trained batch 510 batch loss 3.24977827 batch mAP 0.48828125 batch PCKh 0.486328125\n",
      "Trained batch 511 batch loss 3.21575332 batch mAP 0.330078125 batch PCKh 0.4296875\n",
      "Trained batch 512 batch loss 3.25360894 batch mAP 0.400390625 batch PCKh 0.466796875\n",
      "Trained batch 513 batch loss 3.09557676 batch mAP 0.41796875 batch PCKh 0.373046875\n",
      "Trained batch 514 batch loss 3.14684105 batch mAP 0.4453125 batch PCKh 0.322265625\n",
      "Trained batch 515 batch loss 3.15957165 batch mAP 0.451171875 batch PCKh 0.46875\n",
      "Trained batch 516 batch loss 3.15510368 batch mAP 0.416015625 batch PCKh 0.505859375\n",
      "Trained batch 517 batch loss 3.12416911 batch mAP 0.37890625 batch PCKh 0.474609375\n",
      "Trained batch 518 batch loss 3.17838573 batch mAP 0.443359375 batch PCKh 0.54296875\n",
      "Trained batch 519 batch loss 3.42547441 batch mAP 0.419921875 batch PCKh 0.474609375\n",
      "Trained batch 520 batch loss 3.19504642 batch mAP 0.34765625 batch PCKh 0.517578125\n",
      "Trained batch 521 batch loss 3.26453376 batch mAP 0.353515625 batch PCKh 0.330078125\n",
      "Trained batch 522 batch loss 3.28226471 batch mAP 0.37890625 batch PCKh 0.283203125\n",
      "Trained batch 523 batch loss 3.212497 batch mAP 0.388671875 batch PCKh 0.419921875\n",
      "Trained batch 524 batch loss 3.31362987 batch mAP 0.384765625 batch PCKh 0.171875\n",
      "Trained batch 525 batch loss 3.16042089 batch mAP 0.40625 batch PCKh 0.259765625\n",
      "Trained batch 526 batch loss 3.35527802 batch mAP 0.408203125 batch PCKh 0.349609375\n",
      "Trained batch 527 batch loss 3.09599757 batch mAP 0.416015625 batch PCKh 0.361328125\n",
      "Trained batch 528 batch loss 3.17422104 batch mAP 0.44921875 batch PCKh 0.3828125\n",
      "Trained batch 529 batch loss 3.1208024 batch mAP 0.375 batch PCKh 0.380859375\n",
      "Trained batch 530 batch loss 3.35210562 batch mAP 0.3125 batch PCKh 0.279296875\n",
      "Trained batch 531 batch loss 3.21448612 batch mAP 0.3984375 batch PCKh 0.310546875\n",
      "Trained batch 532 batch loss 3.0753417 batch mAP 0.2734375 batch PCKh 0.404296875\n",
      "Trained batch 533 batch loss 3.25485706 batch mAP 0.380859375 batch PCKh 0.427734375\n",
      "Trained batch 534 batch loss 3.24044776 batch mAP 0.4296875 batch PCKh 0.416015625\n",
      "Trained batch 535 batch loss 3.07489467 batch mAP 0.4375 batch PCKh 0.591796875\n",
      "Trained batch 536 batch loss 3.0255065 batch mAP 0.501953125 batch PCKh 0.515625\n",
      "Trained batch 537 batch loss 3.10294104 batch mAP 0.466796875 batch PCKh 0.390625\n",
      "Trained batch 538 batch loss 3.18082 batch mAP 0.427734375 batch PCKh 0.34765625\n",
      "Trained batch 539 batch loss 3.16444302 batch mAP 0.3828125 batch PCKh 0.478515625\n",
      "Trained batch 540 batch loss 3.1679492 batch mAP 0.3359375 batch PCKh 0.369140625\n",
      "Trained batch 541 batch loss 3.24943662 batch mAP 0.361328125 batch PCKh 0.419921875\n",
      "Trained batch 542 batch loss 3.21793342 batch mAP 0.38671875 batch PCKh 0.470703125\n",
      "Trained batch 543 batch loss 3.23398542 batch mAP 0.376953125 batch PCKh 0.3359375\n",
      "Trained batch 544 batch loss 2.89893055 batch mAP 0.3984375 batch PCKh 0.21484375\n",
      "Trained batch 545 batch loss 3.05370402 batch mAP 0.5 batch PCKh 0.419921875\n",
      "Trained batch 546 batch loss 2.92652941 batch mAP 0.38671875 batch PCKh 0.25390625\n",
      "Trained batch 547 batch loss 3.1752367 batch mAP 0.3359375 batch PCKh 0.37890625\n",
      "Trained batch 548 batch loss 3.16268396 batch mAP 0.40234375 batch PCKh 0.4453125\n",
      "Trained batch 549 batch loss 3.41269112 batch mAP 0.328125 batch PCKh 0.26171875\n",
      "Trained batch 550 batch loss 3.26931691 batch mAP 0.326171875 batch PCKh 0.388671875\n",
      "Trained batch 551 batch loss 3.41935968 batch mAP 0.380859375 batch PCKh 0.26171875\n",
      "Trained batch 552 batch loss 3.31911135 batch mAP 0.32421875 batch PCKh 0.404296875\n",
      "Trained batch 553 batch loss 3.53897572 batch mAP 0.38671875 batch PCKh 0.1796875\n",
      "Trained batch 554 batch loss 3.31347871 batch mAP 0.39453125 batch PCKh 0.197265625\n",
      "Trained batch 555 batch loss 3.27885294 batch mAP 0.408203125 batch PCKh 0.173828125\n",
      "Trained batch 556 batch loss 3.30499506 batch mAP 0.46875 batch PCKh 0.318359375\n",
      "Trained batch 557 batch loss 3.19798 batch mAP 0.5078125 batch PCKh 0.14453125\n",
      "Trained batch 558 batch loss 3.36805511 batch mAP 0.529296875 batch PCKh 0.416015625\n",
      "Trained batch 559 batch loss 3.01190042 batch mAP 0.5078125 batch PCKh 0.142578125\n",
      "Trained batch 560 batch loss 3.2447927 batch mAP 0.490234375 batch PCKh 0.4921875\n",
      "Trained batch 561 batch loss 3.26298046 batch mAP 0.447265625 batch PCKh 0.40234375\n",
      "Trained batch 562 batch loss 3.31771469 batch mAP 0.443359375 batch PCKh 0.361328125\n",
      "Trained batch 563 batch loss 3.23736596 batch mAP 0.46484375 batch PCKh 0.40234375\n",
      "Trained batch 564 batch loss 2.92647028 batch mAP 0.44921875 batch PCKh 0.21875\n",
      "Trained batch 565 batch loss 2.68152905 batch mAP 0.48046875 batch PCKh 0.146484375\n",
      "Trained batch 566 batch loss 3.05919647 batch mAP 0.41015625 batch PCKh 0.22265625\n",
      "Trained batch 567 batch loss 3.15936399 batch mAP 0.48828125 batch PCKh 0.1953125\n",
      "Trained batch 568 batch loss 3.25543451 batch mAP 0.513671875 batch PCKh 0.30078125\n",
      "Trained batch 569 batch loss 3.28945589 batch mAP 0.42578125 batch PCKh 0.2890625\n",
      "Trained batch 570 batch loss 3.27988291 batch mAP 0.51953125 batch PCKh 0.224609375\n",
      "Trained batch 571 batch loss 3.11550236 batch mAP 0.529296875 batch PCKh 0.34765625\n",
      "Trained batch 572 batch loss 3.25573349 batch mAP 0.453125 batch PCKh 0.22265625\n",
      "Trained batch 573 batch loss 3.16907167 batch mAP 0.58203125 batch PCKh 0.59765625\n",
      "Trained batch 574 batch loss 3.31933856 batch mAP 0.5078125 batch PCKh 0.29296875\n",
      "Trained batch 575 batch loss 3.24984598 batch mAP 0.45703125 batch PCKh 0.4609375\n",
      "Trained batch 576 batch loss 3.11637783 batch mAP 0.490234375 batch PCKh 0.470703125\n",
      "Trained batch 577 batch loss 3.08730698 batch mAP 0.5 batch PCKh 0.46875\n",
      "Trained batch 578 batch loss 3.25511169 batch mAP 0.572265625 batch PCKh 0.544921875\n",
      "Trained batch 579 batch loss 3.10422134 batch mAP 0.47265625 batch PCKh 0.32421875\n",
      "Trained batch 580 batch loss 3.08296251 batch mAP 0.51171875 batch PCKh 0.1796875\n",
      "Trained batch 581 batch loss 3.16726112 batch mAP 0.48046875 batch PCKh 0.40234375\n",
      "Trained batch 582 batch loss 3.39759064 batch mAP 0.498046875 batch PCKh 0.232421875\n",
      "Trained batch 583 batch loss 3.14401388 batch mAP 0.46875 batch PCKh 0.39453125\n",
      "Trained batch 584 batch loss 3.21933198 batch mAP 0.427734375 batch PCKh 0.51953125\n",
      "Trained batch 585 batch loss 2.9780817 batch mAP 0.46875 batch PCKh 0.490234375\n",
      "Trained batch 586 batch loss 3.06507754 batch mAP 0.46484375 batch PCKh 0.41796875\n",
      "Trained batch 587 batch loss 3.28772569 batch mAP 0.470703125 batch PCKh 0.439453125\n",
      "Trained batch 588 batch loss 3.43619132 batch mAP 0.470703125 batch PCKh 0.2265625\n",
      "Trained batch 589 batch loss 3.20944905 batch mAP 0.48046875 batch PCKh 0.451171875\n",
      "Trained batch 590 batch loss 2.92660832 batch mAP 0.509765625 batch PCKh 0.41015625\n",
      "Trained batch 591 batch loss 2.94707561 batch mAP 0.373046875 batch PCKh 0.37109375\n",
      "Trained batch 592 batch loss 2.61735988 batch mAP 0.373046875 batch PCKh 0.03515625\n",
      "Trained batch 593 batch loss 3.08120561 batch mAP 0.419921875 batch PCKh 0.36328125\n",
      "Trained batch 594 batch loss 2.90170097 batch mAP 0.36328125 batch PCKh 0.30859375\n",
      "Trained batch 595 batch loss 2.8801322 batch mAP 0.37109375 batch PCKh 0.498046875\n",
      "Trained batch 596 batch loss 2.83032203 batch mAP 0.341796875 batch PCKh 0.353515625\n",
      "Trained batch 597 batch loss 2.65471315 batch mAP 0.376953125 batch PCKh 0.328125\n",
      "Trained batch 598 batch loss 2.84800029 batch mAP 0.376953125 batch PCKh 0.599609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 599 batch loss 2.90619135 batch mAP 0.42578125 batch PCKh 0.4140625\n",
      "Trained batch 600 batch loss 3.38230801 batch mAP 0.361328125 batch PCKh 0.41015625\n",
      "Trained batch 601 batch loss 3.28876877 batch mAP 0.4375 batch PCKh 0.41015625\n",
      "Trained batch 602 batch loss 3.25715065 batch mAP 0.51953125 batch PCKh 0.357421875\n",
      "Trained batch 603 batch loss 3.15683866 batch mAP 0.560546875 batch PCKh 0.583984375\n",
      "Trained batch 604 batch loss 3.45696402 batch mAP 0.482421875 batch PCKh 0.365234375\n",
      "Trained batch 605 batch loss 3.54185104 batch mAP 0.453125 batch PCKh 0.216796875\n",
      "Trained batch 606 batch loss 3.31220865 batch mAP 0.412109375 batch PCKh 0.3828125\n",
      "Trained batch 607 batch loss 3.43693495 batch mAP 0.521484375 batch PCKh 0.3359375\n",
      "Trained batch 608 batch loss 3.2330513 batch mAP 0.451171875 batch PCKh 0.197265625\n",
      "Trained batch 609 batch loss 3.21601939 batch mAP 0.61328125 batch PCKh 0.3125\n",
      "Trained batch 610 batch loss 3.18040466 batch mAP 0.5546875 batch PCKh 0.28125\n",
      "Trained batch 611 batch loss 2.70799923 batch mAP 0.521484375 batch PCKh 0.25390625\n",
      "Trained batch 612 batch loss 3.21851277 batch mAP 0.58984375 batch PCKh 0.41015625\n",
      "Trained batch 613 batch loss 3.23586464 batch mAP 0.462890625 batch PCKh 0.349609375\n",
      "Trained batch 614 batch loss 3.15159702 batch mAP 0.626953125 batch PCKh 0.43359375\n",
      "Trained batch 615 batch loss 3.10707378 batch mAP 0.501953125 batch PCKh 0.201171875\n",
      "Trained batch 616 batch loss 3.03789902 batch mAP 0.54296875 batch PCKh 0.287109375\n",
      "Trained batch 617 batch loss 3.06787062 batch mAP 0.396484375 batch PCKh 0.5\n",
      "Trained batch 618 batch loss 2.83448219 batch mAP 0.439453125 batch PCKh 0.4921875\n",
      "Trained batch 619 batch loss 2.78538 batch mAP 0.505859375 batch PCKh 0.111328125\n",
      "Trained batch 620 batch loss 3.18566275 batch mAP 0.552734375 batch PCKh 0.376953125\n",
      "Trained batch 621 batch loss 3.29420877 batch mAP 0.634765625 batch PCKh 0.357421875\n",
      "Trained batch 622 batch loss 2.99851322 batch mAP 0.55078125 batch PCKh 0.234375\n",
      "Trained batch 623 batch loss 3.06425548 batch mAP 0.58984375 batch PCKh 0.267578125\n",
      "Trained batch 624 batch loss 3.2414465 batch mAP 0.509765625 batch PCKh 0.4453125\n",
      "Trained batch 625 batch loss 3.19614077 batch mAP 0.5 batch PCKh 0.4609375\n",
      "Trained batch 626 batch loss 3.19557405 batch mAP 0.3984375 batch PCKh 0.35546875\n",
      "Trained batch 627 batch loss 3.24039721 batch mAP 0.4296875 batch PCKh 0.390625\n",
      "Trained batch 628 batch loss 3.03215599 batch mAP 0.517578125 batch PCKh 0.435546875\n",
      "Trained batch 629 batch loss 3.08172274 batch mAP 0.490234375 batch PCKh 0.515625\n",
      "Trained batch 630 batch loss 3.01441216 batch mAP 0.4453125 batch PCKh 0.423828125\n",
      "Trained batch 631 batch loss 3.20666599 batch mAP 0.5 batch PCKh 0.34375\n",
      "Trained batch 632 batch loss 3.10292959 batch mAP 0.501953125 batch PCKh 0.396484375\n",
      "Trained batch 633 batch loss 2.90120077 batch mAP 0.580078125 batch PCKh 0.654296875\n",
      "Trained batch 634 batch loss 2.9648273 batch mAP 0.4453125 batch PCKh 0.39453125\n",
      "Trained batch 635 batch loss 3.16334772 batch mAP 0.427734375 batch PCKh 0.330078125\n",
      "Trained batch 636 batch loss 3.18805981 batch mAP 0.41015625 batch PCKh 0.279296875\n",
      "Trained batch 637 batch loss 3.2029171 batch mAP 0.416015625 batch PCKh 0.421875\n",
      "Trained batch 638 batch loss 3.20765448 batch mAP 0.392578125 batch PCKh 0.36328125\n",
      "Trained batch 639 batch loss 3.00630617 batch mAP 0.376953125 batch PCKh 0.255859375\n",
      "Trained batch 640 batch loss 3.19908619 batch mAP 0.3359375 batch PCKh 0.240234375\n",
      "Trained batch 641 batch loss 3.24618149 batch mAP 0.40234375 batch PCKh 0.4609375\n",
      "Trained batch 642 batch loss 3.31036854 batch mAP 0.419921875 batch PCKh 0.337890625\n",
      "Trained batch 643 batch loss 3.32509446 batch mAP 0.380859375 batch PCKh 0.236328125\n",
      "Trained batch 644 batch loss 3.30909252 batch mAP 0.412109375 batch PCKh 0.443359375\n",
      "Trained batch 645 batch loss 3.15816784 batch mAP 0.412109375 batch PCKh 0.48046875\n",
      "Trained batch 646 batch loss 2.81598949 batch mAP 0.3671875 batch PCKh 0.396484375\n",
      "Trained batch 647 batch loss 2.87428021 batch mAP 0.47265625 batch PCKh 0.3984375\n",
      "Trained batch 648 batch loss 2.97326136 batch mAP 0.431640625 batch PCKh 0.162109375\n",
      "Trained batch 649 batch loss 3.01543713 batch mAP 0.40625 batch PCKh 0.3046875\n",
      "Trained batch 650 batch loss 3.21822667 batch mAP 0.505859375 batch PCKh 0.216796875\n",
      "Trained batch 651 batch loss 3.01332164 batch mAP 0.45703125 batch PCKh 0.1953125\n",
      "Trained batch 652 batch loss 2.84730959 batch mAP 0.4375 batch PCKh 0.029296875\n",
      "Trained batch 653 batch loss 3.25155544 batch mAP 0.4140625 batch PCKh 0.232421875\n",
      "Trained batch 654 batch loss 3.23610115 batch mAP 0.466796875 batch PCKh 0.35546875\n",
      "Trained batch 655 batch loss 3.47801447 batch mAP 0.365234375 batch PCKh 0.333984375\n",
      "Trained batch 656 batch loss 3.30511332 batch mAP 0.419921875 batch PCKh 0.271484375\n",
      "Trained batch 657 batch loss 3.19484758 batch mAP 0.37109375 batch PCKh 0.50390625\n",
      "Trained batch 658 batch loss 2.94876194 batch mAP 0.4296875 batch PCKh 0.525390625\n",
      "Trained batch 659 batch loss 2.92401218 batch mAP 0.48828125 batch PCKh 0.6796875\n",
      "Trained batch 660 batch loss 3.10099411 batch mAP 0.423828125 batch PCKh 0.46484375\n",
      "Trained batch 661 batch loss 3.32520151 batch mAP 0.42578125 batch PCKh 0.4453125\n",
      "Trained batch 662 batch loss 3.0617733 batch mAP 0.435546875 batch PCKh 0.572265625\n",
      "Trained batch 663 batch loss 3.25246429 batch mAP 0.39453125 batch PCKh 0.541015625\n",
      "Trained batch 664 batch loss 3.2341094 batch mAP 0.41015625 batch PCKh 0.419921875\n",
      "Trained batch 665 batch loss 3.17097044 batch mAP 0.455078125 batch PCKh 0.345703125\n",
      "Trained batch 666 batch loss 3.18958282 batch mAP 0.515625 batch PCKh 0.283203125\n",
      "Trained batch 667 batch loss 3.11378455 batch mAP 0.427734375 batch PCKh 0.361328125\n",
      "Trained batch 668 batch loss 3.24497223 batch mAP 0.4375 batch PCKh 0.416015625\n",
      "Trained batch 669 batch loss 3.30467796 batch mAP 0.462890625 batch PCKh 0.32421875\n",
      "Trained batch 670 batch loss 3.18679762 batch mAP 0.421875 batch PCKh 0.322265625\n",
      "Trained batch 671 batch loss 3.09548163 batch mAP 0.3984375 batch PCKh 0.35546875\n",
      "Trained batch 672 batch loss 3.10031652 batch mAP 0.4296875 batch PCKh 0.34375\n",
      "Trained batch 673 batch loss 2.50732017 batch mAP 0.375 batch PCKh 0.2265625\n",
      "Trained batch 674 batch loss 3.04859161 batch mAP 0.404296875 batch PCKh 0.419921875\n",
      "Trained batch 675 batch loss 3.18326139 batch mAP 0.421875 batch PCKh 0.421875\n",
      "Trained batch 676 batch loss 3.31237483 batch mAP 0.4921875 batch PCKh 0.625\n",
      "Trained batch 677 batch loss 3.13567019 batch mAP 0.462890625 batch PCKh 0.515625\n",
      "Trained batch 678 batch loss 2.98928547 batch mAP 0.359375 batch PCKh 0.37890625\n",
      "Trained batch 679 batch loss 3.27761936 batch mAP 0.40234375 batch PCKh 0.259765625\n",
      "Trained batch 680 batch loss 3.15360141 batch mAP 0.341796875 batch PCKh 0.193359375\n",
      "Trained batch 681 batch loss 3.04341936 batch mAP 0.3671875 batch PCKh 0.486328125\n",
      "Trained batch 682 batch loss 3.20347595 batch mAP 0.294921875 batch PCKh 0.44921875\n",
      "Trained batch 683 batch loss 3.22423697 batch mAP 0.330078125 batch PCKh 0.375\n",
      "Trained batch 684 batch loss 3.04143381 batch mAP 0.265625 batch PCKh 0.501953125\n",
      "Trained batch 685 batch loss 2.9476347 batch mAP 0.357421875 batch PCKh 0.408203125\n",
      "Trained batch 686 batch loss 3.37860441 batch mAP 0.341796875 batch PCKh 0.24609375\n",
      "Trained batch 687 batch loss 3.03136349 batch mAP 0.447265625 batch PCKh 0.310546875\n",
      "Trained batch 688 batch loss 3.07558656 batch mAP 0.4375 batch PCKh 0.41015625\n",
      "Trained batch 689 batch loss 3.16043711 batch mAP 0.349609375 batch PCKh 0.2109375\n",
      "Trained batch 690 batch loss 3.29952192 batch mAP 0.388671875 batch PCKh 0.283203125\n",
      "Trained batch 691 batch loss 3.23585749 batch mAP 0.42578125 batch PCKh 0.294921875\n",
      "Trained batch 692 batch loss 3.31808543 batch mAP 0.490234375 batch PCKh 0.365234375\n",
      "Trained batch 693 batch loss 3.20356297 batch mAP 0.509765625 batch PCKh 0.447265625\n",
      "Trained batch 694 batch loss 2.93928385 batch mAP 0.4453125 batch PCKh 0.291015625\n",
      "Trained batch 695 batch loss 3.01472902 batch mAP 0.3828125 batch PCKh 0.46484375\n",
      "Trained batch 696 batch loss 2.99257588 batch mAP 0.466796875 batch PCKh 0.255859375\n",
      "Trained batch 697 batch loss 3.03166533 batch mAP 0.421875 batch PCKh 0.4296875\n",
      "Trained batch 698 batch loss 3.08064175 batch mAP 0.392578125 batch PCKh 0.369140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 699 batch loss 3.24750042 batch mAP 0.45703125 batch PCKh 0.3828125\n",
      "Trained batch 700 batch loss 3.3433764 batch mAP 0.42578125 batch PCKh 0.26171875\n",
      "Trained batch 701 batch loss 3.34357691 batch mAP 0.41796875 batch PCKh 0.28125\n",
      "Trained batch 702 batch loss 3.09249282 batch mAP 0.396484375 batch PCKh 0.369140625\n",
      "Trained batch 703 batch loss 3.26376152 batch mAP 0.330078125 batch PCKh 0.32421875\n",
      "Trained batch 704 batch loss 3.27955341 batch mAP 0.435546875 batch PCKh 0.365234375\n",
      "Trained batch 705 batch loss 3.18091297 batch mAP 0.38671875 batch PCKh 0.470703125\n",
      "Trained batch 706 batch loss 3.0554316 batch mAP 0.44921875 batch PCKh 0.39453125\n",
      "Trained batch 707 batch loss 3.04733348 batch mAP 0.4140625 batch PCKh 0.529296875\n",
      "Trained batch 708 batch loss 3.02549481 batch mAP 0.419921875 batch PCKh 0.298828125\n",
      "Trained batch 709 batch loss 3.12109709 batch mAP 0.390625 batch PCKh 0.53125\n",
      "Trained batch 710 batch loss 3.11769557 batch mAP 0.3359375 batch PCKh 0.416015625\n",
      "Trained batch 711 batch loss 3.22933912 batch mAP 0.4375 batch PCKh 0.349609375\n",
      "Trained batch 712 batch loss 3.00307465 batch mAP 0.404296875 batch PCKh 0.552734375\n",
      "Trained batch 713 batch loss 3.07768106 batch mAP 0.392578125 batch PCKh 0.4765625\n",
      "Trained batch 714 batch loss 3.2527771 batch mAP 0.353515625 batch PCKh 0.48828125\n",
      "Trained batch 715 batch loss 3.47428536 batch mAP 0.294921875 batch PCKh 0.3984375\n",
      "Trained batch 716 batch loss 3.10822201 batch mAP 0.328125 batch PCKh 0.361328125\n",
      "Trained batch 717 batch loss 2.89553404 batch mAP 0.287109375 batch PCKh 0.39453125\n",
      "Trained batch 718 batch loss 2.952847 batch mAP 0.30078125 batch PCKh 0.53125\n",
      "Trained batch 719 batch loss 2.75381565 batch mAP 0.3359375 batch PCKh 0.078125\n",
      "Trained batch 720 batch loss 3.0266552 batch mAP 0.3203125 batch PCKh 0.21484375\n",
      "Trained batch 721 batch loss 2.99667406 batch mAP 0.2890625 batch PCKh 0.458984375\n",
      "Trained batch 722 batch loss 3.33270216 batch mAP 0.283203125 batch PCKh 0.330078125\n",
      "Trained batch 723 batch loss 3.19923162 batch mAP 0.318359375 batch PCKh 0.34375\n",
      "Trained batch 724 batch loss 3.37711263 batch mAP 0.376953125 batch PCKh 0.298828125\n",
      "Trained batch 725 batch loss 3.1497817 batch mAP 0.38671875 batch PCKh 0.353515625\n",
      "Trained batch 726 batch loss 3.11082506 batch mAP 0.4140625 batch PCKh 0.349609375\n",
      "Trained batch 727 batch loss 3.11653852 batch mAP 0.423828125 batch PCKh 0.3359375\n",
      "Trained batch 728 batch loss 3.40904927 batch mAP 0.380859375 batch PCKh 0.421875\n",
      "Trained batch 729 batch loss 3.16441703 batch mAP 0.431640625 batch PCKh 0.23828125\n",
      "Trained batch 730 batch loss 3.31923032 batch mAP 0.404296875 batch PCKh 0.369140625\n",
      "Trained batch 731 batch loss 3.2444725 batch mAP 0.30078125 batch PCKh 0.494140625\n",
      "Trained batch 732 batch loss 3.24067616 batch mAP 0.361328125 batch PCKh 0.369140625\n",
      "Trained batch 733 batch loss 3.29735017 batch mAP 0.35546875 batch PCKh 0.416015625\n",
      "Trained batch 734 batch loss 3.20249987 batch mAP 0.3671875 batch PCKh 0.328125\n",
      "Trained batch 735 batch loss 3.27744222 batch mAP 0.453125 batch PCKh 0.3671875\n",
      "Trained batch 736 batch loss 3.0729084 batch mAP 0.427734375 batch PCKh 0.537109375\n",
      "Trained batch 737 batch loss 2.97738552 batch mAP 0.3671875 batch PCKh 0.4296875\n",
      "Trained batch 738 batch loss 3.20878315 batch mAP 0.39453125 batch PCKh 0.357421875\n",
      "Trained batch 739 batch loss 3.24695253 batch mAP 0.326171875 batch PCKh 0.34375\n",
      "Trained batch 740 batch loss 3.25872231 batch mAP 0.333984375 batch PCKh 0.3203125\n",
      "Trained batch 741 batch loss 3.16144133 batch mAP 0.3203125 batch PCKh 0.244140625\n",
      "Trained batch 742 batch loss 3.35209 batch mAP 0.380859375 batch PCKh 0.41796875\n",
      "Trained batch 743 batch loss 3.3833766 batch mAP 0.30859375 batch PCKh 0.38671875\n",
      "Trained batch 744 batch loss 3.15419483 batch mAP 0.28125 batch PCKh 0.48046875\n",
      "Trained batch 745 batch loss 3.16830802 batch mAP 0.279296875 batch PCKh 0.60546875\n",
      "Trained batch 746 batch loss 3.20510411 batch mAP 0.3359375 batch PCKh 0.455078125\n",
      "Trained batch 747 batch loss 3.69344926 batch mAP 0.349609375 batch PCKh 0.119140625\n",
      "Trained batch 748 batch loss 3.46390247 batch mAP 0.36328125 batch PCKh 0.25\n",
      "Trained batch 749 batch loss 3.44064617 batch mAP 0.3125 batch PCKh 0.255859375\n",
      "Trained batch 750 batch loss 3.19444275 batch mAP 0.435546875 batch PCKh 0.3203125\n",
      "Trained batch 751 batch loss 2.88755655 batch mAP 0.400390625 batch PCKh 0.34375\n",
      "Trained batch 752 batch loss 2.89980841 batch mAP 0.423828125 batch PCKh 0.68359375\n",
      "Trained batch 753 batch loss 3.09069657 batch mAP 0.470703125 batch PCKh 0.51953125\n",
      "Trained batch 754 batch loss 3.01923132 batch mAP 0.421875 batch PCKh 0.51171875\n",
      "Trained batch 755 batch loss 2.9383111 batch mAP 0.388671875 batch PCKh 0.212890625\n",
      "Trained batch 756 batch loss 2.58659053 batch mAP 0.33984375 batch PCKh 0.142578125\n",
      "Trained batch 757 batch loss 2.46014738 batch mAP 0.39453125 batch PCKh 0.01171875\n",
      "Trained batch 758 batch loss 2.72352219 batch mAP 0.38671875 batch PCKh 0.083984375\n",
      "Trained batch 759 batch loss 2.83662868 batch mAP 0.361328125 batch PCKh 0.35546875\n",
      "Trained batch 760 batch loss 3.08635902 batch mAP 0.478515625 batch PCKh 0.337890625\n",
      "Trained batch 761 batch loss 3.04953384 batch mAP 0.333984375 batch PCKh 0.234375\n",
      "Trained batch 762 batch loss 2.51261401 batch mAP 0.384765625 batch PCKh 0.134765625\n",
      "Trained batch 763 batch loss 2.48689604 batch mAP 0.353515625 batch PCKh 0.037109375\n",
      "Trained batch 764 batch loss 2.60661125 batch mAP 0.337890625 batch PCKh 0.044921875\n",
      "Trained batch 765 batch loss 2.48818946 batch mAP 0.34375 batch PCKh 0.005859375\n",
      "Trained batch 766 batch loss 2.65389776 batch mAP 0.376953125 batch PCKh 0.001953125\n",
      "Trained batch 767 batch loss 2.61767697 batch mAP 0.380859375 batch PCKh 0.111328125\n",
      "Trained batch 768 batch loss 2.65771341 batch mAP 0.34765625 batch PCKh 0.177734375\n",
      "Trained batch 769 batch loss 3.00271177 batch mAP 0.443359375 batch PCKh 0.388671875\n",
      "Trained batch 770 batch loss 2.77257109 batch mAP 0.4296875 batch PCKh 0.34375\n",
      "Trained batch 771 batch loss 3.04690719 batch mAP 0.3984375 batch PCKh 0.37890625\n",
      "Trained batch 772 batch loss 3.22310519 batch mAP 0.4765625 batch PCKh 0.482421875\n",
      "Trained batch 773 batch loss 3.28969 batch mAP 0.421875 batch PCKh 0.369140625\n",
      "Trained batch 774 batch loss 3.2142067 batch mAP 0.439453125 batch PCKh 0.240234375\n",
      "Trained batch 775 batch loss 3.22818327 batch mAP 0.44140625 batch PCKh 0.28125\n",
      "Trained batch 776 batch loss 3.42408109 batch mAP 0.5 batch PCKh 0.294921875\n",
      "Trained batch 777 batch loss 3.50922394 batch mAP 0.443359375 batch PCKh 0.251953125\n",
      "Trained batch 778 batch loss 3.45645666 batch mAP 0.529296875 batch PCKh 0.392578125\n",
      "Trained batch 779 batch loss 3.29316044 batch mAP 0.392578125 batch PCKh 0.283203125\n",
      "Trained batch 780 batch loss 3.31523371 batch mAP 0.486328125 batch PCKh 0.431640625\n",
      "Trained batch 781 batch loss 3.16562605 batch mAP 0.47265625 batch PCKh 0.4765625\n",
      "Trained batch 782 batch loss 3.21404171 batch mAP 0.462890625 batch PCKh 0.427734375\n",
      "Trained batch 783 batch loss 3.20091462 batch mAP 0.490234375 batch PCKh 0.453125\n",
      "Trained batch 784 batch loss 3.19779444 batch mAP 0.529296875 batch PCKh 0.4609375\n",
      "Trained batch 785 batch loss 3.22777414 batch mAP 0.484375 batch PCKh 0.330078125\n",
      "Trained batch 786 batch loss 3.29525971 batch mAP 0.498046875 batch PCKh 0.3359375\n",
      "Trained batch 787 batch loss 3.28427744 batch mAP 0.48046875 batch PCKh 0.2890625\n",
      "Trained batch 788 batch loss 3.25092578 batch mAP 0.533203125 batch PCKh 0.359375\n",
      "Trained batch 789 batch loss 3.18753839 batch mAP 0.33984375 batch PCKh 0.423828125\n",
      "Trained batch 790 batch loss 3.07767487 batch mAP 0.439453125 batch PCKh 0.43359375\n",
      "Trained batch 791 batch loss 3.19317961 batch mAP 0.3671875 batch PCKh 0.4296875\n",
      "Trained batch 792 batch loss 3.23223305 batch mAP 0.208984375 batch PCKh 0.3359375\n",
      "Trained batch 793 batch loss 3.14017296 batch mAP 0.302734375 batch PCKh 0.33984375\n",
      "Trained batch 794 batch loss 3.1279521 batch mAP 0.36328125 batch PCKh 0.470703125\n",
      "Trained batch 795 batch loss 3.28671932 batch mAP 0.2578125 batch PCKh 0.47265625\n",
      "Trained batch 796 batch loss 3.25741529 batch mAP 0.244140625 batch PCKh 0.421875\n",
      "Trained batch 797 batch loss 3.00846958 batch mAP 0.29296875 batch PCKh 0.4453125\n",
      "Trained batch 798 batch loss 3.01861906 batch mAP 0.333984375 batch PCKh 0.328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 799 batch loss 3.14589834 batch mAP 0.37890625 batch PCKh 0.470703125\n",
      "Trained batch 800 batch loss 3.33440304 batch mAP 0.322265625 batch PCKh 0.46875\n",
      "Trained batch 801 batch loss 3.24154043 batch mAP 0.298828125 batch PCKh 0.39453125\n",
      "Trained batch 802 batch loss 3.23940325 batch mAP 0.26953125 batch PCKh 0.388671875\n",
      "Trained batch 803 batch loss 3.2343843 batch mAP 0.283203125 batch PCKh 0.302734375\n",
      "Trained batch 804 batch loss 3.31770802 batch mAP 0.35546875 batch PCKh 0.275390625\n",
      "Trained batch 805 batch loss 2.84566259 batch mAP 0.3828125 batch PCKh 0.34375\n",
      "Trained batch 806 batch loss 3.1681087 batch mAP 0.384765625 batch PCKh 0.33984375\n",
      "Trained batch 807 batch loss 3.38900614 batch mAP 0.3671875 batch PCKh 0.1796875\n",
      "Trained batch 808 batch loss 3.16234922 batch mAP 0.390625 batch PCKh 0.416015625\n",
      "Trained batch 809 batch loss 3.172153 batch mAP 0.38671875 batch PCKh 0.357421875\n",
      "Trained batch 810 batch loss 3.04857063 batch mAP 0.439453125 batch PCKh 0.392578125\n",
      "Trained batch 811 batch loss 3.20590544 batch mAP 0.43359375 batch PCKh 0.392578125\n",
      "Trained batch 812 batch loss 3.19915748 batch mAP 0.42578125 batch PCKh 0.181640625\n",
      "Trained batch 813 batch loss 3.5040586 batch mAP 0.341796875 batch PCKh 0.349609375\n",
      "Trained batch 814 batch loss 3.36994505 batch mAP 0.412109375 batch PCKh 0.22265625\n",
      "Trained batch 815 batch loss 3.04714322 batch mAP 0.328125 batch PCKh 0.263671875\n",
      "Trained batch 816 batch loss 3.1051476 batch mAP 0.380859375 batch PCKh 0.314453125\n",
      "Trained batch 817 batch loss 2.98263931 batch mAP 0.3828125 batch PCKh 0.232421875\n",
      "Trained batch 818 batch loss 2.98712492 batch mAP 0.41796875 batch PCKh 0.21484375\n",
      "Trained batch 819 batch loss 3.19496536 batch mAP 0.42578125 batch PCKh 0.2734375\n",
      "Trained batch 820 batch loss 3.04117537 batch mAP 0.443359375 batch PCKh 0.45703125\n",
      "Trained batch 821 batch loss 3.21370244 batch mAP 0.384765625 batch PCKh 0.494140625\n",
      "Trained batch 822 batch loss 3.14591813 batch mAP 0.283203125 batch PCKh 0.513671875\n",
      "Trained batch 823 batch loss 3.31318402 batch mAP 0.32421875 batch PCKh 0.349609375\n",
      "Trained batch 824 batch loss 3.33593822 batch mAP 0.341796875 batch PCKh 0.375\n",
      "Trained batch 825 batch loss 3.36310959 batch mAP 0.36328125 batch PCKh 0.31640625\n",
      "Trained batch 826 batch loss 3.01616669 batch mAP 0.375 batch PCKh 0.3671875\n",
      "Trained batch 827 batch loss 2.50512409 batch mAP 0.314453125 batch PCKh 0.099609375\n",
      "Trained batch 828 batch loss 2.84508133 batch mAP 0.43359375 batch PCKh 0.298828125\n",
      "Trained batch 829 batch loss 3.42276025 batch mAP 0.45703125 batch PCKh 0.29296875\n",
      "Trained batch 830 batch loss 3.23251295 batch mAP 0.369140625 batch PCKh 0.2578125\n",
      "Trained batch 831 batch loss 3.06946707 batch mAP 0.333984375 batch PCKh 0.181640625\n",
      "Trained batch 832 batch loss 3.19795752 batch mAP 0.341796875 batch PCKh 0.47265625\n",
      "Trained batch 833 batch loss 3.21515179 batch mAP 0.396484375 batch PCKh 0.306640625\n",
      "Trained batch 834 batch loss 3.44571018 batch mAP 0.373046875 batch PCKh 0.283203125\n",
      "Trained batch 835 batch loss 3.26750183 batch mAP 0.4375 batch PCKh 0.359375\n",
      "Trained batch 836 batch loss 3.34185266 batch mAP 0.46484375 batch PCKh 0.296875\n",
      "Trained batch 837 batch loss 3.30671787 batch mAP 0.47265625 batch PCKh 0.39453125\n",
      "Trained batch 838 batch loss 3.20633316 batch mAP 0.39453125 batch PCKh 0.28125\n",
      "Trained batch 839 batch loss 2.69805431 batch mAP 0.470703125 batch PCKh 0.61328125\n",
      "Trained batch 840 batch loss 2.72764111 batch mAP 0.392578125 batch PCKh 0.5625\n",
      "Trained batch 841 batch loss 3.05880594 batch mAP 0.484375 batch PCKh 0.3359375\n",
      "Trained batch 842 batch loss 3.17187071 batch mAP 0.45703125 batch PCKh 0.546875\n",
      "Trained batch 843 batch loss 2.84109139 batch mAP 0.490234375 batch PCKh 0.265625\n",
      "Trained batch 844 batch loss 2.9852531 batch mAP 0.57421875 batch PCKh 0.404296875\n",
      "Trained batch 845 batch loss 3.14396 batch mAP 0.474609375 batch PCKh 0.322265625\n",
      "Trained batch 846 batch loss 3.19746971 batch mAP 0.353515625 batch PCKh 0.296875\n",
      "Trained batch 847 batch loss 3.19877553 batch mAP 0.296875 batch PCKh 0.333984375\n",
      "Trained batch 848 batch loss 3.28980088 batch mAP 0.328125 batch PCKh 0.341796875\n",
      "Trained batch 849 batch loss 3.14928174 batch mAP 0.4453125 batch PCKh 0.357421875\n",
      "Trained batch 850 batch loss 3.27359366 batch mAP 0.3984375 batch PCKh 0.259765625\n",
      "Trained batch 851 batch loss 3.23680019 batch mAP 0.357421875 batch PCKh 0.234375\n",
      "Trained batch 852 batch loss 3.26672459 batch mAP 0.3125 batch PCKh 0.3671875\n",
      "Trained batch 853 batch loss 3.41207647 batch mAP 0.306640625 batch PCKh 0.3046875\n",
      "Trained batch 854 batch loss 3.33740377 batch mAP 0.34765625 batch PCKh 0.28125\n",
      "Trained batch 855 batch loss 3.27005768 batch mAP 0.296875 batch PCKh 0.25\n",
      "Trained batch 856 batch loss 3.30559731 batch mAP 0.412109375 batch PCKh 0.349609375\n",
      "Trained batch 857 batch loss 3.31723738 batch mAP 0.310546875 batch PCKh 0.39453125\n",
      "Trained batch 858 batch loss 3.34286976 batch mAP 0.3984375 batch PCKh 0.388671875\n",
      "Trained batch 859 batch loss 3.19939971 batch mAP 0.3359375 batch PCKh 0.34765625\n",
      "Trained batch 860 batch loss 3.2894907 batch mAP 0.38671875 batch PCKh 0.365234375\n",
      "Trained batch 861 batch loss 3.16079473 batch mAP 0.404296875 batch PCKh 0.390625\n",
      "Trained batch 862 batch loss 3.18573666 batch mAP 0.5 batch PCKh 0.38671875\n",
      "Trained batch 863 batch loss 3.06156063 batch mAP 0.404296875 batch PCKh 0.376953125\n",
      "Trained batch 864 batch loss 3.11919808 batch mAP 0.43359375 batch PCKh 0.4765625\n",
      "Trained batch 865 batch loss 3.19217157 batch mAP 0.501953125 batch PCKh 0.509765625\n",
      "Trained batch 866 batch loss 3.18353939 batch mAP 0.48828125 batch PCKh 0.599609375\n",
      "Trained batch 867 batch loss 3.09094 batch mAP 0.431640625 batch PCKh 0.431640625\n",
      "Trained batch 868 batch loss 2.93975925 batch mAP 0.40625 batch PCKh 0.228515625\n",
      "Trained batch 869 batch loss 3.00848413 batch mAP 0.39453125 batch PCKh 0.236328125\n",
      "Trained batch 870 batch loss 3.35089397 batch mAP 0.361328125 batch PCKh 0.4609375\n",
      "Trained batch 871 batch loss 3.19586682 batch mAP 0.416015625 batch PCKh 0.216796875\n",
      "Trained batch 872 batch loss 2.7381289 batch mAP 0.322265625 batch PCKh 0.212890625\n",
      "Trained batch 873 batch loss 2.7069242 batch mAP 0.390625 batch PCKh 0.107421875\n",
      "Trained batch 874 batch loss 2.695436 batch mAP 0.427734375 batch PCKh 0.220703125\n",
      "Trained batch 875 batch loss 2.65554285 batch mAP 0.447265625 batch PCKh 0.21484375\n",
      "Trained batch 876 batch loss 2.60814357 batch mAP 0.408203125 batch PCKh 0.22265625\n",
      "Trained batch 877 batch loss 2.65717697 batch mAP 0.443359375 batch PCKh 0.232421875\n",
      "Trained batch 878 batch loss 2.62392735 batch mAP 0.390625 batch PCKh 0.33203125\n",
      "Trained batch 879 batch loss 3.11178041 batch mAP 0.390625 batch PCKh 0.326171875\n",
      "Trained batch 880 batch loss 3.21325684 batch mAP 0.328125 batch PCKh 0.302734375\n",
      "Trained batch 881 batch loss 3.13920975 batch mAP 0.357421875 batch PCKh 0.41015625\n",
      "Trained batch 882 batch loss 3.06045365 batch mAP 0.451171875 batch PCKh 0.20703125\n",
      "Trained batch 883 batch loss 3.249547 batch mAP 0.37109375 batch PCKh 0.21875\n",
      "Trained batch 884 batch loss 3.42506123 batch mAP 0.32421875 batch PCKh 0.34375\n",
      "Trained batch 885 batch loss 3.45320845 batch mAP 0.4140625 batch PCKh 0.298828125\n",
      "Trained batch 886 batch loss 3.57188916 batch mAP 0.416015625 batch PCKh 0.23046875\n",
      "Trained batch 887 batch loss 3.05517912 batch mAP 0.36328125 batch PCKh 0.330078125\n",
      "Trained batch 888 batch loss 3.14450788 batch mAP 0.41796875 batch PCKh 0.33984375\n",
      "Trained batch 889 batch loss 3.18478489 batch mAP 0.458984375 batch PCKh 0.4453125\n",
      "Trained batch 890 batch loss 3.19282055 batch mAP 0.482421875 batch PCKh 0.474609375\n",
      "Trained batch 891 batch loss 3.34494853 batch mAP 0.48828125 batch PCKh 0.388671875\n",
      "Trained batch 892 batch loss 3.13728929 batch mAP 0.45703125 batch PCKh 0.412109375\n",
      "Trained batch 893 batch loss 3.32507658 batch mAP 0.517578125 batch PCKh 0.38671875\n",
      "Trained batch 894 batch loss 3.23897147 batch mAP 0.546875 batch PCKh 0.40625\n",
      "Trained batch 895 batch loss 3.13870215 batch mAP 0.513671875 batch PCKh 0.47265625\n",
      "Trained batch 896 batch loss 3.28414583 batch mAP 0.541015625 batch PCKh 0.400390625\n",
      "Trained batch 897 batch loss 3.24267292 batch mAP 0.58203125 batch PCKh 0.34375\n",
      "Trained batch 898 batch loss 3.19925117 batch mAP 0.58203125 batch PCKh 0.48046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 899 batch loss 3.37324619 batch mAP 0.466796875 batch PCKh 0.28515625\n",
      "Trained batch 900 batch loss 3.31688046 batch mAP 0.50390625 batch PCKh 0.388671875\n",
      "Trained batch 901 batch loss 3.41712451 batch mAP 0.515625 batch PCKh 0.419921875\n",
      "Trained batch 902 batch loss 3.47539043 batch mAP 0.4765625 batch PCKh 0.400390625\n",
      "Trained batch 903 batch loss 3.18765855 batch mAP 0.63671875 batch PCKh 0.501953125\n",
      "Trained batch 904 batch loss 3.0571804 batch mAP 0.46875 batch PCKh 0.408203125\n",
      "Trained batch 905 batch loss 3.16994429 batch mAP 0.482421875 batch PCKh 0.537109375\n",
      "Trained batch 906 batch loss 3.03235674 batch mAP 0.544921875 batch PCKh 0.67578125\n",
      "Trained batch 907 batch loss 3.05998039 batch mAP 0.509765625 batch PCKh 0.44921875\n",
      "Trained batch 908 batch loss 2.87650251 batch mAP 0.55078125 batch PCKh 0.10546875\n",
      "Trained batch 909 batch loss 2.70107722 batch mAP 0.4765625 batch PCKh 0.220703125\n",
      "Trained batch 910 batch loss 2.73215294 batch mAP 0.482421875 batch PCKh 0.30859375\n",
      "Trained batch 911 batch loss 2.97080708 batch mAP 0.376953125 batch PCKh 0.4296875\n",
      "Trained batch 912 batch loss 2.92094588 batch mAP 0.349609375 batch PCKh 0.419921875\n",
      "Trained batch 913 batch loss 3.1330936 batch mAP 0.45703125 batch PCKh 0.427734375\n",
      "Trained batch 914 batch loss 3.06105471 batch mAP 0.3984375 batch PCKh 0.56640625\n",
      "Trained batch 915 batch loss 2.79475498 batch mAP 0.291015625 batch PCKh 0.205078125\n",
      "Trained batch 916 batch loss 3.11175871 batch mAP 0.31640625 batch PCKh 0.3203125\n",
      "Trained batch 917 batch loss 3.20158601 batch mAP 0.349609375 batch PCKh 0.43359375\n",
      "Trained batch 918 batch loss 2.93455172 batch mAP 0.396484375 batch PCKh 0.30078125\n",
      "Trained batch 919 batch loss 2.92372131 batch mAP 0.259765625 batch PCKh 0.365234375\n",
      "Trained batch 920 batch loss 2.88043547 batch mAP 0.29296875 batch PCKh 0.193359375\n",
      "Trained batch 921 batch loss 2.94918156 batch mAP 0.3203125 batch PCKh 0.234375\n",
      "Trained batch 922 batch loss 2.71783876 batch mAP 0.27734375 batch PCKh 0.173828125\n",
      "Trained batch 923 batch loss 3.10203123 batch mAP 0.2109375 batch PCKh 0.353515625\n",
      "Trained batch 924 batch loss 3.0535593 batch mAP 0.22265625 batch PCKh 0.51171875\n",
      "Trained batch 925 batch loss 2.85638046 batch mAP 0.22265625 batch PCKh 0.400390625\n",
      "Trained batch 926 batch loss 2.74293756 batch mAP 0.296875 batch PCKh 0.31640625\n",
      "Trained batch 927 batch loss 2.95203638 batch mAP 0.1953125 batch PCKh 0.3203125\n",
      "Trained batch 928 batch loss 2.9986558 batch mAP 0.267578125 batch PCKh 0.412109375\n",
      "Trained batch 929 batch loss 2.85941577 batch mAP 0.306640625 batch PCKh 0.244140625\n",
      "Trained batch 930 batch loss 2.91859865 batch mAP 0.255859375 batch PCKh 0.166015625\n",
      "Trained batch 931 batch loss 2.65810728 batch mAP 0.3828125 batch PCKh 0.322265625\n",
      "Trained batch 932 batch loss 2.73236966 batch mAP 0.318359375 batch PCKh 0.13671875\n",
      "Trained batch 933 batch loss 2.69081163 batch mAP 0.275390625 batch PCKh 0.275390625\n",
      "Trained batch 934 batch loss 2.80614209 batch mAP 0.30859375 batch PCKh 0.34765625\n",
      "Trained batch 935 batch loss 2.83763647 batch mAP 0.330078125 batch PCKh 0.212890625\n",
      "Trained batch 936 batch loss 2.66287017 batch mAP 0.310546875 batch PCKh 0.23828125\n",
      "Trained batch 937 batch loss 2.82542109 batch mAP 0.26171875 batch PCKh 0.119140625\n",
      "Trained batch 938 batch loss 2.75458956 batch mAP 0.2734375 batch PCKh 0.21875\n",
      "Trained batch 939 batch loss 2.86943913 batch mAP 0.28125 batch PCKh 0.328125\n",
      "Trained batch 940 batch loss 3.19605398 batch mAP 0.291015625 batch PCKh 0.408203125\n",
      "Trained batch 941 batch loss 3.1418395 batch mAP 0.248046875 batch PCKh 0.46484375\n",
      "Trained batch 942 batch loss 2.97246 batch mAP 0.279296875 batch PCKh 0.3828125\n",
      "Trained batch 943 batch loss 3.06274772 batch mAP 0.30078125 batch PCKh 0.3359375\n",
      "Trained batch 944 batch loss 2.90445137 batch mAP 0.259765625 batch PCKh 0.326171875\n",
      "Trained batch 945 batch loss 3.18599749 batch mAP 0.306640625 batch PCKh 0.33984375\n",
      "Trained batch 946 batch loss 3.27781105 batch mAP 0.248046875 batch PCKh 0.26171875\n",
      "Trained batch 947 batch loss 3.18434858 batch mAP 0.26171875 batch PCKh 0.33984375\n",
      "Trained batch 948 batch loss 3.33807969 batch mAP 0.232421875 batch PCKh 0.275390625\n",
      "Trained batch 949 batch loss 3.17563915 batch mAP 0.259765625 batch PCKh 0.38671875\n",
      "Trained batch 950 batch loss 3.2397809 batch mAP 0.310546875 batch PCKh 0.318359375\n",
      "Trained batch 951 batch loss 3.05257416 batch mAP 0.265625 batch PCKh 0.48828125\n",
      "Trained batch 952 batch loss 3.1103723 batch mAP 0.30859375 batch PCKh 0.44921875\n",
      "Trained batch 953 batch loss 3.16813731 batch mAP 0.30859375 batch PCKh 0.4140625\n",
      "Trained batch 954 batch loss 3.08257771 batch mAP 0.2421875 batch PCKh 0.31640625\n",
      "Trained batch 955 batch loss 2.73942471 batch mAP 0.32421875 batch PCKh 0.275390625\n",
      "Trained batch 956 batch loss 2.5818305 batch mAP 0.3125 batch PCKh 0.19140625\n",
      "Trained batch 957 batch loss 2.57095051 batch mAP 0.37109375 batch PCKh 0.046875\n",
      "Trained batch 958 batch loss 2.74104381 batch mAP 0.28515625 batch PCKh 0.169921875\n",
      "Trained batch 959 batch loss 2.87941623 batch mAP 0.236328125 batch PCKh 0.353515625\n",
      "Trained batch 960 batch loss 3.17436528 batch mAP 0.29296875 batch PCKh 0.314453125\n",
      "Trained batch 961 batch loss 3.28759766 batch mAP 0.2734375 batch PCKh 0.3515625\n",
      "Trained batch 962 batch loss 3.09324765 batch mAP 0.2734375 batch PCKh 0.453125\n",
      "Trained batch 963 batch loss 3.22278666 batch mAP 0.271484375 batch PCKh 0.42578125\n",
      "Trained batch 964 batch loss 3.24996805 batch mAP 0.181640625 batch PCKh 0.208984375\n",
      "Trained batch 965 batch loss 3.22300935 batch mAP 0.255859375 batch PCKh 0.3828125\n",
      "Trained batch 966 batch loss 3.22372 batch mAP 0.306640625 batch PCKh 0.48828125\n",
      "Trained batch 967 batch loss 3.17653155 batch mAP 0.314453125 batch PCKh 0.4921875\n",
      "Trained batch 968 batch loss 3.51167822 batch mAP 0.298828125 batch PCKh 0.216796875\n",
      "Trained batch 969 batch loss 3.19727588 batch mAP 0.28515625 batch PCKh 0.51953125\n",
      "Trained batch 970 batch loss 3.36514354 batch mAP 0.24609375 batch PCKh 0.310546875\n",
      "Trained batch 971 batch loss 2.97928238 batch mAP 0.2578125 batch PCKh 0.5\n",
      "Trained batch 972 batch loss 3.02772379 batch mAP 0.337890625 batch PCKh 0.40625\n",
      "Trained batch 973 batch loss 3.26932049 batch mAP 0.353515625 batch PCKh 0.392578125\n",
      "Trained batch 974 batch loss 3.25716591 batch mAP 0.2734375 batch PCKh 0.43359375\n",
      "Trained batch 975 batch loss 3.30105686 batch mAP 0.296875 batch PCKh 0.41015625\n",
      "Trained batch 976 batch loss 3.23283315 batch mAP 0.353515625 batch PCKh 0.443359375\n",
      "Trained batch 977 batch loss 3.21092892 batch mAP 0.353515625 batch PCKh 0.509765625\n",
      "Trained batch 978 batch loss 3.35325265 batch mAP 0.353515625 batch PCKh 0.43359375\n",
      "Trained batch 979 batch loss 2.94004107 batch mAP 0.359375 batch PCKh 0.306640625\n",
      "Trained batch 980 batch loss 3.08003473 batch mAP 0.34375 batch PCKh 0.46875\n",
      "Trained batch 981 batch loss 3.09202719 batch mAP 0.294921875 batch PCKh 0.458984375\n",
      "Trained batch 982 batch loss 3.11251688 batch mAP 0.34375 batch PCKh 0.375\n",
      "Trained batch 983 batch loss 2.7553916 batch mAP 0.34375 batch PCKh 0.28515625\n",
      "Trained batch 984 batch loss 2.69224739 batch mAP 0.34375 batch PCKh 0.056640625\n",
      "Trained batch 985 batch loss 2.64035654 batch mAP 0.3515625 batch PCKh 0.126953125\n",
      "Trained batch 986 batch loss 2.92078638 batch mAP 0.240234375 batch PCKh 0.310546875\n",
      "Trained batch 987 batch loss 2.73006797 batch mAP 0.298828125 batch PCKh 0.21484375\n",
      "Trained batch 988 batch loss 3.30757666 batch mAP 0.291015625 batch PCKh 0.404296875\n",
      "Trained batch 989 batch loss 3.2784214 batch mAP 0.22265625 batch PCKh 0.330078125\n",
      "Trained batch 990 batch loss 3.10117388 batch mAP 0.212890625 batch PCKh 0.271484375\n",
      "Trained batch 991 batch loss 2.99531221 batch mAP 0.263671875 batch PCKh 0.181640625\n",
      "Trained batch 992 batch loss 3.15235472 batch mAP 0.25 batch PCKh 0.390625\n",
      "Trained batch 993 batch loss 3.03316188 batch mAP 0.275390625 batch PCKh 0.41015625\n",
      "Trained batch 994 batch loss 3.30579519 batch mAP 0.28515625 batch PCKh 0.39453125\n",
      "Trained batch 995 batch loss 3.21960449 batch mAP 0.2578125 batch PCKh 0.318359375\n",
      "Trained batch 996 batch loss 3.11696362 batch mAP 0.228515625 batch PCKh 0.42578125\n",
      "Trained batch 997 batch loss 3.05989194 batch mAP 0.251953125 batch PCKh 0.458984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 998 batch loss 2.98061728 batch mAP 0.365234375 batch PCKh 0.08203125\n",
      "Trained batch 999 batch loss 3.26743603 batch mAP 0.396484375 batch PCKh 0.26953125\n",
      "Trained batch 1000 batch loss 2.90461516 batch mAP 0.376953125 batch PCKh 0.353515625\n",
      "Trained batch 1001 batch loss 3.15444565 batch mAP 0.41796875 batch PCKh 0.3515625\n",
      "Trained batch 1002 batch loss 2.89605236 batch mAP 0.42578125 batch PCKh 0.216796875\n",
      "Trained batch 1003 batch loss 3.08180451 batch mAP 0.421875 batch PCKh 0.431640625\n",
      "Trained batch 1004 batch loss 3.08466768 batch mAP 0.365234375 batch PCKh 0.4765625\n",
      "Trained batch 1005 batch loss 3.36550784 batch mAP 0.396484375 batch PCKh 0.34765625\n",
      "Trained batch 1006 batch loss 3.12265539 batch mAP 0.4765625 batch PCKh 0.3515625\n",
      "Trained batch 1007 batch loss 2.56514406 batch mAP 0.419921875 batch PCKh 0.119140625\n",
      "Trained batch 1008 batch loss 2.59659624 batch mAP 0.39453125 batch PCKh 0.267578125\n",
      "Trained batch 1009 batch loss 2.96236086 batch mAP 0.345703125 batch PCKh 0.189453125\n",
      "Trained batch 1010 batch loss 3.10817599 batch mAP 0.34765625 batch PCKh 0.41015625\n",
      "Trained batch 1011 batch loss 3.14993787 batch mAP 0.384765625 batch PCKh 0.46484375\n",
      "Trained batch 1012 batch loss 3.14080453 batch mAP 0.388671875 batch PCKh 0.48828125\n",
      "Trained batch 1013 batch loss 3.03601384 batch mAP 0.458984375 batch PCKh 0.384765625\n",
      "Trained batch 1014 batch loss 3.16679668 batch mAP 0.4140625 batch PCKh 0.326171875\n",
      "Trained batch 1015 batch loss 3.1388464 batch mAP 0.46484375 batch PCKh 0.44140625\n",
      "Trained batch 1016 batch loss 2.97057509 batch mAP 0.46484375 batch PCKh 0.58203125\n",
      "Trained batch 1017 batch loss 2.97345543 batch mAP 0.46484375 batch PCKh 0.5546875\n",
      "Trained batch 1018 batch loss 2.76704502 batch mAP 0.44140625 batch PCKh 0.474609375\n",
      "Trained batch 1019 batch loss 3.12942123 batch mAP 0.392578125 batch PCKh 0.595703125\n",
      "Trained batch 1020 batch loss 3.00815415 batch mAP 0.365234375 batch PCKh 0.421875\n",
      "Trained batch 1021 batch loss 2.95630026 batch mAP 0.3984375 batch PCKh 0.38671875\n",
      "Trained batch 1022 batch loss 3.09284234 batch mAP 0.23828125 batch PCKh 0.2734375\n",
      "Trained batch 1023 batch loss 2.96086216 batch mAP 0.306640625 batch PCKh 0.474609375\n",
      "Trained batch 1024 batch loss 2.92842531 batch mAP 0.291015625 batch PCKh 0.203125\n",
      "Trained batch 1025 batch loss 2.7620182 batch mAP 0.23046875 batch PCKh 0.1171875\n",
      "Trained batch 1026 batch loss 3.04443359 batch mAP 0.275390625 batch PCKh 0.43359375\n",
      "Trained batch 1027 batch loss 2.88285708 batch mAP 0.240234375 batch PCKh 0.60546875\n",
      "Trained batch 1028 batch loss 2.87123179 batch mAP 0.310546875 batch PCKh 0.26953125\n",
      "Trained batch 1029 batch loss 3.21499562 batch mAP 0.169921875 batch PCKh 0.3515625\n",
      "Trained batch 1030 batch loss 3.2049818 batch mAP 0.23046875 batch PCKh 0.396484375\n",
      "Trained batch 1031 batch loss 2.97073364 batch mAP 0.232421875 batch PCKh 0.44921875\n",
      "Trained batch 1032 batch loss 3.27434111 batch mAP 0.224609375 batch PCKh 0.193359375\n",
      "Trained batch 1033 batch loss 3.70067406 batch mAP 0.240234375 batch PCKh 0.23828125\n",
      "Trained batch 1034 batch loss 3.33950233 batch mAP 0.23828125 batch PCKh 0.314453125\n",
      "Trained batch 1035 batch loss 3.27929354 batch mAP 0.216796875 batch PCKh 0.392578125\n",
      "Trained batch 1036 batch loss 3.46529555 batch mAP 0.3046875 batch PCKh 0.248046875\n",
      "Trained batch 1037 batch loss 3.29757857 batch mAP 0.291015625 batch PCKh 0.3828125\n",
      "Trained batch 1038 batch loss 3.09743404 batch mAP 0.27734375 batch PCKh 0.423828125\n",
      "Trained batch 1039 batch loss 3.20084286 batch mAP 0.279296875 batch PCKh 0.388671875\n",
      "Trained batch 1040 batch loss 3.2208972 batch mAP 0.2421875 batch PCKh 0.4453125\n",
      "Trained batch 1041 batch loss 2.72120929 batch mAP 0.26953125 batch PCKh 0.28125\n",
      "Trained batch 1042 batch loss 2.88354921 batch mAP 0.26953125 batch PCKh 0.728515625\n",
      "Trained batch 1043 batch loss 2.99207711 batch mAP 0.28515625 batch PCKh 0.447265625\n",
      "Trained batch 1044 batch loss 3.10315704 batch mAP 0.302734375 batch PCKh 0.498046875\n",
      "Trained batch 1045 batch loss 3.09418058 batch mAP 0.2734375 batch PCKh 0.5859375\n",
      "Trained batch 1046 batch loss 2.76172376 batch mAP 0.251953125 batch PCKh 0.267578125\n",
      "Trained batch 1047 batch loss 2.53852701 batch mAP 0.314453125 batch PCKh 0.177734375\n",
      "Trained batch 1048 batch loss 2.68098497 batch mAP 0.341796875 batch PCKh 0.162109375\n",
      "Trained batch 1049 batch loss 2.67680454 batch mAP 0.232421875 batch PCKh 0.15625\n",
      "Trained batch 1050 batch loss 2.75386882 batch mAP 0.236328125 batch PCKh 0.18359375\n",
      "Trained batch 1051 batch loss 3.17059565 batch mAP 0.22265625 batch PCKh 0.373046875\n",
      "Trained batch 1052 batch loss 3.0173974 batch mAP 0.255859375 batch PCKh 0.345703125\n",
      "Trained batch 1053 batch loss 2.78639889 batch mAP 0.169921875 batch PCKh 0.15234375\n",
      "Trained batch 1054 batch loss 2.81127095 batch mAP 0.193359375 batch PCKh 0.119140625\n",
      "Trained batch 1055 batch loss 2.59617043 batch mAP 0.171875 batch PCKh 0.33984375\n",
      "Trained batch 1056 batch loss 3.0235877 batch mAP 0.171875 batch PCKh 0.349609375\n",
      "Trained batch 1057 batch loss 2.99041176 batch mAP 0.212890625 batch PCKh 0.208984375\n",
      "Trained batch 1058 batch loss 3.06448984 batch mAP 0.1875 batch PCKh 0.232421875\n",
      "Trained batch 1059 batch loss 2.9120121 batch mAP 0.22265625 batch PCKh 0.39453125\n",
      "Trained batch 1060 batch loss 3.1037128 batch mAP 0.23828125 batch PCKh 0.14453125\n",
      "Trained batch 1061 batch loss 2.81118584 batch mAP 0.2890625 batch PCKh 0.40625\n",
      "Trained batch 1062 batch loss 2.99370098 batch mAP 0.3046875 batch PCKh 0.404296875\n",
      "Trained batch 1063 batch loss 3.00211072 batch mAP 0.1640625 batch PCKh 0.46484375\n",
      "Trained batch 1064 batch loss 2.92096472 batch mAP 0.220703125 batch PCKh 0.400390625\n",
      "Trained batch 1065 batch loss 3.05516291 batch mAP 0.208984375 batch PCKh 0.560546875\n",
      "Trained batch 1066 batch loss 2.95371962 batch mAP 0.23046875 batch PCKh 0.345703125\n",
      "Trained batch 1067 batch loss 3.00391817 batch mAP 0.173828125 batch PCKh 0.5\n",
      "Trained batch 1068 batch loss 3.03465652 batch mAP 0.1953125 batch PCKh 0.58203125\n",
      "Trained batch 1069 batch loss 3.30434132 batch mAP 0.21875 batch PCKh 0.283203125\n",
      "Trained batch 1070 batch loss 2.9669075 batch mAP 0.23828125 batch PCKh 0.470703125\n",
      "Trained batch 1071 batch loss 2.47727656 batch mAP 0.236328125 batch PCKh 0.181640625\n",
      "Trained batch 1072 batch loss 2.35262179 batch mAP 0.162109375 batch PCKh 0.08984375\n",
      "Trained batch 1073 batch loss 2.23428631 batch mAP 0.203125 batch PCKh 0.005859375\n",
      "Trained batch 1074 batch loss 2.27789068 batch mAP 0.205078125 batch PCKh 0.001953125\n",
      "Trained batch 1075 batch loss 2.4603188 batch mAP 0.1796875 batch PCKh 0.216796875\n",
      "Trained batch 1076 batch loss 2.85219383 batch mAP 0.162109375 batch PCKh 0.484375\n",
      "Trained batch 1077 batch loss 3.10751295 batch mAP 0.16015625 batch PCKh 0.427734375\n",
      "Trained batch 1078 batch loss 3.42259026 batch mAP 0.16796875 batch PCKh 0.337890625\n",
      "Trained batch 1079 batch loss 3.56510544 batch mAP 0.1953125 batch PCKh 0.236328125\n",
      "Trained batch 1080 batch loss 3.50732803 batch mAP 0.212890625 batch PCKh 0.28515625\n",
      "Trained batch 1081 batch loss 3.46072578 batch mAP 0.162109375 batch PCKh 0.2890625\n",
      "Trained batch 1082 batch loss 3.34950924 batch mAP 0.19140625 batch PCKh 0.26953125\n",
      "Trained batch 1083 batch loss 3.26254439 batch mAP 0.201171875 batch PCKh 0.33203125\n",
      "Trained batch 1084 batch loss 3.18893695 batch mAP 0.28515625 batch PCKh 0.333984375\n",
      "Trained batch 1085 batch loss 3.09703851 batch mAP 0.298828125 batch PCKh 0.482421875\n",
      "Trained batch 1086 batch loss 3.0485549 batch mAP 0.33203125 batch PCKh 0.416015625\n",
      "Trained batch 1087 batch loss 2.89132071 batch mAP 0.326171875 batch PCKh 0.712890625\n",
      "Trained batch 1088 batch loss 3.02419186 batch mAP 0.349609375 batch PCKh 0.53515625\n",
      "Trained batch 1089 batch loss 3.11271167 batch mAP 0.40234375 batch PCKh 0.349609375\n",
      "Trained batch 1090 batch loss 3.25304151 batch mAP 0.4140625 batch PCKh 0.474609375\n",
      "Trained batch 1091 batch loss 2.96693563 batch mAP 0.37890625 batch PCKh 0.271484375\n",
      "Trained batch 1092 batch loss 3.22707367 batch mAP 0.353515625 batch PCKh 0.3203125\n",
      "Trained batch 1093 batch loss 3.25760508 batch mAP 0.392578125 batch PCKh 0.359375\n",
      "Trained batch 1094 batch loss 3.25294113 batch mAP 0.2890625 batch PCKh 0.341796875\n",
      "Trained batch 1095 batch loss 3.17249584 batch mAP 0.3828125 batch PCKh 0.431640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1096 batch loss 3.25718355 batch mAP 0.376953125 batch PCKh 0.251953125\n",
      "Trained batch 1097 batch loss 3.42671347 batch mAP 0.380859375 batch PCKh 0.28515625\n",
      "Trained batch 1098 batch loss 3.43084121 batch mAP 0.359375 batch PCKh 0.2265625\n",
      "Trained batch 1099 batch loss 3.43298054 batch mAP 0.443359375 batch PCKh 0.24609375\n",
      "Trained batch 1100 batch loss 2.9023912 batch mAP 0.408203125 batch PCKh 0.171875\n",
      "Trained batch 1101 batch loss 2.77258563 batch mAP 0.4375 batch PCKh 0.20703125\n",
      "Trained batch 1102 batch loss 2.68870068 batch mAP 0.43359375 batch PCKh 0.10546875\n",
      "Trained batch 1103 batch loss 2.90842414 batch mAP 0.375 batch PCKh 0.3203125\n",
      "Trained batch 1104 batch loss 3.15527439 batch mAP 0.53125 batch PCKh 0.408203125\n",
      "Trained batch 1105 batch loss 2.79589128 batch mAP 0.384765625 batch PCKh 0.287109375\n",
      "Trained batch 1106 batch loss 3.04632878 batch mAP 0.470703125 batch PCKh 0.36328125\n",
      "Trained batch 1107 batch loss 3.09805918 batch mAP 0.546875 batch PCKh 0.509765625\n",
      "Trained batch 1108 batch loss 3.05057478 batch mAP 0.49609375 batch PCKh 0.447265625\n",
      "Trained batch 1109 batch loss 3.08440018 batch mAP 0.388671875 batch PCKh 0.48046875\n",
      "Trained batch 1110 batch loss 3.09511423 batch mAP 0.39453125 batch PCKh 0.384765625\n",
      "Trained batch 1111 batch loss 2.83853626 batch mAP 0.412109375 batch PCKh 0.439453125\n",
      "Trained batch 1112 batch loss 2.96208382 batch mAP 0.318359375 batch PCKh 0.4765625\n",
      "Trained batch 1113 batch loss 3.11674356 batch mAP 0.361328125 batch PCKh 0.466796875\n",
      "Trained batch 1114 batch loss 3.0557797 batch mAP 0.333984375 batch PCKh 0.4453125\n",
      "Trained batch 1115 batch loss 3.10305643 batch mAP 0.3359375 batch PCKh 0.37109375\n",
      "Trained batch 1116 batch loss 3.01397038 batch mAP 0.279296875 batch PCKh 0.404296875\n",
      "Trained batch 1117 batch loss 2.91590571 batch mAP 0.25 batch PCKh 0.552734375\n",
      "Trained batch 1118 batch loss 2.88398361 batch mAP 0.23828125 batch PCKh 0.48828125\n",
      "Trained batch 1119 batch loss 2.91157317 batch mAP 0.24609375 batch PCKh 0.541015625\n",
      "Trained batch 1120 batch loss 3.14649582 batch mAP 0.18359375 batch PCKh 0.34375\n",
      "Trained batch 1121 batch loss 3.01584888 batch mAP 0.150390625 batch PCKh 0.435546875\n",
      "Trained batch 1122 batch loss 3.15360522 batch mAP 0.1875 batch PCKh 0.509765625\n",
      "Trained batch 1123 batch loss 2.94911146 batch mAP 0.169921875 batch PCKh 0.462890625\n",
      "Trained batch 1124 batch loss 2.99923825 batch mAP 0.181640625 batch PCKh 0.34765625\n",
      "Trained batch 1125 batch loss 2.96376371 batch mAP 0.173828125 batch PCKh 0.482421875\n",
      "Trained batch 1126 batch loss 2.99337673 batch mAP 0.1328125 batch PCKh 0.451171875\n",
      "Trained batch 1127 batch loss 2.81762791 batch mAP 0.169921875 batch PCKh 0.501953125\n",
      "Trained batch 1128 batch loss 2.9738543 batch mAP 0.15625 batch PCKh 0.208984375\n",
      "Trained batch 1129 batch loss 3.16284418 batch mAP 0.166015625 batch PCKh 0.369140625\n",
      "Trained batch 1130 batch loss 3.2182765 batch mAP 0.173828125 batch PCKh 0.330078125\n",
      "Trained batch 1131 batch loss 3.20809317 batch mAP 0.19140625 batch PCKh 0.40234375\n",
      "Trained batch 1132 batch loss 3.04283524 batch mAP 0.185546875 batch PCKh 0.44140625\n",
      "Trained batch 1133 batch loss 3.11821842 batch mAP 0.2109375 batch PCKh 0.4609375\n",
      "Trained batch 1134 batch loss 3.18541455 batch mAP 0.23046875 batch PCKh 0.30859375\n",
      "Trained batch 1135 batch loss 3.18147659 batch mAP 0.18359375 batch PCKh 0.41015625\n",
      "Trained batch 1136 batch loss 3.27412605 batch mAP 0.125 batch PCKh 0.490234375\n",
      "Trained batch 1137 batch loss 3.11206937 batch mAP 0.2109375 batch PCKh 0.53515625\n",
      "Trained batch 1138 batch loss 3.11328459 batch mAP 0.177734375 batch PCKh 0.40234375\n",
      "Trained batch 1139 batch loss 2.93534017 batch mAP 0.1875 batch PCKh 0.537109375\n",
      "Trained batch 1140 batch loss 3.04389381 batch mAP 0.197265625 batch PCKh 0.455078125\n",
      "Trained batch 1141 batch loss 3.0093205 batch mAP 0.181640625 batch PCKh 0.435546875\n",
      "Trained batch 1142 batch loss 3.1978128 batch mAP 0.212890625 batch PCKh 0.50390625\n",
      "Trained batch 1143 batch loss 2.98720837 batch mAP 0.181640625 batch PCKh 0.62890625\n",
      "Trained batch 1144 batch loss 3.11581039 batch mAP 0.173828125 batch PCKh 0.419921875\n",
      "Trained batch 1145 batch loss 3.31638789 batch mAP 0.23828125 batch PCKh 0.435546875\n",
      "Trained batch 1146 batch loss 3.07677174 batch mAP 0.20703125 batch PCKh 0.48828125\n",
      "Trained batch 1147 batch loss 3.04130459 batch mAP 0.1875 batch PCKh 0.591796875\n",
      "Trained batch 1148 batch loss 2.88480234 batch mAP 0.171875 batch PCKh 0.638671875\n",
      "Trained batch 1149 batch loss 2.90924168 batch mAP 0.123046875 batch PCKh 0.640625\n",
      "Trained batch 1150 batch loss 2.97862959 batch mAP 0.15625 batch PCKh 0.544921875\n",
      "Trained batch 1151 batch loss 3.099473 batch mAP 0.12890625 batch PCKh 0.55859375\n",
      "Trained batch 1152 batch loss 3.06535244 batch mAP 0.142578125 batch PCKh 0.341796875\n",
      "Trained batch 1153 batch loss 2.98859453 batch mAP 0.142578125 batch PCKh 0.4765625\n",
      "Trained batch 1154 batch loss 3.04740238 batch mAP 0.228515625 batch PCKh 0.404296875\n",
      "Trained batch 1155 batch loss 2.8225646 batch mAP 0.177734375 batch PCKh 0.44140625\n",
      "Trained batch 1156 batch loss 3.08739352 batch mAP 0.15234375 batch PCKh 0.517578125\n",
      "Trained batch 1157 batch loss 3.07286048 batch mAP 0.146484375 batch PCKh 0.39453125\n",
      "Trained batch 1158 batch loss 3.11443377 batch mAP 0.20703125 batch PCKh 0.46875\n",
      "Trained batch 1159 batch loss 2.86465 batch mAP 0.1875 batch PCKh 0.396484375\n",
      "Trained batch 1160 batch loss 2.98713255 batch mAP 0.189453125 batch PCKh 0.455078125\n",
      "Trained batch 1161 batch loss 3.0641737 batch mAP 0.20703125 batch PCKh 0.330078125\n",
      "Trained batch 1162 batch loss 3.05938196 batch mAP 0.21484375 batch PCKh 0.30859375\n",
      "Trained batch 1163 batch loss 3.20224166 batch mAP 0.2109375 batch PCKh 0.451171875\n",
      "Trained batch 1164 batch loss 3.15035439 batch mAP 0.16796875 batch PCKh 0.439453125\n",
      "Trained batch 1165 batch loss 3.16741419 batch mAP 0.162109375 batch PCKh 0.462890625\n",
      "Trained batch 1166 batch loss 3.1624949 batch mAP 0.2265625 batch PCKh 0.416015625\n",
      "Trained batch 1167 batch loss 3.02678204 batch mAP 0.21484375 batch PCKh 0.166015625\n",
      "Trained batch 1168 batch loss 2.99492598 batch mAP 0.248046875 batch PCKh 0.306640625\n",
      "Trained batch 1169 batch loss 3.17454028 batch mAP 0.220703125 batch PCKh 0.5078125\n",
      "Trained batch 1170 batch loss 3.11040258 batch mAP 0.22265625 batch PCKh 0.564453125\n",
      "Trained batch 1171 batch loss 2.97415972 batch mAP 0.2421875 batch PCKh 0.349609375\n",
      "Trained batch 1172 batch loss 3.10956287 batch mAP 0.224609375 batch PCKh 0.251953125\n",
      "Trained batch 1173 batch loss 2.88617182 batch mAP 0.19921875 batch PCKh 0.46875\n",
      "Trained batch 1174 batch loss 2.65780163 batch mAP 0.166015625 batch PCKh 0.59375\n",
      "Trained batch 1175 batch loss 3.4842515 batch mAP 0.236328125 batch PCKh 0.263671875\n",
      "Trained batch 1176 batch loss 3.29770041 batch mAP 0.271484375 batch PCKh 0.28515625\n",
      "Trained batch 1177 batch loss 3.27685547 batch mAP 0.232421875 batch PCKh 0.443359375\n",
      "Trained batch 1178 batch loss 3.259588 batch mAP 0.265625 batch PCKh 0.37109375\n",
      "Trained batch 1179 batch loss 3.27509141 batch mAP 0.32421875 batch PCKh 0.529296875\n",
      "Trained batch 1180 batch loss 3.26322818 batch mAP 0.34765625 batch PCKh 0.37109375\n",
      "Trained batch 1181 batch loss 3.13998842 batch mAP 0.328125 batch PCKh 0.365234375\n",
      "Trained batch 1182 batch loss 3.36293578 batch mAP 0.34375 batch PCKh 0.310546875\n",
      "Trained batch 1183 batch loss 3.29153562 batch mAP 0.38671875 batch PCKh 0.359375\n",
      "Trained batch 1184 batch loss 3.39391327 batch mAP 0.400390625 batch PCKh 0.208984375\n",
      "Trained batch 1185 batch loss 3.24236321 batch mAP 0.376953125 batch PCKh 0.33984375\n",
      "Trained batch 1186 batch loss 2.94814134 batch mAP 0.3359375 batch PCKh 0.3984375\n",
      "Trained batch 1187 batch loss 2.83704805 batch mAP 0.35546875 batch PCKh 0.615234375\n",
      "Trained batch 1188 batch loss 2.79771233 batch mAP 0.560546875 batch PCKh 0.67578125\n",
      "Trained batch 1189 batch loss 2.80090189 batch mAP 0.49609375 batch PCKh 0.708984375\n",
      "Trained batch 1190 batch loss 3.08545065 batch mAP 0.48046875 batch PCKh 0.478515625\n",
      "Trained batch 1191 batch loss 3.14354229 batch mAP 0.353515625 batch PCKh 0.20703125\n",
      "Trained batch 1192 batch loss 3.14631605 batch mAP 0.333984375 batch PCKh 0.46484375\n",
      "Trained batch 1193 batch loss 3.3382163 batch mAP 0.37109375 batch PCKh 0.439453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1194 batch loss 3.02881217 batch mAP 0.32421875 batch PCKh 0.142578125\n",
      "Trained batch 1195 batch loss 2.98875904 batch mAP 0.287109375 batch PCKh 0.29296875\n",
      "Trained batch 1196 batch loss 2.86023712 batch mAP 0.349609375 batch PCKh 0.509765625\n",
      "Trained batch 1197 batch loss 3.34440804 batch mAP 0.302734375 batch PCKh 0.279296875\n",
      "Trained batch 1198 batch loss 3.30440187 batch mAP 0.26171875 batch PCKh 0.32421875\n",
      "Trained batch 1199 batch loss 3.08896589 batch mAP 0.3203125 batch PCKh 0.326171875\n",
      "Trained batch 1200 batch loss 3.08434153 batch mAP 0.2890625 batch PCKh 0.14453125\n",
      "Trained batch 1201 batch loss 3.00541067 batch mAP 0.328125 batch PCKh 0.296875\n",
      "Trained batch 1202 batch loss 3.15094805 batch mAP 0.306640625 batch PCKh 0.376953125\n",
      "Trained batch 1203 batch loss 3.21525478 batch mAP 0.275390625 batch PCKh 0.302734375\n",
      "Trained batch 1204 batch loss 3.03266907 batch mAP 0.26171875 batch PCKh 0.36328125\n",
      "Trained batch 1205 batch loss 3.20770407 batch mAP 0.37109375 batch PCKh 0.38671875\n",
      "Trained batch 1206 batch loss 3.41885114 batch mAP 0.380859375 batch PCKh 0.357421875\n",
      "Trained batch 1207 batch loss 3.52099776 batch mAP 0.37109375 batch PCKh 0.28125\n",
      "Trained batch 1208 batch loss 3.25789833 batch mAP 0.482421875 batch PCKh 0.484375\n",
      "Trained batch 1209 batch loss 2.96317816 batch mAP 0.3671875 batch PCKh 0.447265625\n",
      "Trained batch 1210 batch loss 3.23613358 batch mAP 0.435546875 batch PCKh 0.46484375\n",
      "Trained batch 1211 batch loss 3.28951621 batch mAP 0.55859375 batch PCKh 0.400390625\n",
      "Trained batch 1212 batch loss 3.32462168 batch mAP 0.53125 batch PCKh 0.48046875\n",
      "Trained batch 1213 batch loss 3.09404516 batch mAP 0.41015625 batch PCKh 0.609375\n",
      "Trained batch 1214 batch loss 3.11475897 batch mAP 0.576171875 batch PCKh 0.533203125\n",
      "Trained batch 1215 batch loss 3.15824628 batch mAP 0.5390625 batch PCKh 0.373046875\n",
      "Trained batch 1216 batch loss 3.01390743 batch mAP 0.474609375 batch PCKh 0.439453125\n",
      "Trained batch 1217 batch loss 3.14732528 batch mAP 0.498046875 batch PCKh 0.552734375\n",
      "Trained batch 1218 batch loss 3.07495666 batch mAP 0.419921875 batch PCKh 0.375\n",
      "Trained batch 1219 batch loss 3.19261575 batch mAP 0.427734375 batch PCKh 0.404296875\n",
      "Trained batch 1220 batch loss 3.16830087 batch mAP 0.3671875 batch PCKh 0.302734375\n",
      "Trained batch 1221 batch loss 3.36202216 batch mAP 0.43359375 batch PCKh 0.2421875\n",
      "Trained batch 1222 batch loss 3.04431343 batch mAP 0.244140625 batch PCKh 0.337890625\n",
      "Trained batch 1223 batch loss 3.06052017 batch mAP 0.34375 batch PCKh 0.4453125\n",
      "Trained batch 1224 batch loss 2.86929512 batch mAP 0.35546875 batch PCKh 0.37109375\n",
      "Trained batch 1225 batch loss 2.89378643 batch mAP 0.2578125 batch PCKh 0.458984375\n",
      "Trained batch 1226 batch loss 3.07547092 batch mAP 0.251953125 batch PCKh 0.4765625\n",
      "Trained batch 1227 batch loss 3.05723238 batch mAP 0.26171875 batch PCKh 0.33984375\n",
      "Trained batch 1228 batch loss 2.91043878 batch mAP 0.255859375 batch PCKh 0.416015625\n",
      "Trained batch 1229 batch loss 2.83975816 batch mAP 0.201171875 batch PCKh 0.40625\n",
      "Trained batch 1230 batch loss 2.86061907 batch mAP 0.208984375 batch PCKh 0.33984375\n",
      "Trained batch 1231 batch loss 3.11880779 batch mAP 0.271484375 batch PCKh 0.30078125\n",
      "Trained batch 1232 batch loss 3.15576386 batch mAP 0.296875 batch PCKh 0.41015625\n",
      "Trained batch 1233 batch loss 3.08452606 batch mAP 0.310546875 batch PCKh 0.494140625\n",
      "Trained batch 1234 batch loss 3.15720725 batch mAP 0.310546875 batch PCKh 0.494140625\n",
      "Trained batch 1235 batch loss 2.97907543 batch mAP 0.244140625 batch PCKh 0.40234375\n",
      "Trained batch 1236 batch loss 2.74715352 batch mAP 0.162109375 batch PCKh 0.25390625\n",
      "Trained batch 1237 batch loss 2.87090421 batch mAP 0.216796875 batch PCKh 0.2734375\n",
      "Trained batch 1238 batch loss 2.97169805 batch mAP 0.20703125 batch PCKh 0.400390625\n",
      "Trained batch 1239 batch loss 3.03834295 batch mAP 0.27734375 batch PCKh 0.69140625\n",
      "Trained batch 1240 batch loss 2.93425727 batch mAP 0.24609375 batch PCKh 0.494140625\n",
      "Trained batch 1241 batch loss 2.99929762 batch mAP 0.201171875 batch PCKh 0.58203125\n",
      "Trained batch 1242 batch loss 2.79539871 batch mAP 0.25390625 batch PCKh 0.458984375\n",
      "Trained batch 1243 batch loss 3.22012067 batch mAP 0.193359375 batch PCKh 0.48828125\n",
      "Trained batch 1244 batch loss 2.83249378 batch mAP 0.291015625 batch PCKh 0.431640625\n",
      "Trained batch 1245 batch loss 2.86803436 batch mAP 0.19921875 batch PCKh 0.349609375\n",
      "Trained batch 1246 batch loss 2.91051483 batch mAP 0.19140625 batch PCKh 0.189453125\n",
      "Trained batch 1247 batch loss 2.90802479 batch mAP 0.23828125 batch PCKh 0.234375\n",
      "Trained batch 1248 batch loss 2.82991505 batch mAP 0.263671875 batch PCKh 0.20703125\n",
      "Trained batch 1249 batch loss 2.89688349 batch mAP 0.279296875 batch PCKh 0.3984375\n",
      "Trained batch 1250 batch loss 2.71883535 batch mAP 0.26171875 batch PCKh 0.390625\n",
      "Trained batch 1251 batch loss 2.94104528 batch mAP 0.248046875 batch PCKh 0.48828125\n",
      "Trained batch 1252 batch loss 3.01053476 batch mAP 0.240234375 batch PCKh 0.490234375\n",
      "Trained batch 1253 batch loss 2.72055101 batch mAP 0.248046875 batch PCKh 0.314453125\n",
      "Trained batch 1254 batch loss 2.82121015 batch mAP 0.255859375 batch PCKh 0.349609375\n",
      "Trained batch 1255 batch loss 3.03286242 batch mAP 0.31640625 batch PCKh 0.388671875\n",
      "Trained batch 1256 batch loss 2.96138883 batch mAP 0.205078125 batch PCKh 0.396484375\n",
      "Trained batch 1257 batch loss 3.20287585 batch mAP 0.142578125 batch PCKh 0.42578125\n",
      "Trained batch 1258 batch loss 3.02213526 batch mAP 0.197265625 batch PCKh 0.47265625\n",
      "Trained batch 1259 batch loss 3.21687055 batch mAP 0.24609375 batch PCKh 0.412109375\n",
      "Trained batch 1260 batch loss 3.25868511 batch mAP 0.2109375 batch PCKh 0.333984375\n",
      "Trained batch 1261 batch loss 3.12013912 batch mAP 0.1953125 batch PCKh 0.513671875\n",
      "Trained batch 1262 batch loss 3.12301493 batch mAP 0.138671875 batch PCKh 0.458984375\n",
      "Trained batch 1263 batch loss 2.93610501 batch mAP 0.1640625 batch PCKh 0.4765625\n",
      "Trained batch 1264 batch loss 2.70114636 batch mAP 0.177734375 batch PCKh 0.546875\n",
      "Trained batch 1265 batch loss 2.83596 batch mAP 0.146484375 batch PCKh 0.4140625\n",
      "Trained batch 1266 batch loss 2.80708 batch mAP 0.15234375 batch PCKh 0.541015625\n",
      "Trained batch 1267 batch loss 2.82404757 batch mAP 0.162109375 batch PCKh 0.61328125\n",
      "Trained batch 1268 batch loss 3.04140306 batch mAP 0.158203125 batch PCKh 0.505859375\n",
      "Trained batch 1269 batch loss 3.00663781 batch mAP 0.171875 batch PCKh 0.515625\n",
      "Trained batch 1270 batch loss 3.06018734 batch mAP 0.146484375 batch PCKh 0.455078125\n",
      "Trained batch 1271 batch loss 2.89542747 batch mAP 0.16015625 batch PCKh 0.583984375\n",
      "Trained batch 1272 batch loss 2.76597643 batch mAP 0.1640625 batch PCKh 0.486328125\n",
      "Trained batch 1273 batch loss 2.87406301 batch mAP 0.14453125 batch PCKh 0.541015625\n",
      "Trained batch 1274 batch loss 2.84148169 batch mAP 0.138671875 batch PCKh 0.486328125\n",
      "Trained batch 1275 batch loss 3.23268318 batch mAP 0.12890625 batch PCKh 0.365234375\n",
      "Trained batch 1276 batch loss 3.18865776 batch mAP 0.12890625 batch PCKh 0.212890625\n",
      "Trained batch 1277 batch loss 2.58250666 batch mAP 0.115234375 batch PCKh 0.275390625\n",
      "Trained batch 1278 batch loss 2.92033839 batch mAP 0.1875 batch PCKh 0.53125\n",
      "Trained batch 1279 batch loss 3.19960546 batch mAP 0.13671875 batch PCKh 0.455078125\n",
      "Trained batch 1280 batch loss 2.94072723 batch mAP 0.1484375 batch PCKh 0.662109375\n",
      "Trained batch 1281 batch loss 2.98602104 batch mAP 0.140625 batch PCKh 0.45703125\n",
      "Trained batch 1282 batch loss 3.15056348 batch mAP 0.1171875 batch PCKh 0.43359375\n",
      "Trained batch 1283 batch loss 3.12092829 batch mAP 0.134765625 batch PCKh 0.470703125\n",
      "Trained batch 1284 batch loss 3.13847303 batch mAP 0.154296875 batch PCKh 0.46484375\n",
      "Trained batch 1285 batch loss 3.19294763 batch mAP 0.12109375 batch PCKh 0.501953125\n",
      "Trained batch 1286 batch loss 3.43964052 batch mAP 0.1875 batch PCKh 0.2265625\n",
      "Trained batch 1287 batch loss 3.24446726 batch mAP 0.1328125 batch PCKh 0.32421875\n",
      "Trained batch 1288 batch loss 3.25665665 batch mAP 0.236328125 batch PCKh 0.154296875\n",
      "Trained batch 1289 batch loss 3.4381988 batch mAP 0.224609375 batch PCKh 0.177734375\n",
      "Trained batch 1290 batch loss 3.06413031 batch mAP 0.244140625 batch PCKh 0.439453125\n",
      "Trained batch 1291 batch loss 2.90050364 batch mAP 0.216796875 batch PCKh 0.267578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1292 batch loss 2.59539938 batch mAP 0.267578125 batch PCKh 0.205078125\n",
      "Trained batch 1293 batch loss 2.449332 batch mAP 0.203125 batch PCKh 0.107421875\n",
      "Trained batch 1294 batch loss 2.73450279 batch mAP 0.236328125 batch PCKh 0.076171875\n",
      "Trained batch 1295 batch loss 3.20344782 batch mAP 0.21484375 batch PCKh 0.283203125\n",
      "Trained batch 1296 batch loss 3.1530726 batch mAP 0.224609375 batch PCKh 0.203125\n",
      "Trained batch 1297 batch loss 3.29405379 batch mAP 0.265625 batch PCKh 0.41796875\n",
      "Trained batch 1298 batch loss 3.38388491 batch mAP 0.3203125 batch PCKh 0.26171875\n",
      "Trained batch 1299 batch loss 3.08823967 batch mAP 0.349609375 batch PCKh 0.3125\n",
      "Trained batch 1300 batch loss 3.11820793 batch mAP 0.275390625 batch PCKh 0.521484375\n",
      "Trained batch 1301 batch loss 3.21312714 batch mAP 0.333984375 batch PCKh 0.4375\n",
      "Trained batch 1302 batch loss 3.5604136 batch mAP 0.25 batch PCKh 0.3046875\n",
      "Trained batch 1303 batch loss 3.3747983 batch mAP 0.310546875 batch PCKh 0.537109375\n",
      "Trained batch 1304 batch loss 3.37980056 batch mAP 0.3203125 batch PCKh 0.177734375\n",
      "Trained batch 1305 batch loss 3.44714022 batch mAP 0.34375 batch PCKh 0.259765625\n",
      "Trained batch 1306 batch loss 3.07447052 batch mAP 0.412109375 batch PCKh 0.609375\n",
      "Trained batch 1307 batch loss 3.11744523 batch mAP 0.3984375 batch PCKh 0.384765625\n",
      "Trained batch 1308 batch loss 3.09261107 batch mAP 0.416015625 batch PCKh 0.279296875\n",
      "Trained batch 1309 batch loss 3.21141648 batch mAP 0.33984375 batch PCKh 0.31640625\n",
      "Trained batch 1310 batch loss 3.25236797 batch mAP 0.40625 batch PCKh 0.3046875\n",
      "Trained batch 1311 batch loss 3.15927935 batch mAP 0.43359375 batch PCKh 0.44921875\n",
      "Trained batch 1312 batch loss 3.13167572 batch mAP 0.501953125 batch PCKh 0.1875\n",
      "Trained batch 1313 batch loss 3.05141258 batch mAP 0.494140625 batch PCKh 0.361328125\n",
      "Trained batch 1314 batch loss 2.95843482 batch mAP 0.427734375 batch PCKh 0.521484375\n",
      "Trained batch 1315 batch loss 2.90999365 batch mAP 0.310546875 batch PCKh 0.416015625\n",
      "Trained batch 1316 batch loss 3.02802277 batch mAP 0.419921875 batch PCKh 0.44921875\n",
      "Trained batch 1317 batch loss 2.98343396 batch mAP 0.330078125 batch PCKh 0.45703125\n",
      "Trained batch 1318 batch loss 3.11851788 batch mAP 0.41796875 batch PCKh 0.30859375\n",
      "Trained batch 1319 batch loss 2.89478111 batch mAP 0.373046875 batch PCKh 0.576171875\n",
      "Trained batch 1320 batch loss 2.96009135 batch mAP 0.3046875 batch PCKh 0.484375\n",
      "Trained batch 1321 batch loss 2.7653203 batch mAP 0.19140625 batch PCKh 0.54296875\n",
      "Trained batch 1322 batch loss 3.23783112 batch mAP 0.35546875 batch PCKh 0.34375\n",
      "Trained batch 1323 batch loss 3.00321174 batch mAP 0.359375 batch PCKh 0.43359375\n",
      "Trained batch 1324 batch loss 3.02973986 batch mAP 0.2890625 batch PCKh 0.62890625\n",
      "Trained batch 1325 batch loss 3.21169281 batch mAP 0.205078125 batch PCKh 0.421875\n",
      "Trained batch 1326 batch loss 3.2197032 batch mAP 0.216796875 batch PCKh 0.435546875\n",
      "Trained batch 1327 batch loss 3.1384716 batch mAP 0.16796875 batch PCKh 0.310546875\n",
      "Trained batch 1328 batch loss 3.19617701 batch mAP 0.22265625 batch PCKh 0.46875\n",
      "Trained batch 1329 batch loss 3.13872504 batch mAP 0.12890625 batch PCKh 0.244140625\n",
      "Trained batch 1330 batch loss 3.11851311 batch mAP 0.1640625 batch PCKh 0.587890625\n",
      "Trained batch 1331 batch loss 3.24588609 batch mAP 0.189453125 batch PCKh 0.267578125\n",
      "Trained batch 1332 batch loss 3.22784686 batch mAP 0.220703125 batch PCKh 0.431640625\n",
      "Trained batch 1333 batch loss 3.02605319 batch mAP 0.162109375 batch PCKh 0.486328125\n",
      "Trained batch 1334 batch loss 3.02768159 batch mAP 0.2109375 batch PCKh 0.525390625\n",
      "Trained batch 1335 batch loss 2.89523888 batch mAP 0.185546875 batch PCKh 0.55859375\n",
      "Trained batch 1336 batch loss 2.94112587 batch mAP 0.146484375 batch PCKh 0.490234375\n",
      "Trained batch 1337 batch loss 2.99212289 batch mAP 0.20703125 batch PCKh 0.310546875\n",
      "Trained batch 1338 batch loss 2.88697815 batch mAP 0.236328125 batch PCKh 0.333984375\n",
      "Trained batch 1339 batch loss 2.95212698 batch mAP 0.181640625 batch PCKh 0.234375\n",
      "Trained batch 1340 batch loss 2.78532 batch mAP 0.17578125 batch PCKh 0.34765625\n",
      "Trained batch 1341 batch loss 3.04540777 batch mAP 0.23046875 batch PCKh 0.3046875\n",
      "Trained batch 1342 batch loss 3.04264593 batch mAP 0.208984375 batch PCKh 0.265625\n",
      "Trained batch 1343 batch loss 2.85661936 batch mAP 0.234375 batch PCKh 0.3828125\n",
      "Trained batch 1344 batch loss 2.96523476 batch mAP 0.1875 batch PCKh 0.359375\n",
      "Trained batch 1345 batch loss 2.83214641 batch mAP 0.185546875 batch PCKh 0.52734375\n",
      "Trained batch 1346 batch loss 2.88798714 batch mAP 0.236328125 batch PCKh 0.49609375\n",
      "Trained batch 1347 batch loss 2.94496822 batch mAP 0.1640625 batch PCKh 0.49609375\n",
      "Trained batch 1348 batch loss 3.04654694 batch mAP 0.244140625 batch PCKh 0.5234375\n",
      "Trained batch 1349 batch loss 2.78707242 batch mAP 0.255859375 batch PCKh 0.513671875\n",
      "Trained batch 1350 batch loss 2.97032046 batch mAP 0.205078125 batch PCKh 0.642578125\n",
      "Trained batch 1351 batch loss 2.86357737 batch mAP 0.185546875 batch PCKh 0.7109375\n",
      "Trained batch 1352 batch loss 2.8762188 batch mAP 0.15625 batch PCKh 0.568359375\n",
      "Trained batch 1353 batch loss 3.16764545 batch mAP 0.1015625 batch PCKh 0.48046875\n",
      "Trained batch 1354 batch loss 2.99729705 batch mAP 0.171875 batch PCKh 0.3515625\n",
      "Trained batch 1355 batch loss 3.19627285 batch mAP 0.138671875 batch PCKh 0.333984375\n",
      "Trained batch 1356 batch loss 3.00855684 batch mAP 0.140625 batch PCKh 0.36328125\n",
      "Trained batch 1357 batch loss 3.15323019 batch mAP 0.150390625 batch PCKh 0.330078125\n",
      "Trained batch 1358 batch loss 3.02935266 batch mAP 0.07421875 batch PCKh 0.4296875\n",
      "Trained batch 1359 batch loss 2.74720621 batch mAP 0.07421875 batch PCKh 0.337890625\n",
      "Trained batch 1360 batch loss 3.05790281 batch mAP 0.060546875 batch PCKh 0.4765625\n",
      "Trained batch 1361 batch loss 2.69610214 batch mAP 0.103515625 batch PCKh 0.19140625\n",
      "Trained batch 1362 batch loss 2.75075912 batch mAP 0.171875 batch PCKh 0.203125\n",
      "Trained batch 1363 batch loss 2.73106122 batch mAP 0.158203125 batch PCKh 0.396484375\n",
      "Trained batch 1364 batch loss 2.74054289 batch mAP 0.119140625 batch PCKh 0.158203125\n",
      "Trained batch 1365 batch loss 2.93316 batch mAP 0.134765625 batch PCKh 0.306640625\n",
      "Trained batch 1366 batch loss 2.83046293 batch mAP 0.193359375 batch PCKh 0.634765625\n",
      "Trained batch 1367 batch loss 2.92541528 batch mAP 0.111328125 batch PCKh 0.48828125\n",
      "Trained batch 1368 batch loss 3.00442123 batch mAP 0.1484375 batch PCKh 0.529296875\n",
      "Trained batch 1369 batch loss 3.08129883 batch mAP 0.1875 batch PCKh 0.29296875\n",
      "Trained batch 1370 batch loss 2.97376633 batch mAP 0.125 batch PCKh 0.265625\n",
      "Trained batch 1371 batch loss 3.25934076 batch mAP 0.126953125 batch PCKh 0.43359375\n",
      "Trained batch 1372 batch loss 3.0852356 batch mAP 0.140625 batch PCKh 0.408203125\n",
      "Trained batch 1373 batch loss 2.98086 batch mAP 0.1640625 batch PCKh 0.5625\n",
      "Trained batch 1374 batch loss 2.98276711 batch mAP 0.16015625 batch PCKh 0.59375\n",
      "Trained batch 1375 batch loss 2.99874234 batch mAP 0.2109375 batch PCKh 0.486328125\n",
      "Trained batch 1376 batch loss 2.85035372 batch mAP 0.1796875 batch PCKh 0.37109375\n",
      "Trained batch 1377 batch loss 3.0217483 batch mAP 0.193359375 batch PCKh 0.5078125\n",
      "Trained batch 1378 batch loss 2.98599362 batch mAP 0.224609375 batch PCKh 0.603515625\n",
      "Trained batch 1379 batch loss 2.85887742 batch mAP 0.224609375 batch PCKh 0.255859375\n",
      "Trained batch 1380 batch loss 3.08187532 batch mAP 0.181640625 batch PCKh 0.416015625\n",
      "Trained batch 1381 batch loss 2.91798735 batch mAP 0.216796875 batch PCKh 0.51953125\n",
      "Trained batch 1382 batch loss 2.93917847 batch mAP 0.2265625 batch PCKh 0.373046875\n",
      "Trained batch 1383 batch loss 2.85334182 batch mAP 0.17578125 batch PCKh 0.38671875\n",
      "Trained batch 1384 batch loss 2.96831179 batch mAP 0.18359375 batch PCKh 0.3828125\n",
      "Trained batch 1385 batch loss 2.901968 batch mAP 0.1484375 batch PCKh 0.39453125\n",
      "Trained batch 1386 batch loss 2.82448483 batch mAP 0.16015625 batch PCKh 0.466796875\n",
      "Trained batch 1387 batch loss 3.28449559 batch mAP 0.193359375 batch PCKh 0.396484375\n",
      "Trained batch 1388 batch loss 3.38397312 batch mAP 0.236328125 batch PCKh 0.3671875\n",
      "Trained batch 1389 batch loss 3.27234125 batch mAP 0.142578125 batch PCKh 0.423828125\n",
      "Trained batch 1390 batch loss 2.9553988 batch mAP 0.19140625 batch PCKh 0.37109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1391 batch loss 3.08270288 batch mAP 0.16796875 batch PCKh 0.6015625\n",
      "Trained batch 1392 batch loss 3.14313221 batch mAP 0.216796875 batch PCKh 0.5859375\n",
      "Trained batch 1393 batch loss 3.50666785 batch mAP 0.203125 batch PCKh 0.3828125\n",
      "Trained batch 1394 batch loss 3.26342106 batch mAP 0.212890625 batch PCKh 0.4140625\n",
      "Trained batch 1395 batch loss 2.92547131 batch mAP 0.2109375 batch PCKh 0.376953125\n",
      "Trained batch 1396 batch loss 3.03503275 batch mAP 0.24609375 batch PCKh 0.537109375\n",
      "Trained batch 1397 batch loss 3.25893474 batch mAP 0.259765625 batch PCKh 0.373046875\n",
      "Trained batch 1398 batch loss 3.13772392 batch mAP 0.251953125 batch PCKh 0.412109375\n",
      "Trained batch 1399 batch loss 3.36626935 batch mAP 0.26953125 batch PCKh 0.21484375\n",
      "Trained batch 1400 batch loss 3.10165548 batch mAP 0.208984375 batch PCKh 0.2890625\n",
      "Trained batch 1401 batch loss 3.13300848 batch mAP 0.267578125 batch PCKh 0.49609375\n",
      "Trained batch 1402 batch loss 2.85315847 batch mAP 0.29296875 batch PCKh 0.705078125\n",
      "Trained batch 1403 batch loss 2.96829534 batch mAP 0.287109375 batch PCKh 0.32421875\n",
      "Trained batch 1404 batch loss 2.85247469 batch mAP 0.3046875 batch PCKh 0.517578125\n",
      "Trained batch 1405 batch loss 2.87550974 batch mAP 0.28125 batch PCKh 0.4375\n",
      "Trained batch 1406 batch loss 3.02620506 batch mAP 0.220703125 batch PCKh 0.380859375\n",
      "Trained batch 1407 batch loss 3.1693747 batch mAP 0.271484375 batch PCKh 0.38671875\n",
      "Trained batch 1408 batch loss 3.3660357 batch mAP 0.267578125 batch PCKh 0.330078125\n",
      "Trained batch 1409 batch loss 3.17469573 batch mAP 0.24609375 batch PCKh 0.3828125\n",
      "Trained batch 1410 batch loss 3.26558161 batch mAP 0.26171875 batch PCKh 0.3046875\n",
      "Trained batch 1411 batch loss 3.19348478 batch mAP 0.24609375 batch PCKh 0.34765625\n",
      "Trained batch 1412 batch loss 3.03934622 batch mAP 0.1640625 batch PCKh 0.484375\n",
      "Trained batch 1413 batch loss 3.15875626 batch mAP 0.171875 batch PCKh 0.419921875\n",
      "Trained batch 1414 batch loss 2.93367815 batch mAP 0.224609375 batch PCKh 0.421875\n",
      "Trained batch 1415 batch loss 2.92913055 batch mAP 0.2109375 batch PCKh 0.408203125\n",
      "Trained batch 1416 batch loss 3.06891942 batch mAP 0.2109375 batch PCKh 0.478515625\n",
      "Trained batch 1417 batch loss 3.10535359 batch mAP 0.181640625 batch PCKh 0.458984375\n",
      "Trained batch 1418 batch loss 3.12246227 batch mAP 0.19140625 batch PCKh 0.435546875\n",
      "Trained batch 1419 batch loss 2.78220892 batch mAP 0.1875 batch PCKh 0.33984375\n",
      "Trained batch 1420 batch loss 3.04317284 batch mAP 0.22265625 batch PCKh 0.552734375\n",
      "Trained batch 1421 batch loss 3.17216206 batch mAP 0.197265625 batch PCKh 0.53125\n",
      "Trained batch 1422 batch loss 3.19367242 batch mAP 0.21484375 batch PCKh 0.50390625\n",
      "Trained batch 1423 batch loss 3.24539495 batch mAP 0.189453125 batch PCKh 0.490234375\n",
      "Trained batch 1424 batch loss 3.21830034 batch mAP 0.15234375 batch PCKh 0.212890625\n",
      "Trained batch 1425 batch loss 3.29036164 batch mAP 0.201171875 batch PCKh 0.26171875\n",
      "Trained batch 1426 batch loss 3.07797384 batch mAP 0.228515625 batch PCKh 0.498046875\n",
      "Trained batch 1427 batch loss 2.7721107 batch mAP 0.162109375 batch PCKh 0.392578125\n",
      "Trained batch 1428 batch loss 2.51403379 batch mAP 0.1875 batch PCKh 0.068359375\n",
      "Trained batch 1429 batch loss 2.63557696 batch mAP 0.162109375 batch PCKh 0.166015625\n",
      "Trained batch 1430 batch loss 2.6893 batch mAP 0.2265625 batch PCKh 0.07421875\n",
      "Trained batch 1431 batch loss 2.5009923 batch mAP 0.19140625 batch PCKh 0.171875\n",
      "Trained batch 1432 batch loss 2.95443869 batch mAP 0.2265625 batch PCKh 0.294921875\n",
      "Trained batch 1433 batch loss 2.74337339 batch mAP 0.296875 batch PCKh 0.509765625\n",
      "Trained batch 1434 batch loss 2.98645091 batch mAP 0.275390625 batch PCKh 0.375\n",
      "Trained batch 1435 batch loss 2.90239191 batch mAP 0.234375 batch PCKh 0.466796875\n",
      "Trained batch 1436 batch loss 2.75866699 batch mAP 0.216796875 batch PCKh 0.478515625\n",
      "Trained batch 1437 batch loss 3.13150382 batch mAP 0.3203125 batch PCKh 0.43359375\n",
      "Trained batch 1438 batch loss 3.1259222 batch mAP 0.263671875 batch PCKh 0.45703125\n",
      "Trained batch 1439 batch loss 2.96482539 batch mAP 0.267578125 batch PCKh 0.58984375\n",
      "Trained batch 1440 batch loss 2.96449184 batch mAP 0.255859375 batch PCKh 0.5546875\n",
      "Trained batch 1441 batch loss 3.0118041 batch mAP 0.16796875 batch PCKh 0.529296875\n",
      "Trained batch 1442 batch loss 2.81458092 batch mAP 0.255859375 batch PCKh 0.533203125\n",
      "Trained batch 1443 batch loss 3.07154632 batch mAP 0.2109375 batch PCKh 0.525390625\n",
      "Trained batch 1444 batch loss 3.22596431 batch mAP 0.189453125 batch PCKh 0.44140625\n",
      "Trained batch 1445 batch loss 3.05164456 batch mAP 0.1953125 batch PCKh 0.490234375\n",
      "Trained batch 1446 batch loss 2.95586324 batch mAP 0.162109375 batch PCKh 0.337890625\n",
      "Trained batch 1447 batch loss 3.09576702 batch mAP 0.150390625 batch PCKh 0.498046875\n",
      "Trained batch 1448 batch loss 3.22449207 batch mAP 0.16015625 batch PCKh 0.400390625\n",
      "Trained batch 1449 batch loss 2.99388123 batch mAP 0.126953125 batch PCKh 0.544921875\n",
      "Trained batch 1450 batch loss 3.10710526 batch mAP 0.1328125 batch PCKh 0.38671875\n",
      "Trained batch 1451 batch loss 2.93567443 batch mAP 0.134765625 batch PCKh 0.59375\n",
      "Trained batch 1452 batch loss 2.97887516 batch mAP 0.169921875 batch PCKh 0.515625\n",
      "Trained batch 1453 batch loss 2.96508884 batch mAP 0.16015625 batch PCKh 0.486328125\n",
      "Trained batch 1454 batch loss 2.89052415 batch mAP 0.177734375 batch PCKh 0.435546875\n",
      "Trained batch 1455 batch loss 2.83811593 batch mAP 0.19921875 batch PCKh 0.494140625\n",
      "Trained batch 1456 batch loss 2.58206296 batch mAP 0.203125 batch PCKh 0.435546875\n",
      "Trained batch 1457 batch loss 2.73573422 batch mAP 0.189453125 batch PCKh 0.259765625\n",
      "Trained batch 1458 batch loss 2.68857622 batch mAP 0.173828125 batch PCKh 0.33203125\n",
      "Trained batch 1459 batch loss 2.62270927 batch mAP 0.123046875 batch PCKh 0.22265625\n",
      "Trained batch 1460 batch loss 2.60641956 batch mAP 0.138671875 batch PCKh 0.04296875\n",
      "Trained batch 1461 batch loss 2.95185137 batch mAP 0.166015625 batch PCKh 0.49609375\n",
      "Trained batch 1462 batch loss 2.79768801 batch mAP 0.142578125 batch PCKh 0.498046875\n",
      "Trained batch 1463 batch loss 3.27821207 batch mAP 0.16015625 batch PCKh 0.517578125\n",
      "Trained batch 1464 batch loss 2.93260407 batch mAP 0.171875 batch PCKh 0.572265625\n",
      "Trained batch 1465 batch loss 2.81328 batch mAP 0.17578125 batch PCKh 0.400390625\n",
      "Trained batch 1466 batch loss 2.95015383 batch mAP 0.162109375 batch PCKh 0.396484375\n",
      "Trained batch 1467 batch loss 2.76483035 batch mAP 0.154296875 batch PCKh 0.21875\n",
      "Trained batch 1468 batch loss 2.77314615 batch mAP 0.158203125 batch PCKh 0.375\n",
      "Trained batch 1469 batch loss 2.83462858 batch mAP 0.212890625 batch PCKh 0.138671875\n",
      "Trained batch 1470 batch loss 2.85351491 batch mAP 0.181640625 batch PCKh 0.318359375\n",
      "Trained batch 1471 batch loss 3.17713809 batch mAP 0.201171875 batch PCKh 0.400390625\n",
      "Trained batch 1472 batch loss 3.02956438 batch mAP 0.19140625 batch PCKh 0.435546875\n",
      "Trained batch 1473 batch loss 2.93268275 batch mAP 0.232421875 batch PCKh 0.3359375\n",
      "Trained batch 1474 batch loss 2.5018692 batch mAP 0.263671875 batch PCKh 0.392578125\n",
      "Trained batch 1475 batch loss 2.63146162 batch mAP 0.23046875 batch PCKh 0.470703125\n",
      "Trained batch 1476 batch loss 2.71910954 batch mAP 0.2734375 batch PCKh 0.54296875\n",
      "Trained batch 1477 batch loss 2.68236375 batch mAP 0.26171875 batch PCKh 0.55078125\n",
      "Trained batch 1478 batch loss 2.69852448 batch mAP 0.232421875 batch PCKh 0.576171875\n",
      "Trained batch 1479 batch loss 2.86761904 batch mAP 0.220703125 batch PCKh 0.203125\n",
      "Trained batch 1480 batch loss 2.97309875 batch mAP 0.23828125 batch PCKh 0.515625\n",
      "Trained batch 1481 batch loss 2.8950932 batch mAP 0.17578125 batch PCKh 0.21875\n",
      "Trained batch 1482 batch loss 2.89763951 batch mAP 0.17578125 batch PCKh 0.42578125\n",
      "Trained batch 1483 batch loss 2.93070126 batch mAP 0.1640625 batch PCKh 0.416015625\n",
      "Trained batch 1484 batch loss 3.07974577 batch mAP 0.1875 batch PCKh 0.41015625\n",
      "Trained batch 1485 batch loss 3.00458622 batch mAP 0.177734375 batch PCKh 0.40234375\n",
      "Trained batch 1486 batch loss 2.95156169 batch mAP 0.154296875 batch PCKh 0.546875\n",
      "Trained batch 1487 batch loss 3.05200887 batch mAP 0.181640625 batch PCKh 0.4609375\n",
      "Trained batch 1488 batch loss 2.99426889 batch mAP 0.16015625 batch PCKh 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1489 batch loss 2.95149136 batch mAP 0.158203125 batch PCKh 0.470703125\n",
      "Trained batch 1490 batch loss 2.99679422 batch mAP 0.150390625 batch PCKh 0.576171875\n",
      "Trained batch 1491 batch loss 3.11756039 batch mAP 0.1640625 batch PCKh 0.32421875\n",
      "Trained batch 1492 batch loss 3.19981432 batch mAP 0.134765625 batch PCKh 0.283203125\n",
      "Trained batch 1493 batch loss 3.21107054 batch mAP 0.177734375 batch PCKh 0.255859375\n",
      "Trained batch 1494 batch loss 2.94717288 batch mAP 0.171875 batch PCKh 0.3359375\n",
      "Trained batch 1495 batch loss 2.96593618 batch mAP 0.150390625 batch PCKh 0.37109375\n",
      "Trained batch 1496 batch loss 3.0339179 batch mAP 0.1796875 batch PCKh 0.51953125\n",
      "Trained batch 1497 batch loss 3.09979987 batch mAP 0.20703125 batch PCKh 0.611328125\n",
      "Trained batch 1498 batch loss 2.91070867 batch mAP 0.119140625 batch PCKh 0.357421875\n",
      "Trained batch 1499 batch loss 2.5638845 batch mAP 0.12109375 batch PCKh 0.107421875\n",
      "Trained batch 1500 batch loss 2.77456856 batch mAP 0.1640625 batch PCKh 0.423828125\n",
      "Trained batch 1501 batch loss 2.82272434 batch mAP 0.140625 batch PCKh 0.44921875\n",
      "Trained batch 1502 batch loss 2.70626783 batch mAP 0.2109375 batch PCKh 0.00390625\n",
      "Trained batch 1503 batch loss 2.49670839 batch mAP 0.234375 batch PCKh 0.119140625\n",
      "Trained batch 1504 batch loss 3.18286967 batch mAP 0.205078125 batch PCKh 0.2578125\n",
      "Trained batch 1505 batch loss 3.0312 batch mAP 0.203125 batch PCKh 0.271484375\n",
      "Trained batch 1506 batch loss 2.91629148 batch mAP 0.212890625 batch PCKh 0.47265625\n",
      "Trained batch 1507 batch loss 2.9722209 batch mAP 0.1796875 batch PCKh 0.1328125\n",
      "Trained batch 1508 batch loss 3.06968427 batch mAP 0.24609375 batch PCKh 0.15234375\n",
      "Trained batch 1509 batch loss 3.21221471 batch mAP 0.193359375 batch PCKh 0.349609375\n",
      "Trained batch 1510 batch loss 2.94546843 batch mAP 0.224609375 batch PCKh 0.443359375\n",
      "Trained batch 1511 batch loss 3.16345525 batch mAP 0.240234375 batch PCKh 0.490234375\n",
      "Trained batch 1512 batch loss 3.08704424 batch mAP 0.236328125 batch PCKh 0.40234375\n",
      "Trained batch 1513 batch loss 3.04116106 batch mAP 0.287109375 batch PCKh 0.35546875\n",
      "Trained batch 1514 batch loss 2.56915712 batch mAP 0.28515625 batch PCKh 0.068359375\n",
      "Trained batch 1515 batch loss 2.47898579 batch mAP 0.2890625 batch PCKh 0.08203125\n",
      "Trained batch 1516 batch loss 2.96508408 batch mAP 0.248046875 batch PCKh 0.28125\n",
      "Trained batch 1517 batch loss 2.87869763 batch mAP 0.255859375 batch PCKh 0.615234375\n",
      "Trained batch 1518 batch loss 2.82582974 batch mAP 0.212890625 batch PCKh 0.310546875\n",
      "Trained batch 1519 batch loss 2.65526128 batch mAP 0.2734375 batch PCKh 0.43359375\n",
      "Trained batch 1520 batch loss 2.75256062 batch mAP 0.294921875 batch PCKh 0.35546875\n",
      "Trained batch 1521 batch loss 2.52964067 batch mAP 0.23828125 batch PCKh 0.455078125\n",
      "Trained batch 1522 batch loss 2.75177693 batch mAP 0.306640625 batch PCKh 0.208984375\n",
      "Trained batch 1523 batch loss 2.80631089 batch mAP 0.259765625 batch PCKh 0.43359375\n",
      "Trained batch 1524 batch loss 2.90781546 batch mAP 0.291015625 batch PCKh 0.23828125\n",
      "Trained batch 1525 batch loss 3.21979117 batch mAP 0.26171875 batch PCKh 0.388671875\n",
      "Trained batch 1526 batch loss 3.10583758 batch mAP 0.244140625 batch PCKh 0.376953125\n",
      "Trained batch 1527 batch loss 3.05053711 batch mAP 0.21875 batch PCKh 0.390625\n",
      "Trained batch 1528 batch loss 2.9137187 batch mAP 0.236328125 batch PCKh 0.416015625\n",
      "Trained batch 1529 batch loss 3.00824499 batch mAP 0.25390625 batch PCKh 0.349609375\n",
      "Trained batch 1530 batch loss 2.98339605 batch mAP 0.287109375 batch PCKh 0.41796875\n",
      "Trained batch 1531 batch loss 3.02970576 batch mAP 0.2578125 batch PCKh 0.677734375\n",
      "Trained batch 1532 batch loss 2.78674603 batch mAP 0.232421875 batch PCKh 0.72265625\n",
      "Trained batch 1533 batch loss 2.63437653 batch mAP 0.267578125 batch PCKh 0.296875\n",
      "Trained batch 1534 batch loss 2.66271496 batch mAP 0.283203125 batch PCKh 0.314453125\n",
      "Trained batch 1535 batch loss 2.90844321 batch mAP 0.267578125 batch PCKh 0.599609375\n",
      "Trained batch 1536 batch loss 3.08683681 batch mAP 0.16796875 batch PCKh 0.41796875\n",
      "Trained batch 1537 batch loss 2.86245823 batch mAP 0.15625 batch PCKh 0.453125\n",
      "Trained batch 1538 batch loss 2.90967345 batch mAP 0.18359375 batch PCKh 0.51953125\n",
      "Trained batch 1539 batch loss 3.15101266 batch mAP 0.189453125 batch PCKh 0.408203125\n",
      "Trained batch 1540 batch loss 2.99442768 batch mAP 0.19140625 batch PCKh 0.537109375\n",
      "Trained batch 1541 batch loss 2.9317193 batch mAP 0.22265625 batch PCKh 0.576171875\n",
      "Trained batch 1542 batch loss 2.83744407 batch mAP 0.244140625 batch PCKh 0.630859375\n",
      "Trained batch 1543 batch loss 2.85849452 batch mAP 0.240234375 batch PCKh 0.38671875\n",
      "Trained batch 1544 batch loss 2.72758198 batch mAP 0.1953125 batch PCKh 0.388671875\n",
      "Trained batch 1545 batch loss 2.9041028 batch mAP 0.205078125 batch PCKh 0.29296875\n",
      "Trained batch 1546 batch loss 2.76911211 batch mAP 0.171875 batch PCKh 0.46484375\n",
      "Trained batch 1547 batch loss 3.19460678 batch mAP 0.20703125 batch PCKh 0.31640625\n",
      "Trained batch 1548 batch loss 3.04593182 batch mAP 0.208984375 batch PCKh 0.3359375\n",
      "Trained batch 1549 batch loss 2.99164772 batch mAP 0.24609375 batch PCKh 0.45703125\n",
      "Trained batch 1550 batch loss 3.16765451 batch mAP 0.2890625 batch PCKh 0.36328125\n",
      "Trained batch 1551 batch loss 3.12927055 batch mAP 0.2578125 batch PCKh 0.4765625\n",
      "Trained batch 1552 batch loss 2.97776413 batch mAP 0.28125 batch PCKh 0.5703125\n",
      "Trained batch 1553 batch loss 2.91664982 batch mAP 0.224609375 batch PCKh 0.595703125\n",
      "Trained batch 1554 batch loss 2.7673173 batch mAP 0.171875 batch PCKh 0.708984375\n",
      "Trained batch 1555 batch loss 2.8054862 batch mAP 0.22265625 batch PCKh 0.49609375\n",
      "Trained batch 1556 batch loss 2.80547118 batch mAP 0.2265625 batch PCKh 0.67578125\n",
      "Trained batch 1557 batch loss 2.72534561 batch mAP 0.14453125 batch PCKh 0.69921875\n",
      "Trained batch 1558 batch loss 2.89856219 batch mAP 0.150390625 batch PCKh 0.5546875\n",
      "Trained batch 1559 batch loss 2.72178245 batch mAP 0.150390625 batch PCKh 0.595703125\n",
      "Trained batch 1560 batch loss 2.80656457 batch mAP 0.216796875 batch PCKh 0.701171875\n",
      "Trained batch 1561 batch loss 2.84607673 batch mAP 0.171875 batch PCKh 0.48046875\n",
      "Trained batch 1562 batch loss 3.04117084 batch mAP 0.185546875 batch PCKh 0.326171875\n",
      "Trained batch 1563 batch loss 3.01393366 batch mAP 0.125 batch PCKh 0.322265625\n",
      "Trained batch 1564 batch loss 3.13962889 batch mAP 0.103515625 batch PCKh 0.416015625\n",
      "Trained batch 1565 batch loss 2.7259376 batch mAP 0.091796875 batch PCKh 0.23046875\n",
      "Trained batch 1566 batch loss 2.85513258 batch mAP 0.12890625 batch PCKh 0.5\n",
      "Trained batch 1567 batch loss 2.80508327 batch mAP 0.14453125 batch PCKh 0.6640625\n",
      "Trained batch 1568 batch loss 2.70985842 batch mAP 0.19140625 batch PCKh 0.2890625\n",
      "Trained batch 1569 batch loss 2.52853107 batch mAP 0.1484375 batch PCKh 0.421875\n",
      "Trained batch 1570 batch loss 2.53267741 batch mAP 0.140625 batch PCKh 0.26171875\n",
      "Trained batch 1571 batch loss 2.68698716 batch mAP 0.154296875 batch PCKh 0.322265625\n",
      "Trained batch 1572 batch loss 2.68455458 batch mAP 0.1328125 batch PCKh 0.5859375\n",
      "Trained batch 1573 batch loss 3.05627108 batch mAP 0.1484375 batch PCKh 0.17578125\n",
      "Trained batch 1574 batch loss 3.80625963 batch mAP 0.138671875 batch PCKh 0.16015625\n",
      "Trained batch 1575 batch loss 3.45051908 batch mAP 0.126953125 batch PCKh 0.265625\n",
      "Trained batch 1576 batch loss 3.45464611 batch mAP 0.111328125 batch PCKh 0.28515625\n",
      "Trained batch 1577 batch loss 3.28130484 batch mAP 0.1171875 batch PCKh 0.43359375\n",
      "Trained batch 1578 batch loss 3.05120277 batch mAP 0.189453125 batch PCKh 0.583984375\n",
      "Trained batch 1579 batch loss 2.85602665 batch mAP 0.203125 batch PCKh 0.529296875\n",
      "Trained batch 1580 batch loss 2.9215436 batch mAP 0.328125 batch PCKh 0.556640625\n",
      "Trained batch 1581 batch loss 2.96132 batch mAP 0.26953125 batch PCKh 0.580078125\n",
      "Trained batch 1582 batch loss 3.23638916 batch mAP 0.283203125 batch PCKh 0.38671875\n",
      "Trained batch 1583 batch loss 3.03229427 batch mAP 0.302734375 batch PCKh 0.51171875\n",
      "Trained batch 1584 batch loss 2.80855155 batch mAP 0.310546875 batch PCKh 0.62890625\n",
      "Trained batch 1585 batch loss 2.97135806 batch mAP 0.263671875 batch PCKh 0.47265625\n",
      "Trained batch 1586 batch loss 2.84969187 batch mAP 0.28125 batch PCKh 0.490234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1587 batch loss 2.89132118 batch mAP 0.28515625 batch PCKh 0.49609375\n",
      "Trained batch 1588 batch loss 3.15151596 batch mAP 0.279296875 batch PCKh 0.603515625\n",
      "Trained batch 1589 batch loss 2.92080355 batch mAP 0.263671875 batch PCKh 0.541015625\n",
      "Trained batch 1590 batch loss 2.96657634 batch mAP 0.23046875 batch PCKh 0.4296875\n",
      "Trained batch 1591 batch loss 2.95911789 batch mAP 0.240234375 batch PCKh 0.435546875\n",
      "Trained batch 1592 batch loss 3.08088112 batch mAP 0.19140625 batch PCKh 0.431640625\n",
      "Trained batch 1593 batch loss 2.95060349 batch mAP 0.1171875 batch PCKh 0.54296875\n",
      "Trained batch 1594 batch loss 3.12619352 batch mAP 0.19140625 batch PCKh 0.4765625\n",
      "Trained batch 1595 batch loss 2.82417727 batch mAP 0.20703125 batch PCKh 0.484375\n",
      "Trained batch 1596 batch loss 2.99269867 batch mAP 0.216796875 batch PCKh 0.44140625\n",
      "Trained batch 1597 batch loss 3.08475566 batch mAP 0.119140625 batch PCKh 0.5546875\n",
      "Trained batch 1598 batch loss 2.98001456 batch mAP 0.154296875 batch PCKh 0.42578125\n",
      "Trained batch 1599 batch loss 2.81153488 batch mAP 0.138671875 batch PCKh 0.455078125\n",
      "Trained batch 1600 batch loss 2.81103802 batch mAP 0.14453125 batch PCKh 0.619140625\n",
      "Trained batch 1601 batch loss 2.88016844 batch mAP 0.1171875 batch PCKh 0.490234375\n",
      "Trained batch 1602 batch loss 2.91242552 batch mAP 0.091796875 batch PCKh 0.56640625\n",
      "Trained batch 1603 batch loss 2.93976831 batch mAP 0.111328125 batch PCKh 0.47265625\n",
      "Trained batch 1604 batch loss 2.82939243 batch mAP 0.1015625 batch PCKh 0.46875\n",
      "Trained batch 1605 batch loss 2.8160615 batch mAP 0.0859375 batch PCKh 0.6953125\n",
      "Trained batch 1606 batch loss 2.92988229 batch mAP 0.09375 batch PCKh 0.552734375\n",
      "Trained batch 1607 batch loss 2.89489174 batch mAP 0.07421875 batch PCKh 0.23046875\n",
      "Trained batch 1608 batch loss 2.94321156 batch mAP 0.1171875 batch PCKh 0.369140625\n",
      "Trained batch 1609 batch loss 2.90524626 batch mAP 0.14453125 batch PCKh 0.41796875\n",
      "Trained batch 1610 batch loss 2.95535421 batch mAP 0.119140625 batch PCKh 0.611328125\n",
      "Trained batch 1611 batch loss 2.85804343 batch mAP 0.08984375 batch PCKh 0.5390625\n",
      "Trained batch 1612 batch loss 2.97725391 batch mAP 0.146484375 batch PCKh 0.4453125\n",
      "Trained batch 1613 batch loss 2.76137733 batch mAP 0.099609375 batch PCKh 0.427734375\n",
      "Trained batch 1614 batch loss 2.84657526 batch mAP 0.095703125 batch PCKh 0.60546875\n",
      "Trained batch 1615 batch loss 2.9050777 batch mAP 0.134765625 batch PCKh 0.662109375\n",
      "Trained batch 1616 batch loss 2.91566324 batch mAP 0.103515625 batch PCKh 0.552734375\n",
      "Trained batch 1617 batch loss 2.85187316 batch mAP 0.111328125 batch PCKh 0.58203125\n",
      "Trained batch 1618 batch loss 2.63793898 batch mAP 0.14453125 batch PCKh 0.623046875\n",
      "Trained batch 1619 batch loss 2.85092926 batch mAP 0.134765625 batch PCKh 0.5859375\n",
      "Trained batch 1620 batch loss 2.91919088 batch mAP 0.115234375 batch PCKh 0.52734375\n",
      "Trained batch 1621 batch loss 2.78416419 batch mAP 0.115234375 batch PCKh 0.6328125\n",
      "Trained batch 1622 batch loss 2.94704747 batch mAP 0.09765625 batch PCKh 0.70703125\n",
      "Trained batch 1623 batch loss 2.9492445 batch mAP 0.146484375 batch PCKh 0.421875\n",
      "Trained batch 1624 batch loss 2.83776832 batch mAP 0.05859375 batch PCKh 0.44140625\n",
      "Trained batch 1625 batch loss 2.87669277 batch mAP 0.064453125 batch PCKh 0.45703125\n",
      "Trained batch 1626 batch loss 2.8085084 batch mAP 0.072265625 batch PCKh 0.4375\n",
      "Trained batch 1627 batch loss 2.81218863 batch mAP 0.083984375 batch PCKh 0.373046875\n",
      "Trained batch 1628 batch loss 2.93969965 batch mAP 0.05859375 batch PCKh 0.443359375\n",
      "Trained batch 1629 batch loss 2.795403 batch mAP 0.0546875 batch PCKh 0.55859375\n",
      "Trained batch 1630 batch loss 2.76894665 batch mAP 0.05859375 batch PCKh 0.556640625\n",
      "Trained batch 1631 batch loss 2.92869711 batch mAP 0.099609375 batch PCKh 0.390625\n",
      "Trained batch 1632 batch loss 2.7393558 batch mAP 0.111328125 batch PCKh 0.41796875\n",
      "Trained batch 1633 batch loss 3.45715 batch mAP 0.099609375 batch PCKh 0.115234375\n",
      "Trained batch 1634 batch loss 3.46233845 batch mAP 0.12109375 batch PCKh 0.23828125\n",
      "Trained batch 1635 batch loss 3.10954595 batch mAP 0.1015625 batch PCKh 0.384765625\n",
      "Trained batch 1636 batch loss 3.13494301 batch mAP 0.166015625 batch PCKh 0.41796875\n",
      "Trained batch 1637 batch loss 2.95998025 batch mAP 0.103515625 batch PCKh 0.53125\n",
      "Trained batch 1638 batch loss 3.31142664 batch mAP 0.119140625 batch PCKh 0.31640625\n",
      "Trained batch 1639 batch loss 3.28978825 batch mAP 0.142578125 batch PCKh 0.25390625\n",
      "Trained batch 1640 batch loss 3.39621544 batch mAP 0.15234375 batch PCKh 0.271484375\n",
      "Trained batch 1641 batch loss 3.30620909 batch mAP 0.10546875 batch PCKh 0.265625\n",
      "Trained batch 1642 batch loss 3.26605868 batch mAP 0.15625 batch PCKh 0.30859375\n",
      "Trained batch 1643 batch loss 3.24837017 batch mAP 0.16796875 batch PCKh 0.251953125\n",
      "Trained batch 1644 batch loss 2.91017294 batch mAP 0.255859375 batch PCKh 0.5234375\n",
      "Trained batch 1645 batch loss 3.12284708 batch mAP 0.2265625 batch PCKh 0.40234375\n",
      "Trained batch 1646 batch loss 3.17976952 batch mAP 0.259765625 batch PCKh 0.43359375\n",
      "Trained batch 1647 batch loss 2.79590559 batch mAP 0.255859375 batch PCKh 0.576171875\n",
      "Trained batch 1648 batch loss 3.0523622 batch mAP 0.3203125 batch PCKh 0.57421875\n",
      "Trained batch 1649 batch loss 3.20054 batch mAP 0.30859375 batch PCKh 0.40625\n",
      "Trained batch 1650 batch loss 3.40393305 batch mAP 0.30078125 batch PCKh 0.185546875\n",
      "Trained batch 1651 batch loss 3.30169344 batch mAP 0.37109375 batch PCKh 0.306640625\n",
      "Trained batch 1652 batch loss 3.2905364 batch mAP 0.31640625 batch PCKh 0.314453125\n",
      "Trained batch 1653 batch loss 3.08116674 batch mAP 0.3046875 batch PCKh 0.27734375\n",
      "Trained batch 1654 batch loss 2.77694416 batch mAP 0.265625 batch PCKh 0.248046875\n",
      "Trained batch 1655 batch loss 2.72161651 batch mAP 0.3046875 batch PCKh 0.630859375\n",
      "Trained batch 1656 batch loss 2.77215338 batch mAP 0.291015625 batch PCKh 0.529296875\n",
      "Trained batch 1657 batch loss 2.7145772 batch mAP 0.267578125 batch PCKh 0.591796875\n",
      "Trained batch 1658 batch loss 2.84358358 batch mAP 0.275390625 batch PCKh 0.59765625\n",
      "Trained batch 1659 batch loss 2.70130897 batch mAP 0.2265625 batch PCKh 0.478515625\n",
      "Trained batch 1660 batch loss 2.95630884 batch mAP 0.240234375 batch PCKh 0.607421875\n",
      "Trained batch 1661 batch loss 3.06479192 batch mAP 0.20703125 batch PCKh 0.14453125\n",
      "Trained batch 1662 batch loss 2.90279222 batch mAP 0.18359375 batch PCKh 0.5546875\n",
      "Trained batch 1663 batch loss 2.77685738 batch mAP 0.16796875 batch PCKh 0.24609375\n",
      "Trained batch 1664 batch loss 3.08428478 batch mAP 0.251953125 batch PCKh 0.400390625\n",
      "Trained batch 1665 batch loss 2.72604895 batch mAP 0.24609375 batch PCKh 0.353515625\n",
      "Trained batch 1666 batch loss 2.97048926 batch mAP 0.201171875 batch PCKh 0.55859375\n",
      "Trained batch 1667 batch loss 3.07202601 batch mAP 0.201171875 batch PCKh 0.349609375\n",
      "Trained batch 1668 batch loss 2.86897421 batch mAP 0.177734375 batch PCKh 0.302734375\n",
      "Trained batch 1669 batch loss 3.24771643 batch mAP 0.236328125 batch PCKh 0.208984375\n",
      "Trained batch 1670 batch loss 3.20659876 batch mAP 0.271484375 batch PCKh 0.3359375\n",
      "Trained batch 1671 batch loss 3.27042198 batch mAP 0.16796875 batch PCKh 0.435546875\n",
      "Trained batch 1672 batch loss 3.16892338 batch mAP 0.244140625 batch PCKh 0.384765625\n",
      "Trained batch 1673 batch loss 2.94843578 batch mAP 0.216796875 batch PCKh 0.544921875\n",
      "Trained batch 1674 batch loss 2.93597889 batch mAP 0.19140625 batch PCKh 0.556640625\n",
      "Trained batch 1675 batch loss 2.93957615 batch mAP 0.216796875 batch PCKh 0.64453125\n",
      "Trained batch 1676 batch loss 3.06540036 batch mAP 0.283203125 batch PCKh 0.576171875\n",
      "Trained batch 1677 batch loss 2.83722281 batch mAP 0.22265625 batch PCKh 0.498046875\n",
      "Trained batch 1678 batch loss 2.85998034 batch mAP 0.244140625 batch PCKh 0.4921875\n",
      "Trained batch 1679 batch loss 2.91404223 batch mAP 0.185546875 batch PCKh 0.537109375\n",
      "Trained batch 1680 batch loss 2.9772234 batch mAP 0.1484375 batch PCKh 0.4296875\n",
      "Trained batch 1681 batch loss 2.99501371 batch mAP 0.16015625 batch PCKh 0.517578125\n",
      "Trained batch 1682 batch loss 2.89218426 batch mAP 0.15625 batch PCKh 0.52734375\n",
      "Trained batch 1683 batch loss 3.02362633 batch mAP 0.162109375 batch PCKh 0.400390625\n",
      "Trained batch 1684 batch loss 2.70901465 batch mAP 0.169921875 batch PCKh 0.5859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1685 batch loss 2.80734277 batch mAP 0.115234375 batch PCKh 0.5625\n",
      "Trained batch 1686 batch loss 2.8283143 batch mAP 0.126953125 batch PCKh 0.509765625\n",
      "Trained batch 1687 batch loss 2.63766932 batch mAP 0.05859375 batch PCKh 0.544921875\n",
      "Trained batch 1688 batch loss 2.87649393 batch mAP 0.103515625 batch PCKh 0.544921875\n",
      "Trained batch 1689 batch loss 2.89027429 batch mAP 0.091796875 batch PCKh 0.482421875\n",
      "Trained batch 1690 batch loss 2.73246241 batch mAP 0.1015625 batch PCKh 0.46484375\n",
      "Trained batch 1691 batch loss 2.6146934 batch mAP 0.076171875 batch PCKh 0.494140625\n",
      "Trained batch 1692 batch loss 3.01030254 batch mAP 0.064453125 batch PCKh 0.33984375\n",
      "Trained batch 1693 batch loss 3.28419352 batch mAP 0.10546875 batch PCKh 0.3515625\n",
      "Trained batch 1694 batch loss 3.21548557 batch mAP 0.11328125 batch PCKh 0.380859375\n",
      "Trained batch 1695 batch loss 3.11323977 batch mAP 0.06640625 batch PCKh 0.45703125\n",
      "Trained batch 1696 batch loss 2.92436981 batch mAP 0.109375 batch PCKh 0.265625\n",
      "Trained batch 1697 batch loss 3.10424542 batch mAP 0.111328125 batch PCKh 0.447265625\n",
      "Trained batch 1698 batch loss 3.05359316 batch mAP 0.095703125 batch PCKh 0.619140625\n",
      "Trained batch 1699 batch loss 2.87464261 batch mAP 0.099609375 batch PCKh 0.408203125\n",
      "Trained batch 1700 batch loss 3.09727716 batch mAP 0.111328125 batch PCKh 0.37890625\n",
      "Trained batch 1701 batch loss 3.18566465 batch mAP 0.134765625 batch PCKh 0.396484375\n",
      "Trained batch 1702 batch loss 3.10620952 batch mAP 0.107421875 batch PCKh 0.373046875\n",
      "Trained batch 1703 batch loss 3.29688454 batch mAP 0.1796875 batch PCKh 0.35546875\n",
      "Trained batch 1704 batch loss 2.90321875 batch mAP 0.166015625 batch PCKh 0.6484375\n",
      "Trained batch 1705 batch loss 2.85559177 batch mAP 0.1640625 batch PCKh 0.498046875\n",
      "Trained batch 1706 batch loss 2.95924854 batch mAP 0.220703125 batch PCKh 0.556640625\n",
      "Trained batch 1707 batch loss 3.11737823 batch mAP 0.166015625 batch PCKh 0.3984375\n",
      "Trained batch 1708 batch loss 2.90451431 batch mAP 0.20703125 batch PCKh 0.244140625\n",
      "Trained batch 1709 batch loss 2.83309 batch mAP 0.240234375 batch PCKh 0.18359375\n",
      "Trained batch 1710 batch loss 2.48630953 batch mAP 0.1953125 batch PCKh 0.048828125\n",
      "Trained batch 1711 batch loss 2.5968 batch mAP 0.158203125 batch PCKh 0.32421875\n",
      "Trained batch 1712 batch loss 2.88417149 batch mAP 0.146484375 batch PCKh 0.365234375\n",
      "Trained batch 1713 batch loss 2.35526228 batch mAP 0.14453125 batch PCKh 0.12890625\n",
      "Trained batch 1714 batch loss 2.64794922 batch mAP 0.201171875 batch PCKh 0.39453125\n",
      "Trained batch 1715 batch loss 2.68138981 batch mAP 0.197265625 batch PCKh 0.56640625\n",
      "Trained batch 1716 batch loss 2.67600894 batch mAP 0.197265625 batch PCKh 0.650390625\n",
      "Trained batch 1717 batch loss 2.69588041 batch mAP 0.1953125 batch PCKh 0.388671875\n",
      "Trained batch 1718 batch loss 2.36591363 batch mAP 0.19140625 batch PCKh 0.009765625\n",
      "Trained batch 1719 batch loss 2.27514243 batch mAP 0.1328125 batch PCKh 0.2421875\n",
      "Trained batch 1720 batch loss 2.36732411 batch mAP 0.232421875 batch PCKh 0.017578125\n",
      "Trained batch 1721 batch loss 2.32466364 batch mAP 0.185546875 batch PCKh 0.0078125\n",
      "Trained batch 1722 batch loss 2.32610297 batch mAP 0.16796875 batch PCKh 0.140625\n",
      "Trained batch 1723 batch loss 2.29025888 batch mAP 0.224609375 batch PCKh 0.03125\n",
      "Trained batch 1724 batch loss 2.20739865 batch mAP 0.171875 batch PCKh 0.23046875\n",
      "Trained batch 1725 batch loss 2.85343719 batch mAP 0.111328125 batch PCKh 0.33203125\n",
      "Trained batch 1726 batch loss 3.37189651 batch mAP 0.138671875 batch PCKh 0.46875\n",
      "Trained batch 1727 batch loss 3.11223507 batch mAP 0.138671875 batch PCKh 0.28125\n",
      "Trained batch 1728 batch loss 3.21958017 batch mAP 0.138671875 batch PCKh 0.4140625\n",
      "Trained batch 1729 batch loss 2.95013022 batch mAP 0.076171875 batch PCKh 0.509765625\n",
      "Trained batch 1730 batch loss 2.85770464 batch mAP 0.095703125 batch PCKh 0.2890625\n",
      "Trained batch 1731 batch loss 2.92652345 batch mAP 0.109375 batch PCKh 0.33203125\n",
      "Trained batch 1732 batch loss 2.81325054 batch mAP 0.115234375 batch PCKh 0.3828125\n",
      "Trained batch 1733 batch loss 2.83445311 batch mAP 0.123046875 batch PCKh 0.302734375\n",
      "Trained batch 1734 batch loss 3.01816535 batch mAP 0.142578125 batch PCKh 0.43359375\n",
      "Trained batch 1735 batch loss 3.052495 batch mAP 0.212890625 batch PCKh 0.35546875\n",
      "Trained batch 1736 batch loss 2.78162909 batch mAP 0.181640625 batch PCKh 0.32421875\n",
      "Trained batch 1737 batch loss 2.91760945 batch mAP 0.208984375 batch PCKh 0.30859375\n",
      "Trained batch 1738 batch loss 2.90951467 batch mAP 0.201171875 batch PCKh 0.19921875\n",
      "Trained batch 1739 batch loss 3.16489983 batch mAP 0.197265625 batch PCKh 0.341796875\n",
      "Trained batch 1740 batch loss 3.00831366 batch mAP 0.2421875 batch PCKh 0.453125\n",
      "Trained batch 1741 batch loss 2.99465489 batch mAP 0.21484375 batch PCKh 0.5546875\n",
      "Trained batch 1742 batch loss 2.61602807 batch mAP 0.15625 batch PCKh 0.41015625\n",
      "Trained batch 1743 batch loss 2.89508343 batch mAP 0.158203125 batch PCKh 0.466796875\n",
      "Trained batch 1744 batch loss 2.74967742 batch mAP 0.189453125 batch PCKh 0.41015625\n",
      "Trained batch 1745 batch loss 2.94067168 batch mAP 0.171875 batch PCKh 0.6875\n",
      "Trained batch 1746 batch loss 2.94922 batch mAP 0.166015625 batch PCKh 0.521484375\n",
      "Trained batch 1747 batch loss 3.2202177 batch mAP 0.18359375 batch PCKh 0.552734375\n",
      "Trained batch 1748 batch loss 3.00624967 batch mAP 0.16796875 batch PCKh 0.26171875\n",
      "Trained batch 1749 batch loss 3.16914558 batch mAP 0.166015625 batch PCKh 0.365234375\n",
      "Trained batch 1750 batch loss 3.21832347 batch mAP 0.166015625 batch PCKh 0.466796875\n",
      "Trained batch 1751 batch loss 2.65822411 batch mAP 0.142578125 batch PCKh 0.400390625\n",
      "Trained batch 1752 batch loss 2.82162452 batch mAP 0.16796875 batch PCKh 0.541015625\n",
      "Trained batch 1753 batch loss 2.87831306 batch mAP 0.15234375 batch PCKh 0.38671875\n",
      "Trained batch 1754 batch loss 2.92000222 batch mAP 0.16796875 batch PCKh 0.46875\n",
      "Trained batch 1755 batch loss 2.74086595 batch mAP 0.185546875 batch PCKh 0.220703125\n",
      "Trained batch 1756 batch loss 3.10688019 batch mAP 0.162109375 batch PCKh 0.49609375\n",
      "Trained batch 1757 batch loss 2.8280859 batch mAP 0.146484375 batch PCKh 0.41015625\n",
      "Trained batch 1758 batch loss 2.79574347 batch mAP 0.1484375 batch PCKh 0.5546875\n",
      "Trained batch 1759 batch loss 2.86849165 batch mAP 0.1484375 batch PCKh 0.47265625\n",
      "Trained batch 1760 batch loss 2.69007468 batch mAP 0.1640625 batch PCKh 0.52734375\n",
      "Trained batch 1761 batch loss 2.97488403 batch mAP 0.158203125 batch PCKh 0.421875\n",
      "Trained batch 1762 batch loss 2.83597255 batch mAP 0.162109375 batch PCKh 0.330078125\n",
      "Trained batch 1763 batch loss 2.73192549 batch mAP 0.162109375 batch PCKh 0.228515625\n",
      "Trained batch 1764 batch loss 2.92895579 batch mAP 0.138671875 batch PCKh 0.603515625\n",
      "Trained batch 1765 batch loss 2.9393425 batch mAP 0.177734375 batch PCKh 0.716796875\n",
      "Trained batch 1766 batch loss 3.07800126 batch mAP 0.1640625 batch PCKh 0.296875\n",
      "Trained batch 1767 batch loss 2.93240499 batch mAP 0.14453125 batch PCKh 0.291015625\n",
      "Trained batch 1768 batch loss 2.91896772 batch mAP 0.197265625 batch PCKh 0.3515625\n",
      "Trained batch 1769 batch loss 3.10333538 batch mAP 0.16796875 batch PCKh 0.392578125\n",
      "Trained batch 1770 batch loss 2.85672593 batch mAP 0.16015625 batch PCKh 0.564453125\n",
      "Trained batch 1771 batch loss 2.9483695 batch mAP 0.166015625 batch PCKh 0.564453125\n",
      "Trained batch 1772 batch loss 2.87957692 batch mAP 0.20703125 batch PCKh 0.3359375\n",
      "Trained batch 1773 batch loss 3.27813435 batch mAP 0.189453125 batch PCKh 0.25390625\n",
      "Trained batch 1774 batch loss 3.03135276 batch mAP 0.173828125 batch PCKh 0.27734375\n",
      "Trained batch 1775 batch loss 2.55252767 batch mAP 0.1328125 batch PCKh 0.544921875\n",
      "Trained batch 1776 batch loss 3.42670083 batch mAP 0.134765625 batch PCKh 0.373046875\n",
      "Trained batch 1777 batch loss 2.70741 batch mAP 0.142578125 batch PCKh 0.330078125\n",
      "Trained batch 1778 batch loss 2.86057138 batch mAP 0.177734375 batch PCKh 0.380859375\n",
      "Trained batch 1779 batch loss 2.80170703 batch mAP 0.197265625 batch PCKh 0.4765625\n",
      "Trained batch 1780 batch loss 3.20636654 batch mAP 0.162109375 batch PCKh 0.283203125\n",
      "Trained batch 1781 batch loss 3.02087 batch mAP 0.255859375 batch PCKh 0.333984375\n",
      "Trained batch 1782 batch loss 2.72332883 batch mAP 0.267578125 batch PCKh 0.296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1783 batch loss 2.93603683 batch mAP 0.216796875 batch PCKh 0.341796875\n",
      "Trained batch 1784 batch loss 2.99762225 batch mAP 0.1796875 batch PCKh 0.41796875\n",
      "Trained batch 1785 batch loss 2.805058 batch mAP 0.21484375 batch PCKh 0.404296875\n",
      "Trained batch 1786 batch loss 2.58382559 batch mAP 0.197265625 batch PCKh 0.3125\n",
      "Trained batch 1787 batch loss 2.53451061 batch mAP 0.234375 batch PCKh 0.416015625\n",
      "Trained batch 1788 batch loss 2.66552973 batch mAP 0.228515625 batch PCKh 0.337890625\n",
      "Trained batch 1789 batch loss 2.49928522 batch mAP 0.28125 batch PCKh 0.111328125\n",
      "Trained batch 1790 batch loss 2.95572925 batch mAP 0.26953125 batch PCKh 0.412109375\n",
      "Trained batch 1791 batch loss 2.76786852 batch mAP 0.259765625 batch PCKh 0.30859375\n",
      "Trained batch 1792 batch loss 2.9059236 batch mAP 0.283203125 batch PCKh 0.4296875\n",
      "Trained batch 1793 batch loss 2.68597674 batch mAP 0.228515625 batch PCKh 0.158203125\n",
      "Trained batch 1794 batch loss 3.00553751 batch mAP 0.2265625 batch PCKh 0.453125\n",
      "Trained batch 1795 batch loss 3.03219771 batch mAP 0.22265625 batch PCKh 0.548828125\n",
      "Trained batch 1796 batch loss 3.04682302 batch mAP 0.220703125 batch PCKh 0.46875\n",
      "Trained batch 1797 batch loss 2.9517436 batch mAP 0.205078125 batch PCKh 0.5234375\n",
      "Trained batch 1798 batch loss 3.0577569 batch mAP 0.21875 batch PCKh 0.380859375\n",
      "Trained batch 1799 batch loss 3.03119969 batch mAP 0.259765625 batch PCKh 0.400390625\n",
      "Trained batch 1800 batch loss 2.89250088 batch mAP 0.21875 batch PCKh 0.572265625\n",
      "Trained batch 1801 batch loss 2.91790295 batch mAP 0.220703125 batch PCKh 0.650390625\n",
      "Trained batch 1802 batch loss 2.83511662 batch mAP 0.212890625 batch PCKh 0.6171875\n",
      "Trained batch 1803 batch loss 2.82796717 batch mAP 0.1796875 batch PCKh 0.642578125\n",
      "Trained batch 1804 batch loss 2.81721759 batch mAP 0.169921875 batch PCKh 0.6953125\n",
      "Trained batch 1805 batch loss 2.89425182 batch mAP 0.1640625 batch PCKh 0.58984375\n",
      "Trained batch 1806 batch loss 2.9336729 batch mAP 0.134765625 batch PCKh 0.42578125\n",
      "Trained batch 1807 batch loss 2.92860603 batch mAP 0.169921875 batch PCKh 0.78125\n",
      "Trained batch 1808 batch loss 2.81282043 batch mAP 0.107421875 batch PCKh 0.125\n",
      "Trained batch 1809 batch loss 2.54404449 batch mAP 0.109375 batch PCKh 0.228515625\n",
      "Trained batch 1810 batch loss 2.76391888 batch mAP 0.109375 batch PCKh 0.416015625\n",
      "Trained batch 1811 batch loss 3.00851107 batch mAP 0.080078125 batch PCKh 0.470703125\n",
      "Trained batch 1812 batch loss 3.10396194 batch mAP 0.1171875 batch PCKh 0.552734375\n",
      "Trained batch 1813 batch loss 2.67221189 batch mAP 0.13671875 batch PCKh 0.548828125\n",
      "Trained batch 1814 batch loss 2.9229722 batch mAP 0.0546875 batch PCKh 0.474609375\n",
      "Trained batch 1815 batch loss 3.02484655 batch mAP 0.080078125 batch PCKh 0.31640625\n",
      "Trained batch 1816 batch loss 2.87719107 batch mAP 0.044921875 batch PCKh 0.443359375\n",
      "Trained batch 1817 batch loss 2.86491871 batch mAP 0.134765625 batch PCKh 0.53125\n",
      "Trained batch 1818 batch loss 2.74849868 batch mAP 0.1015625 batch PCKh 0.59375\n",
      "Trained batch 1819 batch loss 2.85351038 batch mAP 0.095703125 batch PCKh 0.470703125\n",
      "Trained batch 1820 batch loss 2.74987411 batch mAP 0.09765625 batch PCKh 0.525390625\n",
      "Trained batch 1821 batch loss 2.65482616 batch mAP 0.125 batch PCKh 0.400390625\n",
      "Trained batch 1822 batch loss 2.77621102 batch mAP 0.13671875 batch PCKh 0.435546875\n",
      "Trained batch 1823 batch loss 2.69043255 batch mAP 0.1328125 batch PCKh 0.576171875\n",
      "Trained batch 1824 batch loss 2.87428212 batch mAP 0.109375 batch PCKh 0.66015625\n",
      "Trained batch 1825 batch loss 2.7726357 batch mAP 0.087890625 batch PCKh 0.4140625\n",
      "Trained batch 1826 batch loss 2.81406546 batch mAP 0.064453125 batch PCKh 0.5703125\n",
      "Trained batch 1827 batch loss 2.86613226 batch mAP 0.09765625 batch PCKh 0.49609375\n",
      "Trained batch 1828 batch loss 2.84577 batch mAP 0.072265625 batch PCKh 0.513671875\n",
      "Trained batch 1829 batch loss 2.78098631 batch mAP 0.111328125 batch PCKh 0.37890625\n",
      "Trained batch 1830 batch loss 3.01442313 batch mAP 0.142578125 batch PCKh 0.533203125\n",
      "Trained batch 1831 batch loss 3.24704289 batch mAP 0.134765625 batch PCKh 0.349609375\n",
      "Trained batch 1832 batch loss 3.34881353 batch mAP 0.126953125 batch PCKh 0.416015625\n",
      "Trained batch 1833 batch loss 3.05063295 batch mAP 0.095703125 batch PCKh 0.5234375\n",
      "Trained batch 1834 batch loss 3.0999825 batch mAP 0.07421875 batch PCKh 0.408203125\n",
      "Trained batch 1835 batch loss 3.05078268 batch mAP 0.072265625 batch PCKh 0.427734375\n",
      "Trained batch 1836 batch loss 2.92592812 batch mAP 0.107421875 batch PCKh 0.53125\n",
      "Trained batch 1837 batch loss 2.92469525 batch mAP 0.10546875 batch PCKh 0.568359375\n",
      "Trained batch 1838 batch loss 2.95204115 batch mAP 0.091796875 batch PCKh 0.890625\n",
      "Trained batch 1839 batch loss 2.83174849 batch mAP 0.125 batch PCKh 0.5078125\n",
      "Trained batch 1840 batch loss 2.99950027 batch mAP 0.13671875 batch PCKh 0.630859375\n",
      "Trained batch 1841 batch loss 2.90135646 batch mAP 0.154296875 batch PCKh 0.431640625\n",
      "Trained batch 1842 batch loss 3.09095311 batch mAP 0.142578125 batch PCKh 0.5\n",
      "Trained batch 1843 batch loss 2.90142822 batch mAP 0.1484375 batch PCKh 0.474609375\n",
      "Trained batch 1844 batch loss 3.16379905 batch mAP 0.171875 batch PCKh 0.296875\n",
      "Trained batch 1845 batch loss 2.97276 batch mAP 0.158203125 batch PCKh 0.42578125\n",
      "Trained batch 1846 batch loss 3.03453231 batch mAP 0.126953125 batch PCKh 0.466796875\n",
      "Trained batch 1847 batch loss 3.26258 batch mAP 0.18359375 batch PCKh 0.34765625\n",
      "Trained batch 1848 batch loss 3.43569446 batch mAP 0.185546875 batch PCKh 0.416015625\n",
      "Trained batch 1849 batch loss 3.21031094 batch mAP 0.1953125 batch PCKh 0.439453125\n",
      "Trained batch 1850 batch loss 3.37308073 batch mAP 0.203125 batch PCKh 0.255859375\n",
      "Trained batch 1851 batch loss 3.36971235 batch mAP 0.2265625 batch PCKh 0.427734375\n",
      "Trained batch 1852 batch loss 3.22762561 batch mAP 0.1796875 batch PCKh 0.291015625\n",
      "Trained batch 1853 batch loss 3.20680761 batch mAP 0.20703125 batch PCKh 0.267578125\n",
      "Trained batch 1854 batch loss 3.21331787 batch mAP 0.212890625 batch PCKh 0.365234375\n",
      "Trained batch 1855 batch loss 3.29547381 batch mAP 0.20703125 batch PCKh 0.30859375\n",
      "Trained batch 1856 batch loss 3.21772718 batch mAP 0.1953125 batch PCKh 0.322265625\n",
      "Trained batch 1857 batch loss 3.06742406 batch mAP 0.2265625 batch PCKh 0.267578125\n",
      "Trained batch 1858 batch loss 3.29306984 batch mAP 0.224609375 batch PCKh 0.171875\n",
      "Trained batch 1859 batch loss 3.25255871 batch mAP 0.302734375 batch PCKh 0.228515625\n",
      "Trained batch 1860 batch loss 3.25248647 batch mAP 0.25390625 batch PCKh 0.259765625\n",
      "Trained batch 1861 batch loss 3.15207052 batch mAP 0.33203125 batch PCKh 0.35546875\n",
      "Trained batch 1862 batch loss 2.99372625 batch mAP 0.361328125 batch PCKh 0.412109375\n",
      "Trained batch 1863 batch loss 2.94401193 batch mAP 0.314453125 batch PCKh 0.396484375\n",
      "Trained batch 1864 batch loss 2.8172698 batch mAP 0.3046875 batch PCKh 0.328125\n",
      "Trained batch 1865 batch loss 2.74927568 batch mAP 0.2890625 batch PCKh 0.3046875\n",
      "Trained batch 1866 batch loss 3.50744629 batch mAP 0.3125 batch PCKh 0.33203125\n",
      "Trained batch 1867 batch loss 3.0059638 batch mAP 0.3359375 batch PCKh 0.513671875\n",
      "Trained batch 1868 batch loss 3.20548201 batch mAP 0.25390625 batch PCKh 0.431640625\n",
      "Trained batch 1869 batch loss 2.86239529 batch mAP 0.25390625 batch PCKh 0.392578125\n",
      "Trained batch 1870 batch loss 3.09095716 batch mAP 0.291015625 batch PCKh 0.4296875\n",
      "Trained batch 1871 batch loss 2.92020607 batch mAP 0.291015625 batch PCKh 0.5078125\n",
      "Trained batch 1872 batch loss 3.04293346 batch mAP 0.17578125 batch PCKh 0.544921875\n",
      "Trained batch 1873 batch loss 3.04112887 batch mAP 0.224609375 batch PCKh 0.390625\n",
      "Trained batch 1874 batch loss 3.11138177 batch mAP 0.171875 batch PCKh 0.4609375\n",
      "Trained batch 1875 batch loss 3.22521043 batch mAP 0.1640625 batch PCKh 0.470703125\n",
      "Trained batch 1876 batch loss 2.98119545 batch mAP 0.15234375 batch PCKh 0.279296875\n",
      "Trained batch 1877 batch loss 2.98543262 batch mAP 0.12890625 batch PCKh 0.248046875\n",
      "Trained batch 1878 batch loss 3.01199102 batch mAP 0.171875 batch PCKh 0.392578125\n",
      "Trained batch 1879 batch loss 2.92506909 batch mAP 0.11328125 batch PCKh 0.2109375\n",
      "Trained batch 1880 batch loss 2.79020071 batch mAP 0.20703125 batch PCKh 0.419921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1881 batch loss 3.16290355 batch mAP 0.169921875 batch PCKh 0.396484375\n",
      "Trained batch 1882 batch loss 3.17039776 batch mAP 0.1953125 batch PCKh 0.40234375\n",
      "Trained batch 1883 batch loss 2.9742012 batch mAP 0.1875 batch PCKh 0.400390625\n",
      "Trained batch 1884 batch loss 2.92069936 batch mAP 0.158203125 batch PCKh 0.42578125\n",
      "Trained batch 1885 batch loss 2.82615519 batch mAP 0.1875 batch PCKh 0.40234375\n",
      "Trained batch 1886 batch loss 2.91325021 batch mAP 0.259765625 batch PCKh 0.35546875\n",
      "Trained batch 1887 batch loss 2.95743561 batch mAP 0.29296875 batch PCKh 0.541015625\n",
      "Trained batch 1888 batch loss 2.93487191 batch mAP 0.24609375 batch PCKh 0.591796875\n",
      "Trained batch 1889 batch loss 2.86976624 batch mAP 0.2578125 batch PCKh 0.4140625\n",
      "Trained batch 1890 batch loss 3.06479049 batch mAP 0.255859375 batch PCKh 0.21484375\n",
      "Trained batch 1891 batch loss 3.04983068 batch mAP 0.22265625 batch PCKh 0.4609375\n",
      "Trained batch 1892 batch loss 2.8121016 batch mAP 0.248046875 batch PCKh 0.576171875\n",
      "Trained batch 1893 batch loss 2.64224958 batch mAP 0.318359375 batch PCKh 0.244140625\n",
      "Trained batch 1894 batch loss 2.79077196 batch mAP 0.2265625 batch PCKh 0.3046875\n",
      "Trained batch 1895 batch loss 2.94531059 batch mAP 0.306640625 batch PCKh 0.62890625\n",
      "Trained batch 1896 batch loss 2.88011217 batch mAP 0.236328125 batch PCKh 0.572265625\n",
      "Trained batch 1897 batch loss 2.57835317 batch mAP 0.2578125 batch PCKh 0.53125\n",
      "Trained batch 1898 batch loss 2.63626909 batch mAP 0.22265625 batch PCKh 0.56640625\n",
      "Trained batch 1899 batch loss 2.6647296 batch mAP 0.228515625 batch PCKh 0.642578125\n",
      "Trained batch 1900 batch loss 2.79598451 batch mAP 0.181640625 batch PCKh 0.658203125\n",
      "Trained batch 1901 batch loss 2.63652396 batch mAP 0.22265625 batch PCKh 0.328125\n",
      "Trained batch 1902 batch loss 2.76861024 batch mAP 0.1328125 batch PCKh 0.318359375\n",
      "Trained batch 1903 batch loss 2.90545225 batch mAP 0.126953125 batch PCKh 0.580078125\n",
      "Trained batch 1904 batch loss 2.87792206 batch mAP 0.162109375 batch PCKh 0.298828125\n",
      "Trained batch 1905 batch loss 2.70087743 batch mAP 0.119140625 batch PCKh 0.34765625\n",
      "Trained batch 1906 batch loss 2.49372435 batch mAP 0.1328125 batch PCKh 0.3046875\n",
      "Trained batch 1907 batch loss 2.63032484 batch mAP 0.150390625 batch PCKh 0.33203125\n",
      "Trained batch 1908 batch loss 2.76297927 batch mAP 0.126953125 batch PCKh 0.412109375\n",
      "Trained batch 1909 batch loss 2.8396337 batch mAP 0.095703125 batch PCKh 0.56640625\n",
      "Trained batch 1910 batch loss 2.91705704 batch mAP 0.12109375 batch PCKh 0.32421875\n",
      "Trained batch 1911 batch loss 2.83299065 batch mAP 0.10546875 batch PCKh 0.462890625\n",
      "Trained batch 1912 batch loss 2.82111216 batch mAP 0.08984375 batch PCKh 0.421875\n",
      "Trained batch 1913 batch loss 2.68285561 batch mAP 0.0859375 batch PCKh 0.28125\n",
      "Trained batch 1914 batch loss 2.8071332 batch mAP 0.107421875 batch PCKh 0.494140625\n",
      "Trained batch 1915 batch loss 2.76784825 batch mAP 0.064453125 batch PCKh 0.404296875\n",
      "Trained batch 1916 batch loss 2.7736094 batch mAP 0.072265625 batch PCKh 0.4375\n",
      "Trained batch 1917 batch loss 2.68713617 batch mAP 0.09765625 batch PCKh 0.298828125\n",
      "Trained batch 1918 batch loss 3.11101484 batch mAP 0.08984375 batch PCKh 0.2890625\n",
      "Trained batch 1919 batch loss 3.16778088 batch mAP 0.08203125 batch PCKh 0.265625\n",
      "Trained batch 1920 batch loss 3.22740269 batch mAP 0.140625 batch PCKh 0.34765625\n",
      "Trained batch 1921 batch loss 3.31859803 batch mAP 0.1171875 batch PCKh 0.345703125\n",
      "Trained batch 1922 batch loss 3.55459023 batch mAP 0.123046875 batch PCKh 0.15625\n",
      "Trained batch 1923 batch loss 3.51520801 batch mAP 0.201171875 batch PCKh 0.16796875\n",
      "Trained batch 1924 batch loss 3.04434347 batch mAP 0.251953125 batch PCKh 0.32421875\n",
      "Trained batch 1925 batch loss 3.20616531 batch mAP 0.20703125 batch PCKh 0.408203125\n",
      "Trained batch 1926 batch loss 3.08453822 batch mAP 0.20703125 batch PCKh 0.56640625\n",
      "Trained batch 1927 batch loss 3.10513926 batch mAP 0.251953125 batch PCKh 0.380859375\n",
      "Trained batch 1928 batch loss 2.94739056 batch mAP 0.2109375 batch PCKh 0.626953125\n",
      "Trained batch 1929 batch loss 3.01971436 batch mAP 0.173828125 batch PCKh 0.48046875\n",
      "Trained batch 1930 batch loss 3.16872668 batch mAP 0.2578125 batch PCKh 0.5390625\n",
      "Trained batch 1931 batch loss 3.0037117 batch mAP 0.224609375 batch PCKh 0.34375\n",
      "Trained batch 1932 batch loss 3.16824675 batch mAP 0.263671875 batch PCKh 0.384765625\n",
      "Trained batch 1933 batch loss 3.00320339 batch mAP 0.228515625 batch PCKh 0.439453125\n",
      "Trained batch 1934 batch loss 3.01908159 batch mAP 0.287109375 batch PCKh 0.44921875\n",
      "Trained batch 1935 batch loss 2.95098543 batch mAP 0.2734375 batch PCKh 0.5\n",
      "Trained batch 1936 batch loss 3.10471916 batch mAP 0.341796875 batch PCKh 0.404296875\n",
      "Trained batch 1937 batch loss 2.9329195 batch mAP 0.333984375 batch PCKh 0.43359375\n",
      "Trained batch 1938 batch loss 2.97842836 batch mAP 0.328125 batch PCKh 0.560546875\n",
      "Trained batch 1939 batch loss 2.87099266 batch mAP 0.330078125 batch PCKh 0.517578125\n",
      "Trained batch 1940 batch loss 3.03470159 batch mAP 0.263671875 batch PCKh 0.50390625\n",
      "Trained batch 1941 batch loss 3.07516575 batch mAP 0.3046875 batch PCKh 0.482421875\n",
      "Trained batch 1942 batch loss 3.26930809 batch mAP 0.3125 batch PCKh 0.376953125\n",
      "Trained batch 1943 batch loss 3.10201502 batch mAP 0.298828125 batch PCKh 0.4375\n",
      "Trained batch 1944 batch loss 3.13283 batch mAP 0.318359375 batch PCKh 0.53125\n",
      "Trained batch 1945 batch loss 2.94643116 batch mAP 0.205078125 batch PCKh 0.61328125\n",
      "Trained batch 1946 batch loss 2.82901597 batch mAP 0.263671875 batch PCKh 0.255859375\n",
      "Trained batch 1947 batch loss 2.90379047 batch mAP 0.28515625 batch PCKh 0.345703125\n",
      "Trained batch 1948 batch loss 2.84153962 batch mAP 0.29296875 batch PCKh 0.466796875\n",
      "Trained batch 1949 batch loss 2.84478951 batch mAP 0.310546875 batch PCKh 0.5625\n",
      "Trained batch 1950 batch loss 2.84951401 batch mAP 0.27734375 batch PCKh 0.693359375\n",
      "Trained batch 1951 batch loss 2.63752627 batch mAP 0.189453125 batch PCKh 0.6640625\n",
      "Trained batch 1952 batch loss 2.87815285 batch mAP 0.2421875 batch PCKh 0.677734375\n",
      "Trained batch 1953 batch loss 3.10454 batch mAP 0.251953125 batch PCKh 0.376953125\n",
      "Trained batch 1954 batch loss 2.89139485 batch mAP 0.2109375 batch PCKh 0.7265625\n",
      "Trained batch 1955 batch loss 2.67911339 batch mAP 0.15234375 batch PCKh 0.642578125\n",
      "Trained batch 1956 batch loss 2.88847876 batch mAP 0.25390625 batch PCKh 0.431640625\n",
      "Trained batch 1957 batch loss 2.8822813 batch mAP 0.279296875 batch PCKh 0.330078125\n",
      "Trained batch 1958 batch loss 2.82994819 batch mAP 0.244140625 batch PCKh 0.55859375\n",
      "Trained batch 1959 batch loss 2.67806149 batch mAP 0.181640625 batch PCKh 0.689453125\n",
      "Trained batch 1960 batch loss 2.77875185 batch mAP 0.271484375 batch PCKh 0.55859375\n",
      "Trained batch 1961 batch loss 2.73376369 batch mAP 0.138671875 batch PCKh 0.646484375\n",
      "Trained batch 1962 batch loss 2.85170054 batch mAP 0.236328125 batch PCKh 0.533203125\n",
      "Trained batch 1963 batch loss 2.91665435 batch mAP 0.1875 batch PCKh 0.484375\n",
      "Trained batch 1964 batch loss 2.78580952 batch mAP 0.15625 batch PCKh 0.41796875\n",
      "Trained batch 1965 batch loss 2.95366216 batch mAP 0.201171875 batch PCKh 0.44921875\n",
      "Trained batch 1966 batch loss 2.85185337 batch mAP 0.123046875 batch PCKh 0.55078125\n",
      "Trained batch 1967 batch loss 2.83497405 batch mAP 0.119140625 batch PCKh 0.484375\n",
      "Trained batch 1968 batch loss 3.01731586 batch mAP 0.134765625 batch PCKh 0.181640625\n",
      "Trained batch 1969 batch loss 2.59578753 batch mAP 0.099609375 batch PCKh 0.24609375\n",
      "Trained batch 1970 batch loss 2.65361023 batch mAP 0.12109375 batch PCKh 0.55859375\n",
      "Trained batch 1971 batch loss 2.88464 batch mAP 0.0859375 batch PCKh 0.537109375\n",
      "Trained batch 1972 batch loss 2.88235664 batch mAP 0.087890625 batch PCKh 0.68359375\n",
      "Trained batch 1973 batch loss 3.01642609 batch mAP 0.115234375 batch PCKh 0.578125\n",
      "Trained batch 1974 batch loss 2.91544 batch mAP 0.125 batch PCKh 0.408203125\n",
      "Trained batch 1975 batch loss 2.84419918 batch mAP 0.1328125 batch PCKh 0.279296875\n",
      "Trained batch 1976 batch loss 2.75240707 batch mAP 0.109375 batch PCKh 0.400390625\n",
      "Trained batch 1977 batch loss 3.24394751 batch mAP 0.083984375 batch PCKh 0.490234375\n",
      "Trained batch 1978 batch loss 3.00826907 batch mAP 0.103515625 batch PCKh 0.404296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1979 batch loss 2.77140188 batch mAP 0.08984375 batch PCKh 0.171875\n",
      "Trained batch 1980 batch loss 2.64050508 batch mAP 0.10546875 batch PCKh 0.158203125\n",
      "Trained batch 1981 batch loss 2.70972419 batch mAP 0.138671875 batch PCKh 0.51171875\n",
      "Trained batch 1982 batch loss 2.84727883 batch mAP 0.083984375 batch PCKh 0.68359375\n",
      "Trained batch 1983 batch loss 2.77258039 batch mAP 0.115234375 batch PCKh 0.62109375\n",
      "Trained batch 1984 batch loss 2.69677258 batch mAP 0.10546875 batch PCKh 0.51953125\n",
      "Trained batch 1985 batch loss 3.11821699 batch mAP 0.126953125 batch PCKh 0.51171875\n",
      "Trained batch 1986 batch loss 3.11151958 batch mAP 0.087890625 batch PCKh 0.4765625\n",
      "Trained batch 1987 batch loss 3.15047598 batch mAP 0.08203125 batch PCKh 0.341796875\n",
      "Trained batch 1988 batch loss 3.26461267 batch mAP 0.107421875 batch PCKh 0.30859375\n",
      "Trained batch 1989 batch loss 3.13413191 batch mAP 0.12890625 batch PCKh 0.501953125\n",
      "Trained batch 1990 batch loss 2.99754524 batch mAP 0.1171875 batch PCKh 0.474609375\n",
      "Trained batch 1991 batch loss 3.02988362 batch mAP 0.142578125 batch PCKh 0.453125\n",
      "Trained batch 1992 batch loss 2.75329161 batch mAP 0.13671875 batch PCKh 0.263671875\n",
      "Trained batch 1993 batch loss 2.74133134 batch mAP 0.162109375 batch PCKh 0.48046875\n",
      "Trained batch 1994 batch loss 2.642344 batch mAP 0.17578125 batch PCKh 0.23828125\n",
      "Trained batch 1995 batch loss 2.77637744 batch mAP 0.185546875 batch PCKh 0.09375\n",
      "Trained batch 1996 batch loss 2.30856752 batch mAP 0.12890625 batch PCKh 0.005859375\n",
      "Trained batch 1997 batch loss 2.66880536 batch mAP 0.18359375 batch PCKh 0.212890625\n",
      "Trained batch 1998 batch loss 2.26714373 batch mAP 0.15234375 batch PCKh 0.083984375\n",
      "Trained batch 1999 batch loss 2.29553318 batch mAP 0.162109375 batch PCKh 0.150390625\n",
      "Trained batch 2000 batch loss 2.81229401 batch mAP 0.140625 batch PCKh 0.353515625\n",
      "Trained batch 2001 batch loss 2.95307708 batch mAP 0.19140625 batch PCKh 0.51171875\n",
      "Trained batch 2002 batch loss 2.74855661 batch mAP 0.107421875 batch PCKh 0.62890625\n",
      "Trained batch 2003 batch loss 2.54356813 batch mAP 0.140625 batch PCKh 0.39453125\n",
      "Trained batch 2004 batch loss 2.93675089 batch mAP 0.140625 batch PCKh 0.59375\n",
      "Trained batch 2005 batch loss 2.92418122 batch mAP 0.134765625 batch PCKh 0.525390625\n",
      "Trained batch 2006 batch loss 2.80116558 batch mAP 0.150390625 batch PCKh 0.462890625\n",
      "Trained batch 2007 batch loss 2.87263513 batch mAP 0.111328125 batch PCKh 0.703125\n",
      "Trained batch 2008 batch loss 2.72921181 batch mAP 0.078125 batch PCKh 0.677734375\n",
      "Trained batch 2009 batch loss 2.92270684 batch mAP 0.138671875 batch PCKh 0.666015625\n",
      "Trained batch 2010 batch loss 2.61958265 batch mAP 0.1484375 batch PCKh 0.404296875\n",
      "Trained batch 2011 batch loss 2.84848976 batch mAP 0.130859375 batch PCKh 0.548828125\n",
      "Trained batch 2012 batch loss 3.10733533 batch mAP 0.080078125 batch PCKh 0.359375\n",
      "Trained batch 2013 batch loss 2.67978287 batch mAP 0.115234375 batch PCKh 0.271484375\n",
      "Trained batch 2014 batch loss 2.94404531 batch mAP 0.1015625 batch PCKh 0.458984375\n",
      "Trained batch 2015 batch loss 2.80443287 batch mAP 0.115234375 batch PCKh 0.41796875\n",
      "Trained batch 2016 batch loss 2.9678266 batch mAP 0.09375 batch PCKh 0.33203125\n",
      "Trained batch 2017 batch loss 3.04948902 batch mAP 0.080078125 batch PCKh 0.326171875\n",
      "Trained batch 2018 batch loss 2.71664429 batch mAP 0.07421875 batch PCKh 0.322265625\n",
      "Trained batch 2019 batch loss 2.59819961 batch mAP 0.142578125 batch PCKh 0.318359375\n",
      "Trained batch 2020 batch loss 2.72152185 batch mAP 0.095703125 batch PCKh 0.47265625\n",
      "Trained batch 2021 batch loss 2.76951814 batch mAP 0.12109375 batch PCKh 0.50390625\n",
      "Trained batch 2022 batch loss 2.84262609 batch mAP 0.091796875 batch PCKh 0.705078125\n",
      "Trained batch 2023 batch loss 2.77472448 batch mAP 0.09765625 batch PCKh 0.583984375\n",
      "Trained batch 2024 batch loss 2.74839091 batch mAP 0.1171875 batch PCKh 0.548828125\n",
      "Trained batch 2025 batch loss 2.77929497 batch mAP 0.048828125 batch PCKh 0.384765625\n",
      "Trained batch 2026 batch loss 3.20263958 batch mAP 0.072265625 batch PCKh 0.498046875\n",
      "Trained batch 2027 batch loss 2.90185785 batch mAP 0.0625 batch PCKh 0.5859375\n",
      "Trained batch 2028 batch loss 3.20760798 batch mAP 0.05078125 batch PCKh 0.486328125\n",
      "Trained batch 2029 batch loss 3.28130293 batch mAP 0.072265625 batch PCKh 0.158203125\n",
      "Trained batch 2030 batch loss 3.14578 batch mAP 0.0859375 batch PCKh 0.357421875\n",
      "Trained batch 2031 batch loss 3.46534395 batch mAP 0.1015625 batch PCKh 0.216796875\n",
      "Trained batch 2032 batch loss 3.04296255 batch mAP 0.125 batch PCKh 0.345703125\n",
      "Trained batch 2033 batch loss 2.97175765 batch mAP 0.126953125 batch PCKh 0.515625\n",
      "Trained batch 2034 batch loss 2.46188951 batch mAP 0.150390625 batch PCKh 0.359375\n",
      "Trained batch 2035 batch loss 2.86894345 batch mAP 0.212890625 batch PCKh 0.634765625\n",
      "Trained batch 2036 batch loss 2.50856853 batch mAP 0.232421875 batch PCKh 0.392578125\n",
      "Trained batch 2037 batch loss 2.4592073 batch mAP 0.28125 batch PCKh 0.26953125\n",
      "Trained batch 2038 batch loss 2.80075693 batch mAP 0.302734375 batch PCKh 0.34375\n",
      "Trained batch 2039 batch loss 2.81750822 batch mAP 0.32421875 batch PCKh 0.5\n",
      "Trained batch 2040 batch loss 2.58709383 batch mAP 0.28515625 batch PCKh 0.638671875\n",
      "Trained batch 2041 batch loss 2.58889747 batch mAP 0.271484375 batch PCKh 0.623046875\n",
      "Trained batch 2042 batch loss 2.44895506 batch mAP 0.26171875 batch PCKh 0.5546875\n",
      "Trained batch 2043 batch loss 2.66223454 batch mAP 0.32421875 batch PCKh 0.515625\n",
      "Trained batch 2044 batch loss 2.87674689 batch mAP 0.255859375 batch PCKh 0.517578125\n",
      "Trained batch 2045 batch loss 3.13286781 batch mAP 0.294921875 batch PCKh 0.275390625\n",
      "Trained batch 2046 batch loss 2.90722799 batch mAP 0.27734375 batch PCKh 0.318359375\n",
      "Trained batch 2047 batch loss 2.93416953 batch mAP 0.2890625 batch PCKh 0.419921875\n",
      "Trained batch 2048 batch loss 2.89516449 batch mAP 0.302734375 batch PCKh 0.359375\n",
      "Trained batch 2049 batch loss 2.91806316 batch mAP 0.265625 batch PCKh 0.45703125\n",
      "Trained batch 2050 batch loss 3.03695202 batch mAP 0.240234375 batch PCKh 0.318359375\n",
      "Trained batch 2051 batch loss 3.33140135 batch mAP 0.302734375 batch PCKh 0.349609375\n",
      "Trained batch 2052 batch loss 3.0247674 batch mAP 0.251953125 batch PCKh 0.375\n",
      "Trained batch 2053 batch loss 2.9328208 batch mAP 0.205078125 batch PCKh 0.494140625\n",
      "Trained batch 2054 batch loss 2.98493171 batch mAP 0.212890625 batch PCKh 0.302734375\n",
      "Trained batch 2055 batch loss 2.92536831 batch mAP 0.203125 batch PCKh 0.33984375\n",
      "Trained batch 2056 batch loss 3.06202459 batch mAP 0.2578125 batch PCKh 0.318359375\n",
      "Trained batch 2057 batch loss 3.12735224 batch mAP 0.216796875 batch PCKh 0.431640625\n",
      "Trained batch 2058 batch loss 3.06294513 batch mAP 0.13671875 batch PCKh 0.3359375\n",
      "Trained batch 2059 batch loss 3.25775743 batch mAP 0.205078125 batch PCKh 0.3046875\n",
      "Trained batch 2060 batch loss 2.92716622 batch mAP 0.236328125 batch PCKh 0.47265625\n",
      "Trained batch 2061 batch loss 2.80102491 batch mAP 0.23046875 batch PCKh 0.58984375\n",
      "Trained batch 2062 batch loss 2.85151863 batch mAP 0.2109375 batch PCKh 0.599609375\n",
      "Trained batch 2063 batch loss 2.83267879 batch mAP 0.197265625 batch PCKh 0.576171875\n",
      "Trained batch 2064 batch loss 2.87523794 batch mAP 0.205078125 batch PCKh 0.681640625\n",
      "Trained batch 2065 batch loss 3.17111969 batch mAP 0.22265625 batch PCKh 0.357421875\n",
      "Trained batch 2066 batch loss 3.17962623 batch mAP 0.173828125 batch PCKh 0.427734375\n",
      "Trained batch 2067 batch loss 2.99103141 batch mAP 0.1640625 batch PCKh 0.55078125\n",
      "Trained batch 2068 batch loss 3.00923705 batch mAP 0.20703125 batch PCKh 0.5859375\n",
      "Trained batch 2069 batch loss 2.80876398 batch mAP 0.138671875 batch PCKh 0.640625\n",
      "Trained batch 2070 batch loss 2.64858961 batch mAP 0.091796875 batch PCKh 0.75\n",
      "Trained batch 2071 batch loss 2.74445629 batch mAP 0.091796875 batch PCKh 0.689453125\n",
      "Trained batch 2072 batch loss 2.64403343 batch mAP 0.078125 batch PCKh 0.7109375\n",
      "Trained batch 2073 batch loss 2.84199333 batch mAP 0.0703125 batch PCKh 0.615234375\n",
      "Trained batch 2074 batch loss 2.77776814 batch mAP 0.0546875 batch PCKh 0.5703125\n",
      "Trained batch 2075 batch loss 2.70104361 batch mAP 0.0859375 batch PCKh 0.61328125\n",
      "Trained batch 2076 batch loss 2.7547884 batch mAP 0.037109375 batch PCKh 0.634765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2077 batch loss 2.71774244 batch mAP 0.078125 batch PCKh 0.544921875\n",
      "Trained batch 2078 batch loss 2.77639818 batch mAP 0.0390625 batch PCKh 0.66015625\n",
      "Trained batch 2079 batch loss 3.0636344 batch mAP 0.044921875 batch PCKh 0.41015625\n",
      "Trained batch 2080 batch loss 2.78351307 batch mAP 0.02734375 batch PCKh 0.587890625\n",
      "Trained batch 2081 batch loss 2.92349195 batch mAP 0.02734375 batch PCKh 0.51171875\n",
      "Trained batch 2082 batch loss 2.8346498 batch mAP 0.05078125 batch PCKh 0.55078125\n",
      "Trained batch 2083 batch loss 2.89138556 batch mAP 0.021484375 batch PCKh 0.57421875\n",
      "Trained batch 2084 batch loss 2.99326706 batch mAP 0.041015625 batch PCKh 0.337890625\n",
      "Trained batch 2085 batch loss 2.7577455 batch mAP 0.0546875 batch PCKh 0.40625\n",
      "Trained batch 2086 batch loss 3.16275263 batch mAP 0.068359375 batch PCKh 0.259765625\n",
      "Trained batch 2087 batch loss 3.24475551 batch mAP 0.0859375 batch PCKh 0.30859375\n",
      "Trained batch 2088 batch loss 2.98118 batch mAP 0.07421875 batch PCKh 0.505859375\n",
      "Trained batch 2089 batch loss 2.75651836 batch mAP 0.09765625 batch PCKh 0.4453125\n",
      "Trained batch 2090 batch loss 2.71496034 batch mAP 0.056640625 batch PCKh 0.7109375\n",
      "Trained batch 2091 batch loss 2.88872552 batch mAP 0.083984375 batch PCKh 0.248046875\n",
      "Trained batch 2092 batch loss 2.97708893 batch mAP 0.09375 batch PCKh 0.48046875\n",
      "Trained batch 2093 batch loss 2.87608814 batch mAP 0.150390625 batch PCKh 0.53125\n",
      "Trained batch 2094 batch loss 2.84736919 batch mAP 0.11328125 batch PCKh 0.501953125\n",
      "Trained batch 2095 batch loss 2.72333288 batch mAP 0.142578125 batch PCKh 0.33203125\n",
      "Trained batch 2096 batch loss 2.79213357 batch mAP 0.123046875 batch PCKh 0.626953125\n",
      "Trained batch 2097 batch loss 2.73935652 batch mAP 0.111328125 batch PCKh 0.3515625\n",
      "Trained batch 2098 batch loss 2.892941 batch mAP 0.111328125 batch PCKh 0.478515625\n",
      "Trained batch 2099 batch loss 2.79711151 batch mAP 0.109375 batch PCKh 0.552734375\n",
      "Trained batch 2100 batch loss 2.81027198 batch mAP 0.169921875 batch PCKh 0.359375\n",
      "Trained batch 2101 batch loss 3.10629034 batch mAP 0.138671875 batch PCKh 0.443359375\n",
      "Trained batch 2102 batch loss 3.2425456 batch mAP 0.119140625 batch PCKh 0.34375\n",
      "Trained batch 2103 batch loss 2.98591185 batch mAP 0.14453125 batch PCKh 0.396484375\n",
      "Trained batch 2104 batch loss 2.84767771 batch mAP 0.166015625 batch PCKh 0.521484375\n",
      "Trained batch 2105 batch loss 3.18392324 batch mAP 0.185546875 batch PCKh 0.43359375\n",
      "Trained batch 2106 batch loss 3.0057869 batch mAP 0.15625 batch PCKh 0.693359375\n",
      "Trained batch 2107 batch loss 3.01382899 batch mAP 0.107421875 batch PCKh 0.34765625\n",
      "Trained batch 2108 batch loss 2.82508898 batch mAP 0.123046875 batch PCKh 0.580078125\n",
      "Trained batch 2109 batch loss 3.12881231 batch mAP 0.158203125 batch PCKh 0.453125\n",
      "Trained batch 2110 batch loss 2.94984388 batch mAP 0.123046875 batch PCKh 0.529296875\n",
      "Trained batch 2111 batch loss 2.9661417 batch mAP 0.1796875 batch PCKh 0.431640625\n",
      "Trained batch 2112 batch loss 3.17142367 batch mAP 0.162109375 batch PCKh 0.4296875\n",
      "Trained batch 2113 batch loss 3.06495285 batch mAP 0.158203125 batch PCKh 0.4609375\n",
      "Trained batch 2114 batch loss 2.94834208 batch mAP 0.193359375 batch PCKh 0.41796875\n",
      "Trained batch 2115 batch loss 3.00285482 batch mAP 0.1796875 batch PCKh 0.322265625\n",
      "Trained batch 2116 batch loss 3.10085106 batch mAP 0.23828125 batch PCKh 0.474609375\n",
      "Trained batch 2117 batch loss 2.96116734 batch mAP 0.189453125 batch PCKh 0.39453125\n",
      "Trained batch 2118 batch loss 2.76001358 batch mAP 0.23046875 batch PCKh 0.34375\n",
      "Trained batch 2119 batch loss 2.82102561 batch mAP 0.2265625 batch PCKh 0.494140625\n",
      "Trained batch 2120 batch loss 2.91059375 batch mAP 0.22265625 batch PCKh 0.546875\n",
      "Trained batch 2121 batch loss 2.66797471 batch mAP 0.197265625 batch PCKh 0.587890625\n",
      "Trained batch 2122 batch loss 2.7481432 batch mAP 0.20703125 batch PCKh 0.431640625\n",
      "Trained batch 2123 batch loss 2.71212268 batch mAP 0.15625 batch PCKh 0.54296875\n",
      "Trained batch 2124 batch loss 2.69445705 batch mAP 0.177734375 batch PCKh 0.451171875\n",
      "Trained batch 2125 batch loss 2.96420717 batch mAP 0.142578125 batch PCKh 0.41015625\n",
      "Trained batch 2126 batch loss 2.84529495 batch mAP 0.12109375 batch PCKh 0.666015625\n",
      "Trained batch 2127 batch loss 2.95251942 batch mAP 0.126953125 batch PCKh 0.580078125\n",
      "Trained batch 2128 batch loss 2.6813128 batch mAP 0.119140625 batch PCKh 0.57421875\n",
      "Trained batch 2129 batch loss 2.88454294 batch mAP 0.080078125 batch PCKh 0.64453125\n",
      "Trained batch 2130 batch loss 2.91342211 batch mAP 0.119140625 batch PCKh 0.619140625\n",
      "Trained batch 2131 batch loss 2.95040107 batch mAP 0.1328125 batch PCKh 0.576171875\n",
      "Trained batch 2132 batch loss 2.7673192 batch mAP 0.1015625 batch PCKh 0.61328125\n",
      "Trained batch 2133 batch loss 2.74843597 batch mAP 0.068359375 batch PCKh 0.5\n",
      "Trained batch 2134 batch loss 2.71098876 batch mAP 0.068359375 batch PCKh 0.595703125\n",
      "Trained batch 2135 batch loss 2.5882659 batch mAP 0.111328125 batch PCKh 0.55078125\n",
      "Trained batch 2136 batch loss 2.67723513 batch mAP 0.060546875 batch PCKh 0.693359375\n",
      "Trained batch 2137 batch loss 2.59815788 batch mAP 0.052734375 batch PCKh 0.669921875\n",
      "Trained batch 2138 batch loss 2.70123148 batch mAP 0.0390625 batch PCKh 0.466796875\n",
      "Trained batch 2139 batch loss 2.83319044 batch mAP 0.060546875 batch PCKh 0.384765625\n",
      "Trained batch 2140 batch loss 3.15921211 batch mAP 0.0546875 batch PCKh 0.33984375\n",
      "Trained batch 2141 batch loss 2.90912271 batch mAP 0.025390625 batch PCKh 0.568359375\n",
      "Trained batch 2142 batch loss 3.02426195 batch mAP 0.0390625 batch PCKh 0.5234375\n",
      "Trained batch 2143 batch loss 3.15064 batch mAP 0.029296875 batch PCKh 0.447265625\n",
      "Trained batch 2144 batch loss 3.42461348 batch mAP 0.046875 batch PCKh 0.20703125\n",
      "Trained batch 2145 batch loss 3.28599596 batch mAP 0.0390625 batch PCKh 0.431640625\n",
      "Trained batch 2146 batch loss 3.30473089 batch mAP 0.060546875 batch PCKh 0.302734375\n",
      "Trained batch 2147 batch loss 3.12373519 batch mAP 0.07421875 batch PCKh 0.494140625\n",
      "Trained batch 2148 batch loss 2.96263242 batch mAP 0.09375 batch PCKh 0.4453125\n",
      "Trained batch 2149 batch loss 2.95654058 batch mAP 0.078125 batch PCKh 0.53125\n",
      "Trained batch 2150 batch loss 2.94803047 batch mAP 0.076171875 batch PCKh 0.533203125\n",
      "Trained batch 2151 batch loss 3.05392289 batch mAP 0.103515625 batch PCKh 0.5\n",
      "Trained batch 2152 batch loss 2.85258102 batch mAP 0.076171875 batch PCKh 0.603515625\n",
      "Trained batch 2153 batch loss 2.69386578 batch mAP 0.125 batch PCKh 0.6328125\n",
      "Trained batch 2154 batch loss 2.59742427 batch mAP 0.07421875 batch PCKh 0.6484375\n",
      "Trained batch 2155 batch loss 2.91328812 batch mAP 0.083984375 batch PCKh 0.615234375\n",
      "Trained batch 2156 batch loss 2.74065375 batch mAP 0.138671875 batch PCKh 0.31640625\n",
      "Trained batch 2157 batch loss 2.95218182 batch mAP 0.1171875 batch PCKh 0.326171875\n",
      "Trained batch 2158 batch loss 3.00229 batch mAP 0.12890625 batch PCKh 0.4375\n",
      "Trained batch 2159 batch loss 3.07978606 batch mAP 0.1015625 batch PCKh 0.400390625\n",
      "Trained batch 2160 batch loss 3.15012932 batch mAP 0.138671875 batch PCKh 0.486328125\n",
      "Trained batch 2161 batch loss 2.85919261 batch mAP 0.130859375 batch PCKh 0.4609375\n",
      "Trained batch 2162 batch loss 2.96027803 batch mAP 0.080078125 batch PCKh 0.53125\n",
      "Trained batch 2163 batch loss 2.94763303 batch mAP 0.060546875 batch PCKh 0.498046875\n",
      "Trained batch 2164 batch loss 2.93766642 batch mAP 0.0625 batch PCKh 0.650390625\n",
      "Trained batch 2165 batch loss 2.90687919 batch mAP 0.09765625 batch PCKh 0.556640625\n",
      "Trained batch 2166 batch loss 2.65520144 batch mAP 0.095703125 batch PCKh 0.578125\n",
      "Trained batch 2167 batch loss 2.88788652 batch mAP 0.06640625 batch PCKh 0.61328125\n",
      "Trained batch 2168 batch loss 2.82924104 batch mAP 0.107421875 batch PCKh 0.384765625\n",
      "Trained batch 2169 batch loss 2.88421822 batch mAP 0.078125 batch PCKh 0.201171875\n",
      "Trained batch 2170 batch loss 2.32269382 batch mAP 0.099609375 batch PCKh 0.14453125\n",
      "Trained batch 2171 batch loss 2.35475421 batch mAP 0.078125 batch PCKh 0.029296875\n",
      "Trained batch 2172 batch loss 2.09208322 batch mAP 0.068359375 batch PCKh 0.017578125\n",
      "Trained batch 2173 batch loss 2.24379969 batch mAP 0.087890625 batch PCKh 0.03125\n",
      "Trained batch 2174 batch loss 2.76712227 batch mAP 0.107421875 batch PCKh 0.2734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2175 batch loss 2.54883814 batch mAP 0.125 batch PCKh 0.265625\n",
      "Trained batch 2176 batch loss 3.20949388 batch mAP 0.09765625 batch PCKh 0.44140625\n",
      "Trained batch 2177 batch loss 3.09858298 batch mAP 0.115234375 batch PCKh 0.349609375\n",
      "Trained batch 2178 batch loss 3.24627447 batch mAP 0.07421875 batch PCKh 0.32421875\n",
      "Trained batch 2179 batch loss 3.4083333 batch mAP 0.08203125 batch PCKh 0.2734375\n",
      "Trained batch 2180 batch loss 3.68835 batch mAP 0.09375 batch PCKh 0.1640625\n",
      "Trained batch 2181 batch loss 3.07895112 batch mAP 0.115234375 batch PCKh 0.140625\n",
      "Trained batch 2182 batch loss 2.72013903 batch mAP 0.166015625 batch PCKh 0.095703125\n",
      "Trained batch 2183 batch loss 2.99587131 batch mAP 0.189453125 batch PCKh 0.474609375\n",
      "Trained batch 2184 batch loss 2.72098494 batch mAP 0.203125 batch PCKh 0.310546875\n",
      "Trained batch 2185 batch loss 2.85264039 batch mAP 0.23046875 batch PCKh 0.330078125\n",
      "Trained batch 2186 batch loss 2.90418553 batch mAP 0.212890625 batch PCKh 0.361328125\n",
      "Trained batch 2187 batch loss 2.81384325 batch mAP 0.18359375 batch PCKh 0.28515625\n",
      "Trained batch 2188 batch loss 3.0819242 batch mAP 0.189453125 batch PCKh 0.52734375\n",
      "Trained batch 2189 batch loss 3.10978937 batch mAP 0.169921875 batch PCKh 0.408203125\n",
      "Trained batch 2190 batch loss 3.11686277 batch mAP 0.216796875 batch PCKh 0.443359375\n",
      "Trained batch 2191 batch loss 2.99481201 batch mAP 0.146484375 batch PCKh 0.447265625\n",
      "Trained batch 2192 batch loss 2.86928988 batch mAP 0.16796875 batch PCKh 0.61328125\n",
      "Trained batch 2193 batch loss 2.9562149 batch mAP 0.16015625 batch PCKh 0.556640625\n",
      "Trained batch 2194 batch loss 2.90613699 batch mAP 0.162109375 batch PCKh 0.6171875\n",
      "Trained batch 2195 batch loss 2.94335032 batch mAP 0.201171875 batch PCKh 0.521484375\n",
      "Trained batch 2196 batch loss 3.0802865 batch mAP 0.15625 batch PCKh 0.455078125\n",
      "Trained batch 2197 batch loss 3.05674148 batch mAP 0.203125 batch PCKh 0.421875\n",
      "Trained batch 2198 batch loss 3.04052663 batch mAP 0.19140625 batch PCKh 0.4921875\n",
      "Trained batch 2199 batch loss 2.76221371 batch mAP 0.173828125 batch PCKh 0.572265625\n",
      "Trained batch 2200 batch loss 2.90609217 batch mAP 0.115234375 batch PCKh 0.5390625\n",
      "Trained batch 2201 batch loss 2.82707167 batch mAP 0.171875 batch PCKh 0.564453125\n",
      "Trained batch 2202 batch loss 2.8330822 batch mAP 0.130859375 batch PCKh 0.607421875\n",
      "Trained batch 2203 batch loss 2.45458603 batch mAP 0.20703125 batch PCKh 0.44921875\n",
      "Trained batch 2204 batch loss 2.85142159 batch mAP 0.16015625 batch PCKh 0.353515625\n",
      "Trained batch 2205 batch loss 2.87586021 batch mAP 0.138671875 batch PCKh 0.517578125\n",
      "Trained batch 2206 batch loss 2.98796 batch mAP 0.0703125 batch PCKh 0.4375\n",
      "Trained batch 2207 batch loss 3.26143265 batch mAP 0.095703125 batch PCKh 0.455078125\n",
      "Trained batch 2208 batch loss 2.9725759 batch mAP 0.064453125 batch PCKh 0.462890625\n",
      "Trained batch 2209 batch loss 2.73351479 batch mAP 0.083984375 batch PCKh 0.548828125\n",
      "Trained batch 2210 batch loss 2.69141436 batch mAP 0.07421875 batch PCKh 0.509765625\n",
      "Trained batch 2211 batch loss 2.42241597 batch mAP 0.0859375 batch PCKh 0.201171875\n",
      "Trained batch 2212 batch loss 2.55008 batch mAP 0.1171875 batch PCKh 0.181640625\n",
      "Trained batch 2213 batch loss 2.6973753 batch mAP 0.103515625 batch PCKh 0.580078125\n",
      "Trained batch 2214 batch loss 3.07804084 batch mAP 0.11328125 batch PCKh 0.759765625\n",
      "Trained batch 2215 batch loss 2.89392757 batch mAP 0.083984375 batch PCKh 0.3125\n",
      "Trained batch 2216 batch loss 2.80122948 batch mAP 0.1328125 batch PCKh 0.341796875\n",
      "Trained batch 2217 batch loss 2.78909731 batch mAP 0.10546875 batch PCKh 0.33984375\n",
      "Trained batch 2218 batch loss 3.23714948 batch mAP 0.09375 batch PCKh 0.341796875\n",
      "Trained batch 2219 batch loss 2.68525076 batch mAP 0.060546875 batch PCKh 0.5703125\n",
      "Trained batch 2220 batch loss 2.83443403 batch mAP 0.1171875 batch PCKh 0.48046875\n",
      "Trained batch 2221 batch loss 2.69802666 batch mAP 0.056640625 batch PCKh 0.52734375\n",
      "Trained batch 2222 batch loss 2.76812458 batch mAP 0.05078125 batch PCKh 0.6171875\n",
      "Trained batch 2223 batch loss 2.75407219 batch mAP 0.064453125 batch PCKh 0.55859375\n",
      "Trained batch 2224 batch loss 2.93822312 batch mAP 0.0859375 batch PCKh 0.34375\n",
      "Trained batch 2225 batch loss 3.00373125 batch mAP 0.095703125 batch PCKh 0.3359375\n",
      "Trained batch 2226 batch loss 2.82254457 batch mAP 0.095703125 batch PCKh 0.529296875\n",
      "Trained batch 2227 batch loss 2.83832765 batch mAP 0.064453125 batch PCKh 0.302734375\n",
      "Trained batch 2228 batch loss 2.87584424 batch mAP 0.103515625 batch PCKh 0.599609375\n",
      "Trained batch 2229 batch loss 2.73119068 batch mAP 0.07421875 batch PCKh 0.23828125\n",
      "Trained batch 2230 batch loss 2.80044341 batch mAP 0.09375 batch PCKh 0.43359375\n",
      "Trained batch 2231 batch loss 2.82706237 batch mAP 0.1171875 batch PCKh 0.48046875\n",
      "Trained batch 2232 batch loss 3.06184793 batch mAP 0.083984375 batch PCKh 0.458984375\n",
      "Trained batch 2233 batch loss 3.06340408 batch mAP 0.119140625 batch PCKh 0.48046875\n",
      "Trained batch 2234 batch loss 2.96116042 batch mAP 0.10546875 batch PCKh 0.21875\n",
      "Trained batch 2235 batch loss 3.06392574 batch mAP 0.119140625 batch PCKh 0.443359375\n",
      "Trained batch 2236 batch loss 3.07019973 batch mAP 0.138671875 batch PCKh 0.525390625\n",
      "Trained batch 2237 batch loss 2.82336926 batch mAP 0.15234375 batch PCKh 0.41015625\n",
      "Trained batch 2238 batch loss 3.22631288 batch mAP 0.11328125 batch PCKh 0.400390625\n",
      "Trained batch 2239 batch loss 3.07772136 batch mAP 0.1328125 batch PCKh 0.35546875\n",
      "Trained batch 2240 batch loss 3.23891878 batch mAP 0.16015625 batch PCKh 0.369140625\n",
      "Trained batch 2241 batch loss 3.09432983 batch mAP 0.166015625 batch PCKh 0.373046875\n",
      "Trained batch 2242 batch loss 2.9988246 batch mAP 0.169921875 batch PCKh 0.34765625\n",
      "Trained batch 2243 batch loss 2.68008137 batch mAP 0.244140625 batch PCKh 0.4140625\n",
      "Trained batch 2244 batch loss 2.8314333 batch mAP 0.12109375 batch PCKh 0.515625\n",
      "Trained batch 2245 batch loss 2.63454366 batch mAP 0.224609375 batch PCKh 0.427734375\n",
      "Trained batch 2246 batch loss 2.71041965 batch mAP 0.21875 batch PCKh 0.55859375\n",
      "Trained batch 2247 batch loss 2.96622062 batch mAP 0.19140625 batch PCKh 0.505859375\n",
      "Trained batch 2248 batch loss 2.71398258 batch mAP 0.1640625 batch PCKh 0.3515625\n",
      "Trained batch 2249 batch loss 2.74764752 batch mAP 0.146484375 batch PCKh 0.5625\n",
      "Trained batch 2250 batch loss 2.71487236 batch mAP 0.107421875 batch PCKh 0.3984375\n",
      "Trained batch 2251 batch loss 2.74979639 batch mAP 0.162109375 batch PCKh 0.501953125\n",
      "Trained batch 2252 batch loss 2.55798149 batch mAP 0.13671875 batch PCKh 0.37890625\n",
      "Trained batch 2253 batch loss 2.88540792 batch mAP 0.111328125 batch PCKh 0.46484375\n",
      "Trained batch 2254 batch loss 2.74709749 batch mAP 0.09765625 batch PCKh 0.48046875\n",
      "Trained batch 2255 batch loss 2.86401534 batch mAP 0.099609375 batch PCKh 0.3359375\n",
      "Trained batch 2256 batch loss 2.83635211 batch mAP 0.1015625 batch PCKh 0.669921875\n",
      "Trained batch 2257 batch loss 2.91750741 batch mAP 0.04296875 batch PCKh 0.5234375\n",
      "Trained batch 2258 batch loss 3.1035161 batch mAP 0.060546875 batch PCKh 0.525390625\n",
      "Trained batch 2259 batch loss 2.89527631 batch mAP 0.046875 batch PCKh 0.5546875\n",
      "Trained batch 2260 batch loss 2.64954281 batch mAP 0.06640625 batch PCKh 0.5\n",
      "Trained batch 2261 batch loss 2.70000029 batch mAP 0.0625 batch PCKh 0.375\n",
      "Trained batch 2262 batch loss 3.01503634 batch mAP 0.080078125 batch PCKh 0.501953125\n",
      "Trained batch 2263 batch loss 2.90041137 batch mAP 0.083984375 batch PCKh 0.365234375\n",
      "Trained batch 2264 batch loss 2.75099325 batch mAP 0.0859375 batch PCKh 0.6015625\n",
      "Trained batch 2265 batch loss 2.9494586 batch mAP 0.14453125 batch PCKh 0.625\n",
      "Trained batch 2266 batch loss 2.95268512 batch mAP 0.08984375 batch PCKh 0.576171875\n",
      "Trained batch 2267 batch loss 3.07622147 batch mAP 0.072265625 batch PCKh 0.53515625\n",
      "Trained batch 2268 batch loss 3.02297592 batch mAP 0.1015625 batch PCKh 0.544921875\n",
      "Trained batch 2269 batch loss 2.82276297 batch mAP 0.1484375 batch PCKh 0.65625\n",
      "Trained batch 2270 batch loss 2.87487149 batch mAP 0.119140625 batch PCKh 0.564453125\n",
      "Trained batch 2271 batch loss 2.85111141 batch mAP 0.15234375 batch PCKh 0.49609375\n",
      "Trained batch 2272 batch loss 2.61072469 batch mAP 0.115234375 batch PCKh 0.40625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2273 batch loss 2.77800226 batch mAP 0.12109375 batch PCKh 0.525390625\n",
      "Trained batch 2274 batch loss 2.75875354 batch mAP 0.142578125 batch PCKh 0.515625\n",
      "Trained batch 2275 batch loss 2.6486063 batch mAP 0.130859375 batch PCKh 0.349609375\n",
      "Trained batch 2276 batch loss 2.88666701 batch mAP 0.109375 batch PCKh 0.271484375\n",
      "Trained batch 2277 batch loss 3.01121712 batch mAP 0.10546875 batch PCKh 0.5546875\n",
      "Trained batch 2278 batch loss 3.05102181 batch mAP 0.125 batch PCKh 0.34375\n",
      "Trained batch 2279 batch loss 3.0287118 batch mAP 0.109375 batch PCKh 0.634765625\n",
      "Trained batch 2280 batch loss 3.02302217 batch mAP 0.08984375 batch PCKh 0.310546875\n",
      "Trained batch 2281 batch loss 2.73126841 batch mAP 0.150390625 batch PCKh 0.302734375\n",
      "Trained batch 2282 batch loss 2.83074856 batch mAP 0.177734375 batch PCKh 0.236328125\n",
      "Trained batch 2283 batch loss 2.98198271 batch mAP 0.166015625 batch PCKh 0.5859375\n",
      "Trained batch 2284 batch loss 2.96240187 batch mAP 0.154296875 batch PCKh 0.32421875\n",
      "Trained batch 2285 batch loss 2.94467163 batch mAP 0.109375 batch PCKh 0.4375\n",
      "Trained batch 2286 batch loss 3.15527296 batch mAP 0.189453125 batch PCKh 0.42578125\n",
      "Trained batch 2287 batch loss 2.69100761 batch mAP 0.234375 batch PCKh 0.55859375\n",
      "Trained batch 2288 batch loss 3.05831885 batch mAP 0.16796875 batch PCKh 0.62890625\n",
      "Trained batch 2289 batch loss 2.79025745 batch mAP 0.185546875 batch PCKh 0.4140625\n",
      "Trained batch 2290 batch loss 2.71058464 batch mAP 0.1171875 batch PCKh 0.05859375\n",
      "Trained batch 2291 batch loss 2.49433279 batch mAP 0.158203125 batch PCKh 0.361328125\n",
      "Trained batch 2292 batch loss 2.75939393 batch mAP 0.158203125 batch PCKh 0.427734375\n",
      "Trained batch 2293 batch loss 2.90676188 batch mAP 0.15234375 batch PCKh 0.42578125\n",
      "Trained batch 2294 batch loss 2.81740355 batch mAP 0.134765625 batch PCKh 0.521484375\n",
      "Trained batch 2295 batch loss 2.86122942 batch mAP 0.20703125 batch PCKh 0.537109375\n",
      "Trained batch 2296 batch loss 3.139709 batch mAP 0.1328125 batch PCKh 0.2890625\n",
      "Trained batch 2297 batch loss 3.05462432 batch mAP 0.140625 batch PCKh 0.27734375\n",
      "Trained batch 2298 batch loss 2.82173657 batch mAP 0.052734375 batch PCKh 0.59375\n",
      "Trained batch 2299 batch loss 2.7415266 batch mAP 0.1328125 batch PCKh 0.46875\n",
      "Trained batch 2300 batch loss 3.1519351 batch mAP 0.1328125 batch PCKh 0.490234375\n",
      "Trained batch 2301 batch loss 3.06131315 batch mAP 0.126953125 batch PCKh 0.375\n",
      "Trained batch 2302 batch loss 2.82140398 batch mAP 0.138671875 batch PCKh 0.205078125\n",
      "Trained batch 2303 batch loss 2.79294538 batch mAP 0.0859375 batch PCKh 0.435546875\n",
      "Trained batch 2304 batch loss 2.66236067 batch mAP 0.111328125 batch PCKh 0.2578125\n",
      "Trained batch 2305 batch loss 2.89282846 batch mAP 0.125 batch PCKh 0.359375\n",
      "Trained batch 2306 batch loss 2.82117748 batch mAP 0.115234375 batch PCKh 0.3984375\n",
      "Trained batch 2307 batch loss 2.76797843 batch mAP 0.12890625 batch PCKh 0.3359375\n",
      "Trained batch 2308 batch loss 3.08106756 batch mAP 0.134765625 batch PCKh 0.42578125\n",
      "Trained batch 2309 batch loss 2.8348453 batch mAP 0.125 batch PCKh 0.37109375\n",
      "Trained batch 2310 batch loss 2.92399883 batch mAP 0.1015625 batch PCKh 0.345703125\n",
      "Trained batch 2311 batch loss 2.97916698 batch mAP 0.09765625 batch PCKh 0.392578125\n",
      "Trained batch 2312 batch loss 2.79898238 batch mAP 0.123046875 batch PCKh 0.296875\n",
      "Trained batch 2313 batch loss 2.72278285 batch mAP 0.076171875 batch PCKh 0.55859375\n",
      "Trained batch 2314 batch loss 2.83970618 batch mAP 0.06640625 batch PCKh 0.56640625\n",
      "Trained batch 2315 batch loss 2.94348478 batch mAP 0.076171875 batch PCKh 0.544921875\n",
      "Trained batch 2316 batch loss 3.00926733 batch mAP 0.107421875 batch PCKh 0.513671875\n",
      "Trained batch 2317 batch loss 3.06140709 batch mAP 0.087890625 batch PCKh 0.44921875\n",
      "Trained batch 2318 batch loss 3.01762342 batch mAP 0.064453125 batch PCKh 0.388671875\n",
      "Trained batch 2319 batch loss 2.98148251 batch mAP 0.103515625 batch PCKh 0.482421875\n",
      "Trained batch 2320 batch loss 3.11048889 batch mAP 0.052734375 batch PCKh 0.392578125\n",
      "Trained batch 2321 batch loss 2.91909695 batch mAP 0.103515625 batch PCKh 0.484375\n",
      "Trained batch 2322 batch loss 2.88849974 batch mAP 0.146484375 batch PCKh 0.3828125\n",
      "Trained batch 2323 batch loss 2.85449338 batch mAP 0.080078125 batch PCKh 0.326171875\n",
      "Trained batch 2324 batch loss 3.08239698 batch mAP 0.099609375 batch PCKh 0.59375\n",
      "Trained batch 2325 batch loss 2.73145795 batch mAP 0.111328125 batch PCKh 0.3046875\n",
      "Trained batch 2326 batch loss 2.77287745 batch mAP 0.11328125 batch PCKh 0.427734375\n",
      "Trained batch 2327 batch loss 2.69820094 batch mAP 0.1484375 batch PCKh 0.26171875\n",
      "Trained batch 2328 batch loss 2.69744015 batch mAP 0.173828125 batch PCKh 0.4375\n",
      "Trained batch 2329 batch loss 2.81760645 batch mAP 0.107421875 batch PCKh 0.447265625\n",
      "Trained batch 2330 batch loss 2.98965549 batch mAP 0.119140625 batch PCKh 0.509765625\n",
      "Trained batch 2331 batch loss 2.66910505 batch mAP 0.1484375 batch PCKh 0.171875\n",
      "Trained batch 2332 batch loss 2.85665345 batch mAP 0.171875 batch PCKh 0.447265625\n",
      "Trained batch 2333 batch loss 2.70650387 batch mAP 0.1171875 batch PCKh 0.25\n",
      "Trained batch 2334 batch loss 2.86650109 batch mAP 0.119140625 batch PCKh 0.630859375\n",
      "Trained batch 2335 batch loss 2.6702919 batch mAP 0.189453125 batch PCKh 0.326171875\n",
      "Trained batch 2336 batch loss 2.80636072 batch mAP 0.150390625 batch PCKh 0.40234375\n",
      "Trained batch 2337 batch loss 2.89228773 batch mAP 0.177734375 batch PCKh 0.470703125\n",
      "Trained batch 2338 batch loss 2.81778383 batch mAP 0.125 batch PCKh 0.4609375\n",
      "Trained batch 2339 batch loss 2.91330385 batch mAP 0.13671875 batch PCKh 0.544921875\n",
      "Trained batch 2340 batch loss 2.81406975 batch mAP 0.1484375 batch PCKh 0.494140625\n",
      "Trained batch 2341 batch loss 2.77663851 batch mAP 0.11328125 batch PCKh 0.34765625\n",
      "Trained batch 2342 batch loss 2.62514567 batch mAP 0.1328125 batch PCKh 0.44140625\n",
      "Trained batch 2343 batch loss 2.9363544 batch mAP 0.109375 batch PCKh 0.31640625\n",
      "Trained batch 2344 batch loss 2.89978862 batch mAP 0.09765625 batch PCKh 0.4921875\n",
      "Trained batch 2345 batch loss 2.55909586 batch mAP 0.103515625 batch PCKh 0.439453125\n",
      "Trained batch 2346 batch loss 2.71120691 batch mAP 0.138671875 batch PCKh 0.201171875\n",
      "Trained batch 2347 batch loss 2.81853175 batch mAP 0.087890625 batch PCKh 0.39453125\n",
      "Trained batch 2348 batch loss 3.3152833 batch mAP 0.0859375 batch PCKh 0.41015625\n",
      "Trained batch 2349 batch loss 3.02056742 batch mAP 0.09375 batch PCKh 0.38671875\n",
      "Trained batch 2350 batch loss 2.71118641 batch mAP 0.09375 batch PCKh 0.50390625\n",
      "Trained batch 2351 batch loss 2.63100076 batch mAP 0.09375 batch PCKh 0.59375\n",
      "Trained batch 2352 batch loss 2.81674123 batch mAP 0.099609375 batch PCKh 0.4375\n",
      "Trained batch 2353 batch loss 2.66922617 batch mAP 0.08984375 batch PCKh 0.494140625\n",
      "Trained batch 2354 batch loss 2.71685576 batch mAP 0.0859375 batch PCKh 0.474609375\n",
      "Trained batch 2355 batch loss 2.78791642 batch mAP 0.068359375 batch PCKh 0.5625\n",
      "Trained batch 2356 batch loss 2.76936626 batch mAP 0.07421875 batch PCKh 0.51171875\n",
      "Trained batch 2357 batch loss 2.81787658 batch mAP 0.064453125 batch PCKh 0.2890625\n",
      "Trained batch 2358 batch loss 3.03618 batch mAP 0.080078125 batch PCKh 0.4296875\n",
      "Trained batch 2359 batch loss 3.05319881 batch mAP 0.025390625 batch PCKh 0.4453125\n",
      "Trained batch 2360 batch loss 2.86033773 batch mAP 0.0625 batch PCKh 0.330078125\n",
      "Trained batch 2361 batch loss 2.50404692 batch mAP 0.044921875 batch PCKh 0.328125\n",
      "Trained batch 2362 batch loss 2.32303667 batch mAP 0.0625 batch PCKh 0.41015625\n",
      "Trained batch 2363 batch loss 2.42587948 batch mAP 0.109375 batch PCKh 0.123046875\n",
      "Trained batch 2364 batch loss 2.28351 batch mAP 0.033203125 batch PCKh 0.234375\n",
      "Trained batch 2365 batch loss 2.72812581 batch mAP 0.0546875 batch PCKh 0.08984375\n",
      "Trained batch 2366 batch loss 2.64519691 batch mAP 0.04296875 batch PCKh 0.166015625\n",
      "Trained batch 2367 batch loss 2.53371596 batch mAP 0.046875 batch PCKh 0.232421875\n",
      "Trained batch 2368 batch loss 2.79989 batch mAP 0.03515625 batch PCKh 0.560546875\n",
      "Trained batch 2369 batch loss 2.64274335 batch mAP 0.06640625 batch PCKh 0.49609375\n",
      "Trained batch 2370 batch loss 2.63722491 batch mAP 0.056640625 batch PCKh 0.390625\n",
      "Trained batch 2371 batch loss 2.52066731 batch mAP 0.087890625 batch PCKh 0.41796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2372 batch loss 2.72636652 batch mAP 0.029296875 batch PCKh 0.48828125\n",
      "Trained batch 2373 batch loss 2.75128341 batch mAP 0.0234375 batch PCKh 0.6328125\n",
      "Trained batch 2374 batch loss 2.94189548 batch mAP 0.06640625 batch PCKh 0.599609375\n",
      "Trained batch 2375 batch loss 2.94282365 batch mAP 0.056640625 batch PCKh 0.6171875\n",
      "Trained batch 2376 batch loss 2.59110594 batch mAP 0.046875 batch PCKh 0.326171875\n",
      "Trained batch 2377 batch loss 3.12519836 batch mAP 0.0546875 batch PCKh 0.390625\n",
      "Trained batch 2378 batch loss 2.97322869 batch mAP 0.087890625 batch PCKh 0.416015625\n",
      "Trained batch 2379 batch loss 2.69467592 batch mAP 0.12109375 batch PCKh 0.21484375\n",
      "Trained batch 2380 batch loss 2.98377657 batch mAP 0.083984375 batch PCKh 0.763671875\n",
      "Trained batch 2381 batch loss 2.97520137 batch mAP 0.125 batch PCKh 0.40234375\n",
      "Trained batch 2382 batch loss 3.33078957 batch mAP 0.13671875 batch PCKh 0.37109375\n",
      "Trained batch 2383 batch loss 3.5498991 batch mAP 0.1484375 batch PCKh 0.203125\n",
      "Trained batch 2384 batch loss 3.1838131 batch mAP 0.125 batch PCKh 0.3984375\n",
      "Trained batch 2385 batch loss 3.36364746 batch mAP 0.138671875 batch PCKh 0.384765625\n",
      "Trained batch 2386 batch loss 3.2031703 batch mAP 0.158203125 batch PCKh 0.353515625\n",
      "Trained batch 2387 batch loss 2.80619025 batch mAP 0.208984375 batch PCKh 0.23046875\n",
      "Trained batch 2388 batch loss 2.7514267 batch mAP 0.19921875 batch PCKh 0.349609375\n",
      "Trained batch 2389 batch loss 2.68581057 batch mAP 0.205078125 batch PCKh 0.36328125\n",
      "Trained batch 2390 batch loss 2.81634736 batch mAP 0.216796875 batch PCKh 0.4921875\n",
      "Trained batch 2391 batch loss 3.02271509 batch mAP 0.298828125 batch PCKh 0.41796875\n",
      "Trained batch 2392 batch loss 3.22050929 batch mAP 0.279296875 batch PCKh 0.291015625\n",
      "Trained batch 2393 batch loss 2.86332703 batch mAP 0.287109375 batch PCKh 0.2109375\n",
      "Trained batch 2394 batch loss 3.07726336 batch mAP 0.2421875 batch PCKh 0.4375\n",
      "Trained batch 2395 batch loss 2.86512947 batch mAP 0.30859375 batch PCKh 0.380859375\n",
      "Trained batch 2396 batch loss 2.90765429 batch mAP 0.27734375 batch PCKh 0.404296875\n",
      "Trained batch 2397 batch loss 2.92486954 batch mAP 0.25 batch PCKh 0.525390625\n",
      "Trained batch 2398 batch loss 2.66453886 batch mAP 0.263671875 batch PCKh 0.494140625\n",
      "Trained batch 2399 batch loss 2.81529117 batch mAP 0.25390625 batch PCKh 0.509765625\n",
      "Trained batch 2400 batch loss 2.75088882 batch mAP 0.19140625 batch PCKh 0.5703125\n",
      "Trained batch 2401 batch loss 2.60730648 batch mAP 0.171875 batch PCKh 0.330078125\n",
      "Trained batch 2402 batch loss 2.81972694 batch mAP 0.138671875 batch PCKh 0.484375\n",
      "Trained batch 2403 batch loss 2.79225683 batch mAP 0.099609375 batch PCKh 0.322265625\n",
      "Trained batch 2404 batch loss 2.58232212 batch mAP 0.087890625 batch PCKh 0.42578125\n",
      "Trained batch 2405 batch loss 2.48994541 batch mAP 0.076171875 batch PCKh 0.626953125\n",
      "Trained batch 2406 batch loss 2.61491656 batch mAP 0.060546875 batch PCKh 0.6875\n",
      "Trained batch 2407 batch loss 2.69656062 batch mAP 0.0703125 batch PCKh 0.638671875\n",
      "Trained batch 2408 batch loss 2.84166336 batch mAP 0.083984375 batch PCKh 0.76953125\n",
      "Trained batch 2409 batch loss 2.65532637 batch mAP 0.060546875 batch PCKh 0.505859375\n",
      "Trained batch 2410 batch loss 2.6193316 batch mAP 0.0859375 batch PCKh 0.349609375\n",
      "Trained batch 2411 batch loss 2.62758684 batch mAP 0.083984375 batch PCKh 0.404296875\n",
      "Trained batch 2412 batch loss 2.79690027 batch mAP 0.087890625 batch PCKh 0.5\n",
      "Trained batch 2413 batch loss 2.43079972 batch mAP 0.05078125 batch PCKh 0.322265625\n",
      "Trained batch 2414 batch loss 2.64150858 batch mAP 0.099609375 batch PCKh 0.5390625\n",
      "Trained batch 2415 batch loss 2.80591059 batch mAP 0.07421875 batch PCKh 0.376953125\n",
      "Trained batch 2416 batch loss 2.82274151 batch mAP 0.119140625 batch PCKh 0.521484375\n",
      "Trained batch 2417 batch loss 2.64157486 batch mAP 0.10546875 batch PCKh 0.3359375\n",
      "Trained batch 2418 batch loss 2.62291241 batch mAP 0.07421875 batch PCKh 0.603515625\n",
      "Trained batch 2419 batch loss 2.83283329 batch mAP 0.05859375 batch PCKh 0.46875\n",
      "Trained batch 2420 batch loss 2.45952559 batch mAP 0.05078125 batch PCKh 0.353515625\n",
      "Trained batch 2421 batch loss 2.90127277 batch mAP 0.08203125 batch PCKh 0.337890625\n",
      "Trained batch 2422 batch loss 3.00168085 batch mAP 0.06640625 batch PCKh 0.4765625\n",
      "Trained batch 2423 batch loss 2.96587443 batch mAP 0.083984375 batch PCKh 0.27734375\n",
      "Trained batch 2424 batch loss 2.89311767 batch mAP 0.083984375 batch PCKh 0.580078125\n",
      "Trained batch 2425 batch loss 2.98969221 batch mAP 0.0625 batch PCKh 0.54296875\n",
      "Trained batch 2426 batch loss 2.8347559 batch mAP 0.064453125 batch PCKh 0.509765625\n",
      "Trained batch 2427 batch loss 2.77143931 batch mAP 0.078125 batch PCKh 0.61328125\n",
      "Trained batch 2428 batch loss 2.78769088 batch mAP 0.11328125 batch PCKh 0.357421875\n",
      "Trained batch 2429 batch loss 2.60822606 batch mAP 0.1171875 batch PCKh 0.564453125\n",
      "Trained batch 2430 batch loss 2.84775639 batch mAP 0.05078125 batch PCKh 0.5234375\n",
      "Trained batch 2431 batch loss 2.85054493 batch mAP 0.06640625 batch PCKh 0.46484375\n",
      "Trained batch 2432 batch loss 2.77952933 batch mAP 0.05859375 batch PCKh 0.51953125\n",
      "Trained batch 2433 batch loss 2.64939213 batch mAP 0.072265625 batch PCKh 0.595703125\n",
      "Trained batch 2434 batch loss 2.70384216 batch mAP 0.091796875 batch PCKh 0.619140625\n",
      "Trained batch 2435 batch loss 2.52642059 batch mAP 0.04296875 batch PCKh 0.705078125\n",
      "Trained batch 2436 batch loss 2.58399677 batch mAP 0.0703125 batch PCKh 0.69921875\n",
      "Trained batch 2437 batch loss 2.9332509 batch mAP 0.03125 batch PCKh 0.54296875\n",
      "Trained batch 2438 batch loss 3.06983662 batch mAP 0.017578125 batch PCKh 0.392578125\n",
      "Trained batch 2439 batch loss 2.98711538 batch mAP 0.052734375 batch PCKh 0.5078125\n",
      "Trained batch 2440 batch loss 2.99186587 batch mAP 0.02734375 batch PCKh 0.5\n",
      "Trained batch 2441 batch loss 2.9458375 batch mAP 0.015625 batch PCKh 0.509765625\n",
      "Trained batch 2442 batch loss 2.89267731 batch mAP 0.01953125 batch PCKh 0.62109375\n",
      "Trained batch 2443 batch loss 2.87829351 batch mAP 0.01171875 batch PCKh 0.60546875\n",
      "Trained batch 2444 batch loss 2.86985874 batch mAP 0.044921875 batch PCKh 0.515625\n",
      "Trained batch 2445 batch loss 2.84815 batch mAP 0.0390625 batch PCKh 0.505859375\n",
      "Trained batch 2446 batch loss 2.65073729 batch mAP 0.046875 batch PCKh 0.7421875\n",
      "Trained batch 2447 batch loss 2.86229396 batch mAP 0.05078125 batch PCKh 0.515625\n",
      "Trained batch 2448 batch loss 3.06442165 batch mAP 0.046875 batch PCKh 0.5234375\n",
      "Trained batch 2449 batch loss 3.07543 batch mAP 0.025390625 batch PCKh 0.357421875\n",
      "Trained batch 2450 batch loss 3.06958199 batch mAP 0.048828125 batch PCKh 0.421875\n",
      "Trained batch 2451 batch loss 2.97022271 batch mAP 0.03125 batch PCKh 0.505859375\n",
      "Trained batch 2452 batch loss 2.81355047 batch mAP 0.060546875 batch PCKh 0.666015625\n",
      "Trained batch 2453 batch loss 2.91058969 batch mAP 0.091796875 batch PCKh 0.34765625\n",
      "Trained batch 2454 batch loss 2.84440637 batch mAP 0.08203125 batch PCKh 0.380859375\n",
      "Trained batch 2455 batch loss 2.78974724 batch mAP 0.099609375 batch PCKh 0.23046875\n",
      "Trained batch 2456 batch loss 2.75686359 batch mAP 0.07421875 batch PCKh 0.515625\n",
      "Trained batch 2457 batch loss 2.79662418 batch mAP 0.099609375 batch PCKh 0.4765625\n",
      "Trained batch 2458 batch loss 2.90543127 batch mAP 0.095703125 batch PCKh 0.46875\n",
      "Trained batch 2459 batch loss 2.549227 batch mAP 0.080078125 batch PCKh 0.21875\n",
      "Trained batch 2460 batch loss 2.61726117 batch mAP 0.095703125 batch PCKh 0.17578125\n",
      "Trained batch 2461 batch loss 2.49085641 batch mAP 0.095703125 batch PCKh 0.3359375\n",
      "Trained batch 2462 batch loss 2.84416389 batch mAP 0.07421875 batch PCKh 0.22265625\n",
      "Trained batch 2463 batch loss 2.92634535 batch mAP 0.0703125 batch PCKh 0.4609375\n",
      "Trained batch 2464 batch loss 2.9828 batch mAP 0.1171875 batch PCKh 0.462890625\n",
      "Trained batch 2465 batch loss 2.82523 batch mAP 0.1328125 batch PCKh 0.60546875\n",
      "Trained batch 2466 batch loss 2.91816092 batch mAP 0.0625 batch PCKh 0.6640625\n",
      "Trained batch 2467 batch loss 2.95500016 batch mAP 0.08984375 batch PCKh 0.541015625\n",
      "Trained batch 2468 batch loss 2.84830809 batch mAP 0.126953125 batch PCKh 0.544921875\n",
      "Trained batch 2469 batch loss 2.67466545 batch mAP 0.095703125 batch PCKh 0.626953125\n",
      "Trained batch 2470 batch loss 2.65424013 batch mAP 0.09765625 batch PCKh 0.390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2471 batch loss 2.68437767 batch mAP 0.07421875 batch PCKh 0.580078125\n",
      "Trained batch 2472 batch loss 2.73651838 batch mAP 0.08203125 batch PCKh 0.296875\n",
      "Trained batch 2473 batch loss 2.6744895 batch mAP 0.1171875 batch PCKh 0.4453125\n",
      "Trained batch 2474 batch loss 2.69112229 batch mAP 0.05859375 batch PCKh 0.44140625\n",
      "Trained batch 2475 batch loss 2.60784316 batch mAP 0.05078125 batch PCKh 0.48046875\n",
      "Trained batch 2476 batch loss 2.52129102 batch mAP 0.09765625 batch PCKh 0.42578125\n",
      "Trained batch 2477 batch loss 2.62166786 batch mAP 0.0234375 batch PCKh 0.55078125\n",
      "Trained batch 2478 batch loss 2.70571184 batch mAP 0.015625 batch PCKh 0.69921875\n",
      "Trained batch 2479 batch loss 2.74239278 batch mAP 0.009765625 batch PCKh 0.568359375\n",
      "Trained batch 2480 batch loss 2.74032307 batch mAP 0.029296875 batch PCKh 0.205078125\n",
      "Trained batch 2481 batch loss 2.85154057 batch mAP 0.021484375 batch PCKh 0.44921875\n",
      "Trained batch 2482 batch loss 3.05626822 batch mAP 0.03125 batch PCKh 0.484375\n",
      "Trained batch 2483 batch loss 2.83757973 batch mAP 0.048828125 batch PCKh 0.40625\n",
      "Trained batch 2484 batch loss 2.63852835 batch mAP 0.041015625 batch PCKh 0.39453125\n",
      "Trained batch 2485 batch loss 2.73554587 batch mAP 0.037109375 batch PCKh 0.275390625\n",
      "Trained batch 2486 batch loss 3.13351727 batch mAP 0.07421875 batch PCKh 0.51953125\n",
      "Trained batch 2487 batch loss 3.06375623 batch mAP 0.05859375 batch PCKh 0.509765625\n",
      "Trained batch 2488 batch loss 2.72923946 batch mAP 0.046875 batch PCKh 0.564453125\n",
      "Trained batch 2489 batch loss 3.08416581 batch mAP 0.056640625 batch PCKh 0.146484375\n",
      "Trained batch 2490 batch loss 2.69542241 batch mAP 0.12890625 batch PCKh 0.435546875\n",
      "Trained batch 2491 batch loss 2.85603905 batch mAP 0.1328125 batch PCKh 0.431640625\n",
      "Trained batch 2492 batch loss 3.2873354 batch mAP 0.056640625 batch PCKh 0.41015625\n",
      "Trained batch 2493 batch loss 3.06445456 batch mAP 0.1171875 batch PCKh 0.5\n",
      "Trained batch 2494 batch loss 3.19171834 batch mAP 0.10546875 batch PCKh 0.330078125\n",
      "Trained batch 2495 batch loss 2.79896975 batch mAP 0.099609375 batch PCKh 0.494140625\n",
      "Trained batch 2496 batch loss 2.78071165 batch mAP 0.109375 batch PCKh 0.587890625\n",
      "Trained batch 2497 batch loss 2.94791698 batch mAP 0.1015625 batch PCKh 0.564453125\n",
      "Trained batch 2498 batch loss 2.82806873 batch mAP 0.169921875 batch PCKh 0.53125\n",
      "Trained batch 2499 batch loss 2.98252869 batch mAP 0.181640625 batch PCKh 0.37890625\n",
      "Trained batch 2500 batch loss 2.97224879 batch mAP 0.111328125 batch PCKh 0.578125\n",
      "Trained batch 2501 batch loss 3.13266516 batch mAP 0.150390625 batch PCKh 0.486328125\n",
      "Trained batch 2502 batch loss 3.05885887 batch mAP 0.15234375 batch PCKh 0.46484375\n",
      "Trained batch 2503 batch loss 3.13703775 batch mAP 0.142578125 batch PCKh 0.392578125\n",
      "Trained batch 2504 batch loss 2.79966736 batch mAP 0.12890625 batch PCKh 0.509765625\n",
      "Trained batch 2505 batch loss 2.87822866 batch mAP 0.140625 batch PCKh 0.3203125\n",
      "Trained batch 2506 batch loss 2.81750202 batch mAP 0.125 batch PCKh 0.40234375\n",
      "Trained batch 2507 batch loss 2.92778516 batch mAP 0.08984375 batch PCKh 0.478515625\n",
      "Trained batch 2508 batch loss 3.25628543 batch mAP 0.1171875 batch PCKh 0.34375\n",
      "Trained batch 2509 batch loss 3.20869899 batch mAP 0.13671875 batch PCKh 0.31640625\n",
      "Trained batch 2510 batch loss 3.13231182 batch mAP 0.166015625 batch PCKh 0.521484375\n",
      "Trained batch 2511 batch loss 3.13169861 batch mAP 0.1328125 batch PCKh 0.439453125\n",
      "Trained batch 2512 batch loss 3.17576337 batch mAP 0.177734375 batch PCKh 0.462890625\n",
      "Trained batch 2513 batch loss 2.72349858 batch mAP 0.18359375 batch PCKh 0.6875\n",
      "Trained batch 2514 batch loss 2.80366874 batch mAP 0.10546875 batch PCKh 0.54296875\n",
      "Trained batch 2515 batch loss 2.91160345 batch mAP 0.146484375 batch PCKh 0.486328125\n",
      "Trained batch 2516 batch loss 3.09130716 batch mAP 0.171875 batch PCKh 0.57421875\n",
      "Trained batch 2517 batch loss 2.85384655 batch mAP 0.1484375 batch PCKh 0.4453125\n",
      "Trained batch 2518 batch loss 3.14145947 batch mAP 0.12109375 batch PCKh 0.376953125\n",
      "Trained batch 2519 batch loss 3.13340759 batch mAP 0.1484375 batch PCKh 0.326171875\n",
      "Trained batch 2520 batch loss 2.74983096 batch mAP 0.1484375 batch PCKh 0.4140625\n",
      "Trained batch 2521 batch loss 2.5918932 batch mAP 0.13671875 batch PCKh 0.388671875\n",
      "Trained batch 2522 batch loss 2.81477737 batch mAP 0.068359375 batch PCKh 0.337890625\n",
      "Trained batch 2523 batch loss 2.60778713 batch mAP 0.083984375 batch PCKh 0.376953125\n",
      "Trained batch 2524 batch loss 2.58392596 batch mAP 0.115234375 batch PCKh 0.453125\n",
      "Trained batch 2525 batch loss 2.69705915 batch mAP 0.134765625 batch PCKh 0.287109375\n",
      "Trained batch 2526 batch loss 2.39496 batch mAP 0.166015625 batch PCKh 0.3828125\n",
      "Trained batch 2527 batch loss 2.82489562 batch mAP 0.0625 batch PCKh 0.59765625\n",
      "Trained batch 2528 batch loss 2.81471777 batch mAP 0.078125 batch PCKh 0.505859375\n",
      "Trained batch 2529 batch loss 2.98038793 batch mAP 0.10546875 batch PCKh 0.408203125\n",
      "Trained batch 2530 batch loss 2.2762661 batch mAP 0.068359375 batch PCKh 0.28125\n",
      "Trained batch 2531 batch loss 2.52974081 batch mAP 0.048828125 batch PCKh 0.33203125\n",
      "Trained batch 2532 batch loss 2.39392638 batch mAP 0.09375 batch PCKh 0.185546875\n",
      "Trained batch 2533 batch loss 2.46722388 batch mAP 0.080078125 batch PCKh 0.34375\n",
      "Trained batch 2534 batch loss 2.27027726 batch mAP 0.080078125 batch PCKh 0.23828125\n",
      "Trained batch 2535 batch loss 2.18954039 batch mAP 0.07421875 batch PCKh 0.12890625\n",
      "Trained batch 2536 batch loss 2.83092427 batch mAP 0.060546875 batch PCKh 0.375\n",
      "Trained batch 2537 batch loss 2.68637228 batch mAP 0.072265625 batch PCKh 0.392578125\n",
      "Trained batch 2538 batch loss 2.91809 batch mAP 0.076171875 batch PCKh 0.453125\n",
      "Trained batch 2539 batch loss 2.38320589 batch mAP 0.048828125 batch PCKh 0.328125\n",
      "Trained batch 2540 batch loss 2.3070693 batch mAP 0.083984375 batch PCKh 0.099609375\n",
      "Trained batch 2541 batch loss 2.46036863 batch mAP 0.046875 batch PCKh 0.587890625\n",
      "Trained batch 2542 batch loss 2.52873492 batch mAP 0.0546875 batch PCKh 0.4453125\n",
      "Trained batch 2543 batch loss 2.95758367 batch mAP 0.052734375 batch PCKh 0.49609375\n",
      "Trained batch 2544 batch loss 2.73284698 batch mAP 0.048828125 batch PCKh 0.314453125\n",
      "Trained batch 2545 batch loss 2.70867491 batch mAP 0.064453125 batch PCKh 0.453125\n",
      "Trained batch 2546 batch loss 2.92038107 batch mAP 0.03125 batch PCKh 0.541015625\n",
      "Trained batch 2547 batch loss 2.8220768 batch mAP 0.029296875 batch PCKh 0.5703125\n",
      "Trained batch 2548 batch loss 2.55891538 batch mAP 0.044921875 batch PCKh 0.314453125\n",
      "Trained batch 2549 batch loss 2.48639488 batch mAP 0.05859375 batch PCKh 0.47265625\n",
      "Trained batch 2550 batch loss 2.4431653 batch mAP 0.04296875 batch PCKh 0.46875\n",
      "Trained batch 2551 batch loss 2.31058073 batch mAP 0.017578125 batch PCKh 0.45703125\n",
      "Trained batch 2552 batch loss 2.91791034 batch mAP 0.044921875 batch PCKh 0.447265625\n",
      "Trained batch 2553 batch loss 2.90690517 batch mAP 0.041015625 batch PCKh 0.654296875\n",
      "Trained batch 2554 batch loss 2.98329782 batch mAP 0.04296875 batch PCKh 0.509765625\n",
      "Trained batch 2555 batch loss 3.15235758 batch mAP 0.076171875 batch PCKh 0.521484375\n",
      "Trained batch 2556 batch loss 2.84900498 batch mAP 0.0703125 batch PCKh 0.330078125\n",
      "Trained batch 2557 batch loss 2.8137188 batch mAP 0.111328125 batch PCKh 0.357421875\n",
      "Trained batch 2558 batch loss 2.90537834 batch mAP 0.068359375 batch PCKh 0.572265625\n",
      "Trained batch 2559 batch loss 3.01641464 batch mAP 0.0703125 batch PCKh 0.578125\n",
      "Trained batch 2560 batch loss 2.95841312 batch mAP 0.072265625 batch PCKh 0.470703125\n",
      "Trained batch 2561 batch loss 2.9827652 batch mAP 0.017578125 batch PCKh 0.4140625\n",
      "Trained batch 2562 batch loss 2.80193782 batch mAP 0.03125 batch PCKh 0.619140625\n",
      "Trained batch 2563 batch loss 3.13132453 batch mAP 0.060546875 batch PCKh 0.3203125\n",
      "Trained batch 2564 batch loss 2.70847893 batch mAP 0.060546875 batch PCKh 0.630859375\n",
      "Trained batch 2565 batch loss 3.00426865 batch mAP 0.078125 batch PCKh 0.36328125\n",
      "Trained batch 2566 batch loss 3.20427752 batch mAP 0.060546875 batch PCKh 0.259765625\n",
      "Trained batch 2567 batch loss 2.97624445 batch mAP 0.083984375 batch PCKh 0.419921875\n",
      "Trained batch 2568 batch loss 2.95203257 batch mAP 0.095703125 batch PCKh 0.47265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2569 batch loss 2.84167647 batch mAP 0.1015625 batch PCKh 0.1171875\n",
      "Trained batch 2570 batch loss 3.01341367 batch mAP 0.14453125 batch PCKh 0.263671875\n",
      "Trained batch 2571 batch loss 2.75060558 batch mAP 0.138671875 batch PCKh 0.57421875\n",
      "Trained batch 2572 batch loss 2.84836102 batch mAP 0.107421875 batch PCKh 0.5625\n",
      "Trained batch 2573 batch loss 2.94572163 batch mAP 0.150390625 batch PCKh 0.55078125\n",
      "Trained batch 2574 batch loss 2.95956945 batch mAP 0.12109375 batch PCKh 0.490234375\n",
      "Trained batch 2575 batch loss 2.97783899 batch mAP 0.107421875 batch PCKh 0.5625\n",
      "Trained batch 2576 batch loss 3.06488419 batch mAP 0.140625 batch PCKh 0.525390625\n",
      "Trained batch 2577 batch loss 2.68971157 batch mAP 0.154296875 batch PCKh 0.630859375\n",
      "Trained batch 2578 batch loss 2.79811406 batch mAP 0.18359375 batch PCKh 0.501953125\n",
      "Trained batch 2579 batch loss 2.86829615 batch mAP 0.083984375 batch PCKh 0.462890625\n",
      "Trained batch 2580 batch loss 3.11935616 batch mAP 0.076171875 batch PCKh 0.35546875\n",
      "Trained batch 2581 batch loss 2.91625142 batch mAP 0.07421875 batch PCKh 0.50390625\n",
      "Trained batch 2582 batch loss 2.69710493 batch mAP 0.0546875 batch PCKh 0.6015625\n",
      "Trained batch 2583 batch loss 2.75946951 batch mAP 0.060546875 batch PCKh 0.576171875\n",
      "Trained batch 2584 batch loss 2.84812355 batch mAP 0.07421875 batch PCKh 0.55078125\n",
      "Trained batch 2585 batch loss 2.77113128 batch mAP 0.06640625 batch PCKh 0.43359375\n",
      "Trained batch 2586 batch loss 3.23003483 batch mAP 0.041015625 batch PCKh 0.40625\n",
      "Trained batch 2587 batch loss 3.03221226 batch mAP 0.03515625 batch PCKh 0.57421875\n",
      "Trained batch 2588 batch loss 2.94035411 batch mAP 0.048828125 batch PCKh 0.51171875\n",
      "Trained batch 2589 batch loss 2.74236321 batch mAP 0.041015625 batch PCKh 0.716796875\n",
      "Trained batch 2590 batch loss 2.69771552 batch mAP 0.056640625 batch PCKh 0.537109375\n",
      "Trained batch 2591 batch loss 2.93397093 batch mAP 0.0625 batch PCKh 0.447265625\n",
      "Trained batch 2592 batch loss 2.71135235 batch mAP 0.056640625 batch PCKh 0.2578125\n",
      "Trained batch 2593 batch loss 3.00445986 batch mAP 0.087890625 batch PCKh 0.498046875\n",
      "Trained batch 2594 batch loss 2.6407342 batch mAP 0.078125 batch PCKh 0.28515625\n",
      "Trained batch 2595 batch loss 3.09176016 batch mAP 0.0625 batch PCKh 0.345703125\n",
      "Trained batch 2596 batch loss 2.99789619 batch mAP 0.08203125 batch PCKh 0.521484375\n",
      "Trained batch 2597 batch loss 2.69664359 batch mAP 0.099609375 batch PCKh 0.33203125\n",
      "Trained batch 2598 batch loss 2.83902764 batch mAP 0.1171875 batch PCKh 0.41015625\n",
      "Trained batch 2599 batch loss 2.78030896 batch mAP 0.078125 batch PCKh 0.482421875\n",
      "Trained batch 2600 batch loss 2.38867688 batch mAP 0.099609375 batch PCKh 0.341796875\n",
      "Trained batch 2601 batch loss 2.55160713 batch mAP 0.080078125 batch PCKh 0.265625\n",
      "Trained batch 2602 batch loss 2.5321703 batch mAP 0.13671875 batch PCKh 0.314453125\n",
      "Trained batch 2603 batch loss 2.51722717 batch mAP 0.08203125 batch PCKh 0.40234375\n",
      "Trained batch 2604 batch loss 2.82723713 batch mAP 0.083984375 batch PCKh 0.251953125\n",
      "Trained batch 2605 batch loss 2.66982365 batch mAP 0.154296875 batch PCKh 0.296875\n",
      "Trained batch 2606 batch loss 2.57191944 batch mAP 0.10546875 batch PCKh 0.33984375\n",
      "Trained batch 2607 batch loss 2.88421392 batch mAP 0.087890625 batch PCKh 0.6328125\n",
      "Trained batch 2608 batch loss 2.86854267 batch mAP 0.119140625 batch PCKh 0.451171875\n",
      "Trained batch 2609 batch loss 2.99203706 batch mAP 0.078125 batch PCKh 0.52734375\n",
      "Trained batch 2610 batch loss 2.80417299 batch mAP 0.076171875 batch PCKh 0.5078125\n",
      "Trained batch 2611 batch loss 2.93016434 batch mAP 0.064453125 batch PCKh 0.58984375\n",
      "Trained batch 2612 batch loss 2.71640778 batch mAP 0.0390625 batch PCKh 0.4453125\n",
      "Trained batch 2613 batch loss 2.71810937 batch mAP 0.076171875 batch PCKh 0.3046875\n",
      "Trained batch 2614 batch loss 2.61536264 batch mAP 0.033203125 batch PCKh 0.38671875\n",
      "Trained batch 2615 batch loss 2.96762562 batch mAP 0.05859375 batch PCKh 0.58203125\n",
      "Trained batch 2616 batch loss 2.81285763 batch mAP 0.05859375 batch PCKh 0.48046875\n",
      "Trained batch 2617 batch loss 2.60383892 batch mAP 0.064453125 batch PCKh 0.5703125\n",
      "Trained batch 2618 batch loss 2.95448828 batch mAP 0.072265625 batch PCKh 0.60546875\n",
      "Trained batch 2619 batch loss 2.7293272 batch mAP 0.056640625 batch PCKh 0.408203125\n",
      "Trained batch 2620 batch loss 2.76871514 batch mAP 0.056640625 batch PCKh 0.25\n",
      "Trained batch 2621 batch loss 2.82817125 batch mAP 0.0546875 batch PCKh 0.33984375\n",
      "Trained batch 2622 batch loss 2.8515842 batch mAP 0.08984375 batch PCKh 0.416015625\n",
      "Trained batch 2623 batch loss 2.56033587 batch mAP 0.0546875 batch PCKh 0.455078125\n",
      "Trained batch 2624 batch loss 2.36071396 batch mAP 0.07421875 batch PCKh 0.294921875\n",
      "Trained batch 2625 batch loss 3.03187 batch mAP 0.076171875 batch PCKh 0.439453125\n",
      "Trained batch 2626 batch loss 2.90218735 batch mAP 0.0703125 batch PCKh 0.5\n",
      "Trained batch 2627 batch loss 3.15239668 batch mAP 0.099609375 batch PCKh 0.337890625\n",
      "Trained batch 2628 batch loss 2.93440914 batch mAP 0.060546875 batch PCKh 0.2890625\n",
      "Trained batch 2629 batch loss 2.80708 batch mAP 0.060546875 batch PCKh 0.482421875\n",
      "Trained batch 2630 batch loss 2.87686348 batch mAP 0.064453125 batch PCKh 0.49609375\n",
      "Trained batch 2631 batch loss 2.75745296 batch mAP 0.068359375 batch PCKh 0.48828125\n",
      "Trained batch 2632 batch loss 2.85207057 batch mAP 0.0546875 batch PCKh 0.40625\n",
      "Trained batch 2633 batch loss 2.78090572 batch mAP 0.052734375 batch PCKh 0.47265625\n",
      "Trained batch 2634 batch loss 2.47491789 batch mAP 0.02734375 batch PCKh 0.408203125\n",
      "Trained batch 2635 batch loss 2.37728214 batch mAP 0.02734375 batch PCKh 0.412109375\n",
      "Trained batch 2636 batch loss 2.61018419 batch mAP 0.013671875 batch PCKh 0.662109375\n",
      "Trained batch 2637 batch loss 2.62462378 batch mAP 0.037109375 batch PCKh 0.484375\n",
      "Trained batch 2638 batch loss 2.81643581 batch mAP 0.05859375 batch PCKh 0.498046875\n",
      "Trained batch 2639 batch loss 2.64065838 batch mAP 0.033203125 batch PCKh 0.6015625\n",
      "Trained batch 2640 batch loss 2.76235652 batch mAP 0.01953125 batch PCKh 0.44140625\n",
      "Trained batch 2641 batch loss 2.60116959 batch mAP 0.013671875 batch PCKh 0.208984375\n",
      "Trained batch 2642 batch loss 2.9370985 batch mAP 0.029296875 batch PCKh 0.197265625\n",
      "Trained batch 2643 batch loss 2.97584963 batch mAP 0.0390625 batch PCKh 0.283203125\n",
      "Trained batch 2644 batch loss 3.00872111 batch mAP 0.041015625 batch PCKh 0.189453125\n",
      "Trained batch 2645 batch loss 3.0878787 batch mAP 0.02734375 batch PCKh 0.3046875\n",
      "Trained batch 2646 batch loss 3.12760878 batch mAP 0.02734375 batch PCKh 0.3515625\n",
      "Trained batch 2647 batch loss 3.11203694 batch mAP 0.013671875 batch PCKh 0.27734375\n",
      "Trained batch 2648 batch loss 2.96711183 batch mAP 0.03125 batch PCKh 0.361328125\n",
      "Trained batch 2649 batch loss 2.93633 batch mAP 0.080078125 batch PCKh 0.5234375\n",
      "Trained batch 2650 batch loss 2.78990579 batch mAP 0.0625 batch PCKh 0.470703125\n",
      "Trained batch 2651 batch loss 2.82212591 batch mAP 0.072265625 batch PCKh 0.505859375\n",
      "Trained batch 2652 batch loss 2.81864262 batch mAP 0.021484375 batch PCKh 0.576171875\n",
      "Trained batch 2653 batch loss 2.79921556 batch mAP 0.0234375 batch PCKh 0.474609375\n",
      "Trained batch 2654 batch loss 2.67365861 batch mAP 0.0625 batch PCKh 0.4453125\n",
      "Trained batch 2655 batch loss 2.92229366 batch mAP 0.1328125 batch PCKh 0.40625\n",
      "Trained batch 2656 batch loss 3.0541997 batch mAP 0.076171875 batch PCKh 0.33203125\n",
      "Trained batch 2657 batch loss 3.21362948 batch mAP 0.0546875 batch PCKh 0.2109375\n",
      "Trained batch 2658 batch loss 2.89870095 batch mAP 0.056640625 batch PCKh 0.50390625\n",
      "Trained batch 2659 batch loss 2.80460525 batch mAP 0.109375 batch PCKh 0.482421875\n",
      "Trained batch 2660 batch loss 2.64924335 batch mAP 0.08203125 batch PCKh 0.232421875\n",
      "Trained batch 2661 batch loss 2.62897444 batch mAP 0.0546875 batch PCKh 0.2421875\n",
      "Trained batch 2662 batch loss 3.15431643 batch mAP 0.068359375 batch PCKh 0.12890625\n",
      "Trained batch 2663 batch loss 3.01860356 batch mAP 0.107421875 batch PCKh 0.3359375\n",
      "Trained batch 2664 batch loss 3.14478564 batch mAP 0.0859375 batch PCKh 0.49609375\n",
      "Trained batch 2665 batch loss 3.03002405 batch mAP 0.078125 batch PCKh 0.40625\n",
      "Trained batch 2666 batch loss 3.10384083 batch mAP 0.103515625 batch PCKh 0.41015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2667 batch loss 2.96880507 batch mAP 0.150390625 batch PCKh 0.509765625\n",
      "Trained batch 2668 batch loss 2.62278366 batch mAP 0.197265625 batch PCKh 0.421875\n",
      "Trained batch 2669 batch loss 2.8441 batch mAP 0.11328125 batch PCKh 0.376953125\n",
      "Trained batch 2670 batch loss 2.74057221 batch mAP 0.12109375 batch PCKh 0.255859375\n",
      "Trained batch 2671 batch loss 2.65562916 batch mAP 0.142578125 batch PCKh 0.5390625\n",
      "Trained batch 2672 batch loss 2.64358 batch mAP 0.166015625 batch PCKh 0.35546875\n",
      "Trained batch 2673 batch loss 2.71382022 batch mAP 0.1171875 batch PCKh 0.388671875\n",
      "Trained batch 2674 batch loss 2.65966821 batch mAP 0.205078125 batch PCKh 0.525390625\n",
      "Trained batch 2675 batch loss 2.92738867 batch mAP 0.111328125 batch PCKh 0.556640625\n",
      "Trained batch 2676 batch loss 2.82896757 batch mAP 0.060546875 batch PCKh 0.62109375\n",
      "Trained batch 2677 batch loss 2.93869495 batch mAP 0.126953125 batch PCKh 0.556640625\n",
      "Trained batch 2678 batch loss 2.8487854 batch mAP 0.08984375 batch PCKh 0.53515625\n",
      "Trained batch 2679 batch loss 3.03693771 batch mAP 0.08203125 batch PCKh 0.5546875\n",
      "Trained batch 2680 batch loss 2.8348155 batch mAP 0.140625 batch PCKh 0.546875\n",
      "Trained batch 2681 batch loss 2.42904139 batch mAP 0.060546875 batch PCKh 0.25390625\n",
      "Trained batch 2682 batch loss 2.47594643 batch mAP 0.08203125 batch PCKh 0.365234375\n",
      "Trained batch 2683 batch loss 2.71702528 batch mAP 0.0625 batch PCKh 0.4296875\n",
      "Trained batch 2684 batch loss 2.98135233 batch mAP 0.052734375 batch PCKh 0.46484375\n",
      "Trained batch 2685 batch loss 2.57862353 batch mAP 0.041015625 batch PCKh 0.58984375\n",
      "Trained batch 2686 batch loss 2.76035213 batch mAP 0.03515625 batch PCKh 0.49609375\n",
      "Trained batch 2687 batch loss 2.70344663 batch mAP 0.080078125 batch PCKh 0.419921875\n",
      "Trained batch 2688 batch loss 2.73429966 batch mAP 0.05859375 batch PCKh 0.484375\n",
      "Trained batch 2689 batch loss 2.70429277 batch mAP 0.009765625 batch PCKh 0.2734375\n",
      "Trained batch 2690 batch loss 2.95426 batch mAP 0.013671875 batch PCKh 0.298828125\n",
      "Trained batch 2691 batch loss 3.06553125 batch mAP 0.03125 batch PCKh 0.48046875\n",
      "Trained batch 2692 batch loss 3.02926874 batch mAP 0.025390625 batch PCKh 0.494140625\n",
      "Trained batch 2693 batch loss 2.52495527 batch mAP 0.03515625 batch PCKh 0.30859375\n",
      "Trained batch 2694 batch loss 2.59767199 batch mAP 0.05078125 batch PCKh 0.068359375\n",
      "Trained batch 2695 batch loss 2.54490376 batch mAP 0.0234375 batch PCKh 0.330078125\n",
      "Trained batch 2696 batch loss 2.25568795 batch mAP 0.044921875 batch PCKh 0.072265625\n",
      "Trained batch 2697 batch loss 2.28308964 batch mAP 0.033203125 batch PCKh 0.16015625\n",
      "Trained batch 2698 batch loss 2.46933746 batch mAP 0.03515625 batch PCKh 0.2109375\n",
      "Trained batch 2699 batch loss 3.02526712 batch mAP 0.06640625 batch PCKh 0.287109375\n",
      "Trained batch 2700 batch loss 2.85392475 batch mAP 0.078125 batch PCKh 0.34375\n",
      "Trained batch 2701 batch loss 3.03550386 batch mAP 0.091796875 batch PCKh 0.365234375\n",
      "Trained batch 2702 batch loss 3.02345085 batch mAP 0.060546875 batch PCKh 0.392578125\n",
      "Trained batch 2703 batch loss 3.00084805 batch mAP 0.078125 batch PCKh 0.330078125\n",
      "Trained batch 2704 batch loss 2.86836767 batch mAP 0.064453125 batch PCKh 0.541015625\n",
      "Trained batch 2705 batch loss 2.94850826 batch mAP 0.09375 batch PCKh 0.5078125\n",
      "Trained batch 2706 batch loss 2.56822085 batch mAP 0.080078125 batch PCKh 0.416015625\n",
      "Trained batch 2707 batch loss 2.7397728 batch mAP 0.08203125 batch PCKh 0.54296875\n",
      "Trained batch 2708 batch loss 2.83136654 batch mAP 0.064453125 batch PCKh 0.4765625\n",
      "Trained batch 2709 batch loss 2.98965454 batch mAP 0.060546875 batch PCKh 0.435546875\n",
      "Trained batch 2710 batch loss 2.86394548 batch mAP 0.044921875 batch PCKh 0.50390625\n",
      "Trained batch 2711 batch loss 2.70782661 batch mAP 0.041015625 batch PCKh 0.474609375\n",
      "Trained batch 2712 batch loss 2.67936087 batch mAP 0.119140625 batch PCKh 0.28125\n",
      "Trained batch 2713 batch loss 2.86433077 batch mAP 0.076171875 batch PCKh 0.423828125\n",
      "Trained batch 2714 batch loss 2.74264956 batch mAP 0.041015625 batch PCKh 0.228515625\n",
      "Trained batch 2715 batch loss 2.84060907 batch mAP 0.05859375 batch PCKh 0.357421875\n",
      "Trained batch 2716 batch loss 2.56759834 batch mAP 0.048828125 batch PCKh 0.279296875\n",
      "Trained batch 2717 batch loss 2.76987791 batch mAP 0.087890625 batch PCKh 0.5\n",
      "Trained batch 2718 batch loss 2.65683126 batch mAP 0.029296875 batch PCKh 0.5078125\n",
      "Trained batch 2719 batch loss 2.76089454 batch mAP 0.0859375 batch PCKh 0.419921875\n",
      "Trained batch 2720 batch loss 2.8020525 batch mAP 0.017578125 batch PCKh 0.60546875\n",
      "Trained batch 2721 batch loss 2.66146708 batch mAP 0.037109375 batch PCKh 0.78125\n",
      "Trained batch 2722 batch loss 2.70198393 batch mAP 0.064453125 batch PCKh 0.65234375\n",
      "Trained batch 2723 batch loss 2.69800258 batch mAP 0.037109375 batch PCKh 0.37890625\n",
      "Trained batch 2724 batch loss 2.72809315 batch mAP 0.02734375 batch PCKh 0.470703125\n",
      "Trained batch 2725 batch loss 2.89673519 batch mAP 0.04296875 batch PCKh 0.634765625\n",
      "Trained batch 2726 batch loss 2.91389322 batch mAP 0.03125 batch PCKh 0.51953125\n",
      "Trained batch 2727 batch loss 2.75387859 batch mAP 0.0390625 batch PCKh 0.556640625\n",
      "Trained batch 2728 batch loss 2.93020415 batch mAP 0.02734375 batch PCKh 0.46875\n",
      "Trained batch 2729 batch loss 2.84138513 batch mAP 0.025390625 batch PCKh 0.490234375\n",
      "Trained batch 2730 batch loss 2.94915438 batch mAP 0.048828125 batch PCKh 0.552734375\n",
      "Trained batch 2731 batch loss 2.79179 batch mAP 0.03125 batch PCKh 0.4453125\n",
      "Trained batch 2732 batch loss 2.77283025 batch mAP 0.03515625 batch PCKh 0.552734375\n",
      "Trained batch 2733 batch loss 3.00978327 batch mAP 0.041015625 batch PCKh 0.54296875\n",
      "Trained batch 2734 batch loss 3.00187016 batch mAP 0.060546875 batch PCKh 0.56640625\n",
      "Trained batch 2735 batch loss 2.89385271 batch mAP 0.015625 batch PCKh 0.4609375\n",
      "Trained batch 2736 batch loss 2.71060085 batch mAP 0.060546875 batch PCKh 0.533203125\n",
      "Trained batch 2737 batch loss 2.77504301 batch mAP 0.015625 batch PCKh 0.341796875\n",
      "Trained batch 2738 batch loss 2.90558457 batch mAP 0.029296875 batch PCKh 0.43359375\n",
      "Trained batch 2739 batch loss 2.7475481 batch mAP 0.041015625 batch PCKh 0.52734375\n",
      "Trained batch 2740 batch loss 2.82299 batch mAP 0.048828125 batch PCKh 0.595703125\n",
      "Trained batch 2741 batch loss 3.02111101 batch mAP 0.0625 batch PCKh 0.49609375\n",
      "Trained batch 2742 batch loss 2.91530943 batch mAP 0.072265625 batch PCKh 0.296875\n",
      "Trained batch 2743 batch loss 3.02588201 batch mAP 0.078125 batch PCKh 0.427734375\n",
      "Trained batch 2744 batch loss 2.95376015 batch mAP 0.060546875 batch PCKh 0.62109375\n",
      "Trained batch 2745 batch loss 2.82972288 batch mAP 0.056640625 batch PCKh 0.552734375\n",
      "Trained batch 2746 batch loss 2.64265537 batch mAP 0.044921875 batch PCKh 0.580078125\n",
      "Trained batch 2747 batch loss 2.56465697 batch mAP 0.099609375 batch PCKh 0.60546875\n",
      "Trained batch 2748 batch loss 2.72375417 batch mAP 0.0546875 batch PCKh 0.658203125\n",
      "Trained batch 2749 batch loss 2.79540586 batch mAP 0.044921875 batch PCKh 0.58984375\n",
      "Trained batch 2750 batch loss 2.71090174 batch mAP 0.04296875 batch PCKh 0.5234375\n",
      "Trained batch 2751 batch loss 2.50903988 batch mAP 0.10546875 batch PCKh 0.580078125\n",
      "Trained batch 2752 batch loss 2.81064987 batch mAP 0.03125 batch PCKh 0.505859375\n",
      "Trained batch 2753 batch loss 2.8088212 batch mAP 0.046875 batch PCKh 0.552734375\n",
      "Trained batch 2754 batch loss 2.9612658 batch mAP 0.048828125 batch PCKh 0.564453125\n",
      "Trained batch 2755 batch loss 3.12713289 batch mAP 0.052734375 batch PCKh 0.470703125\n",
      "Trained batch 2756 batch loss 3.03681827 batch mAP 0.078125 batch PCKh 0.556640625\n",
      "Trained batch 2757 batch loss 3.08116865 batch mAP 0.041015625 batch PCKh 0.431640625\n",
      "Trained batch 2758 batch loss 2.93652797 batch mAP 0.04296875 batch PCKh 0.541015625\n",
      "Trained batch 2759 batch loss 3.00746918 batch mAP 0.03125 batch PCKh 0.4921875\n",
      "Trained batch 2760 batch loss 2.8981421 batch mAP 0.037109375 batch PCKh 0.513671875\n",
      "Trained batch 2761 batch loss 2.81853271 batch mAP 0.0390625 batch PCKh 0.537109375\n",
      "Trained batch 2762 batch loss 3.02048492 batch mAP 0.056640625 batch PCKh 0.443359375\n",
      "Trained batch 2763 batch loss 2.93890047 batch mAP 0.041015625 batch PCKh 0.537109375\n",
      "Trained batch 2764 batch loss 2.80062532 batch mAP 0.05078125 batch PCKh 0.50390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 2765 batch loss 3.07567263 batch mAP 0.060546875 batch PCKh 0.42578125\n",
      "Trained batch 2766 batch loss 2.88468933 batch mAP 0.046875 batch PCKh 0.57421875\n",
      "Trained batch 2767 batch loss 2.86029863 batch mAP 0.0625 batch PCKh 0.375\n",
      "Trained batch 2768 batch loss 2.74216628 batch mAP 0.107421875 batch PCKh 0.447265625\n",
      "Trained batch 2769 batch loss 2.80002117 batch mAP 0.04296875 batch PCKh 0.412109375\n",
      "Trained batch 2770 batch loss 2.68239188 batch mAP 0.087890625 batch PCKh 0.4609375\n",
      "Trained batch 2771 batch loss 2.97239971 batch mAP 0.0625 batch PCKh 0.390625\n",
      "Trained batch 2772 batch loss 2.81880331 batch mAP 0.08984375 batch PCKh 0.41015625\n",
      "Trained batch 2773 batch loss 2.66613579 batch mAP 0.12109375 batch PCKh 0.466796875\n",
      "Trained batch 2774 batch loss 2.6559484 batch mAP 0.109375 batch PCKh 0.5078125\n",
      "Trained batch 2775 batch loss 2.94987273 batch mAP 0.10546875 batch PCKh 0.673828125\n",
      "Trained batch 2776 batch loss 3.11800861 batch mAP 0.109375 batch PCKh 0.6328125\n",
      "Epoch 1 train loss 3.036715030670166 train mAP 0.3019498884677887 train PCKh 0.384084552526474\n",
      "Validated batch 1 batch loss 2.7511425 batch mAP 0.130859375 batch PCKh 0.521484375\n",
      "Validated batch 2 batch loss 2.85769606 batch mAP 0.130859375 batch PCKh 0.560546875\n",
      "Validated batch 3 batch loss 2.91192913 batch mAP 0.359375 batch PCKh 0.61328125\n",
      "Validated batch 4 batch loss 2.77892017 batch mAP 0.095703125 batch PCKh 0.5625\n",
      "Validated batch 5 batch loss 2.80827761 batch mAP 0.04296875 batch PCKh 0.455078125\n",
      "Validated batch 6 batch loss 2.47538853 batch mAP 0.07421875 batch PCKh 0.5078125\n",
      "Validated batch 7 batch loss 3.00301528 batch mAP 0.21875 batch PCKh 0.439453125\n",
      "Validated batch 8 batch loss 2.94636583 batch mAP 0.14453125 batch PCKh 0.435546875\n",
      "Validated batch 9 batch loss 2.74512053 batch mAP 0.037109375 batch PCKh 0.5625\n",
      "Validated batch 10 batch loss 2.79800105 batch mAP 0.09375 batch PCKh 0.580078125\n",
      "Validated batch 11 batch loss 2.82218981 batch mAP 0.009765625 batch PCKh 0.6796875\n",
      "Validated batch 12 batch loss 2.7984271 batch mAP 0 batch PCKh 0.576171875\n",
      "Validated batch 13 batch loss 2.97109795 batch mAP 0.326171875 batch PCKh 0.529296875\n",
      "Validated batch 14 batch loss 3.0964632 batch mAP 0.16796875 batch PCKh 0.396484375\n",
      "Validated batch 15 batch loss 2.87138319 batch mAP 0.232421875 batch PCKh 0.40234375\n",
      "Validated batch 16 batch loss 2.88764906 batch mAP 0.083984375 batch PCKh 0.537109375\n",
      "Validated batch 17 batch loss 2.96550274 batch mAP 0.130859375 batch PCKh 0.43359375\n",
      "Validated batch 18 batch loss 2.97367048 batch mAP 0.17578125 batch PCKh 0.388671875\n",
      "Validated batch 19 batch loss 2.92797637 batch mAP 0.048828125 batch PCKh 0.435546875\n",
      "Validated batch 20 batch loss 2.87971 batch mAP 0.021484375 batch PCKh 0.5546875\n",
      "Validated batch 21 batch loss 2.81958818 batch mAP 0.052734375 batch PCKh 0.57421875\n",
      "Validated batch 22 batch loss 3.06846857 batch mAP 0.044921875 batch PCKh 0.40625\n",
      "Validated batch 23 batch loss 2.73942947 batch mAP 0.138671875 batch PCKh 0.517578125\n",
      "Validated batch 24 batch loss 2.79833698 batch mAP 0.294921875 batch PCKh 0.4296875\n",
      "Validated batch 25 batch loss 2.62600374 batch mAP 0.169921875 batch PCKh 0.240234375\n",
      "Validated batch 26 batch loss 2.88692141 batch mAP 0.130859375 batch PCKh 0.603515625\n",
      "Validated batch 27 batch loss 2.91517186 batch mAP 0.103515625 batch PCKh 0.390625\n",
      "Validated batch 28 batch loss 3.06797624 batch mAP 0.203125 batch PCKh 0.369140625\n",
      "Validated batch 29 batch loss 2.95304894 batch mAP 0.15625 batch PCKh 0.228515625\n",
      "Validated batch 30 batch loss 2.79644632 batch mAP 0.16015625 batch PCKh 0.4765625\n",
      "Validated batch 31 batch loss 2.85048914 batch mAP 0.205078125 batch PCKh 0.322265625\n",
      "Validated batch 32 batch loss 2.75077415 batch mAP 0.119140625 batch PCKh 0.365234375\n",
      "Validated batch 33 batch loss 3.04137182 batch mAP 0.369140625 batch PCKh 0.31640625\n",
      "Validated batch 34 batch loss 3.12420273 batch mAP 0.294921875 batch PCKh 0.1640625\n",
      "Validated batch 35 batch loss 2.43718266 batch mAP 0.12890625 batch PCKh 0.1484375\n",
      "Validated batch 36 batch loss 2.58187532 batch mAP 0.01953125 batch PCKh 0.365234375\n",
      "Validated batch 37 batch loss 2.70085907 batch mAP 0.13671875 batch PCKh 0.177734375\n",
      "Validated batch 38 batch loss 3.10414433 batch mAP 0.193359375 batch PCKh 0.46484375\n",
      "Validated batch 39 batch loss 2.68510365 batch mAP 0.044921875 batch PCKh 0.595703125\n",
      "Validated batch 40 batch loss 2.61658192 batch mAP 0.150390625 batch PCKh 0.533203125\n",
      "Validated batch 41 batch loss 2.67765045 batch mAP 0.146484375 batch PCKh 0.48046875\n",
      "Validated batch 42 batch loss 3.01068068 batch mAP 0.19140625 batch PCKh 0.404296875\n",
      "Validated batch 43 batch loss 3.01930952 batch mAP 0.0078125 batch PCKh 0.43359375\n",
      "Validated batch 44 batch loss 3.0570538 batch mAP 0.064453125 batch PCKh 0.5\n",
      "Validated batch 45 batch loss 2.61623359 batch mAP 0.078125 batch PCKh 0.443359375\n",
      "Validated batch 46 batch loss 2.76672029 batch mAP 0.0234375 batch PCKh 0.6015625\n",
      "Validated batch 47 batch loss 2.94539809 batch mAP 0.103515625 batch PCKh 0.423828125\n",
      "Validated batch 48 batch loss 3.01433182 batch mAP 0.158203125 batch PCKh 0.3671875\n",
      "Validated batch 49 batch loss 3.02784634 batch mAP 0.11328125 batch PCKh 0.474609375\n",
      "Validated batch 50 batch loss 2.68682814 batch mAP 0.076171875 batch PCKh 0.3984375\n",
      "Validated batch 51 batch loss 2.48843336 batch mAP 0.048828125 batch PCKh 0.6015625\n",
      "Validated batch 52 batch loss 2.87141609 batch mAP 0.25390625 batch PCKh 0.3046875\n",
      "Validated batch 53 batch loss 2.5375309 batch mAP 0.150390625 batch PCKh 0.365234375\n",
      "Validated batch 54 batch loss 2.68832159 batch mAP 0.205078125 batch PCKh 0.41796875\n",
      "Validated batch 55 batch loss 2.87007904 batch mAP 0.125 batch PCKh 0.537109375\n",
      "Validated batch 56 batch loss 2.69621754 batch mAP 0.0234375 batch PCKh 0.236328125\n",
      "Validated batch 57 batch loss 2.85075951 batch mAP 0.1484375 batch PCKh 0.451171875\n",
      "Validated batch 58 batch loss 2.82304144 batch mAP 0.005859375 batch PCKh 0.513671875\n",
      "Validated batch 59 batch loss 2.82130766 batch mAP 0.01953125 batch PCKh 0.412109375\n",
      "Validated batch 60 batch loss 2.53564382 batch mAP 0.060546875 batch PCKh 0.232421875\n",
      "Validated batch 61 batch loss 2.73549986 batch mAP 0.125 batch PCKh 0.482421875\n",
      "Validated batch 62 batch loss 2.7339263 batch mAP 0.11328125 batch PCKh 0.37109375\n",
      "Validated batch 63 batch loss 2.45843 batch mAP 0.177734375 batch PCKh 0.2109375\n",
      "Validated batch 64 batch loss 3.12042 batch mAP 0.197265625 batch PCKh 0.439453125\n",
      "Validated batch 65 batch loss 3.02882934 batch mAP 0.271484375 batch PCKh 0.279296875\n",
      "Validated batch 66 batch loss 2.76997066 batch mAP 0.1484375 batch PCKh 0.4140625\n",
      "Validated batch 67 batch loss 2.76761675 batch mAP 0.078125 batch PCKh 0.427734375\n",
      "Validated batch 68 batch loss 2.62547326 batch mAP 0.076171875 batch PCKh 0.392578125\n",
      "Validated batch 69 batch loss 2.5935154 batch mAP 0.052734375 batch PCKh 0.275390625\n",
      "Validated batch 70 batch loss 2.63774562 batch mAP 0.181640625 batch PCKh 0.599609375\n",
      "Validated batch 71 batch loss 2.78594112 batch mAP 0.26171875 batch PCKh 0.388671875\n",
      "Validated batch 72 batch loss 2.85019159 batch mAP 0.111328125 batch PCKh 0.408203125\n",
      "Validated batch 73 batch loss 3.0407896 batch mAP 0.2890625 batch PCKh 0.47265625\n",
      "Validated batch 74 batch loss 2.74902725 batch mAP 0.36328125 batch PCKh 0.333984375\n",
      "Validated batch 75 batch loss 3.11690235 batch mAP 0.158203125 batch PCKh 0.384765625\n",
      "Validated batch 76 batch loss 2.99310398 batch mAP 0.4765625 batch PCKh 0.310546875\n",
      "Validated batch 77 batch loss 2.99922 batch mAP 0.548828125 batch PCKh 0.376953125\n",
      "Validated batch 78 batch loss 2.82506752 batch mAP 0.21875 batch PCKh 0.396484375\n",
      "Validated batch 79 batch loss 2.59628439 batch mAP 0.173828125 batch PCKh 0.484375\n",
      "Validated batch 80 batch loss 2.8859787 batch mAP 0.083984375 batch PCKh 0.447265625\n",
      "Validated batch 81 batch loss 3.16891217 batch mAP 0.025390625 batch PCKh 0.5\n",
      "Validated batch 82 batch loss 2.96819305 batch mAP 0.4296875 batch PCKh 0.21875\n",
      "Validated batch 83 batch loss 2.72162247 batch mAP 0.130859375 batch PCKh 0.392578125\n",
      "Validated batch 84 batch loss 2.98217607 batch mAP 0.037109375 batch PCKh 0.46875\n",
      "Validated batch 85 batch loss 3.10691857 batch mAP 0.205078125 batch PCKh 0.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 86 batch loss 2.5342381 batch mAP 0.2109375 batch PCKh 0.19921875\n",
      "Validated batch 87 batch loss 3.14090419 batch mAP 0.251953125 batch PCKh 0.365234375\n",
      "Validated batch 88 batch loss 2.83322835 batch mAP 0.1328125 batch PCKh 0.556640625\n",
      "Validated batch 89 batch loss 2.36977816 batch mAP 0.09375 batch PCKh 0.134765625\n",
      "Validated batch 90 batch loss 2.4693625 batch mAP 0.080078125 batch PCKh 0.29296875\n",
      "Validated batch 91 batch loss 2.83538795 batch mAP 0.158203125 batch PCKh 0.505859375\n",
      "Validated batch 92 batch loss 3.05167794 batch mAP 0.060546875 batch PCKh 0.5234375\n",
      "Validated batch 93 batch loss 2.82891369 batch mAP 0.06640625 batch PCKh 0.46484375\n",
      "Validated batch 94 batch loss 2.75490379 batch mAP 0.150390625 batch PCKh 0.296875\n",
      "Validated batch 95 batch loss 2.6582551 batch mAP 0.03125 batch PCKh 0.4609375\n",
      "Validated batch 96 batch loss 2.73888016 batch mAP 0.255859375 batch PCKh 0.3203125\n",
      "Validated batch 97 batch loss 2.78798556 batch mAP 0.041015625 batch PCKh 0.611328125\n",
      "Validated batch 98 batch loss 2.86830378 batch mAP 0.11328125 batch PCKh 0.375\n",
      "Validated batch 99 batch loss 2.6849792 batch mAP 0.0078125 batch PCKh 0.51171875\n",
      "Validated batch 100 batch loss 2.58301783 batch mAP 0.037109375 batch PCKh 0.484375\n",
      "Validated batch 101 batch loss 2.80382895 batch mAP 0.140625 batch PCKh 0.33984375\n",
      "Validated batch 102 batch loss 2.83316946 batch mAP 0.1328125 batch PCKh 0.6484375\n",
      "Validated batch 103 batch loss 2.82073879 batch mAP 0.095703125 batch PCKh 0.25390625\n",
      "Validated batch 104 batch loss 2.91600561 batch mAP 0.1015625 batch PCKh 0.451171875\n",
      "Validated batch 105 batch loss 2.83964 batch mAP 0.060546875 batch PCKh 0.560546875\n",
      "Validated batch 106 batch loss 2.99407768 batch mAP 0.1015625 batch PCKh 0.513671875\n",
      "Validated batch 107 batch loss 2.84109259 batch mAP 0.12109375 batch PCKh 0.3828125\n",
      "Validated batch 108 batch loss 3.19629931 batch mAP 0.31640625 batch PCKh 0.287109375\n",
      "Validated batch 109 batch loss 2.92957973 batch mAP 0.220703125 batch PCKh 0.44140625\n",
      "Validated batch 110 batch loss 2.93293953 batch mAP 0.033203125 batch PCKh 0.419921875\n",
      "Validated batch 111 batch loss 2.96679592 batch mAP 0.3125 batch PCKh 0.369140625\n",
      "Validated batch 112 batch loss 2.85385561 batch mAP 0.12890625 batch PCKh 0.58984375\n",
      "Validated batch 113 batch loss 2.94679594 batch mAP 0.1796875 batch PCKh 0.44140625\n",
      "Validated batch 114 batch loss 2.74771166 batch mAP 0.126953125 batch PCKh 0.384765625\n",
      "Validated batch 115 batch loss 2.95780706 batch mAP 0.181640625 batch PCKh 0.44921875\n",
      "Validated batch 116 batch loss 2.98310423 batch mAP 0.18359375 batch PCKh 0.2890625\n",
      "Validated batch 117 batch loss 3.04490829 batch mAP 0.185546875 batch PCKh 0.515625\n",
      "Validated batch 118 batch loss 2.78966641 batch mAP 0.32421875 batch PCKh 0.306640625\n",
      "Validated batch 119 batch loss 2.97152662 batch mAP 0.17578125 batch PCKh 0.41796875\n",
      "Validated batch 120 batch loss 2.92788601 batch mAP 0.09765625 batch PCKh 0.455078125\n",
      "Validated batch 121 batch loss 2.94155741 batch mAP 0.05859375 batch PCKh 0.4140625\n",
      "Validated batch 122 batch loss 2.94405365 batch mAP 0.056640625 batch PCKh 0.453125\n",
      "Validated batch 123 batch loss 3.06257677 batch mAP 0.166015625 batch PCKh 0.544921875\n",
      "Validated batch 124 batch loss 2.69454932 batch mAP 0.078125 batch PCKh 0.283203125\n",
      "Validated batch 125 batch loss 3.11386395 batch mAP 0.189453125 batch PCKh 0.435546875\n",
      "Validated batch 126 batch loss 2.7606473 batch mAP 0.064453125 batch PCKh 0.35546875\n",
      "Validated batch 127 batch loss 2.66838574 batch mAP 0.107421875 batch PCKh 0.541015625\n",
      "Validated batch 128 batch loss 2.43000269 batch mAP 0.109375 batch PCKh 0.2265625\n",
      "Validated batch 129 batch loss 2.81440973 batch mAP 0.1171875 batch PCKh 0.435546875\n",
      "Validated batch 130 batch loss 2.82904196 batch mAP 0.072265625 batch PCKh 0.5390625\n",
      "Validated batch 131 batch loss 2.9043026 batch mAP 0.1640625 batch PCKh 0.4765625\n",
      "Validated batch 132 batch loss 2.91589761 batch mAP 0.0234375 batch PCKh 0.453125\n",
      "Validated batch 133 batch loss 3.04379177 batch mAP 0.095703125 batch PCKh 0.537109375\n",
      "Validated batch 134 batch loss 2.78982115 batch mAP 0.16015625 batch PCKh 0.48828125\n",
      "Validated batch 135 batch loss 2.85519409 batch mAP 0.046875 batch PCKh 0.45703125\n",
      "Validated batch 136 batch loss 3.03653908 batch mAP 0.044921875 batch PCKh 0.4453125\n",
      "Validated batch 137 batch loss 2.88102126 batch mAP 0.1171875 batch PCKh 0.439453125\n",
      "Validated batch 138 batch loss 2.63812351 batch mAP 0.064453125 batch PCKh 0.255859375\n",
      "Validated batch 139 batch loss 2.81160069 batch mAP 0.041015625 batch PCKh 0.55859375\n",
      "Validated batch 140 batch loss 2.78346777 batch mAP 0.125 batch PCKh 0.501953125\n",
      "Validated batch 141 batch loss 2.75035429 batch mAP 0.1328125 batch PCKh 0.3125\n",
      "Validated batch 142 batch loss 2.47503233 batch mAP 0.0234375 batch PCKh 0.638671875\n",
      "Validated batch 143 batch loss 2.91553497 batch mAP 0.22265625 batch PCKh 0.359375\n",
      "Validated batch 144 batch loss 2.68629479 batch mAP 0.279296875 batch PCKh 0.24609375\n",
      "Validated batch 145 batch loss 2.74985909 batch mAP 0.1640625 batch PCKh 0.33203125\n",
      "Validated batch 146 batch loss 2.68897796 batch mAP 0.119140625 batch PCKh 0.478515625\n",
      "Validated batch 147 batch loss 2.86217165 batch mAP 0.0859375 batch PCKh 0.697265625\n",
      "Validated batch 148 batch loss 2.70713758 batch mAP 0.12109375 batch PCKh 0.5390625\n",
      "Validated batch 149 batch loss 2.79740572 batch mAP 0.177734375 batch PCKh 0.48828125\n",
      "Validated batch 150 batch loss 2.79087114 batch mAP 0.19921875 batch PCKh 0.447265625\n",
      "Validated batch 151 batch loss 2.89376163 batch mAP 0.162109375 batch PCKh 0.494140625\n",
      "Validated batch 152 batch loss 2.843647 batch mAP 0.251953125 batch PCKh 0.158203125\n",
      "Validated batch 153 batch loss 2.60599947 batch mAP 0.0859375 batch PCKh 0.513671875\n",
      "Validated batch 154 batch loss 2.73989153 batch mAP 0.041015625 batch PCKh 0.572265625\n",
      "Validated batch 155 batch loss 2.9538281 batch mAP 0.197265625 batch PCKh 0.390625\n",
      "Validated batch 156 batch loss 2.78238297 batch mAP 0.12890625 batch PCKh 0.259765625\n",
      "Validated batch 157 batch loss 2.72135448 batch mAP 0.02734375 batch PCKh 0.62109375\n",
      "Validated batch 158 batch loss 2.96408844 batch mAP 0.107421875 batch PCKh 0.478515625\n",
      "Validated batch 159 batch loss 3.09282207 batch mAP 0.03125 batch PCKh 0.44140625\n",
      "Validated batch 160 batch loss 2.85568762 batch mAP 0.05859375 batch PCKh 0.5546875\n",
      "Validated batch 161 batch loss 3.094275 batch mAP 0.244140625 batch PCKh 0.4375\n",
      "Validated batch 162 batch loss 2.61449957 batch mAP 0.212890625 batch PCKh 0.4296875\n",
      "Validated batch 163 batch loss 2.59837031 batch mAP 0.1484375 batch PCKh 0.376953125\n",
      "Validated batch 164 batch loss 2.91599894 batch mAP 0.109375 batch PCKh 0.466796875\n",
      "Validated batch 165 batch loss 2.75029325 batch mAP 0.109375 batch PCKh 0.451171875\n",
      "Validated batch 166 batch loss 2.57120371 batch mAP 0.0078125 batch PCKh 0.53125\n",
      "Validated batch 167 batch loss 2.85402107 batch mAP 0.14453125 batch PCKh 0.412109375\n",
      "Validated batch 168 batch loss 2.8285203 batch mAP 0.0703125 batch PCKh 0.33984375\n",
      "Validated batch 169 batch loss 2.71632886 batch mAP 0.10546875 batch PCKh 0.412109375\n",
      "Validated batch 170 batch loss 2.83746624 batch mAP 0.150390625 batch PCKh 0.333984375\n",
      "Validated batch 171 batch loss 3.10766029 batch mAP 0.25 batch PCKh 0.326171875\n",
      "Validated batch 172 batch loss 3.02177405 batch mAP 0.1640625 batch PCKh 0.390625\n",
      "Validated batch 173 batch loss 2.47390413 batch mAP 0.10546875 batch PCKh 0.19140625\n",
      "Validated batch 174 batch loss 3.03796601 batch mAP 0.095703125 batch PCKh 0.435546875\n",
      "Validated batch 175 batch loss 2.93644524 batch mAP 0.125 batch PCKh 0.443359375\n",
      "Validated batch 176 batch loss 2.5218873 batch mAP 0.072265625 batch PCKh 0.390625\n",
      "Validated batch 177 batch loss 2.88981748 batch mAP 0.171875 batch PCKh 0.41796875\n",
      "Validated batch 178 batch loss 2.92219234 batch mAP 0.24609375 batch PCKh 0.248046875\n",
      "Validated batch 179 batch loss 2.74323034 batch mAP 0.138671875 batch PCKh 0.4609375\n",
      "Validated batch 180 batch loss 2.88217044 batch mAP 0.06640625 batch PCKh 0.201171875\n",
      "Validated batch 181 batch loss 2.96443868 batch mAP 0.099609375 batch PCKh 0.322265625\n",
      "Validated batch 182 batch loss 2.44069886 batch mAP 0.0703125 batch PCKh 0.6640625\n",
      "Validated batch 183 batch loss 2.8903966 batch mAP 0.025390625 batch PCKh 0.404296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 184 batch loss 2.83298874 batch mAP 0.083984375 batch PCKh 0.333984375\n",
      "Validated batch 185 batch loss 2.7016449 batch mAP 0.078125 batch PCKh 0.515625\n",
      "Validated batch 186 batch loss 2.82035279 batch mAP 0.033203125 batch PCKh 0.427734375\n",
      "Validated batch 187 batch loss 2.90333223 batch mAP 0.03125 batch PCKh 0.2578125\n",
      "Validated batch 188 batch loss 2.86035442 batch mAP 0.22265625 batch PCKh 0.4140625\n",
      "Validated batch 189 batch loss 2.89141703 batch mAP 0.345703125 batch PCKh 0.390625\n",
      "Validated batch 190 batch loss 2.72474146 batch mAP 0.119140625 batch PCKh 0.357421875\n",
      "Validated batch 191 batch loss 2.28008223 batch mAP 0.1015625 batch PCKh 0.1171875\n",
      "Validated batch 192 batch loss 2.66475439 batch mAP 0.060546875 batch PCKh 0.306640625\n",
      "Validated batch 193 batch loss 2.8908329 batch mAP 0.05859375 batch PCKh 0.650390625\n",
      "Validated batch 194 batch loss 2.4789319 batch mAP 0.05078125 batch PCKh 0.193359375\n",
      "Validated batch 195 batch loss 3.03882456 batch mAP 0.05078125 batch PCKh 0.5234375\n",
      "Validated batch 196 batch loss 2.93328 batch mAP 0.140625 batch PCKh 0.41796875\n",
      "Validated batch 197 batch loss 2.75765371 batch mAP 0.16796875 batch PCKh 0.4609375\n",
      "Validated batch 198 batch loss 2.69752765 batch mAP 0.083984375 batch PCKh 0.466796875\n",
      "Validated batch 199 batch loss 2.87624693 batch mAP 0.24609375 batch PCKh 0.435546875\n",
      "Validated batch 200 batch loss 2.51840425 batch mAP 0.037109375 batch PCKh 0.50390625\n",
      "Validated batch 201 batch loss 2.67168808 batch mAP 0.060546875 batch PCKh 0.60546875\n",
      "Validated batch 202 batch loss 2.84042215 batch mAP 0.158203125 batch PCKh 0.51171875\n",
      "Validated batch 203 batch loss 2.85585213 batch mAP 0.123046875 batch PCKh 0.431640625\n",
      "Validated batch 204 batch loss 2.77358365 batch mAP 0.048828125 batch PCKh 0.404296875\n",
      "Validated batch 205 batch loss 2.93311024 batch mAP 0.08984375 batch PCKh 0.478515625\n",
      "Validated batch 206 batch loss 2.60399485 batch mAP 0.025390625 batch PCKh 0.529296875\n",
      "Validated batch 207 batch loss 2.9101305 batch mAP 0.08203125 batch PCKh 0.3515625\n",
      "Validated batch 208 batch loss 2.73977304 batch mAP 0.119140625 batch PCKh 0.19140625\n",
      "Validated batch 209 batch loss 2.78842735 batch mAP 0.095703125 batch PCKh 0.341796875\n",
      "Validated batch 210 batch loss 2.93293118 batch mAP 0.15625 batch PCKh 0.345703125\n",
      "Validated batch 211 batch loss 2.7625196 batch mAP 0.05859375 batch PCKh 0.4453125\n",
      "Validated batch 212 batch loss 2.7976222 batch mAP 0.083984375 batch PCKh 0.498046875\n",
      "Validated batch 213 batch loss 2.96469975 batch mAP 0.041015625 batch PCKh 0.603515625\n",
      "Validated batch 214 batch loss 2.88068 batch mAP 0.080078125 batch PCKh 0.41796875\n",
      "Validated batch 215 batch loss 3.18410015 batch mAP 0.185546875 batch PCKh 0.328125\n",
      "Validated batch 216 batch loss 3.08099508 batch mAP 0.208984375 batch PCKh 0.400390625\n",
      "Validated batch 217 batch loss 2.7856853 batch mAP 0.044921875 batch PCKh 0.607421875\n",
      "Validated batch 218 batch loss 2.56064248 batch mAP 0.19140625 batch PCKh 0.384765625\n",
      "Validated batch 219 batch loss 3.21068215 batch mAP 0.14453125 batch PCKh 0.36328125\n",
      "Validated batch 220 batch loss 2.88681507 batch mAP 0.2109375 batch PCKh 0.283203125\n",
      "Validated batch 221 batch loss 2.86151028 batch mAP 0.056640625 batch PCKh 0.4140625\n",
      "Validated batch 222 batch loss 2.52879596 batch mAP 0.044921875 batch PCKh 0.56640625\n",
      "Validated batch 223 batch loss 2.64678121 batch mAP 0.306640625 batch PCKh 0.431640625\n",
      "Validated batch 224 batch loss 2.92577648 batch mAP 0.185546875 batch PCKh 0.556640625\n",
      "Validated batch 225 batch loss 2.97672772 batch mAP 0.39453125 batch PCKh 0.490234375\n",
      "Validated batch 226 batch loss 2.67456436 batch mAP 0.015625 batch PCKh 0.533203125\n",
      "Validated batch 227 batch loss 2.74597931 batch mAP 0.087890625 batch PCKh 0.5234375\n",
      "Validated batch 228 batch loss 2.93059182 batch mAP 0.173828125 batch PCKh 0.509765625\n",
      "Validated batch 229 batch loss 3.07098389 batch mAP 0.11328125 batch PCKh 0.4453125\n",
      "Validated batch 230 batch loss 2.92837524 batch mAP 0.12890625 batch PCKh 0.44921875\n",
      "Validated batch 231 batch loss 3.00682139 batch mAP 0.2421875 batch PCKh 0.546875\n",
      "Validated batch 232 batch loss 2.50405025 batch mAP 0.19921875 batch PCKh 0.197265625\n",
      "Validated batch 233 batch loss 2.76165438 batch mAP 0.11328125 batch PCKh 0.529296875\n",
      "Validated batch 234 batch loss 3.01172137 batch mAP 0.13671875 batch PCKh 0.431640625\n",
      "Validated batch 235 batch loss 3.08247828 batch mAP 0.1015625 batch PCKh 0.408203125\n",
      "Validated batch 236 batch loss 2.77294302 batch mAP 0.154296875 batch PCKh 0.53515625\n",
      "Validated batch 237 batch loss 2.92439556 batch mAP 0.134765625 batch PCKh 0.427734375\n",
      "Validated batch 238 batch loss 2.71193266 batch mAP 0.044921875 batch PCKh 0.423828125\n",
      "Validated batch 239 batch loss 2.85503364 batch mAP 0.03515625 batch PCKh 0.40625\n",
      "Validated batch 240 batch loss 2.98825026 batch mAP 0.0625 batch PCKh 0.376953125\n",
      "Validated batch 241 batch loss 3.18200278 batch mAP 0.095703125 batch PCKh 0.42578125\n",
      "Validated batch 242 batch loss 2.98623371 batch mAP 0.052734375 batch PCKh 0.509765625\n",
      "Validated batch 243 batch loss 2.73320341 batch mAP 0.025390625 batch PCKh 0.587890625\n",
      "Validated batch 244 batch loss 2.45626688 batch mAP 0.0703125 batch PCKh 0.703125\n",
      "Validated batch 245 batch loss 2.82849646 batch mAP 0.01953125 batch PCKh 0.5234375\n",
      "Validated batch 246 batch loss 2.92769957 batch mAP 0.0234375 batch PCKh 0.583984375\n",
      "Validated batch 247 batch loss 2.97532248 batch mAP 0.0625 batch PCKh 0.41015625\n",
      "Validated batch 248 batch loss 2.62021208 batch mAP 0.162109375 batch PCKh 0.150390625\n",
      "Validated batch 249 batch loss 2.78155446 batch mAP 0.17578125 batch PCKh 0.484375\n",
      "Validated batch 250 batch loss 2.94844 batch mAP 0.119140625 batch PCKh 0.2578125\n",
      "Validated batch 251 batch loss 2.93100119 batch mAP 0.02734375 batch PCKh 0.37109375\n",
      "Validated batch 252 batch loss 2.8693676 batch mAP 0.0859375 batch PCKh 0.427734375\n",
      "Validated batch 253 batch loss 2.7749896 batch mAP 0.025390625 batch PCKh 0.576171875\n",
      "Validated batch 254 batch loss 2.70697379 batch mAP 0.076171875 batch PCKh 0.56640625\n",
      "Validated batch 255 batch loss 2.14046741 batch mAP 0.03125 batch PCKh 0.146484375\n",
      "Validated batch 256 batch loss 2.54397631 batch mAP 0.021484375 batch PCKh 0.357421875\n",
      "Validated batch 257 batch loss 2.92020822 batch mAP 0.056640625 batch PCKh 0.55078125\n",
      "Validated batch 258 batch loss 2.87308717 batch mAP 0.111328125 batch PCKh 0.5390625\n",
      "Validated batch 259 batch loss 2.88168406 batch mAP 0.05859375 batch PCKh 0.572265625\n",
      "Validated batch 260 batch loss 2.67801785 batch mAP 0.056640625 batch PCKh 0.525390625\n",
      "Validated batch 261 batch loss 2.98830175 batch mAP 0.1875 batch PCKh 0.533203125\n",
      "Validated batch 262 batch loss 2.77621794 batch mAP 0.24609375 batch PCKh 0.3203125\n",
      "Validated batch 263 batch loss 2.87299299 batch mAP 0.126953125 batch PCKh 0.26953125\n",
      "Validated batch 264 batch loss 2.87022281 batch mAP 0.05078125 batch PCKh 0.662109375\n",
      "Validated batch 265 batch loss 2.78488183 batch mAP 0.0390625 batch PCKh 0.48046875\n",
      "Validated batch 266 batch loss 2.67670131 batch mAP 0.02734375 batch PCKh 0.427734375\n",
      "Validated batch 267 batch loss 3.02649069 batch mAP 0.171875 batch PCKh 0.416015625\n",
      "Validated batch 268 batch loss 2.7669096 batch mAP 0.056640625 batch PCKh 0.50390625\n",
      "Validated batch 269 batch loss 3.07987905 batch mAP 0.15625 batch PCKh 0.408203125\n",
      "Validated batch 270 batch loss 3.10695767 batch mAP 0.2109375 batch PCKh 0.494140625\n",
      "Validated batch 271 batch loss 2.91119695 batch mAP 0.072265625 batch PCKh 0.478515625\n",
      "Validated batch 272 batch loss 2.92592907 batch mAP 0.021484375 batch PCKh 0.623046875\n",
      "Validated batch 273 batch loss 2.96006179 batch mAP 0.015625 batch PCKh 0.51171875\n",
      "Validated batch 274 batch loss 2.90890193 batch mAP 0.1171875 batch PCKh 0.427734375\n",
      "Validated batch 275 batch loss 2.59604788 batch mAP 0.0625 batch PCKh 0.447265625\n",
      "Validated batch 276 batch loss 2.61014342 batch mAP 0.166015625 batch PCKh 0.3515625\n",
      "Validated batch 277 batch loss 2.85895586 batch mAP 0.08203125 batch PCKh 0.51171875\n",
      "Validated batch 278 batch loss 2.97983789 batch mAP 0.08203125 batch PCKh 0.498046875\n",
      "Validated batch 279 batch loss 2.85708714 batch mAP 0.048828125 batch PCKh 0.380859375\n",
      "Validated batch 280 batch loss 2.82893372 batch mAP 0.095703125 batch PCKh 0.513671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 281 batch loss 2.94192219 batch mAP 0.12109375 batch PCKh 0.40625\n",
      "Validated batch 282 batch loss 2.73824501 batch mAP 0.18359375 batch PCKh 0.5625\n",
      "Validated batch 283 batch loss 2.88189292 batch mAP 0.0234375 batch PCKh 0.55859375\n",
      "Validated batch 284 batch loss 2.72178292 batch mAP 0.015625 batch PCKh 0.5078125\n",
      "Validated batch 285 batch loss 2.77500343 batch mAP 0.046875 batch PCKh 0.6953125\n",
      "Validated batch 286 batch loss 2.63242292 batch mAP 0.072265625 batch PCKh 0.5546875\n",
      "Validated batch 287 batch loss 2.98245239 batch mAP 0.068359375 batch PCKh 0.458984375\n",
      "Validated batch 288 batch loss 2.86494207 batch mAP 0.07421875 batch PCKh 0.5390625\n",
      "Validated batch 289 batch loss 2.76299953 batch mAP 0.1015625 batch PCKh 0.548828125\n",
      "Validated batch 290 batch loss 2.8200736 batch mAP 0.08984375 batch PCKh 0.50390625\n",
      "Validated batch 291 batch loss 2.79563284 batch mAP 0.0390625 batch PCKh 0.30859375\n",
      "Validated batch 292 batch loss 2.51478672 batch mAP 0.140625 batch PCKh 0.46484375\n",
      "Validated batch 293 batch loss 2.65177512 batch mAP 0.048828125 batch PCKh 0.353515625\n",
      "Validated batch 294 batch loss 2.81814146 batch mAP 0.0078125 batch PCKh 0.515625\n",
      "Validated batch 295 batch loss 2.94736314 batch mAP 0.123046875 batch PCKh 0.494140625\n",
      "Validated batch 296 batch loss 2.75923705 batch mAP 0.06640625 batch PCKh 0.455078125\n",
      "Validated batch 297 batch loss 2.8423605 batch mAP 0.0703125 batch PCKh 0.453125\n",
      "Validated batch 298 batch loss 2.85662127 batch mAP 0.224609375 batch PCKh 0.4296875\n",
      "Validated batch 299 batch loss 2.98407221 batch mAP 0.20703125 batch PCKh 0.501953125\n",
      "Validated batch 300 batch loss 2.81123161 batch mAP 0.126953125 batch PCKh 0.390625\n",
      "Validated batch 301 batch loss 2.82698965 batch mAP 0.12109375 batch PCKh 0.283203125\n",
      "Validated batch 302 batch loss 2.95572853 batch mAP 0.134765625 batch PCKh 0.361328125\n",
      "Validated batch 303 batch loss 2.69787836 batch mAP 0.263671875 batch PCKh 0.244140625\n",
      "Validated batch 304 batch loss 2.70231748 batch mAP 0.212890625 batch PCKh 0.49609375\n",
      "Validated batch 305 batch loss 2.90824151 batch mAP 0.23046875 batch PCKh 0.25390625\n",
      "Validated batch 306 batch loss 2.62786388 batch mAP 0.09765625 batch PCKh 0.42578125\n",
      "Validated batch 307 batch loss 2.88366342 batch mAP 0.134765625 batch PCKh 0.392578125\n",
      "Validated batch 308 batch loss 2.90138578 batch mAP 0.16796875 batch PCKh 0.552734375\n",
      "Validated batch 309 batch loss 2.79011726 batch mAP 0.146484375 batch PCKh 0.388671875\n",
      "Validated batch 310 batch loss 2.93709612 batch mAP 0.166015625 batch PCKh 0.3984375\n",
      "Validated batch 311 batch loss 2.72869253 batch mAP 0.25390625 batch PCKh 0.376953125\n",
      "Validated batch 312 batch loss 2.98206 batch mAP 0.177734375 batch PCKh 0.447265625\n",
      "Validated batch 313 batch loss 2.8762784 batch mAP 0.044921875 batch PCKh 0.5859375\n",
      "Validated batch 314 batch loss 3.02386498 batch mAP 0.10546875 batch PCKh 0.46484375\n",
      "Validated batch 315 batch loss 3.2310133 batch mAP 0.2578125 batch PCKh 0.341796875\n",
      "Validated batch 316 batch loss 3.23066092 batch mAP 0.265625 batch PCKh 0.439453125\n",
      "Validated batch 317 batch loss 3.23164082 batch mAP 0.244140625 batch PCKh 0.314453125\n",
      "Validated batch 318 batch loss 3.01025391 batch mAP 0.056640625 batch PCKh 0.537109375\n",
      "Validated batch 319 batch loss 2.76357651 batch mAP 0.11328125 batch PCKh 0.451171875\n",
      "Validated batch 320 batch loss 2.93065119 batch mAP 0.15625 batch PCKh 0.455078125\n",
      "Validated batch 321 batch loss 2.6103332 batch mAP 0.2265625 batch PCKh 0.31640625\n",
      "Validated batch 322 batch loss 2.61392283 batch mAP 0.064453125 batch PCKh 0.4765625\n",
      "Validated batch 323 batch loss 2.61869 batch mAP 0.021484375 batch PCKh 0.55078125\n",
      "Validated batch 324 batch loss 2.87193441 batch mAP 0.044921875 batch PCKh 0.646484375\n",
      "Validated batch 325 batch loss 2.9566617 batch mAP 0.21484375 batch PCKh 0.44140625\n",
      "Validated batch 326 batch loss 2.7028439 batch mAP 0.109375 batch PCKh 0.560546875\n",
      "Validated batch 327 batch loss 3.00193691 batch mAP 0.205078125 batch PCKh 0.4140625\n",
      "Validated batch 328 batch loss 2.65428591 batch mAP 0.056640625 batch PCKh 0.44140625\n",
      "Validated batch 329 batch loss 2.70995116 batch mAP 0.099609375 batch PCKh 0.623046875\n",
      "Validated batch 330 batch loss 2.6948638 batch mAP 0.06640625 batch PCKh 0.375\n",
      "Validated batch 331 batch loss 2.64706564 batch mAP 0.1171875 batch PCKh 0.30078125\n",
      "Validated batch 332 batch loss 2.87440872 batch mAP 0.1484375 batch PCKh 0.38671875\n",
      "Validated batch 333 batch loss 3.06733179 batch mAP 0.3359375 batch PCKh 0.40234375\n",
      "Validated batch 334 batch loss 3.18785381 batch mAP 0.228515625 batch PCKh 0.439453125\n",
      "Validated batch 335 batch loss 2.62886047 batch mAP 0.171875 batch PCKh 0.3046875\n",
      "Validated batch 336 batch loss 2.73061132 batch mAP 0.103515625 batch PCKh 0.455078125\n",
      "Validated batch 337 batch loss 2.75391531 batch mAP 0.072265625 batch PCKh 0.580078125\n",
      "Validated batch 338 batch loss 3.00599241 batch mAP 0.1796875 batch PCKh 0.59765625\n",
      "Validated batch 339 batch loss 2.88761568 batch mAP 0.08203125 batch PCKh 0.513671875\n",
      "Validated batch 340 batch loss 2.73009109 batch mAP 0.033203125 batch PCKh 0.482421875\n",
      "Validated batch 341 batch loss 3.17073512 batch mAP 0.078125 batch PCKh 0.283203125\n",
      "Validated batch 342 batch loss 2.74816489 batch mAP 0.05859375 batch PCKh 0.408203125\n",
      "Validated batch 343 batch loss 2.79009581 batch mAP 0.08203125 batch PCKh 0.669921875\n",
      "Validated batch 344 batch loss 2.75280547 batch mAP 0.05078125 batch PCKh 0.6875\n",
      "Validated batch 345 batch loss 2.81499958 batch mAP 0.072265625 batch PCKh 0.533203125\n",
      "Validated batch 346 batch loss 2.59953451 batch mAP 0.052734375 batch PCKh 0.451171875\n",
      "Validated batch 347 batch loss 2.59997272 batch mAP 0.06640625 batch PCKh 0.373046875\n",
      "Validated batch 348 batch loss 2.95060587 batch mAP 0.13671875 batch PCKh 0.205078125\n",
      "Validated batch 349 batch loss 3.30760527 batch mAP 0.048828125 batch PCKh 0.4453125\n",
      "Validated batch 350 batch loss 2.52502179 batch mAP 0.046875 batch PCKh 0.509765625\n",
      "Validated batch 351 batch loss 2.73216629 batch mAP 0.39453125 batch PCKh 0.283203125\n",
      "Validated batch 352 batch loss 2.97880793 batch mAP 0.208984375 batch PCKh 0.5234375\n",
      "Validated batch 353 batch loss 2.73870468 batch mAP 0.13671875 batch PCKh 0.390625\n",
      "Validated batch 354 batch loss 2.95974731 batch mAP 0.01953125 batch PCKh 0.560546875\n",
      "Validated batch 355 batch loss 2.93835354 batch mAP 0.06640625 batch PCKh 0.541015625\n",
      "Validated batch 356 batch loss 2.84815431 batch mAP 0.2265625 batch PCKh 0.376953125\n",
      "Validated batch 357 batch loss 2.89444447 batch mAP 0.02734375 batch PCKh 0.55859375\n",
      "Validated batch 358 batch loss 2.5116303 batch mAP 0.12890625 batch PCKh 0.3359375\n",
      "Validated batch 359 batch loss 2.49126053 batch mAP 0.107421875 batch PCKh 0.474609375\n",
      "Validated batch 360 batch loss 2.72169638 batch mAP 0.033203125 batch PCKh 0.408203125\n",
      "Validated batch 361 batch loss 2.76029897 batch mAP 0.1484375 batch PCKh 0.34375\n",
      "Validated batch 362 batch loss 2.89570427 batch mAP 0.052734375 batch PCKh 0.509765625\n",
      "Validated batch 363 batch loss 2.63526177 batch mAP 0.03125 batch PCKh 0.509765625\n",
      "Validated batch 364 batch loss 2.81877947 batch mAP 0.046875 batch PCKh 0.6171875\n",
      "Validated batch 365 batch loss 2.77673292 batch mAP 0.076171875 batch PCKh 0.314453125\n",
      "Validated batch 366 batch loss 2.56131172 batch mAP 0.103515625 batch PCKh 0.1640625\n",
      "Validated batch 367 batch loss 3.119555 batch mAP 0.193359375 batch PCKh 0.521484375\n",
      "Validated batch 368 batch loss 2.70112038 batch mAP 0.1328125 batch PCKh 0.21875\n",
      "Validated batch 369 batch loss 3.0760591 batch mAP 0.212890625 batch PCKh 0.38671875\n",
      "Epoch 1 val loss 2.828590154647827 val mAP 0.12471947073936462 val PCKh 0.4356209635734558\n",
      "Epoch 1 completed in 1648.18 seconds\n",
      "Model /aiffel/aiffel/model_weight/GD08/i_model-epoch-1-loss-2.8286.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.75989389 batch mAP 0.09765625 batch PCKh 0.703125\n",
      "Trained batch 2 batch loss 3.02903032 batch mAP 0.083984375 batch PCKh 0.58203125\n",
      "Trained batch 3 batch loss 2.56734681 batch mAP 0.09375 batch PCKh 0.37109375\n",
      "Trained batch 4 batch loss 2.85109138 batch mAP 0.140625 batch PCKh 0.421875\n",
      "Trained batch 5 batch loss 2.72791719 batch mAP 0.10546875 batch PCKh 0.34765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 6 batch loss 2.90646148 batch mAP 0.05859375 batch PCKh 0.466796875\n",
      "Trained batch 7 batch loss 2.49458218 batch mAP 0.064453125 batch PCKh 0.6328125\n",
      "Trained batch 8 batch loss 2.82022476 batch mAP 0.103515625 batch PCKh 0.578125\n",
      "Trained batch 9 batch loss 2.62552547 batch mAP 0.03125 batch PCKh 0.564453125\n",
      "Trained batch 10 batch loss 2.76614523 batch mAP 0.048828125 batch PCKh 0.529296875\n",
      "Trained batch 11 batch loss 2.69235086 batch mAP 0.033203125 batch PCKh 0.6328125\n",
      "Trained batch 12 batch loss 3.01174021 batch mAP 0.080078125 batch PCKh 0.345703125\n",
      "Trained batch 13 batch loss 2.72119164 batch mAP 0.041015625 batch PCKh 0.421875\n",
      "Trained batch 14 batch loss 2.81539536 batch mAP 0.048828125 batch PCKh 0.451171875\n",
      "Trained batch 15 batch loss 2.65113974 batch mAP 0.048828125 batch PCKh 0.400390625\n",
      "Trained batch 16 batch loss 2.90593863 batch mAP 0.1171875 batch PCKh 0.736328125\n",
      "Trained batch 17 batch loss 2.93920279 batch mAP 0.05078125 batch PCKh 0.330078125\n",
      "Trained batch 18 batch loss 2.70757651 batch mAP 0.03125 batch PCKh 0.375\n",
      "Trained batch 19 batch loss 2.77657843 batch mAP 0.04296875 batch PCKh 0.61328125\n",
      "Trained batch 20 batch loss 2.83396912 batch mAP 0.048828125 batch PCKh 0.41796875\n",
      "Trained batch 21 batch loss 2.68846297 batch mAP 0.041015625 batch PCKh 0.2578125\n",
      "Trained batch 22 batch loss 3.04279208 batch mAP 0.048828125 batch PCKh 0.38671875\n",
      "Trained batch 23 batch loss 2.9485023 batch mAP 0.08203125 batch PCKh 0.470703125\n",
      "Trained batch 24 batch loss 2.77549648 batch mAP 0.083984375 batch PCKh 0.44921875\n",
      "Trained batch 25 batch loss 2.9283514 batch mAP 0.0703125 batch PCKh 0.513671875\n",
      "Trained batch 26 batch loss 2.93860245 batch mAP 0.0625 batch PCKh 0.423828125\n",
      "Trained batch 27 batch loss 2.92973518 batch mAP 0.064453125 batch PCKh 0.439453125\n",
      "Trained batch 28 batch loss 3.12207365 batch mAP 0.083984375 batch PCKh 0.40625\n",
      "Trained batch 29 batch loss 3.07548213 batch mAP 0.087890625 batch PCKh 0.4296875\n",
      "Trained batch 30 batch loss 2.82739687 batch mAP 0.09765625 batch PCKh 0.275390625\n",
      "Trained batch 31 batch loss 2.67036319 batch mAP 0.107421875 batch PCKh 0.33984375\n",
      "Trained batch 32 batch loss 2.72657275 batch mAP 0.078125 batch PCKh 0.4921875\n",
      "Trained batch 33 batch loss 2.66189027 batch mAP 0.123046875 batch PCKh 0.5390625\n",
      "Trained batch 34 batch loss 2.82204914 batch mAP 0.0625 batch PCKh 0.58203125\n",
      "Trained batch 35 batch loss 2.64803743 batch mAP 0.064453125 batch PCKh 0.525390625\n",
      "Trained batch 36 batch loss 2.7205143 batch mAP 0.078125 batch PCKh 0.56640625\n",
      "Trained batch 37 batch loss 2.73352909 batch mAP 0.0703125 batch PCKh 0.396484375\n",
      "Trained batch 38 batch loss 2.58346272 batch mAP 0.064453125 batch PCKh 0.51953125\n",
      "Trained batch 39 batch loss 2.4516964 batch mAP 0.087890625 batch PCKh 0.38671875\n",
      "Trained batch 40 batch loss 2.48361874 batch mAP 0.06640625 batch PCKh 0.36328125\n",
      "Trained batch 41 batch loss 2.95760202 batch mAP 0.05078125 batch PCKh 0.548828125\n",
      "Trained batch 42 batch loss 2.70013094 batch mAP 0.056640625 batch PCKh 0.400390625\n",
      "Trained batch 43 batch loss 2.71519041 batch mAP 0.04296875 batch PCKh 0.55859375\n",
      "Trained batch 44 batch loss 2.76864576 batch mAP 0.046875 batch PCKh 0.45703125\n",
      "Trained batch 45 batch loss 3.00941133 batch mAP 0.03515625 batch PCKh 0.466796875\n",
      "Trained batch 46 batch loss 3.03205657 batch mAP 0.03515625 batch PCKh 0.35546875\n",
      "Trained batch 47 batch loss 2.52410936 batch mAP 0.048828125 batch PCKh 0.423828125\n",
      "Trained batch 48 batch loss 2.792521 batch mAP 0.052734375 batch PCKh 0.60546875\n",
      "Trained batch 49 batch loss 2.69693351 batch mAP 0.03125 batch PCKh 0.423828125\n",
      "Trained batch 50 batch loss 2.58640718 batch mAP 0.037109375 batch PCKh 0.4296875\n",
      "Trained batch 51 batch loss 2.90305 batch mAP 0.03515625 batch PCKh 0.52734375\n",
      "Trained batch 52 batch loss 2.91021323 batch mAP 0.033203125 batch PCKh 0.505859375\n",
      "Trained batch 53 batch loss 2.94688416 batch mAP 0.037109375 batch PCKh 0.46875\n",
      "Trained batch 54 batch loss 2.62506866 batch mAP 0.1015625 batch PCKh 0.509765625\n",
      "Trained batch 55 batch loss 2.98672462 batch mAP 0.0546875 batch PCKh 0.4765625\n",
      "Trained batch 56 batch loss 2.51584148 batch mAP 0.060546875 batch PCKh 0.470703125\n",
      "Trained batch 57 batch loss 2.70255017 batch mAP 0.099609375 batch PCKh 0.6640625\n",
      "Trained batch 58 batch loss 2.66164327 batch mAP 0.0546875 batch PCKh 0.744140625\n",
      "Trained batch 59 batch loss 2.83638334 batch mAP 0.04296875 batch PCKh 0.533203125\n",
      "Trained batch 60 batch loss 2.64870787 batch mAP 0.041015625 batch PCKh 0.498046875\n",
      "Trained batch 61 batch loss 2.55331278 batch mAP 0.0234375 batch PCKh 0.525390625\n",
      "Trained batch 62 batch loss 2.68550873 batch mAP 0.0546875 batch PCKh 0.599609375\n",
      "Trained batch 63 batch loss 2.4724369 batch mAP 0.046875 batch PCKh 0.6015625\n",
      "Trained batch 64 batch loss 2.68572164 batch mAP 0.064453125 batch PCKh 0.59375\n",
      "Trained batch 65 batch loss 2.91972947 batch mAP 0.0390625 batch PCKh 0.51953125\n",
      "Trained batch 66 batch loss 2.91849709 batch mAP 0.025390625 batch PCKh 0.46484375\n",
      "Trained batch 67 batch loss 2.92106462 batch mAP 0.05859375 batch PCKh 0.640625\n",
      "Trained batch 68 batch loss 3.07290411 batch mAP 0.02734375 batch PCKh 0.50390625\n",
      "Trained batch 69 batch loss 2.89213133 batch mAP 0.015625 batch PCKh 0.486328125\n",
      "Trained batch 70 batch loss 2.67945623 batch mAP 0.033203125 batch PCKh 0.568359375\n",
      "Trained batch 71 batch loss 2.81286597 batch mAP 0.033203125 batch PCKh 0.59375\n",
      "Trained batch 72 batch loss 2.82162213 batch mAP 0.01953125 batch PCKh 0.572265625\n",
      "Trained batch 73 batch loss 2.92264915 batch mAP 0.046875 batch PCKh 0.529296875\n",
      "Trained batch 74 batch loss 2.88725924 batch mAP 0.068359375 batch PCKh 0.4140625\n",
      "Trained batch 75 batch loss 2.8568337 batch mAP 0.03515625 batch PCKh 0.4296875\n",
      "Trained batch 76 batch loss 2.58879185 batch mAP 0.05859375 batch PCKh 0.478515625\n",
      "Trained batch 77 batch loss 2.82812834 batch mAP 0.02734375 batch PCKh 0.396484375\n",
      "Trained batch 78 batch loss 2.75302601 batch mAP 0.052734375 batch PCKh 0.5703125\n",
      "Trained batch 79 batch loss 2.64518142 batch mAP 0.056640625 batch PCKh 0.43359375\n",
      "Trained batch 80 batch loss 2.63471317 batch mAP 0.037109375 batch PCKh 0.44921875\n",
      "Trained batch 81 batch loss 2.75635457 batch mAP 0.052734375 batch PCKh 0.4375\n",
      "Trained batch 82 batch loss 2.72676206 batch mAP 0.04296875 batch PCKh 0.48828125\n",
      "Trained batch 83 batch loss 2.84508085 batch mAP 0.091796875 batch PCKh 0.35546875\n",
      "Trained batch 84 batch loss 2.51091671 batch mAP 0.0390625 batch PCKh 0.56640625\n",
      "Trained batch 85 batch loss 2.51410866 batch mAP 0.119140625 batch PCKh 0.609375\n",
      "Trained batch 86 batch loss 2.90013599 batch mAP 0.048828125 batch PCKh 0.70703125\n",
      "Trained batch 87 batch loss 2.8437562 batch mAP 0.0546875 batch PCKh 0.546875\n",
      "Trained batch 88 batch loss 3.0083952 batch mAP 0.06640625 batch PCKh 0.45703125\n",
      "Trained batch 89 batch loss 2.63304758 batch mAP 0.07421875 batch PCKh 0.37890625\n",
      "Trained batch 90 batch loss 2.93006253 batch mAP 0.087890625 batch PCKh 0.44921875\n",
      "Trained batch 91 batch loss 2.81321645 batch mAP 0.076171875 batch PCKh 0.337890625\n",
      "Trained batch 92 batch loss 3.00905013 batch mAP 0.08203125 batch PCKh 0.2890625\n",
      "Trained batch 93 batch loss 2.51500654 batch mAP 0.05859375 batch PCKh 0.5\n",
      "Trained batch 94 batch loss 2.6599865 batch mAP 0.064453125 batch PCKh 0.49609375\n",
      "Trained batch 95 batch loss 2.62656188 batch mAP 0.072265625 batch PCKh 0.396484375\n",
      "Trained batch 96 batch loss 2.56327057 batch mAP 0.056640625 batch PCKh 0.541015625\n",
      "Trained batch 97 batch loss 2.62269878 batch mAP 0.064453125 batch PCKh 0.513671875\n",
      "Trained batch 98 batch loss 2.63716555 batch mAP 0.04296875 batch PCKh 0.580078125\n",
      "Trained batch 99 batch loss 2.7470305 batch mAP 0.083984375 batch PCKh 0.482421875\n",
      "Trained batch 100 batch loss 2.71501517 batch mAP 0.068359375 batch PCKh 0.580078125\n",
      "Trained batch 101 batch loss 2.3788197 batch mAP 0.046875 batch PCKh 0.517578125\n",
      "Trained batch 102 batch loss 2.38296056 batch mAP 0.044921875 batch PCKh 0.595703125\n",
      "Trained batch 103 batch loss 2.73129559 batch mAP 0.029296875 batch PCKh 0.68359375\n",
      "Trained batch 104 batch loss 2.56240749 batch mAP 0.033203125 batch PCKh 0.4609375\n",
      "Trained batch 105 batch loss 2.63424969 batch mAP 0.0390625 batch PCKh 0.568359375\n",
      "Trained batch 106 batch loss 2.38531137 batch mAP 0.060546875 batch PCKh 0.515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 107 batch loss 2.810987 batch mAP 0.03515625 batch PCKh 0.50390625\n",
      "Trained batch 108 batch loss 2.80662107 batch mAP 0.0234375 batch PCKh 0.439453125\n",
      "Trained batch 109 batch loss 2.61450148 batch mAP 0.013671875 batch PCKh 0.6015625\n",
      "Trained batch 110 batch loss 2.55365443 batch mAP 0.025390625 batch PCKh 0.416015625\n",
      "Trained batch 111 batch loss 2.54190779 batch mAP 0.052734375 batch PCKh 0.416015625\n",
      "Trained batch 112 batch loss 2.66823 batch mAP 0.0390625 batch PCKh 0.2578125\n",
      "Trained batch 113 batch loss 2.91949987 batch mAP 0.04296875 batch PCKh 0.4609375\n",
      "Trained batch 114 batch loss 2.82283711 batch mAP 0.03515625 batch PCKh 0.51171875\n",
      "Trained batch 115 batch loss 2.81960487 batch mAP 0.041015625 batch PCKh 0.60546875\n",
      "Trained batch 116 batch loss 3.12194729 batch mAP 0.04296875 batch PCKh 0.31640625\n",
      "Trained batch 117 batch loss 2.90319085 batch mAP 0.0625 batch PCKh 0.466796875\n",
      "Trained batch 118 batch loss 2.88089561 batch mAP 0.046875 batch PCKh 0.453125\n",
      "Trained batch 119 batch loss 3.06684351 batch mAP 0.056640625 batch PCKh 0.453125\n",
      "Trained batch 120 batch loss 3.18092012 batch mAP 0.05859375 batch PCKh 0.328125\n",
      "Trained batch 121 batch loss 2.87690401 batch mAP 0.06640625 batch PCKh 0.439453125\n",
      "Trained batch 122 batch loss 2.59753799 batch mAP 0.041015625 batch PCKh 0.384765625\n",
      "Trained batch 123 batch loss 2.85050797 batch mAP 0.087890625 batch PCKh 0.353515625\n",
      "Trained batch 124 batch loss 2.9857614 batch mAP 0.078125 batch PCKh 0.5078125\n",
      "Trained batch 125 batch loss 2.79676938 batch mAP 0.080078125 batch PCKh 0.162109375\n",
      "Trained batch 126 batch loss 3.00111222 batch mAP 0.041015625 batch PCKh 0.5859375\n",
      "Trained batch 127 batch loss 2.8693428 batch mAP 0.0546875 batch PCKh 0.560546875\n",
      "Trained batch 128 batch loss 2.74796677 batch mAP 0.06640625 batch PCKh 0.58203125\n",
      "Trained batch 129 batch loss 2.79996061 batch mAP 0.03125 batch PCKh 0.5703125\n",
      "Trained batch 130 batch loss 2.56849098 batch mAP 0.037109375 batch PCKh 0.54296875\n",
      "Trained batch 131 batch loss 3.08283806 batch mAP 0.0625 batch PCKh 0.31640625\n",
      "Trained batch 132 batch loss 2.95513201 batch mAP 0.05859375 batch PCKh 0.54296875\n",
      "Trained batch 133 batch loss 2.99802208 batch mAP 0.046875 batch PCKh 0.564453125\n",
      "Trained batch 134 batch loss 2.57381082 batch mAP 0.056640625 batch PCKh 0.279296875\n",
      "Trained batch 135 batch loss 2.65114641 batch mAP 0.09765625 batch PCKh 0.23046875\n",
      "Trained batch 136 batch loss 2.3384707 batch mAP 0.060546875 batch PCKh 0.15234375\n",
      "Trained batch 137 batch loss 2.33721256 batch mAP 0.083984375 batch PCKh 0.021484375\n",
      "Trained batch 138 batch loss 2.33599067 batch mAP 0.0625 batch PCKh 0.21875\n",
      "Trained batch 139 batch loss 2.35354137 batch mAP 0.052734375 batch PCKh 0.23046875\n",
      "Trained batch 140 batch loss 2.84554243 batch mAP 0.0703125 batch PCKh 0.25390625\n",
      "Trained batch 141 batch loss 2.86346626 batch mAP 0.107421875 batch PCKh 0.35546875\n",
      "Trained batch 142 batch loss 2.97558928 batch mAP 0.134765625 batch PCKh 0.48828125\n",
      "Trained batch 143 batch loss 2.89881611 batch mAP 0.0703125 batch PCKh 0.310546875\n",
      "Trained batch 144 batch loss 2.84034586 batch mAP 0.078125 batch PCKh 0.3359375\n",
      "Trained batch 145 batch loss 2.69264078 batch mAP 0.09375 batch PCKh 0.57421875\n",
      "Trained batch 146 batch loss 2.75484514 batch mAP 0.099609375 batch PCKh 0.46875\n",
      "Trained batch 147 batch loss 2.80901122 batch mAP 0.0625 batch PCKh 0.482421875\n",
      "Trained batch 148 batch loss 2.70919824 batch mAP 0.07421875 batch PCKh 0.484375\n",
      "Trained batch 149 batch loss 2.73135781 batch mAP 0.064453125 batch PCKh 0.5625\n",
      "Trained batch 150 batch loss 2.8578887 batch mAP 0.0546875 batch PCKh 0.447265625\n",
      "Trained batch 151 batch loss 2.77134848 batch mAP 0.044921875 batch PCKh 0.537109375\n",
      "Trained batch 152 batch loss 2.76923585 batch mAP 0.060546875 batch PCKh 0.498046875\n",
      "Trained batch 153 batch loss 2.34850359 batch mAP 0.078125 batch PCKh 0.365234375\n",
      "Trained batch 154 batch loss 3.08899379 batch mAP 0.056640625 batch PCKh 0.33203125\n",
      "Trained batch 155 batch loss 2.61665702 batch mAP 0.03515625 batch PCKh 0.22265625\n",
      "Trained batch 156 batch loss 2.5142653 batch mAP 0.046875 batch PCKh 0.20703125\n",
      "Trained batch 157 batch loss 2.73037672 batch mAP 0.060546875 batch PCKh 0.435546875\n",
      "Trained batch 158 batch loss 2.60360384 batch mAP 0.08203125 batch PCKh 0.482421875\n",
      "Trained batch 159 batch loss 2.68621445 batch mAP 0.048828125 batch PCKh 0.576171875\n",
      "Trained batch 160 batch loss 2.66500592 batch mAP 0.08984375 batch PCKh 0.5703125\n",
      "Trained batch 161 batch loss 2.68967605 batch mAP 0.0546875 batch PCKh 0.708984375\n",
      "Trained batch 162 batch loss 2.6746881 batch mAP 0.03515625 batch PCKh 0.60546875\n",
      "Trained batch 163 batch loss 2.65961313 batch mAP 0.052734375 batch PCKh 0.54296875\n",
      "Trained batch 164 batch loss 2.61315346 batch mAP 0.041015625 batch PCKh 0.5390625\n",
      "Trained batch 165 batch loss 2.75158787 batch mAP 0.046875 batch PCKh 0.60546875\n",
      "Trained batch 166 batch loss 2.67803931 batch mAP 0.0390625 batch PCKh 0.556640625\n",
      "Trained batch 167 batch loss 2.70496154 batch mAP 0.013671875 batch PCKh 0.5703125\n",
      "Trained batch 168 batch loss 2.64588714 batch mAP 0.01953125 batch PCKh 0.54296875\n",
      "Trained batch 169 batch loss 2.7834022 batch mAP 0.021484375 batch PCKh 0.5390625\n",
      "Trained batch 170 batch loss 2.85883951 batch mAP 0.01953125 batch PCKh 0.46484375\n",
      "Trained batch 171 batch loss 2.70461702 batch mAP 0.013671875 batch PCKh 0.529296875\n",
      "Trained batch 172 batch loss 2.71337199 batch mAP 0.021484375 batch PCKh 0.537109375\n",
      "Trained batch 173 batch loss 2.79291773 batch mAP 0.01953125 batch PCKh 0.611328125\n",
      "Trained batch 174 batch loss 2.67136693 batch mAP 0.01953125 batch PCKh 0.44921875\n",
      "Trained batch 175 batch loss 2.51251554 batch mAP 0.009765625 batch PCKh 0.6015625\n",
      "Trained batch 176 batch loss 2.60215735 batch mAP 0.01171875 batch PCKh 0.478515625\n",
      "Trained batch 177 batch loss 2.60848141 batch mAP 0.005859375 batch PCKh 0.630859375\n",
      "Trained batch 178 batch loss 2.49088812 batch mAP 0.0078125 batch PCKh 0.568359375\n",
      "Trained batch 179 batch loss 2.66615868 batch mAP 0.00390625 batch PCKh 0.62890625\n",
      "Trained batch 180 batch loss 2.71902466 batch mAP 0.01171875 batch PCKh 0.61328125\n",
      "Trained batch 181 batch loss 2.61380434 batch mAP 0.0078125 batch PCKh 0.662109375\n",
      "Trained batch 182 batch loss 2.61237526 batch mAP 0.009765625 batch PCKh 0.560546875\n",
      "Trained batch 183 batch loss 2.80577731 batch mAP 0.01171875 batch PCKh 0.501953125\n",
      "Trained batch 184 batch loss 2.59291887 batch mAP 0.04296875 batch PCKh 0.572265625\n",
      "Trained batch 185 batch loss 2.66180325 batch mAP 0.03125 batch PCKh 0.66015625\n",
      "Trained batch 186 batch loss 2.98675442 batch mAP 0.033203125 batch PCKh 0.4453125\n",
      "Trained batch 187 batch loss 2.75388432 batch mAP 0.04296875 batch PCKh 0.45703125\n",
      "Trained batch 188 batch loss 3.08303428 batch mAP 0.052734375 batch PCKh 0.384765625\n",
      "Trained batch 189 batch loss 2.80146217 batch mAP 0.02734375 batch PCKh 0.603515625\n",
      "Trained batch 190 batch loss 2.84800243 batch mAP 0.0390625 batch PCKh 0.583984375\n",
      "Trained batch 191 batch loss 2.9892838 batch mAP 0.060546875 batch PCKh 0.494140625\n",
      "Trained batch 192 batch loss 2.75986 batch mAP 0.0390625 batch PCKh 0.54296875\n",
      "Trained batch 193 batch loss 2.71754122 batch mAP 0.052734375 batch PCKh 0.62890625\n",
      "Trained batch 194 batch loss 2.7475903 batch mAP 0.029296875 batch PCKh 0.533203125\n",
      "Trained batch 195 batch loss 2.73111486 batch mAP 0.03125 batch PCKh 0.60546875\n",
      "Trained batch 196 batch loss 2.95422 batch mAP 0.080078125 batch PCKh 0.5234375\n",
      "Trained batch 197 batch loss 2.40319204 batch mAP 0.0703125 batch PCKh 0.421875\n",
      "Trained batch 198 batch loss 2.45471978 batch mAP 0.044921875 batch PCKh 0.318359375\n",
      "Trained batch 199 batch loss 2.60566092 batch mAP 0.04296875 batch PCKh 0.529296875\n",
      "Trained batch 200 batch loss 2.49105978 batch mAP 0.04296875 batch PCKh 0.23828125\n",
      "Trained batch 201 batch loss 2.63698435 batch mAP 0.0390625 batch PCKh 0.23828125\n",
      "Trained batch 202 batch loss 2.47189283 batch mAP 0.033203125 batch PCKh 0.2109375\n",
      "Trained batch 203 batch loss 2.71227551 batch mAP 0.052734375 batch PCKh 0.30859375\n",
      "Trained batch 204 batch loss 3.00880098 batch mAP 0.04296875 batch PCKh 0.4375\n",
      "Trained batch 205 batch loss 3.06575489 batch mAP 0.048828125 batch PCKh 0.517578125\n",
      "Trained batch 206 batch loss 2.9348917 batch mAP 0.0625 batch PCKh 0.533203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 207 batch loss 2.82580423 batch mAP 0.072265625 batch PCKh 0.47265625\n",
      "Trained batch 208 batch loss 2.69576502 batch mAP 0.05078125 batch PCKh 0.40625\n",
      "Trained batch 209 batch loss 2.62998223 batch mAP 0.07421875 batch PCKh 0.22265625\n",
      "Trained batch 210 batch loss 2.70022249 batch mAP 0.095703125 batch PCKh 0.470703125\n",
      "Trained batch 211 batch loss 2.68812776 batch mAP 0.103515625 batch PCKh 0.228515625\n",
      "Trained batch 212 batch loss 2.65906143 batch mAP 0.130859375 batch PCKh 0.314453125\n",
      "Trained batch 213 batch loss 2.61193013 batch mAP 0.08203125 batch PCKh 0.408203125\n",
      "Trained batch 214 batch loss 3.04752207 batch mAP 0.087890625 batch PCKh 0.431640625\n",
      "Trained batch 215 batch loss 2.71690083 batch mAP 0.158203125 batch PCKh 0.45703125\n",
      "Trained batch 216 batch loss 2.21034503 batch mAP 0.142578125 batch PCKh 0.265625\n",
      "Trained batch 217 batch loss 2.37942052 batch mAP 0.107421875 batch PCKh 0.48046875\n",
      "Trained batch 218 batch loss 2.77699614 batch mAP 0.1484375 batch PCKh 0.5703125\n",
      "Trained batch 219 batch loss 2.71064329 batch mAP 0.115234375 batch PCKh 0.513671875\n",
      "Trained batch 220 batch loss 2.71452379 batch mAP 0.10546875 batch PCKh 0.552734375\n",
      "Trained batch 221 batch loss 2.76770449 batch mAP 0.099609375 batch PCKh 0.546875\n",
      "Trained batch 222 batch loss 2.98600793 batch mAP 0.107421875 batch PCKh 0.41015625\n",
      "Trained batch 223 batch loss 2.92656 batch mAP 0.107421875 batch PCKh 0.564453125\n",
      "Trained batch 224 batch loss 2.81474066 batch mAP 0.107421875 batch PCKh 0.541015625\n",
      "Trained batch 225 batch loss 2.57859278 batch mAP 0.044921875 batch PCKh 0.515625\n",
      "Trained batch 226 batch loss 2.62235212 batch mAP 0.052734375 batch PCKh 0.576171875\n",
      "Trained batch 227 batch loss 2.79368901 batch mAP 0.07421875 batch PCKh 0.5625\n",
      "Trained batch 228 batch loss 2.79936647 batch mAP 0.0390625 batch PCKh 0.501953125\n",
      "Trained batch 229 batch loss 2.54602075 batch mAP 0.04296875 batch PCKh 0.41015625\n",
      "Trained batch 230 batch loss 2.97417927 batch mAP 0.01953125 batch PCKh 0.525390625\n",
      "Trained batch 231 batch loss 2.8405273 batch mAP 0.01171875 batch PCKh 0.669921875\n",
      "Trained batch 232 batch loss 3.17383289 batch mAP 0.005859375 batch PCKh 0.513671875\n",
      "Trained batch 233 batch loss 2.58224797 batch mAP 0.013671875 batch PCKh 0.490234375\n",
      "Trained batch 234 batch loss 2.75187683 batch mAP 0.01953125 batch PCKh 0.529296875\n",
      "Trained batch 235 batch loss 3.11184239 batch mAP 0.0390625 batch PCKh 0.46484375\n",
      "Trained batch 236 batch loss 2.51761532 batch mAP 0.015625 batch PCKh 0.546875\n",
      "Trained batch 237 batch loss 3.05244923 batch mAP 0.013671875 batch PCKh 0.4609375\n",
      "Trained batch 238 batch loss 2.94448 batch mAP 0.013671875 batch PCKh 0.568359375\n",
      "Trained batch 239 batch loss 3.51786351 batch mAP 0.021484375 batch PCKh 0.3515625\n",
      "Trained batch 240 batch loss 2.63147736 batch mAP 0.021484375 batch PCKh 0.49609375\n",
      "Trained batch 241 batch loss 2.4963963 batch mAP 0.05078125 batch PCKh 0.380859375\n",
      "Trained batch 242 batch loss 2.75141668 batch mAP 0.05859375 batch PCKh 0.431640625\n",
      "Trained batch 243 batch loss 2.87381 batch mAP 0.056640625 batch PCKh 0.384765625\n",
      "Trained batch 244 batch loss 2.93696523 batch mAP 0.10546875 batch PCKh 0.5078125\n",
      "Trained batch 245 batch loss 2.72038579 batch mAP 0.033203125 batch PCKh 0.509765625\n",
      "Trained batch 246 batch loss 2.68725967 batch mAP 0.056640625 batch PCKh 0.48046875\n",
      "Trained batch 247 batch loss 3.01193309 batch mAP 0.037109375 batch PCKh 0.333984375\n",
      "Trained batch 248 batch loss 2.90188813 batch mAP 0.0859375 batch PCKh 0.55859375\n",
      "Trained batch 249 batch loss 2.97576571 batch mAP 0.068359375 batch PCKh 0.30859375\n",
      "Trained batch 250 batch loss 3.24604321 batch mAP 0.0703125 batch PCKh 0.326171875\n",
      "Trained batch 251 batch loss 3.27104092 batch mAP 0.080078125 batch PCKh 0.49609375\n",
      "Trained batch 252 batch loss 2.58757782 batch mAP 0.099609375 batch PCKh 0.236328125\n",
      "Trained batch 253 batch loss 2.52664399 batch mAP 0.083984375 batch PCKh 0.216796875\n",
      "Trained batch 254 batch loss 2.99479914 batch mAP 0.0859375 batch PCKh 0.404296875\n",
      "Trained batch 255 batch loss 2.85932207 batch mAP 0.119140625 batch PCKh 0.279296875\n",
      "Trained batch 256 batch loss 2.84338427 batch mAP 0.14453125 batch PCKh 0.548828125\n",
      "Trained batch 257 batch loss 2.56244206 batch mAP 0.119140625 batch PCKh 0.525390625\n",
      "Trained batch 258 batch loss 2.74375224 batch mAP 0.08203125 batch PCKh 0.595703125\n",
      "Trained batch 259 batch loss 2.82184029 batch mAP 0.07421875 batch PCKh 0.5546875\n",
      "Trained batch 260 batch loss 2.85253048 batch mAP 0.12109375 batch PCKh 0.482421875\n",
      "Trained batch 261 batch loss 2.8462615 batch mAP 0.1171875 batch PCKh 0.564453125\n",
      "Trained batch 262 batch loss 2.85601759 batch mAP 0.126953125 batch PCKh 0.361328125\n",
      "Trained batch 263 batch loss 2.73510885 batch mAP 0.05859375 batch PCKh 0.302734375\n",
      "Trained batch 264 batch loss 2.74533367 batch mAP 0.126953125 batch PCKh 0.310546875\n",
      "Trained batch 265 batch loss 2.52295923 batch mAP 0.0546875 batch PCKh 0.33203125\n",
      "Trained batch 266 batch loss 3.16798019 batch mAP 0.07421875 batch PCKh 0.513671875\n",
      "Trained batch 267 batch loss 2.69583464 batch mAP 0.083984375 batch PCKh 0.359375\n",
      "Trained batch 268 batch loss 2.84381342 batch mAP 0.14453125 batch PCKh 0.509765625\n",
      "Trained batch 269 batch loss 2.53168273 batch mAP 0.068359375 batch PCKh 0.62890625\n",
      "Trained batch 270 batch loss 2.68050957 batch mAP 0.0703125 batch PCKh 0.419921875\n",
      "Trained batch 271 batch loss 2.38112307 batch mAP 0.068359375 batch PCKh 0.400390625\n",
      "Trained batch 272 batch loss 2.83332586 batch mAP 0.0859375 batch PCKh 0.541015625\n",
      "Trained batch 273 batch loss 2.65156698 batch mAP 0.056640625 batch PCKh 0.7109375\n",
      "Trained batch 274 batch loss 2.74864602 batch mAP 0.0859375 batch PCKh 0.55078125\n",
      "Trained batch 275 batch loss 2.79814911 batch mAP 0.0859375 batch PCKh 0.240234375\n",
      "Trained batch 276 batch loss 3.00923061 batch mAP 0.08984375 batch PCKh 0.49609375\n",
      "Trained batch 277 batch loss 2.85165763 batch mAP 0.048828125 batch PCKh 0.5078125\n",
      "Trained batch 278 batch loss 2.5486629 batch mAP 0.056640625 batch PCKh 0.4453125\n",
      "Trained batch 279 batch loss 2.82374287 batch mAP 0.064453125 batch PCKh 0.318359375\n",
      "Trained batch 280 batch loss 2.3241241 batch mAP 0.048828125 batch PCKh 0.294921875\n",
      "Trained batch 281 batch loss 2.15083551 batch mAP 0.0390625 batch PCKh 0.244140625\n",
      "Trained batch 282 batch loss 2.2054317 batch mAP 0.078125 batch PCKh 0.12890625\n",
      "Trained batch 283 batch loss 2.49807453 batch mAP 0.0234375 batch PCKh 0.08984375\n",
      "Trained batch 284 batch loss 2.36562872 batch mAP 0.0625 batch PCKh 0.16796875\n",
      "Trained batch 285 batch loss 2.66800261 batch mAP 0.03125 batch PCKh 0.400390625\n",
      "Trained batch 286 batch loss 2.79522419 batch mAP 0.037109375 batch PCKh 0.4296875\n",
      "Trained batch 287 batch loss 2.66595626 batch mAP 0.044921875 batch PCKh 0.455078125\n",
      "Trained batch 288 batch loss 2.49305868 batch mAP 0.0546875 batch PCKh 0.3515625\n",
      "Trained batch 289 batch loss 2.55610275 batch mAP 0.041015625 batch PCKh 0.515625\n",
      "Trained batch 290 batch loss 2.59999681 batch mAP 0.041015625 batch PCKh 0.64453125\n",
      "Trained batch 291 batch loss 2.65774775 batch mAP 0.01953125 batch PCKh 0.517578125\n",
      "Trained batch 292 batch loss 2.95065451 batch mAP 0.03125 batch PCKh 0.537109375\n",
      "Trained batch 293 batch loss 2.89534307 batch mAP 0.060546875 batch PCKh 0.49609375\n",
      "Trained batch 294 batch loss 2.62901497 batch mAP 0.03125 batch PCKh 0.693359375\n",
      "Trained batch 295 batch loss 2.9381485 batch mAP 0.033203125 batch PCKh 0.4296875\n",
      "Trained batch 296 batch loss 2.93799067 batch mAP 0.04296875 batch PCKh 0.37109375\n",
      "Trained batch 297 batch loss 2.84639645 batch mAP 0.056640625 batch PCKh 0.359375\n",
      "Trained batch 298 batch loss 2.54279232 batch mAP 0.03515625 batch PCKh 0.591796875\n",
      "Trained batch 299 batch loss 2.97274256 batch mAP 0.08203125 batch PCKh 0.5546875\n",
      "Trained batch 300 batch loss 3.13374805 batch mAP 0.0546875 batch PCKh 0.3671875\n",
      "Trained batch 301 batch loss 3.54269838 batch mAP 0.0625 batch PCKh 0.205078125\n",
      "Trained batch 302 batch loss 2.98793983 batch mAP 0.056640625 batch PCKh 0.44921875\n",
      "Trained batch 303 batch loss 3.50595951 batch mAP 0.0703125 batch PCKh 0.357421875\n",
      "Trained batch 304 batch loss 2.84699225 batch mAP 0.099609375 batch PCKh 0.56640625\n",
      "Trained batch 305 batch loss 2.79214501 batch mAP 0.091796875 batch PCKh 0.46875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 306 batch loss 2.85645056 batch mAP 0.123046875 batch PCKh 0.34765625\n",
      "Trained batch 307 batch loss 2.8195312 batch mAP 0.134765625 batch PCKh 0.466796875\n",
      "Trained batch 308 batch loss 2.67284155 batch mAP 0.16796875 batch PCKh 0.3515625\n",
      "Trained batch 309 batch loss 2.53987026 batch mAP 0.12109375 batch PCKh 0.4765625\n",
      "Trained batch 310 batch loss 2.77731609 batch mAP 0.1640625 batch PCKh 0.5859375\n",
      "Trained batch 311 batch loss 2.80878973 batch mAP 0.09375 batch PCKh 0.421875\n",
      "Trained batch 312 batch loss 2.44322371 batch mAP 0.1171875 batch PCKh 0.330078125\n",
      "Trained batch 313 batch loss 2.66031551 batch mAP 0.125 batch PCKh 0.212890625\n",
      "Trained batch 314 batch loss 3.31634259 batch mAP 0.1015625 batch PCKh 0.306640625\n",
      "Trained batch 315 batch loss 2.83038187 batch mAP 0.0546875 batch PCKh 0.39453125\n",
      "Trained batch 316 batch loss 2.94777536 batch mAP 0.130859375 batch PCKh 0.396484375\n",
      "Trained batch 317 batch loss 3.38115501 batch mAP 0.107421875 batch PCKh 0.265625\n",
      "Trained batch 318 batch loss 3.06947422 batch mAP 0.072265625 batch PCKh 0.265625\n",
      "Trained batch 319 batch loss 2.88668561 batch mAP 0.09375 batch PCKh 0.3671875\n",
      "Trained batch 320 batch loss 3.25561571 batch mAP 0.107421875 batch PCKh 0.265625\n",
      "Trained batch 321 batch loss 3.04954505 batch mAP 0.154296875 batch PCKh 0.470703125\n",
      "Trained batch 322 batch loss 2.76808214 batch mAP 0.162109375 batch PCKh 0.443359375\n",
      "Trained batch 323 batch loss 2.86471391 batch mAP 0.1171875 batch PCKh 0.52734375\n",
      "Trained batch 324 batch loss 2.91938639 batch mAP 0.1328125 batch PCKh 0.54296875\n",
      "Trained batch 325 batch loss 3.08513975 batch mAP 0.140625 batch PCKh 0.5625\n",
      "Trained batch 326 batch loss 3.03773141 batch mAP 0.150390625 batch PCKh 0.455078125\n",
      "Trained batch 327 batch loss 2.89070487 batch mAP 0.126953125 batch PCKh 0.390625\n",
      "Trained batch 328 batch loss 2.89896488 batch mAP 0.15625 batch PCKh 0.42578125\n",
      "Trained batch 329 batch loss 2.74351168 batch mAP 0.11328125 batch PCKh 0.751953125\n",
      "Trained batch 330 batch loss 2.73558402 batch mAP 0.169921875 batch PCKh 0.267578125\n",
      "Trained batch 331 batch loss 2.93145823 batch mAP 0.1171875 batch PCKh 0.474609375\n",
      "Trained batch 332 batch loss 2.74985933 batch mAP 0.205078125 batch PCKh 0.486328125\n",
      "Trained batch 333 batch loss 2.75419426 batch mAP 0.130859375 batch PCKh 0.58984375\n",
      "Trained batch 334 batch loss 2.66325212 batch mAP 0.1484375 batch PCKh 0.51953125\n",
      "Trained batch 335 batch loss 2.95690274 batch mAP 0.19140625 batch PCKh 0.55859375\n",
      "Trained batch 336 batch loss 3.0545392 batch mAP 0.2265625 batch PCKh 0.390625\n",
      "Trained batch 337 batch loss 2.94485378 batch mAP 0.18359375 batch PCKh 0.52734375\n",
      "Trained batch 338 batch loss 2.93527961 batch mAP 0.181640625 batch PCKh 0.416015625\n",
      "Trained batch 339 batch loss 2.78945351 batch mAP 0.18359375 batch PCKh 0.609375\n",
      "Trained batch 340 batch loss 2.8411355 batch mAP 0.123046875 batch PCKh 0.568359375\n",
      "Trained batch 341 batch loss 2.70851421 batch mAP 0.169921875 batch PCKh 0.423828125\n",
      "Trained batch 342 batch loss 2.68022323 batch mAP 0.078125 batch PCKh 0.267578125\n",
      "Trained batch 343 batch loss 2.59522 batch mAP 0.12109375 batch PCKh 0.66015625\n",
      "Trained batch 344 batch loss 2.67716122 batch mAP 0.095703125 batch PCKh 0.576171875\n",
      "Trained batch 345 batch loss 2.71950269 batch mAP 0.099609375 batch PCKh 0.607421875\n",
      "Trained batch 346 batch loss 2.61892366 batch mAP 0.1171875 batch PCKh 0.673828125\n",
      "Trained batch 347 batch loss 2.582479 batch mAP 0.107421875 batch PCKh 0.619140625\n",
      "Trained batch 348 batch loss 2.81673384 batch mAP 0.0703125 batch PCKh 0.6171875\n",
      "Trained batch 349 batch loss 2.78092718 batch mAP 0.056640625 batch PCKh 0.70703125\n",
      "Trained batch 350 batch loss 2.70111012 batch mAP 0.064453125 batch PCKh 0.54296875\n",
      "Trained batch 351 batch loss 2.68945408 batch mAP 0.11328125 batch PCKh 0.349609375\n",
      "Trained batch 352 batch loss 2.88643503 batch mAP 0.06640625 batch PCKh 0.4765625\n",
      "Trained batch 353 batch loss 2.84147978 batch mAP 0.044921875 batch PCKh 0.6015625\n",
      "Trained batch 354 batch loss 2.66340113 batch mAP 0.099609375 batch PCKh 0.50390625\n",
      "Trained batch 355 batch loss 2.85616279 batch mAP 0.037109375 batch PCKh 0.42578125\n",
      "Trained batch 356 batch loss 2.69584846 batch mAP 0.064453125 batch PCKh 0.642578125\n",
      "Trained batch 357 batch loss 2.86986876 batch mAP 0.06640625 batch PCKh 0.59375\n",
      "Trained batch 358 batch loss 3.05271792 batch mAP 0.0390625 batch PCKh 0.5\n",
      "Trained batch 359 batch loss 2.7983408 batch mAP 0.044921875 batch PCKh 0.625\n",
      "Trained batch 360 batch loss 2.75870371 batch mAP 0.06640625 batch PCKh 0.638671875\n",
      "Trained batch 361 batch loss 2.69336653 batch mAP 0.109375 batch PCKh 0.646484375\n",
      "Trained batch 362 batch loss 2.63530779 batch mAP 0.099609375 batch PCKh 0.423828125\n",
      "Trained batch 363 batch loss 2.76054072 batch mAP 0.056640625 batch PCKh 0.513671875\n",
      "Trained batch 364 batch loss 2.59285927 batch mAP 0.08203125 batch PCKh 0.478515625\n",
      "Trained batch 365 batch loss 2.55492115 batch mAP 0.056640625 batch PCKh 0.5703125\n",
      "Trained batch 366 batch loss 2.54398632 batch mAP 0.052734375 batch PCKh 0.427734375\n",
      "Trained batch 367 batch loss 2.60322785 batch mAP 0.05859375 batch PCKh 0.455078125\n",
      "Trained batch 368 batch loss 2.90349388 batch mAP 0.05078125 batch PCKh 0.287109375\n",
      "Trained batch 369 batch loss 2.8186636 batch mAP 0.068359375 batch PCKh 0.376953125\n",
      "Trained batch 370 batch loss 2.95450592 batch mAP 0.056640625 batch PCKh 0.71484375\n",
      "Trained batch 371 batch loss 2.89104152 batch mAP 0.064453125 batch PCKh 0.509765625\n",
      "Trained batch 372 batch loss 2.72711635 batch mAP 0.087890625 batch PCKh 0.1875\n",
      "Trained batch 373 batch loss 2.81345201 batch mAP 0.072265625 batch PCKh 0.32421875\n",
      "Trained batch 374 batch loss 2.80274606 batch mAP 0.091796875 batch PCKh 0.427734375\n",
      "Trained batch 375 batch loss 2.93502879 batch mAP 0.087890625 batch PCKh 0.56640625\n",
      "Trained batch 376 batch loss 2.98834062 batch mAP 0.056640625 batch PCKh 0.537109375\n",
      "Trained batch 377 batch loss 2.68968105 batch mAP 0.1015625 batch PCKh 0.421875\n",
      "Trained batch 378 batch loss 2.87989092 batch mAP 0.12890625 batch PCKh 0.580078125\n",
      "Trained batch 379 batch loss 2.92379785 batch mAP 0.119140625 batch PCKh 0.51171875\n",
      "Trained batch 380 batch loss 2.82469559 batch mAP 0.0703125 batch PCKh 0.40625\n",
      "Trained batch 381 batch loss 2.37756968 batch mAP 0.0546875 batch PCKh 0.19921875\n",
      "Trained batch 382 batch loss 2.48776174 batch mAP 0.064453125 batch PCKh 0.302734375\n",
      "Trained batch 383 batch loss 2.42555737 batch mAP 0.0703125 batch PCKh 0.44140625\n",
      "Trained batch 384 batch loss 2.61442852 batch mAP 0.078125 batch PCKh 0.333984375\n",
      "Trained batch 385 batch loss 2.78920507 batch mAP 0.11328125 batch PCKh 0.53515625\n",
      "Trained batch 386 batch loss 2.88249254 batch mAP 0.1015625 batch PCKh 0.5390625\n",
      "Trained batch 387 batch loss 3.08631063 batch mAP 0.1015625 batch PCKh 0.4140625\n",
      "Trained batch 388 batch loss 2.86395288 batch mAP 0.1015625 batch PCKh 0.52734375\n",
      "Trained batch 389 batch loss 2.80724335 batch mAP 0.05859375 batch PCKh 0.50390625\n",
      "Trained batch 390 batch loss 2.73563719 batch mAP 0.080078125 batch PCKh 0.5859375\n",
      "Trained batch 391 batch loss 2.87277699 batch mAP 0.064453125 batch PCKh 0.498046875\n",
      "Trained batch 392 batch loss 2.9193182 batch mAP 0.068359375 batch PCKh 0.55859375\n",
      "Trained batch 393 batch loss 3.11509228 batch mAP 0.048828125 batch PCKh 0.34375\n",
      "Trained batch 394 batch loss 3.09237957 batch mAP 0.078125 batch PCKh 0.2890625\n",
      "Trained batch 395 batch loss 3.08734822 batch mAP 0.046875 batch PCKh 0.453125\n",
      "Trained batch 396 batch loss 2.87870097 batch mAP 0.08984375 batch PCKh 0.603515625\n",
      "Trained batch 397 batch loss 2.93961477 batch mAP 0.119140625 batch PCKh 0.5234375\n",
      "Trained batch 398 batch loss 2.78801298 batch mAP 0.0546875 batch PCKh 0.509765625\n",
      "Trained batch 399 batch loss 2.50080323 batch mAP 0.056640625 batch PCKh 0.16796875\n",
      "Trained batch 400 batch loss 2.55103302 batch mAP 0.05859375 batch PCKh 0.373046875\n",
      "Trained batch 401 batch loss 2.76080394 batch mAP 0.064453125 batch PCKh 0.390625\n",
      "Trained batch 402 batch loss 2.7748456 batch mAP 0.048828125 batch PCKh 0.607421875\n",
      "Trained batch 403 batch loss 2.64634275 batch mAP 0.072265625 batch PCKh 0.455078125\n",
      "Trained batch 404 batch loss 2.8796134 batch mAP 0.048828125 batch PCKh 0.51953125\n",
      "Trained batch 405 batch loss 2.61070108 batch mAP 0.0625 batch PCKh 0.447265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 406 batch loss 2.70204186 batch mAP 0.068359375 batch PCKh 0.6875\n",
      "Trained batch 407 batch loss 2.64174843 batch mAP 0.078125 batch PCKh 0.57421875\n",
      "Trained batch 408 batch loss 2.8066411 batch mAP 0.0546875 batch PCKh 0.525390625\n",
      "Trained batch 409 batch loss 2.81522131 batch mAP 0.041015625 batch PCKh 0.4765625\n",
      "Trained batch 410 batch loss 3.07927847 batch mAP 0.03125 batch PCKh 0.50390625\n",
      "Trained batch 411 batch loss 2.89984703 batch mAP 0.05078125 batch PCKh 0.162109375\n",
      "Trained batch 412 batch loss 2.65504479 batch mAP 0.05078125 batch PCKh 0.439453125\n",
      "Trained batch 413 batch loss 2.72055244 batch mAP 0.056640625 batch PCKh 0.224609375\n",
      "Trained batch 414 batch loss 2.87119961 batch mAP 0.033203125 batch PCKh 0.404296875\n",
      "Trained batch 415 batch loss 2.71561575 batch mAP 0.07421875 batch PCKh 0.3828125\n",
      "Trained batch 416 batch loss 2.77034521 batch mAP 0.068359375 batch PCKh 0.529296875\n",
      "Trained batch 417 batch loss 2.93112898 batch mAP 0.05859375 batch PCKh 0.55078125\n",
      "Trained batch 418 batch loss 2.64845467 batch mAP 0.025390625 batch PCKh 0.615234375\n",
      "Trained batch 419 batch loss 2.35912228 batch mAP 0.029296875 batch PCKh 0.685546875\n",
      "Trained batch 420 batch loss 2.69535208 batch mAP 0.05078125 batch PCKh 0.5078125\n",
      "Trained batch 421 batch loss 2.92766881 batch mAP 0.07421875 batch PCKh 0.595703125\n",
      "Trained batch 422 batch loss 2.65880489 batch mAP 0.025390625 batch PCKh 0.599609375\n",
      "Trained batch 423 batch loss 2.84792471 batch mAP 0.017578125 batch PCKh 0.572265625\n",
      "Trained batch 424 batch loss 2.5820117 batch mAP 0.041015625 batch PCKh 0.63671875\n",
      "Trained batch 425 batch loss 2.6823287 batch mAP 0.01953125 batch PCKh 0.615234375\n",
      "Trained batch 426 batch loss 2.54967189 batch mAP 0.02734375 batch PCKh 0.58984375\n",
      "Trained batch 427 batch loss 2.49593067 batch mAP 0.01171875 batch PCKh 0.5625\n",
      "Trained batch 428 batch loss 2.57735968 batch mAP 0.009765625 batch PCKh 0.646484375\n",
      "Trained batch 429 batch loss 2.53117371 batch mAP 0.01171875 batch PCKh 0.66015625\n",
      "Trained batch 430 batch loss 2.60317469 batch mAP 0.0625 batch PCKh 0.625\n",
      "Trained batch 431 batch loss 2.51182604 batch mAP 0.041015625 batch PCKh 0.603515625\n",
      "Trained batch 432 batch loss 2.7780652 batch mAP 0.0625 batch PCKh 0.748046875\n",
      "Trained batch 433 batch loss 3.05506611 batch mAP 0.0078125 batch PCKh 0.666015625\n",
      "Trained batch 434 batch loss 3.19370985 batch mAP 0.01953125 batch PCKh 0.466796875\n",
      "Trained batch 435 batch loss 2.48223257 batch mAP 0.017578125 batch PCKh 0.734375\n",
      "Trained batch 436 batch loss 2.68338537 batch mAP 0.02734375 batch PCKh 0.6875\n",
      "Trained batch 437 batch loss 2.83471155 batch mAP 0.0234375 batch PCKh 0.5625\n",
      "Trained batch 438 batch loss 2.58144808 batch mAP 0.046875 batch PCKh 0.41796875\n",
      "Trained batch 439 batch loss 3.21364117 batch mAP 0.017578125 batch PCKh 0.349609375\n",
      "Trained batch 440 batch loss 2.64967537 batch mAP 0.033203125 batch PCKh 0.38671875\n",
      "Trained batch 441 batch loss 2.86062288 batch mAP 0.00390625 batch PCKh 0.634765625\n",
      "Trained batch 442 batch loss 3.01030374 batch mAP 0.041015625 batch PCKh 0.36328125\n",
      "Trained batch 443 batch loss 3.03495479 batch mAP 0.03515625 batch PCKh 0.380859375\n",
      "Trained batch 444 batch loss 3.16816211 batch mAP 0.0234375 batch PCKh 0.458984375\n",
      "Trained batch 445 batch loss 3.244169 batch mAP 0.04296875 batch PCKh 0.392578125\n",
      "Trained batch 446 batch loss 3.03208447 batch mAP 0.064453125 batch PCKh 0.30859375\n",
      "Trained batch 447 batch loss 3.31307793 batch mAP 0.068359375 batch PCKh 0.271484375\n",
      "Trained batch 448 batch loss 3.26030326 batch mAP 0.064453125 batch PCKh 0.22265625\n",
      "Trained batch 449 batch loss 2.75260282 batch mAP 0.115234375 batch PCKh 0.40234375\n",
      "Trained batch 450 batch loss 2.64414024 batch mAP 0.103515625 batch PCKh 0.154296875\n",
      "Trained batch 451 batch loss 2.48804617 batch mAP 0.146484375 batch PCKh 0.0625\n",
      "Trained batch 452 batch loss 2.65178394 batch mAP 0.22265625 batch PCKh 0.330078125\n",
      "Trained batch 453 batch loss 2.75414228 batch mAP 0.19921875 batch PCKh 0.560546875\n",
      "Trained batch 454 batch loss 2.68656921 batch mAP 0.18359375 batch PCKh 0.474609375\n",
      "Trained batch 455 batch loss 2.80111384 batch mAP 0.197265625 batch PCKh 0.455078125\n",
      "Trained batch 456 batch loss 2.69853568 batch mAP 0.181640625 batch PCKh 0.626953125\n",
      "Trained batch 457 batch loss 2.53606701 batch mAP 0.142578125 batch PCKh 0.53125\n",
      "Trained batch 458 batch loss 2.48393059 batch mAP 0.1875 batch PCKh 0.388671875\n",
      "Trained batch 459 batch loss 2.74321246 batch mAP 0.12890625 batch PCKh 0.5859375\n",
      "Trained batch 460 batch loss 2.58145761 batch mAP 0.107421875 batch PCKh 0.53125\n",
      "Trained batch 461 batch loss 2.67306733 batch mAP 0.123046875 batch PCKh 0.611328125\n",
      "Trained batch 462 batch loss 2.60173702 batch mAP 0.068359375 batch PCKh 0.58203125\n",
      "Trained batch 463 batch loss 2.79704857 batch mAP 0.076171875 batch PCKh 0.439453125\n",
      "Trained batch 464 batch loss 2.60359192 batch mAP 0.07421875 batch PCKh 0.474609375\n",
      "Trained batch 465 batch loss 2.50573277 batch mAP 0.068359375 batch PCKh 0.591796875\n",
      "Trained batch 466 batch loss 2.36906695 batch mAP 0.048828125 batch PCKh 0.578125\n",
      "Trained batch 467 batch loss 2.31636739 batch mAP 0.017578125 batch PCKh 0.556640625\n",
      "Trained batch 468 batch loss 2.34066486 batch mAP 0.0078125 batch PCKh 0.654296875\n",
      "Trained batch 469 batch loss 2.49335861 batch mAP 0.052734375 batch PCKh 0.57421875\n",
      "Trained batch 470 batch loss 2.84209156 batch mAP 0.052734375 batch PCKh 0.451171875\n",
      "Trained batch 471 batch loss 2.80573201 batch mAP 0.0234375 batch PCKh 0.44140625\n",
      "Trained batch 472 batch loss 2.85576725 batch mAP 0.0234375 batch PCKh 0.513671875\n",
      "Trained batch 473 batch loss 2.82417631 batch mAP 0.1015625 batch PCKh 0.4921875\n",
      "Trained batch 474 batch loss 2.46709108 batch mAP 0.0078125 batch PCKh 0.57421875\n",
      "Trained batch 475 batch loss 2.52919 batch mAP 0.001953125 batch PCKh 0.546875\n",
      "Trained batch 476 batch loss 2.55732965 batch mAP 0.005859375 batch PCKh 0.5703125\n",
      "Trained batch 477 batch loss 2.38615203 batch mAP 0.03515625 batch PCKh 0.482421875\n",
      "Trained batch 478 batch loss 2.84517908 batch mAP 0.01953125 batch PCKh 0.533203125\n",
      "Trained batch 479 batch loss 2.97033119 batch mAP 0.013671875 batch PCKh 0.60546875\n",
      "Trained batch 480 batch loss 2.73583531 batch mAP 0.046875 batch PCKh 0.54296875\n",
      "Trained batch 481 batch loss 2.93374586 batch mAP 0.029296875 batch PCKh 0.583984375\n",
      "Trained batch 482 batch loss 2.97719 batch mAP 0.0234375 batch PCKh 0.55859375\n",
      "Trained batch 483 batch loss 2.97773123 batch mAP 0.048828125 batch PCKh 0.49609375\n",
      "Trained batch 484 batch loss 3.39056635 batch mAP 0.05078125 batch PCKh 0.275390625\n",
      "Trained batch 485 batch loss 2.73806024 batch mAP 0.0390625 batch PCKh 0.546875\n",
      "Trained batch 486 batch loss 2.89708185 batch mAP 0.04296875 batch PCKh 0.5234375\n",
      "Trained batch 487 batch loss 2.73337936 batch mAP 0.064453125 batch PCKh 0.603515625\n",
      "Trained batch 488 batch loss 3.18801713 batch mAP 0.0703125 batch PCKh 0.2890625\n",
      "Trained batch 489 batch loss 3.09224653 batch mAP 0.056640625 batch PCKh 0.287109375\n",
      "Trained batch 490 batch loss 2.74201727 batch mAP 0.0703125 batch PCKh 0.689453125\n",
      "Trained batch 491 batch loss 2.7695682 batch mAP 0.12109375 batch PCKh 0.6328125\n",
      "Trained batch 492 batch loss 2.7650423 batch mAP 0.1484375 batch PCKh 0.4765625\n",
      "Trained batch 493 batch loss 2.66762495 batch mAP 0.12890625 batch PCKh 0.365234375\n",
      "Trained batch 494 batch loss 2.78339195 batch mAP 0.095703125 batch PCKh 0.634765625\n",
      "Trained batch 495 batch loss 2.70534039 batch mAP 0.08984375 batch PCKh 0.45703125\n",
      "Trained batch 496 batch loss 3.05317807 batch mAP 0.09765625 batch PCKh 0.53125\n",
      "Trained batch 497 batch loss 3.22732401 batch mAP 0.07421875 batch PCKh 0.369140625\n",
      "Trained batch 498 batch loss 3.12851143 batch mAP 0.072265625 batch PCKh 0.357421875\n",
      "Trained batch 499 batch loss 3.12807798 batch mAP 0.08203125 batch PCKh 0.333984375\n",
      "Trained batch 500 batch loss 2.90712452 batch mAP 0.091796875 batch PCKh 0.541015625\n",
      "Trained batch 501 batch loss 2.75610542 batch mAP 0.080078125 batch PCKh 0.564453125\n",
      "Trained batch 502 batch loss 2.78589487 batch mAP 0.115234375 batch PCKh 0.40234375\n",
      "Trained batch 503 batch loss 2.691154 batch mAP 0.056640625 batch PCKh 0.525390625\n",
      "Trained batch 504 batch loss 2.74485874 batch mAP 0.05859375 batch PCKh 0.619140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 505 batch loss 2.90440941 batch mAP 0.0234375 batch PCKh 0.537109375\n",
      "Trained batch 506 batch loss 2.80820298 batch mAP 0.048828125 batch PCKh 0.529296875\n",
      "Trained batch 507 batch loss 2.86495543 batch mAP 0.03515625 batch PCKh 0.55078125\n",
      "Trained batch 508 batch loss 2.62636 batch mAP 0.03515625 batch PCKh 0.4375\n",
      "Trained batch 509 batch loss 2.94227576 batch mAP 0.04296875 batch PCKh 0.53515625\n",
      "Trained batch 510 batch loss 2.95112944 batch mAP 0.03515625 batch PCKh 0.615234375\n",
      "Trained batch 511 batch loss 3.11048722 batch mAP 0.056640625 batch PCKh 0.53515625\n",
      "Trained batch 512 batch loss 2.97106433 batch mAP 0.083984375 batch PCKh 0.458984375\n",
      "Trained batch 513 batch loss 3.02801 batch mAP 0.08203125 batch PCKh 0.3359375\n",
      "Trained batch 514 batch loss 3.1397295 batch mAP 0.078125 batch PCKh 0.263671875\n",
      "Trained batch 515 batch loss 2.79346561 batch mAP 0.10546875 batch PCKh 0.486328125\n",
      "Trained batch 516 batch loss 2.64599848 batch mAP 0.0625 batch PCKh 0.52734375\n",
      "Trained batch 517 batch loss 2.58226752 batch mAP 0.091796875 batch PCKh 0.30078125\n",
      "Trained batch 518 batch loss 2.55993271 batch mAP 0.060546875 batch PCKh 0.0625\n",
      "Trained batch 519 batch loss 2.42049074 batch mAP 0.07421875 batch PCKh 0.189453125\n",
      "Trained batch 520 batch loss 2.23637605 batch mAP 0.1171875 batch PCKh 0.103515625\n",
      "Trained batch 521 batch loss 2.52091169 batch mAP 0.05859375 batch PCKh 0.36328125\n",
      "Trained batch 522 batch loss 2.74348402 batch mAP 0.064453125 batch PCKh 0.40234375\n",
      "Trained batch 523 batch loss 2.9160738 batch mAP 0.076171875 batch PCKh 0.556640625\n",
      "Trained batch 524 batch loss 2.90873718 batch mAP 0.103515625 batch PCKh 0.361328125\n",
      "Trained batch 525 batch loss 2.9262352 batch mAP 0.078125 batch PCKh 0.603515625\n",
      "Trained batch 526 batch loss 3.09161258 batch mAP 0.0859375 batch PCKh 0.533203125\n",
      "Trained batch 527 batch loss 2.68506908 batch mAP 0.08203125 batch PCKh 0.66015625\n",
      "Trained batch 528 batch loss 2.74898863 batch mAP 0.044921875 batch PCKh 0.72265625\n",
      "Trained batch 529 batch loss 2.80051899 batch mAP 0.080078125 batch PCKh 0.60546875\n",
      "Trained batch 530 batch loss 3.4735415 batch mAP 0.041015625 batch PCKh 0.28125\n",
      "Trained batch 531 batch loss 3.38861275 batch mAP 0.052734375 batch PCKh 0.28125\n",
      "Trained batch 532 batch loss 3.1719017 batch mAP 0.041015625 batch PCKh 0.30078125\n",
      "Trained batch 533 batch loss 3.25412512 batch mAP 0.10546875 batch PCKh 0.458984375\n",
      "Trained batch 534 batch loss 2.66535187 batch mAP 0.0625 batch PCKh 0.2421875\n",
      "Trained batch 535 batch loss 2.6086762 batch mAP 0.044921875 batch PCKh 0.53125\n",
      "Trained batch 536 batch loss 2.89597368 batch mAP 0.052734375 batch PCKh 0.58984375\n",
      "Trained batch 537 batch loss 2.7096796 batch mAP 0.05078125 batch PCKh 0.474609375\n",
      "Trained batch 538 batch loss 2.48896956 batch mAP 0.052734375 batch PCKh 0.42578125\n",
      "Trained batch 539 batch loss 2.36030436 batch mAP 0.0859375 batch PCKh 0.2578125\n",
      "Trained batch 540 batch loss 2.1163125 batch mAP 0.0703125 batch PCKh 0.046875\n",
      "Trained batch 541 batch loss 2.14719105 batch mAP 0.0546875 batch PCKh 0.0390625\n",
      "Trained batch 542 batch loss 2.6590054 batch mAP 0.083984375 batch PCKh 0.546875\n",
      "Trained batch 543 batch loss 2.77264595 batch mAP 0.0859375 batch PCKh 0.373046875\n",
      "Trained batch 544 batch loss 2.73167181 batch mAP 0.119140625 batch PCKh 0.314453125\n",
      "Trained batch 545 batch loss 2.53986454 batch mAP 0.07421875 batch PCKh 0.169921875\n",
      "Trained batch 546 batch loss 2.10697412 batch mAP 0.126953125 batch PCKh 0.01953125\n",
      "Trained batch 547 batch loss 2.11130476 batch mAP 0.076171875 batch PCKh 0.037109375\n",
      "Trained batch 548 batch loss 2.16360092 batch mAP 0.0390625 batch PCKh 0.04296875\n",
      "Trained batch 549 batch loss 2.26497936 batch mAP 0.07421875 batch PCKh 0.0234375\n",
      "Trained batch 550 batch loss 2.27769 batch mAP 0.08984375 batch PCKh 0.25\n",
      "Trained batch 551 batch loss 2.26444197 batch mAP 0.068359375 batch PCKh 0.119140625\n",
      "Trained batch 552 batch loss 2.52673674 batch mAP 0.056640625 batch PCKh 0.458984375\n",
      "Trained batch 553 batch loss 2.38276029 batch mAP 0.048828125 batch PCKh 0.263671875\n",
      "Trained batch 554 batch loss 2.83796501 batch mAP 0.05078125 batch PCKh 0.529296875\n",
      "Trained batch 555 batch loss 2.69361925 batch mAP 0.037109375 batch PCKh 0.697265625\n",
      "Trained batch 556 batch loss 2.53462887 batch mAP 0.044921875 batch PCKh 0.587890625\n",
      "Trained batch 557 batch loss 2.96804833 batch mAP 0.052734375 batch PCKh 0.396484375\n",
      "Trained batch 558 batch loss 2.91978097 batch mAP 0.037109375 batch PCKh 0.373046875\n",
      "Trained batch 559 batch loss 2.76830029 batch mAP 0.0546875 batch PCKh 0.521484375\n",
      "Trained batch 560 batch loss 2.96903205 batch mAP 0.033203125 batch PCKh 0.556640625\n",
      "Trained batch 561 batch loss 3.22801805 batch mAP 0.080078125 batch PCKh 0.384765625\n",
      "Trained batch 562 batch loss 3.00698209 batch mAP 0.044921875 batch PCKh 0.318359375\n",
      "Trained batch 563 batch loss 3.03336835 batch mAP 0.046875 batch PCKh 0.56640625\n",
      "Trained batch 564 batch loss 2.90685892 batch mAP 0.056640625 batch PCKh 0.416015625\n",
      "Trained batch 565 batch loss 2.88731909 batch mAP 0.083984375 batch PCKh 0.4609375\n",
      "Trained batch 566 batch loss 3.02039719 batch mAP 0.072265625 batch PCKh 0.380859375\n",
      "Trained batch 567 batch loss 2.97104263 batch mAP 0.068359375 batch PCKh 0.525390625\n",
      "Trained batch 568 batch loss 2.85861063 batch mAP 0.0703125 batch PCKh 0.548828125\n",
      "Trained batch 569 batch loss 2.73165131 batch mAP 0.05859375 batch PCKh 0.484375\n",
      "Trained batch 570 batch loss 2.50797892 batch mAP 0.126953125 batch PCKh 0.49609375\n",
      "Trained batch 571 batch loss 2.45280051 batch mAP 0.11328125 batch PCKh 0.5703125\n",
      "Trained batch 572 batch loss 2.31098723 batch mAP 0.103515625 batch PCKh 0.630859375\n",
      "Trained batch 573 batch loss 2.63950348 batch mAP 0.056640625 batch PCKh 0.60546875\n",
      "Trained batch 574 batch loss 2.76222277 batch mAP 0.046875 batch PCKh 0.4765625\n",
      "Trained batch 575 batch loss 2.59156418 batch mAP 0.02734375 batch PCKh 0.6875\n",
      "Trained batch 576 batch loss 2.62295079 batch mAP 0.04296875 batch PCKh 0.576171875\n",
      "Trained batch 577 batch loss 2.64177656 batch mAP 0.009765625 batch PCKh 0.619140625\n",
      "Trained batch 578 batch loss 2.6026454 batch mAP 0.015625 batch PCKh 0.37890625\n",
      "Trained batch 579 batch loss 2.68169498 batch mAP 0 batch PCKh 0.6953125\n",
      "Trained batch 580 batch loss 2.5848043 batch mAP 0.013671875 batch PCKh 0.38671875\n",
      "Trained batch 581 batch loss 2.90793586 batch mAP 0.0234375 batch PCKh 0.509765625\n",
      "Trained batch 582 batch loss 3.04937983 batch mAP 0.021484375 batch PCKh 0.359375\n",
      "Trained batch 583 batch loss 2.42356181 batch mAP 0.013671875 batch PCKh 0.404296875\n",
      "Trained batch 584 batch loss 2.53050494 batch mAP 0.03125 batch PCKh 0.48046875\n",
      "Trained batch 585 batch loss 2.89753771 batch mAP 0.029296875 batch PCKh 0.640625\n",
      "Trained batch 586 batch loss 2.78513765 batch mAP 0.015625 batch PCKh 0.46484375\n",
      "Trained batch 587 batch loss 2.7205224 batch mAP 0.037109375 batch PCKh 0.5234375\n",
      "Trained batch 588 batch loss 2.7650094 batch mAP 0.0390625 batch PCKh 0.591796875\n",
      "Trained batch 589 batch loss 2.85247874 batch mAP 0.033203125 batch PCKh 0.57421875\n",
      "Trained batch 590 batch loss 2.90879655 batch mAP 0.029296875 batch PCKh 0.49609375\n",
      "Trained batch 591 batch loss 2.92129517 batch mAP 0.048828125 batch PCKh 0.564453125\n",
      "Trained batch 592 batch loss 3.25056076 batch mAP 0.05078125 batch PCKh 0.31640625\n",
      "Trained batch 593 batch loss 3.15463877 batch mAP 0.029296875 batch PCKh 0.421875\n",
      "Trained batch 594 batch loss 3.26919436 batch mAP 0.056640625 batch PCKh 0.25\n",
      "Trained batch 595 batch loss 3.14677262 batch mAP 0.0546875 batch PCKh 0.275390625\n",
      "Trained batch 596 batch loss 2.60063839 batch mAP 0.060546875 batch PCKh 0.361328125\n",
      "Trained batch 597 batch loss 2.72525287 batch mAP 0.041015625 batch PCKh 0.171875\n",
      "Trained batch 598 batch loss 2.61601758 batch mAP 0.06640625 batch PCKh 0.24609375\n",
      "Trained batch 599 batch loss 2.18114972 batch mAP 0.095703125 batch PCKh 0.01171875\n",
      "Trained batch 600 batch loss 2.56389832 batch mAP 0.107421875 batch PCKh 0.2421875\n",
      "Trained batch 601 batch loss 2.72824478 batch mAP 0.076171875 batch PCKh 0.404296875\n",
      "Trained batch 602 batch loss 3.13632607 batch mAP 0.072265625 batch PCKh 0.4375\n",
      "Trained batch 603 batch loss 3.30302739 batch mAP 0.083984375 batch PCKh 0.3828125\n",
      "Trained batch 604 batch loss 3.34017682 batch mAP 0.13671875 batch PCKh 0.33203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 605 batch loss 3.14049 batch mAP 0.181640625 batch PCKh 0.2265625\n",
      "Trained batch 606 batch loss 2.80972028 batch mAP 0.162109375 batch PCKh 0.525390625\n",
      "Trained batch 607 batch loss 2.8025918 batch mAP 0.107421875 batch PCKh 0.677734375\n",
      "Trained batch 608 batch loss 2.7217114 batch mAP 0.20703125 batch PCKh 0.623046875\n",
      "Trained batch 609 batch loss 2.82039022 batch mAP 0.21875 batch PCKh 0.462890625\n",
      "Trained batch 610 batch loss 2.78379726 batch mAP 0.220703125 batch PCKh 0.6171875\n",
      "Trained batch 611 batch loss 2.96960783 batch mAP 0.1640625 batch PCKh 0.53125\n",
      "Trained batch 612 batch loss 3.05453444 batch mAP 0.17578125 batch PCKh 0.42578125\n",
      "Trained batch 613 batch loss 2.68999815 batch mAP 0.142578125 batch PCKh 0.330078125\n",
      "Trained batch 614 batch loss 3.11656165 batch mAP 0.134765625 batch PCKh 0.41796875\n",
      "Trained batch 615 batch loss 3.49404931 batch mAP 0.125 batch PCKh 0.21484375\n",
      "Trained batch 616 batch loss 3.42649746 batch mAP 0.134765625 batch PCKh 0.259765625\n",
      "Trained batch 617 batch loss 3.25489759 batch mAP 0.09375 batch PCKh 0.240234375\n",
      "Trained batch 618 batch loss 2.96009755 batch mAP 0.154296875 batch PCKh 0.470703125\n",
      "Trained batch 619 batch loss 2.85619688 batch mAP 0.09375 batch PCKh 0.560546875\n",
      "Trained batch 620 batch loss 3.00897789 batch mAP 0.111328125 batch PCKh 0.2578125\n",
      "Trained batch 621 batch loss 3.0630703 batch mAP 0.173828125 batch PCKh 0.296875\n",
      "Trained batch 622 batch loss 2.91940784 batch mAP 0.1015625 batch PCKh 0.580078125\n",
      "Trained batch 623 batch loss 2.91407585 batch mAP 0.201171875 batch PCKh 0.43359375\n",
      "Trained batch 624 batch loss 2.69825125 batch mAP 0.1875 batch PCKh 0.388671875\n",
      "Trained batch 625 batch loss 2.44529 batch mAP 0.216796875 batch PCKh 0.287109375\n",
      "Trained batch 626 batch loss 2.52278137 batch mAP 0.17578125 batch PCKh 0.251953125\n",
      "Trained batch 627 batch loss 2.5726366 batch mAP 0.181640625 batch PCKh 0.369140625\n",
      "Trained batch 628 batch loss 2.88969326 batch mAP 0.23046875 batch PCKh 0.404296875\n",
      "Trained batch 629 batch loss 2.74768424 batch mAP 0.1484375 batch PCKh 0.4375\n",
      "Trained batch 630 batch loss 3.02641106 batch mAP 0.216796875 batch PCKh 0.3359375\n",
      "Trained batch 631 batch loss 2.97396255 batch mAP 0.134765625 batch PCKh 0.33203125\n",
      "Trained batch 632 batch loss 3.03232455 batch mAP 0.134765625 batch PCKh 0.333984375\n",
      "Trained batch 633 batch loss 2.81944871 batch mAP 0.22265625 batch PCKh 0.490234375\n",
      "Trained batch 634 batch loss 2.80600667 batch mAP 0.138671875 batch PCKh 0.212890625\n",
      "Trained batch 635 batch loss 2.69907856 batch mAP 0.1875 batch PCKh 0.533203125\n",
      "Trained batch 636 batch loss 2.86020803 batch mAP 0.166015625 batch PCKh 0.388671875\n",
      "Trained batch 637 batch loss 2.64471698 batch mAP 0.185546875 batch PCKh 0.3203125\n",
      "Trained batch 638 batch loss 2.38201475 batch mAP 0.1796875 batch PCKh 0.33203125\n",
      "Trained batch 639 batch loss 2.66357374 batch mAP 0.12890625 batch PCKh 0.673828125\n",
      "Trained batch 640 batch loss 2.63698053 batch mAP 0.19921875 batch PCKh 0.4140625\n",
      "Trained batch 641 batch loss 2.6790185 batch mAP 0.140625 batch PCKh 0.669921875\n",
      "Trained batch 642 batch loss 2.34568357 batch mAP 0.072265625 batch PCKh 0.44140625\n",
      "Trained batch 643 batch loss 2.60566688 batch mAP 0.072265625 batch PCKh 0.23046875\n",
      "Trained batch 644 batch loss 2.46245551 batch mAP 0.041015625 batch PCKh 0.654296875\n",
      "Trained batch 645 batch loss 2.76504254 batch mAP 0.025390625 batch PCKh 0.646484375\n",
      "Trained batch 646 batch loss 2.51809025 batch mAP 0.091796875 batch PCKh 0.39453125\n",
      "Trained batch 647 batch loss 2.41743183 batch mAP 0.037109375 batch PCKh 0.52734375\n",
      "Trained batch 648 batch loss 2.62054873 batch mAP 0.056640625 batch PCKh 0.568359375\n",
      "Trained batch 649 batch loss 2.65369272 batch mAP 0.048828125 batch PCKh 0.521484375\n",
      "Trained batch 650 batch loss 2.67116022 batch mAP 0.041015625 batch PCKh 0.509765625\n",
      "Trained batch 651 batch loss 2.32142 batch mAP 0.03515625 batch PCKh 0.40234375\n",
      "Trained batch 652 batch loss 2.13912725 batch mAP 0.083984375 batch PCKh 0.349609375\n",
      "Trained batch 653 batch loss 2.34299374 batch mAP 0.068359375 batch PCKh 0.6328125\n",
      "Trained batch 654 batch loss 2.36772537 batch mAP 0.0625 batch PCKh 0.69140625\n",
      "Trained batch 655 batch loss 3.11899614 batch mAP 0.029296875 batch PCKh 0.279296875\n",
      "Trained batch 656 batch loss 3.13455892 batch mAP 0.03515625 batch PCKh 0.52734375\n",
      "Trained batch 657 batch loss 2.94685364 batch mAP 0.029296875 batch PCKh 0.544921875\n",
      "Trained batch 658 batch loss 3.05884099 batch mAP 0.01953125 batch PCKh 0.54296875\n",
      "Trained batch 659 batch loss 2.83219957 batch mAP 0.013671875 batch PCKh 0.458984375\n",
      "Trained batch 660 batch loss 2.72768855 batch mAP 0.013671875 batch PCKh 0.4921875\n",
      "Trained batch 661 batch loss 2.92549109 batch mAP 0.013671875 batch PCKh 0.3515625\n",
      "Trained batch 662 batch loss 2.96089268 batch mAP 0.009765625 batch PCKh 0.431640625\n",
      "Trained batch 663 batch loss 3.02723718 batch mAP 0.021484375 batch PCKh 0.513671875\n",
      "Trained batch 664 batch loss 2.861063 batch mAP 0.025390625 batch PCKh 0.55078125\n",
      "Trained batch 665 batch loss 2.9859097 batch mAP 0.05078125 batch PCKh 0.47265625\n",
      "Trained batch 666 batch loss 2.60417724 batch mAP 0.064453125 batch PCKh 0.70703125\n",
      "Trained batch 667 batch loss 2.5382514 batch mAP 0.091796875 batch PCKh 0.58984375\n",
      "Trained batch 668 batch loss 2.41404819 batch mAP 0.076171875 batch PCKh 0.755859375\n",
      "Trained batch 669 batch loss 2.55395174 batch mAP 0.06640625 batch PCKh 0.54296875\n",
      "Trained batch 670 batch loss 2.68745708 batch mAP 0.037109375 batch PCKh 0.544921875\n",
      "Trained batch 671 batch loss 2.83419585 batch mAP 0.056640625 batch PCKh 0.599609375\n",
      "Trained batch 672 batch loss 3.00310087 batch mAP 0.07421875 batch PCKh 0.462890625\n",
      "Trained batch 673 batch loss 2.80488563 batch mAP 0.126953125 batch PCKh 0.59765625\n",
      "Trained batch 674 batch loss 2.66224575 batch mAP 0.052734375 batch PCKh 0.486328125\n",
      "Trained batch 675 batch loss 2.74277592 batch mAP 0.060546875 batch PCKh 0.36328125\n",
      "Trained batch 676 batch loss 2.66749501 batch mAP 0.109375 batch PCKh 0.611328125\n",
      "Trained batch 677 batch loss 2.97444057 batch mAP 0.0390625 batch PCKh 0.4296875\n",
      "Trained batch 678 batch loss 3.17182016 batch mAP 0.052734375 batch PCKh 0.337890625\n",
      "Trained batch 679 batch loss 2.64914584 batch mAP 0.078125 batch PCKh 0.32421875\n",
      "Trained batch 680 batch loss 2.77112055 batch mAP 0.052734375 batch PCKh 0.474609375\n",
      "Trained batch 681 batch loss 2.774019 batch mAP 0.046875 batch PCKh 0.197265625\n",
      "Trained batch 682 batch loss 3.07681227 batch mAP 0.0625 batch PCKh 0.34765625\n",
      "Trained batch 683 batch loss 2.78791571 batch mAP 0.0625 batch PCKh 0.458984375\n",
      "Trained batch 684 batch loss 3.00991297 batch mAP 0.087890625 batch PCKh 0.58203125\n",
      "Trained batch 685 batch loss 3.08670664 batch mAP 0.10546875 batch PCKh 0.498046875\n",
      "Trained batch 686 batch loss 2.90848422 batch mAP 0.11328125 batch PCKh 0.48828125\n",
      "Trained batch 687 batch loss 3.13400769 batch mAP 0.115234375 batch PCKh 0.474609375\n",
      "Trained batch 688 batch loss 2.93265176 batch mAP 0.166015625 batch PCKh 0.59765625\n",
      "Trained batch 689 batch loss 2.82165551 batch mAP 0.107421875 batch PCKh 0.603515625\n",
      "Trained batch 690 batch loss 2.91820192 batch mAP 0.12109375 batch PCKh 0.474609375\n",
      "Trained batch 691 batch loss 3.02505136 batch mAP 0.109375 batch PCKh 0.478515625\n",
      "Trained batch 692 batch loss 2.88637114 batch mAP 0.146484375 batch PCKh 0.658203125\n",
      "Trained batch 693 batch loss 2.84086943 batch mAP 0.1171875 batch PCKh 0.583984375\n",
      "Trained batch 694 batch loss 2.94674325 batch mAP 0.1484375 batch PCKh 0.427734375\n",
      "Trained batch 695 batch loss 2.90794849 batch mAP 0.126953125 batch PCKh 0.423828125\n",
      "Trained batch 696 batch loss 3.04647446 batch mAP 0.15234375 batch PCKh 0.39453125\n",
      "Trained batch 697 batch loss 2.70257449 batch mAP 0.140625 batch PCKh 0.23828125\n",
      "Trained batch 698 batch loss 2.37041497 batch mAP 0.109375 batch PCKh 0.447265625\n",
      "Trained batch 699 batch loss 2.92663097 batch mAP 0.115234375 batch PCKh 0.4921875\n",
      "Trained batch 700 batch loss 2.82651067 batch mAP 0.11328125 batch PCKh 0.625\n",
      "Trained batch 701 batch loss 2.43286109 batch mAP 0.099609375 batch PCKh 0.322265625\n",
      "Trained batch 702 batch loss 2.67949414 batch mAP 0.09765625 batch PCKh 0.546875\n",
      "Trained batch 703 batch loss 2.44103384 batch mAP 0.09765625 batch PCKh 0.19921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 704 batch loss 2.64335012 batch mAP 0.08203125 batch PCKh 0.1953125\n",
      "Trained batch 705 batch loss 2.59066558 batch mAP 0.06640625 batch PCKh 0.47265625\n",
      "Trained batch 706 batch loss 2.86717319 batch mAP 0.111328125 batch PCKh 0.412109375\n",
      "Trained batch 707 batch loss 2.42570591 batch mAP 0.083984375 batch PCKh 0.349609375\n",
      "Trained batch 708 batch loss 2.67163467 batch mAP 0.099609375 batch PCKh 0.216796875\n",
      "Trained batch 709 batch loss 2.48874521 batch mAP 0.095703125 batch PCKh 0.33984375\n",
      "Trained batch 710 batch loss 2.89215708 batch mAP 0.052734375 batch PCKh 0.4765625\n",
      "Trained batch 711 batch loss 2.78006864 batch mAP 0.07421875 batch PCKh 0.541015625\n",
      "Trained batch 712 batch loss 2.81883955 batch mAP 0.05078125 batch PCKh 0.560546875\n",
      "Trained batch 713 batch loss 2.69785047 batch mAP 0.10546875 batch PCKh 0.599609375\n",
      "Trained batch 714 batch loss 2.82183695 batch mAP 0.087890625 batch PCKh 0.486328125\n",
      "Trained batch 715 batch loss 2.74780464 batch mAP 0.095703125 batch PCKh 0.564453125\n",
      "Trained batch 716 batch loss 2.76219082 batch mAP 0.09375 batch PCKh 0.62109375\n",
      "Trained batch 717 batch loss 2.63457561 batch mAP 0.0703125 batch PCKh 0.67578125\n",
      "Trained batch 718 batch loss 2.56974721 batch mAP 0.05078125 batch PCKh 0.646484375\n",
      "Trained batch 719 batch loss 2.52029562 batch mAP 0.03515625 batch PCKh 0.671875\n",
      "Trained batch 720 batch loss 2.55318475 batch mAP 0.078125 batch PCKh 0.744140625\n",
      "Trained batch 721 batch loss 2.70475173 batch mAP 0.033203125 batch PCKh 0.560546875\n",
      "Trained batch 722 batch loss 2.56484199 batch mAP 0.068359375 batch PCKh 0.626953125\n",
      "Trained batch 723 batch loss 2.87615967 batch mAP 0.0234375 batch PCKh 0.27734375\n",
      "Trained batch 724 batch loss 2.72113609 batch mAP 0.02734375 batch PCKh 0.419921875\n",
      "Trained batch 725 batch loss 2.23281336 batch mAP 0.021484375 batch PCKh 0.34375\n",
      "Trained batch 726 batch loss 2.53767443 batch mAP 0.021484375 batch PCKh 0.34765625\n",
      "Trained batch 727 batch loss 3.20213485 batch mAP 0.025390625 batch PCKh 0.46875\n",
      "Trained batch 728 batch loss 2.65876174 batch mAP 0.095703125 batch PCKh 0.62890625\n",
      "Trained batch 729 batch loss 2.72791 batch mAP 0.0234375 batch PCKh 0.498046875\n",
      "Trained batch 730 batch loss 2.57568145 batch mAP 0.03515625 batch PCKh 0.46484375\n",
      "Trained batch 731 batch loss 2.82705212 batch mAP 0.037109375 batch PCKh 0.45703125\n",
      "Trained batch 732 batch loss 2.58913088 batch mAP 0.02734375 batch PCKh 0.51171875\n",
      "Trained batch 733 batch loss 2.88593507 batch mAP 0.037109375 batch PCKh 0.568359375\n",
      "Trained batch 734 batch loss 2.67888546 batch mAP 0.048828125 batch PCKh 0.5078125\n",
      "Trained batch 735 batch loss 2.68567371 batch mAP 0.056640625 batch PCKh 0.501953125\n",
      "Trained batch 736 batch loss 2.43094468 batch mAP 0.041015625 batch PCKh 0.595703125\n",
      "Trained batch 737 batch loss 2.46967983 batch mAP 0.150390625 batch PCKh 0.41015625\n",
      "Trained batch 738 batch loss 2.65228844 batch mAP 0.119140625 batch PCKh 0.40625\n",
      "Trained batch 739 batch loss 2.67229843 batch mAP 0.048828125 batch PCKh 0.330078125\n",
      "Trained batch 740 batch loss 2.62013769 batch mAP 0.072265625 batch PCKh 0.333984375\n",
      "Trained batch 741 batch loss 2.23285365 batch mAP 0.064453125 batch PCKh 0.431640625\n",
      "Trained batch 742 batch loss 2.79329443 batch mAP 0.033203125 batch PCKh 0.494140625\n",
      "Trained batch 743 batch loss 3.09277678 batch mAP 0.107421875 batch PCKh 0.390625\n",
      "Trained batch 744 batch loss 3.02795458 batch mAP 0.080078125 batch PCKh 0.443359375\n",
      "Trained batch 745 batch loss 2.68895054 batch mAP 0.078125 batch PCKh 0.423828125\n",
      "Trained batch 746 batch loss 2.79893064 batch mAP 0.044921875 batch PCKh 0.345703125\n",
      "Trained batch 747 batch loss 2.85582852 batch mAP 0.04296875 batch PCKh 0.466796875\n",
      "Trained batch 748 batch loss 2.67816401 batch mAP 0.08984375 batch PCKh 0.380859375\n",
      "Trained batch 749 batch loss 2.90959764 batch mAP 0.08984375 batch PCKh 0.705078125\n",
      "Trained batch 750 batch loss 2.55530691 batch mAP 0.0859375 batch PCKh 0.529296875\n",
      "Trained batch 751 batch loss 2.28900647 batch mAP 0.091796875 batch PCKh 0.1640625\n",
      "Trained batch 752 batch loss 2.79312897 batch mAP 0.048828125 batch PCKh 0.537109375\n",
      "Trained batch 753 batch loss 2.64829493 batch mAP 0.06640625 batch PCKh 0.427734375\n",
      "Trained batch 754 batch loss 2.54587913 batch mAP 0.044921875 batch PCKh 0.607421875\n",
      "Trained batch 755 batch loss 2.64557433 batch mAP 0.046875 batch PCKh 0.4921875\n",
      "Trained batch 756 batch loss 2.41177821 batch mAP 0.037109375 batch PCKh 0.296875\n",
      "Trained batch 757 batch loss 2.39336252 batch mAP 0.037109375 batch PCKh 0.73828125\n",
      "Trained batch 758 batch loss 2.48172331 batch mAP 0.05078125 batch PCKh 0.7265625\n",
      "Trained batch 759 batch loss 2.51124454 batch mAP 0.0234375 batch PCKh 0.578125\n",
      "Trained batch 760 batch loss 2.72658157 batch mAP 0.064453125 batch PCKh 0.5546875\n",
      "Trained batch 761 batch loss 2.64303231 batch mAP 0.044921875 batch PCKh 0.501953125\n",
      "Trained batch 762 batch loss 2.39392138 batch mAP 0.04296875 batch PCKh 0.3046875\n",
      "Trained batch 763 batch loss 2.75128651 batch mAP 0.044921875 batch PCKh 0.578125\n",
      "Trained batch 764 batch loss 2.77428985 batch mAP 0.07421875 batch PCKh 0.576171875\n",
      "Trained batch 765 batch loss 2.4749198 batch mAP 0.02734375 batch PCKh 0.494140625\n",
      "Trained batch 766 batch loss 2.25388908 batch mAP 0.033203125 batch PCKh 0.25390625\n",
      "Trained batch 767 batch loss 2.54349899 batch mAP 0.078125 batch PCKh 0.484375\n",
      "Trained batch 768 batch loss 2.54709387 batch mAP 0.056640625 batch PCKh 0.419921875\n",
      "Trained batch 769 batch loss 2.82610321 batch mAP 0.048828125 batch PCKh 0.53125\n",
      "Trained batch 770 batch loss 2.66879153 batch mAP 0.015625 batch PCKh 0.5078125\n",
      "Trained batch 771 batch loss 2.33488035 batch mAP 0.01953125 batch PCKh 0.34375\n",
      "Trained batch 772 batch loss 2.40308857 batch mAP 0.01953125 batch PCKh 0.54296875\n",
      "Trained batch 773 batch loss 3.07504177 batch mAP 0.03125 batch PCKh 0.49609375\n",
      "Trained batch 774 batch loss 2.92262602 batch mAP 0.01953125 batch PCKh 0.30859375\n",
      "Trained batch 775 batch loss 2.82644486 batch mAP 0.041015625 batch PCKh 0.505859375\n",
      "Trained batch 776 batch loss 2.73936868 batch mAP 0.015625 batch PCKh 0.55859375\n",
      "Trained batch 777 batch loss 2.69357657 batch mAP 0.052734375 batch PCKh 0.345703125\n",
      "Trained batch 778 batch loss 2.64330769 batch mAP 0.02734375 batch PCKh 0.5625\n",
      "Trained batch 779 batch loss 2.54259539 batch mAP 0.04296875 batch PCKh 0.626953125\n",
      "Trained batch 780 batch loss 2.78277636 batch mAP 0.046875 batch PCKh 0.576171875\n",
      "Trained batch 781 batch loss 2.58222818 batch mAP 0.080078125 batch PCKh 0.529296875\n",
      "Trained batch 782 batch loss 2.40064216 batch mAP 0.076171875 batch PCKh 0.451171875\n",
      "Trained batch 783 batch loss 2.51028872 batch mAP 0.064453125 batch PCKh 0.513671875\n",
      "Trained batch 784 batch loss 2.43170881 batch mAP 0.005859375 batch PCKh 0.5546875\n",
      "Trained batch 785 batch loss 2.42151308 batch mAP 0.013671875 batch PCKh 0.544921875\n",
      "Trained batch 786 batch loss 2.68459034 batch mAP 0.005859375 batch PCKh 0.576171875\n",
      "Trained batch 787 batch loss 2.55985212 batch mAP 0.00390625 batch PCKh 0.66796875\n",
      "Trained batch 788 batch loss 2.46953535 batch mAP 0.0234375 batch PCKh 0.30859375\n",
      "Trained batch 789 batch loss 2.71412086 batch mAP 0.00390625 batch PCKh 0.4765625\n",
      "Trained batch 790 batch loss 2.80803895 batch mAP 0.025390625 batch PCKh 0.494140625\n",
      "Trained batch 791 batch loss 2.62105513 batch mAP 0.009765625 batch PCKh 0.3515625\n",
      "Trained batch 792 batch loss 2.77311444 batch mAP 0.005859375 batch PCKh 0.482421875\n",
      "Trained batch 793 batch loss 2.77639246 batch mAP 0.029296875 batch PCKh 0.376953125\n",
      "Trained batch 794 batch loss 3.00318 batch mAP 0.025390625 batch PCKh 0.404296875\n",
      "Trained batch 795 batch loss 2.80272985 batch mAP 0.017578125 batch PCKh 0.625\n",
      "Trained batch 796 batch loss 2.64023161 batch mAP 0.025390625 batch PCKh 0.46875\n",
      "Trained batch 797 batch loss 2.88600135 batch mAP 0.03125 batch PCKh 0.255859375\n",
      "Trained batch 798 batch loss 3.02531433 batch mAP 0.04296875 batch PCKh 0.32421875\n",
      "Trained batch 799 batch loss 2.49219704 batch mAP 0.109375 batch PCKh 0.517578125\n",
      "Trained batch 800 batch loss 2.83464718 batch mAP 0.07421875 batch PCKh 0.375\n",
      "Trained batch 801 batch loss 2.9190712 batch mAP 0.064453125 batch PCKh 0.423828125\n",
      "Trained batch 802 batch loss 2.78964496 batch mAP 0.05078125 batch PCKh 0.416015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 803 batch loss 2.92656684 batch mAP 0.078125 batch PCKh 0.4765625\n",
      "Trained batch 804 batch loss 2.5482173 batch mAP 0.0546875 batch PCKh 0.654296875\n",
      "Trained batch 805 batch loss 2.8256526 batch mAP 0.091796875 batch PCKh 0.50390625\n",
      "Trained batch 806 batch loss 2.94492221 batch mAP 0.126953125 batch PCKh 0.5703125\n",
      "Trained batch 807 batch loss 2.75895905 batch mAP 0.12109375 batch PCKh 0.521484375\n",
      "Trained batch 808 batch loss 2.78356051 batch mAP 0.037109375 batch PCKh 0.5703125\n",
      "Trained batch 809 batch loss 2.88292837 batch mAP 0.05859375 batch PCKh 0.501953125\n",
      "Trained batch 810 batch loss 2.91985679 batch mAP 0.064453125 batch PCKh 0.5859375\n",
      "Trained batch 811 batch loss 3.01891184 batch mAP 0.0546875 batch PCKh 0.490234375\n",
      "Trained batch 812 batch loss 2.76263142 batch mAP 0.04296875 batch PCKh 0.544921875\n",
      "Trained batch 813 batch loss 2.53713465 batch mAP 0.06640625 batch PCKh 0.546875\n",
      "Trained batch 814 batch loss 2.64780402 batch mAP 0.046875 batch PCKh 0.232421875\n",
      "Trained batch 815 batch loss 2.84360147 batch mAP 0.041015625 batch PCKh 0.564453125\n",
      "Trained batch 816 batch loss 2.88441372 batch mAP 0.0546875 batch PCKh 0.3515625\n",
      "Trained batch 817 batch loss 2.98118973 batch mAP 0.0625 batch PCKh 0.44921875\n",
      "Trained batch 818 batch loss 2.96605015 batch mAP 0.099609375 batch PCKh 0.513671875\n",
      "Trained batch 819 batch loss 3.18819118 batch mAP 0.103515625 batch PCKh 0.52734375\n",
      "Trained batch 820 batch loss 2.97410727 batch mAP 0.08203125 batch PCKh 0.419921875\n",
      "Trained batch 821 batch loss 2.55131817 batch mAP 0.13671875 batch PCKh 0.623046875\n",
      "Trained batch 822 batch loss 2.80998969 batch mAP 0.09375 batch PCKh 0.525390625\n",
      "Trained batch 823 batch loss 2.635391 batch mAP 0.041015625 batch PCKh 0.70703125\n",
      "Trained batch 824 batch loss 2.81948161 batch mAP 0.0390625 batch PCKh 0.5703125\n",
      "Trained batch 825 batch loss 2.71877074 batch mAP 0.080078125 batch PCKh 0.482421875\n",
      "Trained batch 826 batch loss 2.57706642 batch mAP 0.046875 batch PCKh 0.603515625\n",
      "Trained batch 827 batch loss 2.59544134 batch mAP 0.015625 batch PCKh 0.533203125\n",
      "Trained batch 828 batch loss 2.76518607 batch mAP 0.05859375 batch PCKh 0.515625\n",
      "Trained batch 829 batch loss 2.78394699 batch mAP 0.02734375 batch PCKh 0.421875\n",
      "Trained batch 830 batch loss 2.64295435 batch mAP 0.08203125 batch PCKh 0.595703125\n",
      "Trained batch 831 batch loss 2.98205185 batch mAP 0.048828125 batch PCKh 0.412109375\n",
      "Trained batch 832 batch loss 2.67993855 batch mAP 0.068359375 batch PCKh 0.42578125\n",
      "Trained batch 833 batch loss 2.710361 batch mAP 0.052734375 batch PCKh 0.333984375\n",
      "Trained batch 834 batch loss 2.71899557 batch mAP 0.05078125 batch PCKh 0.330078125\n",
      "Trained batch 835 batch loss 3.06951475 batch mAP 0.033203125 batch PCKh 0.35546875\n",
      "Trained batch 836 batch loss 2.68824649 batch mAP 0.05078125 batch PCKh 0.568359375\n",
      "Trained batch 837 batch loss 2.54270029 batch mAP 0.046875 batch PCKh 0.46875\n",
      "Trained batch 838 batch loss 2.63008642 batch mAP 0.052734375 batch PCKh 0.57421875\n",
      "Trained batch 839 batch loss 3.00584936 batch mAP 0.10546875 batch PCKh 0.453125\n",
      "Trained batch 840 batch loss 2.89924455 batch mAP 0.037109375 batch PCKh 0.55078125\n",
      "Trained batch 841 batch loss 2.70861602 batch mAP 0.052734375 batch PCKh 0.4453125\n",
      "Trained batch 842 batch loss 2.59416628 batch mAP 0.017578125 batch PCKh 0.66796875\n",
      "Trained batch 843 batch loss 2.63260603 batch mAP 0.07421875 batch PCKh 0.662109375\n",
      "Trained batch 844 batch loss 2.79695439 batch mAP 0.015625 batch PCKh 0.490234375\n",
      "Trained batch 845 batch loss 2.83485055 batch mAP 0.033203125 batch PCKh 0.505859375\n",
      "Trained batch 846 batch loss 2.75178671 batch mAP 0.033203125 batch PCKh 0.478515625\n",
      "Trained batch 847 batch loss 2.83618903 batch mAP 0.013671875 batch PCKh 0.533203125\n",
      "Trained batch 848 batch loss 2.71909142 batch mAP 0.048828125 batch PCKh 0.59765625\n",
      "Trained batch 849 batch loss 2.69213915 batch mAP 0.01171875 batch PCKh 0.48828125\n",
      "Trained batch 850 batch loss 2.85674334 batch mAP 0.0234375 batch PCKh 0.4453125\n",
      "Trained batch 851 batch loss 2.96395397 batch mAP 0.01171875 batch PCKh 0.5859375\n",
      "Trained batch 852 batch loss 2.67786288 batch mAP 0.005859375 batch PCKh 0.65234375\n",
      "Trained batch 853 batch loss 2.75392628 batch mAP 0.017578125 batch PCKh 0.55859375\n",
      "Trained batch 854 batch loss 2.70702815 batch mAP 0.01171875 batch PCKh 0.61328125\n",
      "Trained batch 855 batch loss 2.51915979 batch mAP 0.0078125 batch PCKh 0.671875\n",
      "Trained batch 856 batch loss 2.89470339 batch mAP 0.0234375 batch PCKh 0.572265625\n",
      "Trained batch 857 batch loss 2.73264 batch mAP 0.015625 batch PCKh 0.333984375\n",
      "Trained batch 858 batch loss 2.98854566 batch mAP 0.0234375 batch PCKh 0.333984375\n",
      "Trained batch 859 batch loss 2.62776184 batch mAP 0.037109375 batch PCKh 0.3359375\n",
      "Trained batch 860 batch loss 2.83251905 batch mAP 0.037109375 batch PCKh 0.27734375\n",
      "Trained batch 861 batch loss 2.47830296 batch mAP 0.0234375 batch PCKh 0.56640625\n",
      "Trained batch 862 batch loss 2.43935347 batch mAP 0.03125 batch PCKh 0.306640625\n",
      "Trained batch 863 batch loss 2.88446665 batch mAP 0.064453125 batch PCKh 0.359375\n",
      "Trained batch 864 batch loss 2.49435878 batch mAP 0.052734375 batch PCKh 0.478515625\n",
      "Trained batch 865 batch loss 2.50345349 batch mAP 0.0546875 batch PCKh 0.45703125\n",
      "Trained batch 866 batch loss 2.50915289 batch mAP 0.0390625 batch PCKh 0.32421875\n",
      "Trained batch 867 batch loss 2.47479606 batch mAP 0.04296875 batch PCKh 0.0703125\n",
      "Trained batch 868 batch loss 3.27920151 batch mAP 0.025390625 batch PCKh 0.205078125\n",
      "Trained batch 869 batch loss 3.12743592 batch mAP 0.07421875 batch PCKh 0.326171875\n",
      "Trained batch 870 batch loss 3.01805019 batch mAP 0.1015625 batch PCKh 0.5\n",
      "Trained batch 871 batch loss 2.84736562 batch mAP 0.083984375 batch PCKh 0.47265625\n",
      "Trained batch 872 batch loss 2.90811658 batch mAP 0.068359375 batch PCKh 0.53125\n",
      "Trained batch 873 batch loss 2.80117965 batch mAP 0.1015625 batch PCKh 0.458984375\n",
      "Trained batch 874 batch loss 2.65702939 batch mAP 0.09375 batch PCKh 0.33984375\n",
      "Trained batch 875 batch loss 3.18829298 batch mAP 0.13671875 batch PCKh 0.326171875\n",
      "Trained batch 876 batch loss 2.9560349 batch mAP 0.146484375 batch PCKh 0.357421875\n",
      "Trained batch 877 batch loss 2.8018198 batch mAP 0.099609375 batch PCKh 0.607421875\n",
      "Trained batch 878 batch loss 2.88255548 batch mAP 0.107421875 batch PCKh 0.427734375\n",
      "Trained batch 879 batch loss 2.77197647 batch mAP 0.103515625 batch PCKh 0.373046875\n",
      "Trained batch 880 batch loss 2.89648676 batch mAP 0.134765625 batch PCKh 0.587890625\n",
      "Trained batch 881 batch loss 2.69257069 batch mAP 0.123046875 batch PCKh 0.453125\n",
      "Trained batch 882 batch loss 2.49791789 batch mAP 0.064453125 batch PCKh 0.408203125\n",
      "Trained batch 883 batch loss 2.62562323 batch mAP 0.078125 batch PCKh 0.54296875\n",
      "Trained batch 884 batch loss 2.53247619 batch mAP 0.115234375 batch PCKh 0.443359375\n",
      "Trained batch 885 batch loss 2.95878315 batch mAP 0.173828125 batch PCKh 0.408203125\n",
      "Trained batch 886 batch loss 2.46018672 batch mAP 0.1015625 batch PCKh 0.578125\n",
      "Trained batch 887 batch loss 2.48063684 batch mAP 0.138671875 batch PCKh 0.54296875\n",
      "Trained batch 888 batch loss 2.85871553 batch mAP 0.044921875 batch PCKh 0.537109375\n",
      "Trained batch 889 batch loss 2.95077443 batch mAP 0.044921875 batch PCKh 0.693359375\n",
      "Trained batch 890 batch loss 2.50611782 batch mAP 0.01171875 batch PCKh 0.63671875\n",
      "Trained batch 891 batch loss 2.83266234 batch mAP 0.00390625 batch PCKh 0.568359375\n",
      "Trained batch 892 batch loss 2.9769783 batch mAP 0.013671875 batch PCKh 0.509765625\n",
      "Trained batch 893 batch loss 2.89917755 batch mAP 0.03515625 batch PCKh 0.421875\n",
      "Trained batch 894 batch loss 2.99999356 batch mAP 0.056640625 batch PCKh 0.4765625\n",
      "Trained batch 895 batch loss 2.81078291 batch mAP 0.0234375 batch PCKh 0.48828125\n",
      "Trained batch 896 batch loss 2.93816 batch mAP 0.068359375 batch PCKh 0.5703125\n",
      "Trained batch 897 batch loss 2.84910274 batch mAP 0.064453125 batch PCKh 0.462890625\n",
      "Trained batch 898 batch loss 3.12614942 batch mAP 0.015625 batch PCKh 0.337890625\n",
      "Trained batch 899 batch loss 2.84104133 batch mAP 0.00390625 batch PCKh 0.494140625\n",
      "Trained batch 900 batch loss 2.68632245 batch mAP 0.015625 batch PCKh 0.572265625\n",
      "Trained batch 901 batch loss 2.53309417 batch mAP 0.078125 batch PCKh 0.580078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 902 batch loss 2.41325116 batch mAP 0.0390625 batch PCKh 0.634765625\n",
      "Trained batch 903 batch loss 2.57721853 batch mAP 0.095703125 batch PCKh 0.568359375\n",
      "Trained batch 904 batch loss 2.53760934 batch mAP 0.091796875 batch PCKh 0.3046875\n",
      "Trained batch 905 batch loss 2.69736362 batch mAP 0.080078125 batch PCKh 0.3203125\n",
      "Trained batch 906 batch loss 2.67853951 batch mAP 0.046875 batch PCKh 0.37109375\n",
      "Trained batch 907 batch loss 2.90854073 batch mAP 0.037109375 batch PCKh 0.419921875\n",
      "Trained batch 908 batch loss 2.49895811 batch mAP 0.037109375 batch PCKh 0.19140625\n",
      "Trained batch 909 batch loss 2.80843091 batch mAP 0.044921875 batch PCKh 0.353515625\n",
      "Trained batch 910 batch loss 2.69339705 batch mAP 0.05078125 batch PCKh 0.533203125\n",
      "Trained batch 911 batch loss 2.62442255 batch mAP 0.029296875 batch PCKh 0.611328125\n",
      "Trained batch 912 batch loss 2.63627338 batch mAP 0.068359375 batch PCKh 0.626953125\n",
      "Trained batch 913 batch loss 2.50267339 batch mAP 0.017578125 batch PCKh 0.640625\n",
      "Trained batch 914 batch loss 2.46702385 batch mAP 0.02734375 batch PCKh 0.49609375\n",
      "Trained batch 915 batch loss 2.62858295 batch mAP 0.00390625 batch PCKh 0.595703125\n",
      "Trained batch 916 batch loss 2.51277399 batch mAP 0.05078125 batch PCKh 0.380859375\n",
      "Trained batch 917 batch loss 2.94817877 batch mAP 0.001953125 batch PCKh 0.314453125\n",
      "Trained batch 918 batch loss 2.36200023 batch mAP 0.009765625 batch PCKh 0.412109375\n",
      "Trained batch 919 batch loss 2.5512681 batch mAP 0.0234375 batch PCKh 0.560546875\n",
      "Trained batch 920 batch loss 2.97219634 batch mAP 0.00390625 batch PCKh 0.38671875\n",
      "Trained batch 921 batch loss 2.77216768 batch mAP 0.00390625 batch PCKh 0.44140625\n",
      "Trained batch 922 batch loss 2.82285547 batch mAP 0.01953125 batch PCKh 0.568359375\n",
      "Trained batch 923 batch loss 2.7767477 batch mAP 0.015625 batch PCKh 0.544921875\n",
      "Trained batch 924 batch loss 2.65713859 batch mAP 0.048828125 batch PCKh 0.568359375\n",
      "Trained batch 925 batch loss 2.65315056 batch mAP 0.046875 batch PCKh 0.62109375\n",
      "Trained batch 926 batch loss 2.69508123 batch mAP 0.029296875 batch PCKh 0.525390625\n",
      "Trained batch 927 batch loss 2.58351874 batch mAP 0.048828125 batch PCKh 0.4765625\n",
      "Trained batch 928 batch loss 3.40162396 batch mAP 0.041015625 batch PCKh 0.32421875\n",
      "Trained batch 929 batch loss 2.83773851 batch mAP 0.048828125 batch PCKh 0.494140625\n",
      "Trained batch 930 batch loss 2.60387182 batch mAP 0.04296875 batch PCKh 0.30078125\n",
      "Trained batch 931 batch loss 2.57295942 batch mAP 0.044921875 batch PCKh 0.388671875\n",
      "Trained batch 932 batch loss 2.83540463 batch mAP 0.02734375 batch PCKh 0.345703125\n",
      "Trained batch 933 batch loss 3.01345682 batch mAP 0.046875 batch PCKh 0.697265625\n",
      "Trained batch 934 batch loss 2.6446507 batch mAP 0.017578125 batch PCKh 0.408203125\n",
      "Trained batch 935 batch loss 2.75909281 batch mAP 0.037109375 batch PCKh 0.326171875\n",
      "Trained batch 936 batch loss 2.4069972 batch mAP 0.017578125 batch PCKh 0.287109375\n",
      "Trained batch 937 batch loss 2.40519357 batch mAP 0.044921875 batch PCKh 0.126953125\n",
      "Trained batch 938 batch loss 2.50995803 batch mAP 0.02734375 batch PCKh 0.228515625\n",
      "Trained batch 939 batch loss 2.70817804 batch mAP 0.0546875 batch PCKh 0.390625\n",
      "Trained batch 940 batch loss 2.69348621 batch mAP 0.06640625 batch PCKh 0.2265625\n",
      "Trained batch 941 batch loss 2.61990452 batch mAP 0.056640625 batch PCKh 0.375\n",
      "Trained batch 942 batch loss 2.72951531 batch mAP 0.076171875 batch PCKh 0.404296875\n",
      "Trained batch 943 batch loss 2.66213489 batch mAP 0.083984375 batch PCKh 0.314453125\n",
      "Trained batch 944 batch loss 2.98202801 batch mAP 0.08984375 batch PCKh 0.197265625\n",
      "Trained batch 945 batch loss 2.80438423 batch mAP 0.072265625 batch PCKh 0.40625\n",
      "Trained batch 946 batch loss 2.85407281 batch mAP 0.05859375 batch PCKh 0.484375\n",
      "Trained batch 947 batch loss 2.90368366 batch mAP 0.021484375 batch PCKh 0.583984375\n",
      "Trained batch 948 batch loss 2.77667236 batch mAP 0.099609375 batch PCKh 0.595703125\n",
      "Trained batch 949 batch loss 2.96507049 batch mAP 0.099609375 batch PCKh 0.3203125\n",
      "Trained batch 950 batch loss 2.48628163 batch mAP 0.064453125 batch PCKh 0.203125\n",
      "Trained batch 951 batch loss 2.25806212 batch mAP 0.033203125 batch PCKh 0.123046875\n",
      "Trained batch 952 batch loss 2.50034213 batch mAP 0.03515625 batch PCKh 0.361328125\n",
      "Trained batch 953 batch loss 2.84611487 batch mAP 0.076171875 batch PCKh 0.44921875\n",
      "Trained batch 954 batch loss 2.91075134 batch mAP 0.041015625 batch PCKh 0.482421875\n",
      "Trained batch 955 batch loss 2.87278891 batch mAP 0.13671875 batch PCKh 0.388671875\n",
      "Trained batch 956 batch loss 3.17898321 batch mAP 0.0703125 batch PCKh 0.345703125\n",
      "Trained batch 957 batch loss 2.65332055 batch mAP 0.087890625 batch PCKh 0.296875\n",
      "Trained batch 958 batch loss 2.75278902 batch mAP 0.06640625 batch PCKh 0.43359375\n",
      "Trained batch 959 batch loss 2.57309389 batch mAP 0.09375 batch PCKh 0.466796875\n",
      "Trained batch 960 batch loss 2.87496471 batch mAP 0.091796875 batch PCKh 0.384765625\n",
      "Trained batch 961 batch loss 2.74272251 batch mAP 0.080078125 batch PCKh 0.412109375\n",
      "Trained batch 962 batch loss 2.38363552 batch mAP 0.07421875 batch PCKh 0.23828125\n",
      "Trained batch 963 batch loss 2.37670422 batch mAP 0.099609375 batch PCKh 0.4453125\n",
      "Trained batch 964 batch loss 2.62652421 batch mAP 0.1015625 batch PCKh 0.45703125\n",
      "Trained batch 965 batch loss 2.78349328 batch mAP 0.05859375 batch PCKh 0.544921875\n",
      "Trained batch 966 batch loss 2.6945076 batch mAP 0.048828125 batch PCKh 0.59765625\n",
      "Trained batch 967 batch loss 2.78628111 batch mAP 0.046875 batch PCKh 0.48828125\n",
      "Trained batch 968 batch loss 2.5593729 batch mAP 0.041015625 batch PCKh 0.42578125\n",
      "Trained batch 969 batch loss 2.33356237 batch mAP 0.041015625 batch PCKh 0.34375\n",
      "Trained batch 970 batch loss 2.09782958 batch mAP 0.037109375 batch PCKh 0.244140625\n",
      "Trained batch 971 batch loss 2.21137047 batch mAP 0.01953125 batch PCKh 0.091796875\n",
      "Trained batch 972 batch loss 2.12679982 batch mAP 0.060546875 batch PCKh 0.130859375\n",
      "Trained batch 973 batch loss 2.37798047 batch mAP 0.0234375 batch PCKh 0.287109375\n",
      "Trained batch 974 batch loss 2.54016542 batch mAP 0.01953125 batch PCKh 0.166015625\n",
      "Trained batch 975 batch loss 2.35467196 batch mAP 0.01171875 batch PCKh 0.5546875\n",
      "Trained batch 976 batch loss 2.5808053 batch mAP 0.03125 batch PCKh 0.572265625\n",
      "Trained batch 977 batch loss 2.62934732 batch mAP 0.03515625 batch PCKh 0.349609375\n",
      "Trained batch 978 batch loss 2.43790293 batch mAP 0.0078125 batch PCKh 0.134765625\n",
      "Trained batch 979 batch loss 2.21873856 batch mAP 0.0078125 batch PCKh 0.54296875\n",
      "Trained batch 980 batch loss 2.19794202 batch mAP 0.009765625 batch PCKh 0.302734375\n",
      "Trained batch 981 batch loss 2.56288385 batch mAP 0.03125 batch PCKh 0.578125\n",
      "Trained batch 982 batch loss 2.41630888 batch mAP 0.01953125 batch PCKh 0.625\n",
      "Trained batch 983 batch loss 2.66388535 batch mAP 0.015625 batch PCKh 0.509765625\n",
      "Trained batch 984 batch loss 2.7983067 batch mAP 0.029296875 batch PCKh 0.390625\n",
      "Trained batch 985 batch loss 2.62535191 batch mAP 0.009765625 batch PCKh 0.56640625\n",
      "Trained batch 986 batch loss 2.78520441 batch mAP 0.009765625 batch PCKh 0.595703125\n",
      "Trained batch 987 batch loss 2.25424814 batch mAP 0.015625 batch PCKh 0.3984375\n",
      "Trained batch 988 batch loss 2.43140864 batch mAP 0.02734375 batch PCKh 0.609375\n",
      "Trained batch 989 batch loss 2.19216394 batch mAP 0.013671875 batch PCKh 0.40234375\n",
      "Trained batch 990 batch loss 2.42773771 batch mAP 0.00390625 batch PCKh 0.5625\n",
      "Trained batch 991 batch loss 2.85398507 batch mAP 0.013671875 batch PCKh 0.55859375\n",
      "Trained batch 992 batch loss 2.71154475 batch mAP 0.01171875 batch PCKh 0.47265625\n",
      "Trained batch 993 batch loss 3.00503349 batch mAP 0.033203125 batch PCKh 0.513671875\n",
      "Trained batch 994 batch loss 2.7049396 batch mAP 0.021484375 batch PCKh 0.46875\n",
      "Trained batch 995 batch loss 2.79605246 batch mAP 0.025390625 batch PCKh 0.380859375\n",
      "Trained batch 996 batch loss 2.67230749 batch mAP 0.01953125 batch PCKh 0.587890625\n",
      "Trained batch 997 batch loss 2.7795949 batch mAP 0.052734375 batch PCKh 0.4765625\n",
      "Trained batch 998 batch loss 2.63686228 batch mAP 0.0625 batch PCKh 0.376953125\n",
      "Trained batch 999 batch loss 2.49485636 batch mAP 0.021484375 batch PCKh 0.01953125\n",
      "Trained batch 1000 batch loss 2.60573769 batch mAP 0.021484375 batch PCKh 0.107421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1001 batch loss 2.62916684 batch mAP 0.01953125 batch PCKh 0.30078125\n",
      "Trained batch 1002 batch loss 3.08875084 batch mAP 0.044921875 batch PCKh 0.443359375\n",
      "Trained batch 1003 batch loss 3.34268355 batch mAP 0.04296875 batch PCKh 0.333984375\n",
      "Trained batch 1004 batch loss 3.00923181 batch mAP 0.052734375 batch PCKh 0.328125\n",
      "Trained batch 1005 batch loss 2.63348484 batch mAP 0.0859375 batch PCKh 0.486328125\n",
      "Trained batch 1006 batch loss 2.46857643 batch mAP 0.015625 batch PCKh 0.412109375\n",
      "Trained batch 1007 batch loss 2.53861237 batch mAP 0.017578125 batch PCKh 0.3046875\n",
      "Trained batch 1008 batch loss 2.44291139 batch mAP 0.060546875 batch PCKh 0.591796875\n",
      "Trained batch 1009 batch loss 2.53458667 batch mAP 0.0703125 batch PCKh 0.564453125\n",
      "Trained batch 1010 batch loss 2.3807745 batch mAP 0.080078125 batch PCKh 0.564453125\n",
      "Trained batch 1011 batch loss 2.42769265 batch mAP 0.078125 batch PCKh 0.560546875\n",
      "Trained batch 1012 batch loss 2.75246572 batch mAP 0.0703125 batch PCKh 0.515625\n",
      "Trained batch 1013 batch loss 2.62493324 batch mAP 0.033203125 batch PCKh 0.421875\n",
      "Trained batch 1014 batch loss 2.76050711 batch mAP 0.041015625 batch PCKh 0.53515625\n",
      "Trained batch 1015 batch loss 3.24527359 batch mAP 0.03125 batch PCKh 0.30078125\n",
      "Trained batch 1016 batch loss 2.67890596 batch mAP 0.072265625 batch PCKh 0.515625\n",
      "Trained batch 1017 batch loss 2.66692734 batch mAP 0.02734375 batch PCKh 0.466796875\n",
      "Trained batch 1018 batch loss 2.97718525 batch mAP 0.005859375 batch PCKh 0.37890625\n",
      "Trained batch 1019 batch loss 2.67713404 batch mAP 0.005859375 batch PCKh 0.5390625\n",
      "Trained batch 1020 batch loss 2.63694692 batch mAP 0.029296875 batch PCKh 0.490234375\n",
      "Trained batch 1021 batch loss 2.74647164 batch mAP 0.0703125 batch PCKh 0.462890625\n",
      "Trained batch 1022 batch loss 2.46076202 batch mAP 0.072265625 batch PCKh 0.44140625\n",
      "Trained batch 1023 batch loss 2.8648355 batch mAP 0.07421875 batch PCKh 0.482421875\n",
      "Trained batch 1024 batch loss 2.42893505 batch mAP 0.048828125 batch PCKh 0.517578125\n",
      "Trained batch 1025 batch loss 2.69949961 batch mAP 0.02734375 batch PCKh 0.306640625\n",
      "Trained batch 1026 batch loss 2.90202951 batch mAP 0.0234375 batch PCKh 0.359375\n",
      "Trained batch 1027 batch loss 2.71909571 batch mAP 0.025390625 batch PCKh 0.3046875\n",
      "Trained batch 1028 batch loss 2.9442935 batch mAP 0.03515625 batch PCKh 0.330078125\n",
      "Trained batch 1029 batch loss 2.87391472 batch mAP 0.044921875 batch PCKh 0.44140625\n",
      "Trained batch 1030 batch loss 3.08627248 batch mAP 0.04296875 batch PCKh 0.28515625\n",
      "Trained batch 1031 batch loss 2.83163667 batch mAP 0.06640625 batch PCKh 0.3515625\n",
      "Trained batch 1032 batch loss 2.92645073 batch mAP 0.04296875 batch PCKh 0.494140625\n",
      "Trained batch 1033 batch loss 2.89412308 batch mAP 0.04296875 batch PCKh 0.490234375\n",
      "Trained batch 1034 batch loss 2.59541702 batch mAP 0.091796875 batch PCKh 0.412109375\n",
      "Trained batch 1035 batch loss 2.49799109 batch mAP 0.091796875 batch PCKh 0.396484375\n",
      "Trained batch 1036 batch loss 2.72099638 batch mAP 0.076171875 batch PCKh 0.642578125\n",
      "Trained batch 1037 batch loss 2.39930868 batch mAP 0.0703125 batch PCKh 0.3359375\n",
      "Trained batch 1038 batch loss 2.45280266 batch mAP 0.09765625 batch PCKh 0.556640625\n",
      "Trained batch 1039 batch loss 2.58122015 batch mAP 0.080078125 batch PCKh 0.68359375\n",
      "Trained batch 1040 batch loss 2.71378636 batch mAP 0.1015625 batch PCKh 0.2109375\n",
      "Trained batch 1041 batch loss 3.02366686 batch mAP 0.1015625 batch PCKh 0.412109375\n",
      "Trained batch 1042 batch loss 2.90631938 batch mAP 0.111328125 batch PCKh 0.474609375\n",
      "Trained batch 1043 batch loss 2.90736055 batch mAP 0.083984375 batch PCKh 0.54296875\n",
      "Trained batch 1044 batch loss 2.56257463 batch mAP 0.078125 batch PCKh 0.732421875\n",
      "Trained batch 1045 batch loss 2.80872464 batch mAP 0.064453125 batch PCKh 0.478515625\n",
      "Trained batch 1046 batch loss 2.49607325 batch mAP 0.0625 batch PCKh 0.4296875\n",
      "Trained batch 1047 batch loss 2.468225 batch mAP 0.0625 batch PCKh 0.537109375\n",
      "Trained batch 1048 batch loss 2.53559303 batch mAP 0.087890625 batch PCKh 0.50390625\n",
      "Trained batch 1049 batch loss 2.71708703 batch mAP 0.1328125 batch PCKh 0.6015625\n",
      "Trained batch 1050 batch loss 2.7406249 batch mAP 0.087890625 batch PCKh 0.50390625\n",
      "Trained batch 1051 batch loss 2.62898922 batch mAP 0.044921875 batch PCKh 0.693359375\n",
      "Trained batch 1052 batch loss 2.53163886 batch mAP 0.072265625 batch PCKh 0.548828125\n",
      "Trained batch 1053 batch loss 2.47876024 batch mAP 0.107421875 batch PCKh 0.46875\n",
      "Trained batch 1054 batch loss 2.95763 batch mAP 0.0625 batch PCKh 0.271484375\n",
      "Trained batch 1055 batch loss 2.59215617 batch mAP 0.05859375 batch PCKh 0.642578125\n",
      "Trained batch 1056 batch loss 2.56467772 batch mAP 0.05859375 batch PCKh 0.416015625\n",
      "Trained batch 1057 batch loss 2.28398037 batch mAP 0.03515625 batch PCKh 0.166015625\n",
      "Trained batch 1058 batch loss 2.38247538 batch mAP 0.08203125 batch PCKh 0.220703125\n",
      "Trained batch 1059 batch loss 2.43311262 batch mAP 0.099609375 batch PCKh 0.23046875\n",
      "Trained batch 1060 batch loss 1.94997621 batch mAP 0.06640625 batch PCKh 0.126953125\n",
      "Trained batch 1061 batch loss 1.8831315 batch mAP 0.08203125 batch PCKh 0.044921875\n",
      "Trained batch 1062 batch loss 2.00987291 batch mAP 0.041015625 batch PCKh 0.03515625\n",
      "Trained batch 1063 batch loss 2.79403305 batch mAP 0.046875 batch PCKh 0.509765625\n",
      "Trained batch 1064 batch loss 2.91560483 batch mAP 0.076171875 batch PCKh 0.4296875\n",
      "Trained batch 1065 batch loss 2.662467 batch mAP 0.0546875 batch PCKh 0.3046875\n",
      "Trained batch 1066 batch loss 2.99843669 batch mAP 0.095703125 batch PCKh 0.45703125\n",
      "Trained batch 1067 batch loss 2.84496021 batch mAP 0.09765625 batch PCKh 0.525390625\n",
      "Trained batch 1068 batch loss 2.54876566 batch mAP 0.095703125 batch PCKh 0.46484375\n",
      "Trained batch 1069 batch loss 3.03418183 batch mAP 0.087890625 batch PCKh 0.423828125\n",
      "Trained batch 1070 batch loss 2.54966021 batch mAP 0.099609375 batch PCKh 0.53125\n",
      "Trained batch 1071 batch loss 2.56946039 batch mAP 0.060546875 batch PCKh 0.572265625\n",
      "Trained batch 1072 batch loss 2.72162771 batch mAP 0.087890625 batch PCKh 0.62109375\n",
      "Trained batch 1073 batch loss 2.7499032 batch mAP 0.064453125 batch PCKh 0.591796875\n",
      "Trained batch 1074 batch loss 2.56330013 batch mAP 0.078125 batch PCKh 0.3984375\n",
      "Trained batch 1075 batch loss 2.39034653 batch mAP 0.10546875 batch PCKh 0.173828125\n",
      "Trained batch 1076 batch loss 2.3547473 batch mAP 0.083984375 batch PCKh 0.18359375\n",
      "Trained batch 1077 batch loss 2.64138985 batch mAP 0.060546875 batch PCKh 0.490234375\n",
      "Trained batch 1078 batch loss 2.51351881 batch mAP 0.091796875 batch PCKh 0.431640625\n",
      "Trained batch 1079 batch loss 2.78935099 batch mAP 0.0546875 batch PCKh 0.37109375\n",
      "Trained batch 1080 batch loss 2.57434034 batch mAP 0.0859375 batch PCKh 0.181640625\n",
      "Trained batch 1081 batch loss 2.62158871 batch mAP 0.08203125 batch PCKh 0.453125\n",
      "Trained batch 1082 batch loss 2.90192628 batch mAP 0.05078125 batch PCKh 0.201171875\n",
      "Trained batch 1083 batch loss 2.43848038 batch mAP 0.025390625 batch PCKh 0.4453125\n",
      "Trained batch 1084 batch loss 2.65353322 batch mAP 0.037109375 batch PCKh 0.38671875\n",
      "Trained batch 1085 batch loss 3.04452658 batch mAP 0.025390625 batch PCKh 0.453125\n",
      "Trained batch 1086 batch loss 2.85129786 batch mAP 0.05859375 batch PCKh 0.548828125\n",
      "Trained batch 1087 batch loss 3.36494064 batch mAP 0.025390625 batch PCKh 0.33203125\n",
      "Trained batch 1088 batch loss 2.67612982 batch mAP 0.080078125 batch PCKh 0.541015625\n",
      "Trained batch 1089 batch loss 2.64962 batch mAP 0.05859375 batch PCKh 0.58984375\n",
      "Trained batch 1090 batch loss 2.67935157 batch mAP 0.044921875 batch PCKh 0.548828125\n",
      "Trained batch 1091 batch loss 2.70394182 batch mAP 0.01171875 batch PCKh 0.568359375\n",
      "Trained batch 1092 batch loss 2.92113304 batch mAP 0.05859375 batch PCKh 0.478515625\n",
      "Trained batch 1093 batch loss 2.93583202 batch mAP 0.04296875 batch PCKh 0.58203125\n",
      "Trained batch 1094 batch loss 2.87540674 batch mAP 0.03125 batch PCKh 0.6328125\n",
      "Trained batch 1095 batch loss 2.66577697 batch mAP 0.064453125 batch PCKh 0.306640625\n",
      "Trained batch 1096 batch loss 2.92598081 batch mAP 0.033203125 batch PCKh 0.486328125\n",
      "Trained batch 1097 batch loss 2.87057304 batch mAP 0.0703125 batch PCKh 0.318359375\n",
      "Trained batch 1098 batch loss 2.38183355 batch mAP 0.0390625 batch PCKh 0.16796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1099 batch loss 2.8629806 batch mAP 0.037109375 batch PCKh 0.5\n",
      "Trained batch 1100 batch loss 2.87854838 batch mAP 0.080078125 batch PCKh 0.466796875\n",
      "Trained batch 1101 batch loss 2.82697058 batch mAP 0.033203125 batch PCKh 0.54296875\n",
      "Trained batch 1102 batch loss 2.98650599 batch mAP 0.076171875 batch PCKh 0.552734375\n",
      "Trained batch 1103 batch loss 2.6116786 batch mAP 0.080078125 batch PCKh 0.240234375\n",
      "Trained batch 1104 batch loss 2.53082156 batch mAP 0.0546875 batch PCKh 0.44140625\n",
      "Trained batch 1105 batch loss 2.68861818 batch mAP 0.046875 batch PCKh 0.51171875\n",
      "Trained batch 1106 batch loss 2.79067731 batch mAP 0.12109375 batch PCKh 0.548828125\n",
      "Trained batch 1107 batch loss 2.79589128 batch mAP 0.13671875 batch PCKh 0.53515625\n",
      "Trained batch 1108 batch loss 2.51815653 batch mAP 0.06640625 batch PCKh 0.556640625\n",
      "Trained batch 1109 batch loss 2.86195517 batch mAP 0.064453125 batch PCKh 0.576171875\n",
      "Trained batch 1110 batch loss 2.91607404 batch mAP 0.0546875 batch PCKh 0.38671875\n",
      "Trained batch 1111 batch loss 2.90317917 batch mAP 0.048828125 batch PCKh 0.2890625\n",
      "Trained batch 1112 batch loss 2.49225473 batch mAP 0.064453125 batch PCKh 0.328125\n",
      "Trained batch 1113 batch loss 2.38049364 batch mAP 0.0859375 batch PCKh 0.3125\n",
      "Trained batch 1114 batch loss 2.45420289 batch mAP 0.056640625 batch PCKh 0.7578125\n",
      "Trained batch 1115 batch loss 2.54294968 batch mAP 0.08203125 batch PCKh 0.58984375\n",
      "Trained batch 1116 batch loss 2.52915525 batch mAP 0.076171875 batch PCKh 0.70703125\n",
      "Trained batch 1117 batch loss 2.23058653 batch mAP 0.03515625 batch PCKh 0.517578125\n",
      "Trained batch 1118 batch loss 2.39375925 batch mAP 0.033203125 batch PCKh 0.52734375\n",
      "Trained batch 1119 batch loss 2.53200746 batch mAP 0.02734375 batch PCKh 0.748046875\n",
      "Trained batch 1120 batch loss 2.6456337 batch mAP 0.03125 batch PCKh 0.6015625\n",
      "Trained batch 1121 batch loss 2.52923203 batch mAP 0.033203125 batch PCKh 0.552734375\n",
      "Trained batch 1122 batch loss 2.51085687 batch mAP 0.0234375 batch PCKh 0.4375\n",
      "Trained batch 1123 batch loss 2.57318592 batch mAP 0.01953125 batch PCKh 0.431640625\n",
      "Trained batch 1124 batch loss 2.51630592 batch mAP 0.060546875 batch PCKh 0.166015625\n",
      "Trained batch 1125 batch loss 2.36276436 batch mAP 0.017578125 batch PCKh 0.44921875\n",
      "Trained batch 1126 batch loss 2.27593255 batch mAP 0.0390625 batch PCKh 0.4375\n",
      "Trained batch 1127 batch loss 2.38845897 batch mAP 0.009765625 batch PCKh 0.44140625\n",
      "Trained batch 1128 batch loss 2.45889354 batch mAP 0.029296875 batch PCKh 0.462890625\n",
      "Trained batch 1129 batch loss 2.74311972 batch mAP 0.009765625 batch PCKh 0.603515625\n",
      "Trained batch 1130 batch loss 2.54269385 batch mAP 0.029296875 batch PCKh 0.58984375\n",
      "Trained batch 1131 batch loss 2.98931122 batch mAP 0.013671875 batch PCKh 0.4921875\n",
      "Trained batch 1132 batch loss 2.92443037 batch mAP 0.015625 batch PCKh 0.455078125\n",
      "Trained batch 1133 batch loss 3.22884512 batch mAP 0.015625 batch PCKh 0.349609375\n",
      "Trained batch 1134 batch loss 3.4104321 batch mAP 0.01953125 batch PCKh 0.349609375\n",
      "Trained batch 1135 batch loss 3.41719484 batch mAP 0.01953125 batch PCKh 0.234375\n",
      "Trained batch 1136 batch loss 2.73677349 batch mAP 0.02734375 batch PCKh 0.375\n",
      "Trained batch 1137 batch loss 2.96539068 batch mAP 0.05078125 batch PCKh 0.498046875\n",
      "Trained batch 1138 batch loss 2.93067694 batch mAP 0.044921875 batch PCKh 0.43359375\n",
      "Trained batch 1139 batch loss 2.89228487 batch mAP 0.052734375 batch PCKh 0.443359375\n",
      "Trained batch 1140 batch loss 2.536484 batch mAP 0.064453125 batch PCKh 0.55859375\n",
      "Trained batch 1141 batch loss 2.53997445 batch mAP 0.083984375 batch PCKh 0.560546875\n",
      "Trained batch 1142 batch loss 2.56219769 batch mAP 0.12109375 batch PCKh 0.318359375\n",
      "Trained batch 1143 batch loss 2.90330648 batch mAP 0.09765625 batch PCKh 0.509765625\n",
      "Trained batch 1144 batch loss 2.56499362 batch mAP 0.083984375 batch PCKh 0.5\n",
      "Trained batch 1145 batch loss 2.41402292 batch mAP 0.05859375 batch PCKh 0.466796875\n",
      "Trained batch 1146 batch loss 2.81182313 batch mAP 0.0546875 batch PCKh 0.51953125\n",
      "Trained batch 1147 batch loss 3.01690125 batch mAP 0.072265625 batch PCKh 0.390625\n",
      "Trained batch 1148 batch loss 2.59400272 batch mAP 0.05859375 batch PCKh 0.69921875\n",
      "Trained batch 1149 batch loss 2.82199025 batch mAP 0.072265625 batch PCKh 0.556640625\n",
      "Trained batch 1150 batch loss 2.63820624 batch mAP 0.03125 batch PCKh 0.548828125\n",
      "Trained batch 1151 batch loss 2.32248807 batch mAP 0.064453125 batch PCKh 0.494140625\n",
      "Trained batch 1152 batch loss 2.66991901 batch mAP 0.03125 batch PCKh 0.544921875\n",
      "Trained batch 1153 batch loss 2.54768515 batch mAP 0.03125 batch PCKh 0.630859375\n",
      "Trained batch 1154 batch loss 2.31680369 batch mAP 0.09375 batch PCKh 0.607421875\n",
      "Trained batch 1155 batch loss 2.57482862 batch mAP 0.052734375 batch PCKh 0.51953125\n",
      "Trained batch 1156 batch loss 2.31251025 batch mAP 0.015625 batch PCKh 0.63671875\n",
      "Trained batch 1157 batch loss 2.3644793 batch mAP 0.005859375 batch PCKh 0.6015625\n",
      "Trained batch 1158 batch loss 2.61467576 batch mAP 0.01953125 batch PCKh 0.638671875\n",
      "Trained batch 1159 batch loss 2.81092191 batch mAP 0.015625 batch PCKh 0.55078125\n",
      "Trained batch 1160 batch loss 3.05044484 batch mAP 0.025390625 batch PCKh 0.37109375\n",
      "Trained batch 1161 batch loss 3.46059752 batch mAP 0.015625 batch PCKh 0.23828125\n",
      "Trained batch 1162 batch loss 3.25891471 batch mAP 0.02734375 batch PCKh 0.41015625\n",
      "Trained batch 1163 batch loss 3.23815441 batch mAP 0.015625 batch PCKh 0.513671875\n",
      "Trained batch 1164 batch loss 3.29887843 batch mAP 0.03125 batch PCKh 0.419921875\n",
      "Trained batch 1165 batch loss 3.04203033 batch mAP 0.025390625 batch PCKh 0.482421875\n",
      "Trained batch 1166 batch loss 2.85604882 batch mAP 0.037109375 batch PCKh 0.716796875\n",
      "Trained batch 1167 batch loss 2.88431978 batch mAP 0.07421875 batch PCKh 0.32421875\n",
      "Trained batch 1168 batch loss 3.04077816 batch mAP 0.052734375 batch PCKh 0.494140625\n",
      "Trained batch 1169 batch loss 3.04623365 batch mAP 0.10546875 batch PCKh 0.396484375\n",
      "Trained batch 1170 batch loss 3.24533272 batch mAP 0.08984375 batch PCKh 0.431640625\n",
      "Trained batch 1171 batch loss 2.9625926 batch mAP 0.05078125 batch PCKh 0.326171875\n",
      "Trained batch 1172 batch loss 2.72859406 batch mAP 0.08984375 batch PCKh 0.296875\n",
      "Trained batch 1173 batch loss 3.10054731 batch mAP 0.146484375 batch PCKh 0.259765625\n",
      "Trained batch 1174 batch loss 2.93863225 batch mAP 0.1328125 batch PCKh 0.396484375\n",
      "Trained batch 1175 batch loss 2.20414567 batch mAP 0.14453125 batch PCKh 0.12890625\n",
      "Trained batch 1176 batch loss 2.51022387 batch mAP 0.125 batch PCKh 0.42578125\n",
      "Trained batch 1177 batch loss 2.76693654 batch mAP 0.12890625 batch PCKh 0.455078125\n",
      "Trained batch 1178 batch loss 2.91012716 batch mAP 0.107421875 batch PCKh 0.416015625\n",
      "Trained batch 1179 batch loss 2.83629441 batch mAP 0.13671875 batch PCKh 0.455078125\n",
      "Trained batch 1180 batch loss 2.81839 batch mAP 0.134765625 batch PCKh 0.25390625\n",
      "Trained batch 1181 batch loss 2.50603533 batch mAP 0.111328125 batch PCKh 0.173828125\n",
      "Trained batch 1182 batch loss 2.30956507 batch mAP 0.080078125 batch PCKh 0.048828125\n",
      "Trained batch 1183 batch loss 2.19668055 batch mAP 0.0546875 batch PCKh 0.033203125\n",
      "Trained batch 1184 batch loss 2.98559332 batch mAP 0.07421875 batch PCKh 0.31640625\n",
      "Trained batch 1185 batch loss 2.97797608 batch mAP 0.109375 batch PCKh 0.3671875\n",
      "Trained batch 1186 batch loss 3.05149388 batch mAP 0.1015625 batch PCKh 0.541015625\n",
      "Trained batch 1187 batch loss 2.98591256 batch mAP 0.0546875 batch PCKh 0.568359375\n",
      "Trained batch 1188 batch loss 2.81316686 batch mAP 0.0859375 batch PCKh 0.46875\n",
      "Trained batch 1189 batch loss 2.72309399 batch mAP 0.072265625 batch PCKh 0.58203125\n",
      "Trained batch 1190 batch loss 2.63138103 batch mAP 0.1328125 batch PCKh 0.63671875\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file, history = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(history['train_loss'], 'r')\n",
    "plt.plot(history['val_loss'], 'y')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf97c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(history['train_map'], 'r')\n",
    "plt.plot(history['val_map'], 'y')\n",
    "plt.title('Model mAP')\n",
    "plt.ylabel('mAP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(history['train_pckh'], 'r')\n",
    "plt.plot(history['val_pckh'], 'y')\n",
    "plt.title('Model PCKh')\n",
    "plt.ylabel('PCKh')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history/history_sh_10.json', 'w') as f:\n",
    "    pd.DataFrame(history).to_json(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
