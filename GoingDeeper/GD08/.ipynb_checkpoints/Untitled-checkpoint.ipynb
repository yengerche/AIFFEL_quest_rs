{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a7bb2b",
   "metadata": {},
   "source": [
    "## 23-2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d0e94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있습니다\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/node_data/GD08/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "# MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "MODEL_PATH = os.getenv('HOME') + '/aiffel/model_weight/GD08'\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b93eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533d0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e0425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/node_data/GD08/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170171a",
   "metadata": {},
   "source": [
    "## 23-3. TFRecord 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90373410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff16b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35277b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac9ff077",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3c7b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450971f6",
   "metadata": {},
   "source": [
    "## 23-4. Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f772786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 02:31:13,912\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.92gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/node_data/GD08/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/node_data/GD08/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m \n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m \n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m \n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m start to build tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=131)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=132)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8a58d",
   "metadata": {},
   "source": [
    "## 23-5. data label 로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36fcfe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=130)\u001b[0m finished building tf records for /aiffel/aiffel/node_data/GD08/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "def parse_tfexample(example):\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "        'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    return tf.io.parse_single_example(example, image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8276862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_roi(image, features, margin=0.2):\n",
    "    img_shape = tf.shape(image)\n",
    "    img_height = img_shape[0]\n",
    "    img_width = img_shape[1]\n",
    "    img_depth = img_shape[2]\n",
    "\n",
    "    keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "    keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "    center_x = features['image/object/center/x']\n",
    "    center_y = features['image/object/center/y']\n",
    "    body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "    # keypoint 중 유효한값(visible = 1) 만 사용합니다.\n",
    "    masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "    masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "    # min, max 값을 찾습니다.\n",
    "    keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "    keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "    keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "    keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "    # 높이 값을 이용해서 x, y 위치를 재조정 합니다. 박스를 정사각형으로 사용하기 위해 아래와 같이 사용합니다.\n",
    "    xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "    ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "    # 이미지 크기를 벗어나는 점을 재조정 해줍니다.\n",
    "    effective_xmin = xmin if xmin > 0 else 0\n",
    "    effective_ymin = ymin if ymin > 0 else 0\n",
    "    effective_xmax = xmax if xmax < img_width else img_width\n",
    "    effective_ymax = ymax if ymax < img_height else img_height\n",
    "    effective_height = effective_ymax - effective_ymin\n",
    "    effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "    image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "    new_shape = tf.shape(image)\n",
    "    new_height = new_shape[0]\n",
    "    new_width = new_shape[1]\n",
    "\n",
    "    effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "    effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "    return image, effective_keypoint_x, effective_keypoint_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ccbadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_guassian(height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "    heatmap = tf.zeros((height, width))\n",
    "\n",
    "    xmin = x0 - 3 * sigma\n",
    "    ymin = y0 - 3 * sigma\n",
    "    xmax = x0 + 3 * sigma\n",
    "    ymax = y0 + 3 * sigma\n",
    "    \n",
    "    if xmin >= width or ymin >= height or xmax < 0 or ymax < 0 or visibility == 0:\n",
    "        return heatmap\n",
    "\n",
    "    size = 6 * sigma + 1\n",
    "    x, y = tf.meshgrid(tf.range(0, 6 * sigma + 1, 1), tf.range(0, 6 * sigma + 1, 1), indexing='xy')\n",
    "\n",
    "    center_x = size // 2\n",
    "    center_y = size // 2\n",
    "\n",
    "    gaussian_patch = tf.cast(tf.math.exp(\n",
    "        -(tf.math.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale,\n",
    "                             dtype=tf.float32)\n",
    "\n",
    "    patch_xmin = tf.math.maximum(0, -xmin)\n",
    "    patch_ymin = tf.math.maximum(0, -ymin)\n",
    "    patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "    patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "    heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "    heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "    heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "    heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "    indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "    updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for j in tf.range(patch_ymin, patch_ymax):\n",
    "        for i in tf.range(patch_xmin, patch_xmax):\n",
    "            indices = indices.write(count, [heatmap_ymin + j, heatmap_xmin + i])\n",
    "            updates = updates.write(count, gaussian_patch[j][i])\n",
    "            count += 1\n",
    "\n",
    "    heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def make_heatmaps(features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "    v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "    x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "    y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "    num_heatmap = heatmap_shape[2]\n",
    "    heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "    for i in range(num_heatmap):\n",
    "        gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "        heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "    heatmaps = heatmap_array.stack()\n",
    "    heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0])  # change to (64, 64, 16)\n",
    "\n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ae615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837690e",
   "metadata": {},
   "source": [
    "## 23-6. 모델을 학습해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3962a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c64d5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b41617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5be6e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfa441",
   "metadata": {},
   "source": [
    "## 23-7. 학습 엔진 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d62b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = MODEL_PATH + '/model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27929c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "748fd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97149c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.49702239 epoch total loss 2.49702239\n",
      "Trained batch 2 batch loss 2.54110551 epoch total loss 2.51906395\n",
      "Trained batch 3 batch loss 2.50863361 epoch total loss 2.51558709\n",
      "Trained batch 4 batch loss 2.52499461 epoch total loss 2.51793909\n",
      "Trained batch 5 batch loss 2.44842696 epoch total loss 2.50403666\n",
      "Trained batch 6 batch loss 2.35022879 epoch total loss 2.47840214\n",
      "Trained batch 7 batch loss 2.31007051 epoch total loss 2.45435476\n",
      "Trained batch 8 batch loss 2.09883 epoch total loss 2.40991402\n",
      "Trained batch 9 batch loss 2.13480854 epoch total loss 2.37934661\n",
      "Trained batch 10 batch loss 2.16158509 epoch total loss 2.35757041\n",
      "Trained batch 11 batch loss 2.13173056 epoch total loss 2.33703971\n",
      "Trained batch 12 batch loss 2.17206168 epoch total loss 2.32329154\n",
      "Trained batch 13 batch loss 2.09623647 epoch total loss 2.30582571\n",
      "Trained batch 14 batch loss 1.97244191 epoch total loss 2.2820127\n",
      "Trained batch 15 batch loss 2.0267148 epoch total loss 2.26499271\n",
      "Trained batch 16 batch loss 1.91596818 epoch total loss 2.24317884\n",
      "Trained batch 17 batch loss 2.11212826 epoch total loss 2.23547\n",
      "Trained batch 18 batch loss 2.08272076 epoch total loss 2.22698402\n",
      "Trained batch 19 batch loss 2.02034712 epoch total loss 2.21610832\n",
      "Trained batch 20 batch loss 2.10475183 epoch total loss 2.21054053\n",
      "Trained batch 21 batch loss 2.09463072 epoch total loss 2.20502114\n",
      "Trained batch 22 batch loss 1.97179389 epoch total loss 2.19441986\n",
      "Trained batch 23 batch loss 1.85122824 epoch total loss 2.17949843\n",
      "Trained batch 24 batch loss 1.80984616 epoch total loss 2.16409612\n",
      "Trained batch 25 batch loss 1.73535633 epoch total loss 2.14694667\n",
      "Trained batch 26 batch loss 1.76664126 epoch total loss 2.13231945\n",
      "Trained batch 27 batch loss 1.694628 epoch total loss 2.11610866\n",
      "Trained batch 28 batch loss 1.97734618 epoch total loss 2.11115289\n",
      "Trained batch 29 batch loss 1.8696624 epoch total loss 2.10282564\n",
      "Trained batch 30 batch loss 1.95209527 epoch total loss 2.09780121\n",
      "Trained batch 31 batch loss 1.94816208 epoch total loss 2.09297419\n",
      "Trained batch 32 batch loss 1.95083463 epoch total loss 2.08853245\n",
      "Trained batch 33 batch loss 1.90624177 epoch total loss 2.08300853\n",
      "Trained batch 34 batch loss 1.84962153 epoch total loss 2.07614422\n",
      "Trained batch 35 batch loss 1.8371439 epoch total loss 2.06931567\n",
      "Trained batch 36 batch loss 1.88004911 epoch total loss 2.0640583\n",
      "Trained batch 37 batch loss 1.84386206 epoch total loss 2.05810714\n",
      "Trained batch 38 batch loss 1.8543098 epoch total loss 2.05274391\n",
      "Trained batch 39 batch loss 1.85988069 epoch total loss 2.04779863\n",
      "Trained batch 40 batch loss 1.92082965 epoch total loss 2.04462457\n",
      "Trained batch 41 batch loss 1.872262 epoch total loss 2.04042053\n",
      "Trained batch 42 batch loss 1.7771 epoch total loss 2.03415108\n",
      "Trained batch 43 batch loss 1.80671263 epoch total loss 2.02886176\n",
      "Trained batch 44 batch loss 1.59867454 epoch total loss 2.01908469\n",
      "Trained batch 45 batch loss 1.63559628 epoch total loss 2.01056266\n",
      "Trained batch 46 batch loss 1.73191071 epoch total loss 2.00450492\n",
      "Trained batch 47 batch loss 1.74017072 epoch total loss 1.99888086\n",
      "Trained batch 48 batch loss 1.78100824 epoch total loss 1.99434185\n",
      "Trained batch 49 batch loss 1.79430175 epoch total loss 1.99025941\n",
      "Trained batch 50 batch loss 1.8139205 epoch total loss 1.9867326\n",
      "Trained batch 51 batch loss 1.83548379 epoch total loss 1.98376703\n",
      "Trained batch 52 batch loss 1.82453513 epoch total loss 1.98070478\n",
      "Trained batch 53 batch loss 1.86299145 epoch total loss 1.9784838\n",
      "Trained batch 54 batch loss 1.84007931 epoch total loss 1.9759208\n",
      "Trained batch 55 batch loss 1.74653554 epoch total loss 1.97175014\n",
      "Trained batch 56 batch loss 1.82943118 epoch total loss 1.96920872\n",
      "Trained batch 57 batch loss 1.59197366 epoch total loss 1.96259058\n",
      "Trained batch 58 batch loss 1.71748412 epoch total loss 1.95836461\n",
      "Trained batch 59 batch loss 1.75118 epoch total loss 1.95485294\n",
      "Trained batch 60 batch loss 1.83008575 epoch total loss 1.95277357\n",
      "Trained batch 61 batch loss 1.69557023 epoch total loss 1.94855714\n",
      "Trained batch 62 batch loss 1.63868546 epoch total loss 1.94355917\n",
      "Trained batch 63 batch loss 1.63450825 epoch total loss 1.93865359\n",
      "Trained batch 64 batch loss 1.6689676 epoch total loss 1.93443978\n",
      "Trained batch 65 batch loss 1.71411514 epoch total loss 1.93105018\n",
      "Trained batch 66 batch loss 1.77534461 epoch total loss 1.92869103\n",
      "Trained batch 67 batch loss 1.74448633 epoch total loss 1.92594182\n",
      "Trained batch 68 batch loss 1.7922523 epoch total loss 1.92397583\n",
      "Trained batch 69 batch loss 1.76438677 epoch total loss 1.92166293\n",
      "Trained batch 70 batch loss 1.80130267 epoch total loss 1.91994345\n",
      "Trained batch 71 batch loss 1.78645205 epoch total loss 1.91806328\n",
      "Trained batch 72 batch loss 1.66284561 epoch total loss 1.91451859\n",
      "Trained batch 73 batch loss 1.71845806 epoch total loss 1.91183281\n",
      "Trained batch 74 batch loss 1.6788913 epoch total loss 1.90868497\n",
      "Trained batch 75 batch loss 1.74326 epoch total loss 1.90647924\n",
      "Trained batch 76 batch loss 1.69694161 epoch total loss 1.90372229\n",
      "Trained batch 77 batch loss 1.63210678 epoch total loss 1.90019488\n",
      "Trained batch 78 batch loss 1.68146479 epoch total loss 1.89739048\n",
      "Trained batch 79 batch loss 1.65741491 epoch total loss 1.89435279\n",
      "Trained batch 80 batch loss 1.72001183 epoch total loss 1.89217353\n",
      "Trained batch 81 batch loss 1.72241044 epoch total loss 1.89007771\n",
      "Trained batch 82 batch loss 1.72572589 epoch total loss 1.88807344\n",
      "Trained batch 83 batch loss 1.68797374 epoch total loss 1.88566256\n",
      "Trained batch 84 batch loss 1.72694528 epoch total loss 1.88377309\n",
      "Trained batch 85 batch loss 1.71530676 epoch total loss 1.88179111\n",
      "Trained batch 86 batch loss 1.61678708 epoch total loss 1.87870967\n",
      "Trained batch 87 batch loss 1.52890515 epoch total loss 1.87468886\n",
      "Trained batch 88 batch loss 1.48287761 epoch total loss 1.87023652\n",
      "Trained batch 89 batch loss 1.58387959 epoch total loss 1.86701894\n",
      "Trained batch 90 batch loss 1.73281419 epoch total loss 1.86552787\n",
      "Trained batch 91 batch loss 1.74917293 epoch total loss 1.86424923\n",
      "Trained batch 92 batch loss 1.74436355 epoch total loss 1.86294627\n",
      "Trained batch 93 batch loss 1.68642282 epoch total loss 1.8610481\n",
      "Trained batch 94 batch loss 1.52340174 epoch total loss 1.85745609\n",
      "Trained batch 95 batch loss 1.59201694 epoch total loss 1.85466194\n",
      "Trained batch 96 batch loss 1.75577843 epoch total loss 1.85363197\n",
      "Trained batch 97 batch loss 1.74466801 epoch total loss 1.85250866\n",
      "Trained batch 98 batch loss 1.76169288 epoch total loss 1.85158193\n",
      "Trained batch 99 batch loss 1.65752339 epoch total loss 1.84962165\n",
      "Trained batch 100 batch loss 1.55511546 epoch total loss 1.84667659\n",
      "Trained batch 101 batch loss 1.65551066 epoch total loss 1.84478402\n",
      "Trained batch 102 batch loss 1.70394206 epoch total loss 1.84340322\n",
      "Trained batch 103 batch loss 1.73229206 epoch total loss 1.84232438\n",
      "Trained batch 104 batch loss 1.71225023 epoch total loss 1.84107375\n",
      "Trained batch 105 batch loss 1.69356275 epoch total loss 1.83966875\n",
      "Trained batch 106 batch loss 1.68293893 epoch total loss 1.8381902\n",
      "Trained batch 107 batch loss 1.70134258 epoch total loss 1.8369112\n",
      "Trained batch 108 batch loss 1.65493917 epoch total loss 1.8352263\n",
      "Trained batch 109 batch loss 1.60925603 epoch total loss 1.83315313\n",
      "Trained batch 110 batch loss 1.70731485 epoch total loss 1.8320092\n",
      "Trained batch 111 batch loss 1.71996069 epoch total loss 1.83099973\n",
      "Trained batch 112 batch loss 1.67380071 epoch total loss 1.82959616\n",
      "Trained batch 113 batch loss 1.69649434 epoch total loss 1.82841814\n",
      "Trained batch 114 batch loss 1.74356747 epoch total loss 1.82767379\n",
      "Trained batch 115 batch loss 1.69245231 epoch total loss 1.82649803\n",
      "Trained batch 116 batch loss 1.68302369 epoch total loss 1.82526124\n",
      "Trained batch 117 batch loss 1.70843923 epoch total loss 1.82426274\n",
      "Trained batch 118 batch loss 1.73510528 epoch total loss 1.82350719\n",
      "Trained batch 119 batch loss 1.57830477 epoch total loss 1.82144666\n",
      "Trained batch 120 batch loss 1.55742359 epoch total loss 1.81924641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 121 batch loss 1.67897177 epoch total loss 1.8180871\n",
      "Trained batch 122 batch loss 1.7025969 epoch total loss 1.81714046\n",
      "Trained batch 123 batch loss 1.76104414 epoch total loss 1.81668437\n",
      "Trained batch 124 batch loss 1.71854472 epoch total loss 1.81589293\n",
      "Trained batch 125 batch loss 1.64256656 epoch total loss 1.81450629\n",
      "Trained batch 126 batch loss 1.63776469 epoch total loss 1.81310368\n",
      "Trained batch 127 batch loss 1.60722113 epoch total loss 1.81148255\n",
      "Trained batch 128 batch loss 1.75413215 epoch total loss 1.81103456\n",
      "Trained batch 129 batch loss 1.7458657 epoch total loss 1.81052935\n",
      "Trained batch 130 batch loss 1.67092133 epoch total loss 1.80945551\n",
      "Trained batch 131 batch loss 1.65195143 epoch total loss 1.80825317\n",
      "Trained batch 132 batch loss 1.65652895 epoch total loss 1.80710375\n",
      "Trained batch 133 batch loss 1.68969345 epoch total loss 1.80622101\n",
      "Trained batch 134 batch loss 1.63701284 epoch total loss 1.80495822\n",
      "Trained batch 135 batch loss 1.6234777 epoch total loss 1.8036139\n",
      "Trained batch 136 batch loss 1.76019216 epoch total loss 1.80329454\n",
      "Trained batch 137 batch loss 1.77240992 epoch total loss 1.80306923\n",
      "Trained batch 138 batch loss 1.76783419 epoch total loss 1.80281389\n",
      "Trained batch 139 batch loss 1.75965095 epoch total loss 1.80250335\n",
      "Trained batch 140 batch loss 1.62240922 epoch total loss 1.80121684\n",
      "Trained batch 141 batch loss 1.69340014 epoch total loss 1.80045223\n",
      "Trained batch 142 batch loss 1.69921422 epoch total loss 1.79973936\n",
      "Trained batch 143 batch loss 1.68314767 epoch total loss 1.79892397\n",
      "Trained batch 144 batch loss 1.62805 epoch total loss 1.79773736\n",
      "Trained batch 145 batch loss 1.64349866 epoch total loss 1.79667354\n",
      "Trained batch 146 batch loss 1.73351789 epoch total loss 1.79624104\n",
      "Trained batch 147 batch loss 1.74113727 epoch total loss 1.79586625\n",
      "Trained batch 148 batch loss 1.75930381 epoch total loss 1.79561925\n",
      "Trained batch 149 batch loss 1.64470935 epoch total loss 1.79460645\n",
      "Trained batch 150 batch loss 1.62482548 epoch total loss 1.79347456\n",
      "Trained batch 151 batch loss 1.64429462 epoch total loss 1.79248655\n",
      "Trained batch 152 batch loss 1.58634019 epoch total loss 1.7911303\n",
      "Trained batch 153 batch loss 1.61075544 epoch total loss 1.78995132\n",
      "Trained batch 154 batch loss 1.57144487 epoch total loss 1.78853238\n",
      "Trained batch 155 batch loss 1.65711737 epoch total loss 1.78768444\n",
      "Trained batch 156 batch loss 1.56681085 epoch total loss 1.78626859\n",
      "Trained batch 157 batch loss 1.63408911 epoch total loss 1.7852993\n",
      "Trained batch 158 batch loss 1.56744266 epoch total loss 1.78392053\n",
      "Trained batch 159 batch loss 1.64573848 epoch total loss 1.78305149\n",
      "Trained batch 160 batch loss 1.60650122 epoch total loss 1.78194809\n",
      "Trained batch 161 batch loss 1.59686887 epoch total loss 1.78079844\n",
      "Trained batch 162 batch loss 1.69207537 epoch total loss 1.78025079\n",
      "Trained batch 163 batch loss 1.60623145 epoch total loss 1.77918327\n",
      "Trained batch 164 batch loss 1.54506123 epoch total loss 1.77775574\n",
      "Trained batch 165 batch loss 1.59945047 epoch total loss 1.77667511\n",
      "Trained batch 166 batch loss 1.72906351 epoch total loss 1.77638829\n",
      "Trained batch 167 batch loss 1.71181393 epoch total loss 1.77600169\n",
      "Trained batch 168 batch loss 1.71795213 epoch total loss 1.77565622\n",
      "Trained batch 169 batch loss 1.69832337 epoch total loss 1.7751987\n",
      "Trained batch 170 batch loss 1.58932793 epoch total loss 1.77410531\n",
      "Trained batch 171 batch loss 1.60643935 epoch total loss 1.77312481\n",
      "Trained batch 172 batch loss 1.57200885 epoch total loss 1.77195561\n",
      "Trained batch 173 batch loss 1.51034307 epoch total loss 1.77044344\n",
      "Trained batch 174 batch loss 1.62223792 epoch total loss 1.76959157\n",
      "Trained batch 175 batch loss 1.48160815 epoch total loss 1.76794589\n",
      "Trained batch 176 batch loss 1.70771861 epoch total loss 1.76760375\n",
      "Trained batch 177 batch loss 1.5887326 epoch total loss 1.76659334\n",
      "Trained batch 178 batch loss 1.65339446 epoch total loss 1.76595724\n",
      "Trained batch 179 batch loss 1.68926752 epoch total loss 1.7655288\n",
      "Trained batch 180 batch loss 1.70482063 epoch total loss 1.76519167\n",
      "Trained batch 181 batch loss 1.56641078 epoch total loss 1.7640934\n",
      "Trained batch 182 batch loss 1.53416634 epoch total loss 1.76283014\n",
      "Trained batch 183 batch loss 1.59498894 epoch total loss 1.76191306\n",
      "Trained batch 184 batch loss 1.62096262 epoch total loss 1.76114702\n",
      "Trained batch 185 batch loss 1.67102432 epoch total loss 1.76065981\n",
      "Trained batch 186 batch loss 1.67538691 epoch total loss 1.76020145\n",
      "Trained batch 187 batch loss 1.69353747 epoch total loss 1.7598449\n",
      "Trained batch 188 batch loss 1.73872399 epoch total loss 1.75973272\n",
      "Trained batch 189 batch loss 1.73521841 epoch total loss 1.75960302\n",
      "Trained batch 190 batch loss 1.73616767 epoch total loss 1.75947976\n",
      "Trained batch 191 batch loss 1.62453425 epoch total loss 1.75877321\n",
      "Trained batch 192 batch loss 1.68331289 epoch total loss 1.75838029\n",
      "Trained batch 193 batch loss 1.68094206 epoch total loss 1.75797904\n",
      "Trained batch 194 batch loss 1.63318825 epoch total loss 1.75733566\n",
      "Trained batch 195 batch loss 1.56278515 epoch total loss 1.756338\n",
      "Trained batch 196 batch loss 1.44492352 epoch total loss 1.75474906\n",
      "Trained batch 197 batch loss 1.53879368 epoch total loss 1.75365281\n",
      "Trained batch 198 batch loss 1.62345576 epoch total loss 1.75299513\n",
      "Trained batch 199 batch loss 1.69378603 epoch total loss 1.75269771\n",
      "Trained batch 200 batch loss 1.66517818 epoch total loss 1.75226009\n",
      "Trained batch 201 batch loss 1.64726758 epoch total loss 1.75173783\n",
      "Trained batch 202 batch loss 1.75046539 epoch total loss 1.75173151\n",
      "Trained batch 203 batch loss 1.75207901 epoch total loss 1.75173318\n",
      "Trained batch 204 batch loss 1.64457119 epoch total loss 1.75120783\n",
      "Trained batch 205 batch loss 1.68183148 epoch total loss 1.75086939\n",
      "Trained batch 206 batch loss 1.71216202 epoch total loss 1.75068152\n",
      "Trained batch 207 batch loss 1.70721829 epoch total loss 1.75047147\n",
      "Trained batch 208 batch loss 1.64159989 epoch total loss 1.74994802\n",
      "Trained batch 209 batch loss 1.65820408 epoch total loss 1.7495091\n",
      "Trained batch 210 batch loss 1.44984162 epoch total loss 1.74808204\n",
      "Trained batch 211 batch loss 1.4933778 epoch total loss 1.74687493\n",
      "Trained batch 212 batch loss 1.60805094 epoch total loss 1.74622011\n",
      "Trained batch 213 batch loss 1.59396935 epoch total loss 1.74550533\n",
      "Trained batch 214 batch loss 1.68128145 epoch total loss 1.74520516\n",
      "Trained batch 215 batch loss 1.64799666 epoch total loss 1.74475312\n",
      "Trained batch 216 batch loss 1.64544344 epoch total loss 1.74429333\n",
      "Trained batch 217 batch loss 1.56970751 epoch total loss 1.74348879\n",
      "Trained batch 218 batch loss 1.48591125 epoch total loss 1.74230719\n",
      "Trained batch 219 batch loss 1.59763634 epoch total loss 1.74164653\n",
      "Trained batch 220 batch loss 1.56355023 epoch total loss 1.74083698\n",
      "Trained batch 221 batch loss 1.50716031 epoch total loss 1.73977959\n",
      "Trained batch 222 batch loss 1.59489501 epoch total loss 1.73912704\n",
      "Trained batch 223 batch loss 1.64953625 epoch total loss 1.7387253\n",
      "Trained batch 224 batch loss 1.52787936 epoch total loss 1.73778415\n",
      "Trained batch 225 batch loss 1.61865926 epoch total loss 1.73725462\n",
      "Trained batch 226 batch loss 1.55678725 epoch total loss 1.73645616\n",
      "Trained batch 227 batch loss 1.45044589 epoch total loss 1.73519611\n",
      "Trained batch 228 batch loss 1.47860169 epoch total loss 1.73407078\n",
      "Trained batch 229 batch loss 1.4705894 epoch total loss 1.73292017\n",
      "Trained batch 230 batch loss 1.57088375 epoch total loss 1.73221564\n",
      "Trained batch 231 batch loss 1.52862382 epoch total loss 1.73133433\n",
      "Trained batch 232 batch loss 1.57930171 epoch total loss 1.73067904\n",
      "Trained batch 233 batch loss 1.60953104 epoch total loss 1.73015916\n",
      "Trained batch 234 batch loss 1.58083439 epoch total loss 1.72952104\n",
      "Trained batch 235 batch loss 1.6008085 epoch total loss 1.72897327\n",
      "Trained batch 236 batch loss 1.64303589 epoch total loss 1.72860909\n",
      "Trained batch 237 batch loss 1.73707008 epoch total loss 1.72864473\n",
      "Trained batch 238 batch loss 1.75285053 epoch total loss 1.72874641\n",
      "Trained batch 239 batch loss 1.73456299 epoch total loss 1.72877073\n",
      "Trained batch 240 batch loss 1.56411874 epoch total loss 1.72808468\n",
      "Trained batch 241 batch loss 1.61017954 epoch total loss 1.72759545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 242 batch loss 1.67285061 epoch total loss 1.72736919\n",
      "Trained batch 243 batch loss 1.66066873 epoch total loss 1.72709477\n",
      "Trained batch 244 batch loss 1.66324103 epoch total loss 1.72683299\n",
      "Trained batch 245 batch loss 1.65649843 epoch total loss 1.72654593\n",
      "Trained batch 246 batch loss 1.64005208 epoch total loss 1.72619426\n",
      "Trained batch 247 batch loss 1.72039247 epoch total loss 1.72617078\n",
      "Trained batch 248 batch loss 1.66702294 epoch total loss 1.72593236\n",
      "Trained batch 249 batch loss 1.6674031 epoch total loss 1.72569728\n",
      "Trained batch 250 batch loss 1.55867159 epoch total loss 1.72502923\n",
      "Trained batch 251 batch loss 1.49458635 epoch total loss 1.72411108\n",
      "Trained batch 252 batch loss 1.48701227 epoch total loss 1.72317016\n",
      "Trained batch 253 batch loss 1.42908025 epoch total loss 1.72200775\n",
      "Trained batch 254 batch loss 1.51489639 epoch total loss 1.72119236\n",
      "Trained batch 255 batch loss 1.55630231 epoch total loss 1.72054577\n",
      "Trained batch 256 batch loss 1.52326334 epoch total loss 1.71977508\n",
      "Trained batch 257 batch loss 1.53942239 epoch total loss 1.7190733\n",
      "Trained batch 258 batch loss 1.55716109 epoch total loss 1.71844578\n",
      "Trained batch 259 batch loss 1.55752 epoch total loss 1.71782446\n",
      "Trained batch 260 batch loss 1.41364622 epoch total loss 1.71665454\n",
      "Trained batch 261 batch loss 1.24585116 epoch total loss 1.71485066\n",
      "Trained batch 262 batch loss 1.2803551 epoch total loss 1.71319234\n",
      "Trained batch 263 batch loss 1.40840602 epoch total loss 1.71203351\n",
      "Trained batch 264 batch loss 1.65961146 epoch total loss 1.71183491\n",
      "Trained batch 265 batch loss 1.718418 epoch total loss 1.7118597\n",
      "Trained batch 266 batch loss 1.73325193 epoch total loss 1.71194\n",
      "Trained batch 267 batch loss 1.67454565 epoch total loss 1.7118001\n",
      "Trained batch 268 batch loss 1.47696733 epoch total loss 1.71092379\n",
      "Trained batch 269 batch loss 1.59345233 epoch total loss 1.71048713\n",
      "Trained batch 270 batch loss 1.71289551 epoch total loss 1.71049607\n",
      "Trained batch 271 batch loss 1.68680668 epoch total loss 1.71040857\n",
      "Trained batch 272 batch loss 1.70705986 epoch total loss 1.71039629\n",
      "Trained batch 273 batch loss 1.6695925 epoch total loss 1.7102468\n",
      "Trained batch 274 batch loss 1.7017771 epoch total loss 1.71021593\n",
      "Trained batch 275 batch loss 1.62701404 epoch total loss 1.70991337\n",
      "Trained batch 276 batch loss 1.66890037 epoch total loss 1.70976484\n",
      "Trained batch 277 batch loss 1.60775185 epoch total loss 1.70939648\n",
      "Trained batch 278 batch loss 1.58928084 epoch total loss 1.70896447\n",
      "Trained batch 279 batch loss 1.64644802 epoch total loss 1.70874047\n",
      "Trained batch 280 batch loss 1.68206823 epoch total loss 1.70864522\n",
      "Trained batch 281 batch loss 1.48734224 epoch total loss 1.70785761\n",
      "Trained batch 282 batch loss 1.47811747 epoch total loss 1.70704293\n",
      "Trained batch 283 batch loss 1.6625874 epoch total loss 1.70688593\n",
      "Trained batch 284 batch loss 1.65304291 epoch total loss 1.70669627\n",
      "Trained batch 285 batch loss 1.67037785 epoch total loss 1.70656884\n",
      "Trained batch 286 batch loss 1.64263415 epoch total loss 1.70634532\n",
      "Trained batch 287 batch loss 1.60212421 epoch total loss 1.70598221\n",
      "Trained batch 288 batch loss 1.56939554 epoch total loss 1.70550787\n",
      "Trained batch 289 batch loss 1.67230821 epoch total loss 1.70539296\n",
      "Trained batch 290 batch loss 1.67940712 epoch total loss 1.70530343\n",
      "Trained batch 291 batch loss 1.71525979 epoch total loss 1.70533764\n",
      "Trained batch 292 batch loss 1.71799195 epoch total loss 1.70538104\n",
      "Trained batch 293 batch loss 1.68702567 epoch total loss 1.70531833\n",
      "Trained batch 294 batch loss 1.68574965 epoch total loss 1.70525181\n",
      "Trained batch 295 batch loss 1.66665208 epoch total loss 1.70512092\n",
      "Trained batch 296 batch loss 1.5842526 epoch total loss 1.70471263\n",
      "Trained batch 297 batch loss 1.55948448 epoch total loss 1.70422363\n",
      "Trained batch 298 batch loss 1.62641931 epoch total loss 1.70396256\n",
      "Trained batch 299 batch loss 1.7397933 epoch total loss 1.70408249\n",
      "Trained batch 300 batch loss 1.68795633 epoch total loss 1.70402873\n",
      "Trained batch 301 batch loss 1.63310266 epoch total loss 1.70379317\n",
      "Trained batch 302 batch loss 1.62792635 epoch total loss 1.70354199\n",
      "Trained batch 303 batch loss 1.63359582 epoch total loss 1.70331109\n",
      "Trained batch 304 batch loss 1.642699 epoch total loss 1.70311177\n",
      "Trained batch 305 batch loss 1.66658545 epoch total loss 1.70299196\n",
      "Trained batch 306 batch loss 1.641482 epoch total loss 1.70279086\n",
      "Trained batch 307 batch loss 1.63827133 epoch total loss 1.70258069\n",
      "Trained batch 308 batch loss 1.61367273 epoch total loss 1.70229197\n",
      "Trained batch 309 batch loss 1.62384319 epoch total loss 1.70203805\n",
      "Trained batch 310 batch loss 1.60288 epoch total loss 1.70171821\n",
      "Trained batch 311 batch loss 1.56882584 epoch total loss 1.70129097\n",
      "Trained batch 312 batch loss 1.62169623 epoch total loss 1.70103586\n",
      "Trained batch 313 batch loss 1.58042288 epoch total loss 1.70065069\n",
      "Trained batch 314 batch loss 1.56802273 epoch total loss 1.70022821\n",
      "Trained batch 315 batch loss 1.52736378 epoch total loss 1.69967937\n",
      "Trained batch 316 batch loss 1.5117445 epoch total loss 1.69908452\n",
      "Trained batch 317 batch loss 1.53101158 epoch total loss 1.69855428\n",
      "Trained batch 318 batch loss 1.59165776 epoch total loss 1.69821823\n",
      "Trained batch 319 batch loss 1.5621593 epoch total loss 1.69779158\n",
      "Trained batch 320 batch loss 1.53771842 epoch total loss 1.69729137\n",
      "Trained batch 321 batch loss 1.56644797 epoch total loss 1.6968838\n",
      "Trained batch 322 batch loss 1.53372312 epoch total loss 1.69637716\n",
      "Trained batch 323 batch loss 1.49424052 epoch total loss 1.69575143\n",
      "Trained batch 324 batch loss 1.57419658 epoch total loss 1.6953764\n",
      "Trained batch 325 batch loss 1.65530038 epoch total loss 1.69525301\n",
      "Trained batch 326 batch loss 1.73220479 epoch total loss 1.69536626\n",
      "Trained batch 327 batch loss 1.57797086 epoch total loss 1.69500709\n",
      "Trained batch 328 batch loss 1.647627 epoch total loss 1.69486272\n",
      "Trained batch 329 batch loss 1.65527105 epoch total loss 1.69474244\n",
      "Trained batch 330 batch loss 1.59604979 epoch total loss 1.69444335\n",
      "Trained batch 331 batch loss 1.68756485 epoch total loss 1.6944226\n",
      "Trained batch 332 batch loss 1.66584873 epoch total loss 1.69433653\n",
      "Trained batch 333 batch loss 1.60887909 epoch total loss 1.69407988\n",
      "Trained batch 334 batch loss 1.52991414 epoch total loss 1.69358838\n",
      "Trained batch 335 batch loss 1.41271567 epoch total loss 1.69275\n",
      "Trained batch 336 batch loss 1.40241194 epoch total loss 1.69188583\n",
      "Trained batch 337 batch loss 1.47490859 epoch total loss 1.69124198\n",
      "Trained batch 338 batch loss 1.35646272 epoch total loss 1.69025147\n",
      "Trained batch 339 batch loss 1.37683535 epoch total loss 1.68932688\n",
      "Trained batch 340 batch loss 1.3174808 epoch total loss 1.68823338\n",
      "Trained batch 341 batch loss 1.269701 epoch total loss 1.687006\n",
      "Trained batch 342 batch loss 1.55057919 epoch total loss 1.68660712\n",
      "Trained batch 343 batch loss 1.55352521 epoch total loss 1.6862191\n",
      "Trained batch 344 batch loss 1.64764237 epoch total loss 1.68610704\n",
      "Trained batch 345 batch loss 1.5597508 epoch total loss 1.68574083\n",
      "Trained batch 346 batch loss 1.60706806 epoch total loss 1.68551338\n",
      "Trained batch 347 batch loss 1.54789817 epoch total loss 1.68511677\n",
      "Trained batch 348 batch loss 1.57410336 epoch total loss 1.68479776\n",
      "Trained batch 349 batch loss 1.56076014 epoch total loss 1.68444228\n",
      "Trained batch 350 batch loss 1.64585114 epoch total loss 1.68433213\n",
      "Trained batch 351 batch loss 1.63461852 epoch total loss 1.68419051\n",
      "Trained batch 352 batch loss 1.68548679 epoch total loss 1.68419421\n",
      "Trained batch 353 batch loss 1.67409432 epoch total loss 1.6841656\n",
      "Trained batch 354 batch loss 1.61267233 epoch total loss 1.68396354\n",
      "Trained batch 355 batch loss 1.56525421 epoch total loss 1.68362916\n",
      "Trained batch 356 batch loss 1.56329167 epoch total loss 1.6832912\n",
      "Trained batch 357 batch loss 1.61564362 epoch total loss 1.68310165\n",
      "Trained batch 358 batch loss 1.66927636 epoch total loss 1.68306303\n",
      "Trained batch 359 batch loss 1.64722574 epoch total loss 1.68296313\n",
      "Trained batch 360 batch loss 1.70948672 epoch total loss 1.6830368\n",
      "Trained batch 361 batch loss 1.7721138 epoch total loss 1.68328345\n",
      "Trained batch 362 batch loss 1.7297821 epoch total loss 1.68341196\n",
      "Trained batch 363 batch loss 1.72174454 epoch total loss 1.68351758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 364 batch loss 1.6668818 epoch total loss 1.6834718\n",
      "Trained batch 365 batch loss 1.78058541 epoch total loss 1.68373787\n",
      "Trained batch 366 batch loss 1.73662484 epoch total loss 1.68388247\n",
      "Trained batch 367 batch loss 1.69045854 epoch total loss 1.68390024\n",
      "Trained batch 368 batch loss 1.55387247 epoch total loss 1.68354702\n",
      "Trained batch 369 batch loss 1.61811972 epoch total loss 1.68336964\n",
      "Trained batch 370 batch loss 1.47471488 epoch total loss 1.68280578\n",
      "Trained batch 371 batch loss 1.61180258 epoch total loss 1.68261445\n",
      "Trained batch 372 batch loss 1.65110254 epoch total loss 1.68252969\n",
      "Trained batch 373 batch loss 1.55992401 epoch total loss 1.68220103\n",
      "Trained batch 374 batch loss 1.56025362 epoch total loss 1.68187499\n",
      "Trained batch 375 batch loss 1.48564136 epoch total loss 1.68135178\n",
      "Trained batch 376 batch loss 1.51377368 epoch total loss 1.68090606\n",
      "Trained batch 377 batch loss 1.60347652 epoch total loss 1.68070066\n",
      "Trained batch 378 batch loss 1.53725088 epoch total loss 1.6803211\n",
      "Trained batch 379 batch loss 1.65097523 epoch total loss 1.68024373\n",
      "Trained batch 380 batch loss 1.61884594 epoch total loss 1.68008208\n",
      "Trained batch 381 batch loss 1.55114102 epoch total loss 1.67974377\n",
      "Trained batch 382 batch loss 1.54206419 epoch total loss 1.67938328\n",
      "Trained batch 383 batch loss 1.60776579 epoch total loss 1.67919636\n",
      "Trained batch 384 batch loss 1.59483385 epoch total loss 1.67897666\n",
      "Trained batch 385 batch loss 1.62310433 epoch total loss 1.67883158\n",
      "Trained batch 386 batch loss 1.6350565 epoch total loss 1.67871821\n",
      "Trained batch 387 batch loss 1.61009192 epoch total loss 1.67854095\n",
      "Trained batch 388 batch loss 1.53049445 epoch total loss 1.67815948\n",
      "Trained batch 389 batch loss 1.44701087 epoch total loss 1.67756522\n",
      "Trained batch 390 batch loss 1.61376309 epoch total loss 1.67740166\n",
      "Trained batch 391 batch loss 1.62346745 epoch total loss 1.67726374\n",
      "Trained batch 392 batch loss 1.61936891 epoch total loss 1.67711604\n",
      "Trained batch 393 batch loss 1.59920406 epoch total loss 1.67691779\n",
      "Trained batch 394 batch loss 1.41716254 epoch total loss 1.67625856\n",
      "Trained batch 395 batch loss 1.39001536 epoch total loss 1.67553389\n",
      "Trained batch 396 batch loss 1.51756918 epoch total loss 1.67513502\n",
      "Trained batch 397 batch loss 1.38560581 epoch total loss 1.67440569\n",
      "Trained batch 398 batch loss 1.62631869 epoch total loss 1.67428493\n",
      "Trained batch 399 batch loss 1.63775682 epoch total loss 1.67419338\n",
      "Trained batch 400 batch loss 1.59556377 epoch total loss 1.67399693\n",
      "Trained batch 401 batch loss 1.56785893 epoch total loss 1.67373228\n",
      "Trained batch 402 batch loss 1.66208315 epoch total loss 1.67370331\n",
      "Trained batch 403 batch loss 1.59682226 epoch total loss 1.67351246\n",
      "Trained batch 404 batch loss 1.59524798 epoch total loss 1.67331886\n",
      "Trained batch 405 batch loss 1.64480102 epoch total loss 1.67324841\n",
      "Trained batch 406 batch loss 1.58542776 epoch total loss 1.67303216\n",
      "Trained batch 407 batch loss 1.51173472 epoch total loss 1.67263579\n",
      "Trained batch 408 batch loss 1.49501467 epoch total loss 1.67220032\n",
      "Trained batch 409 batch loss 1.58160341 epoch total loss 1.67197883\n",
      "Trained batch 410 batch loss 1.53610265 epoch total loss 1.67164755\n",
      "Trained batch 411 batch loss 1.50022399 epoch total loss 1.67123044\n",
      "Trained batch 412 batch loss 1.52113187 epoch total loss 1.67086613\n",
      "Trained batch 413 batch loss 1.64661443 epoch total loss 1.67080736\n",
      "Trained batch 414 batch loss 1.59586859 epoch total loss 1.6706264\n",
      "Trained batch 415 batch loss 1.57083356 epoch total loss 1.67038608\n",
      "Trained batch 416 batch loss 1.51284051 epoch total loss 1.67000723\n",
      "Trained batch 417 batch loss 1.58334661 epoch total loss 1.66979957\n",
      "Trained batch 418 batch loss 1.60906243 epoch total loss 1.66965425\n",
      "Trained batch 419 batch loss 1.5341959 epoch total loss 1.66933095\n",
      "Trained batch 420 batch loss 1.5130142 epoch total loss 1.66895866\n",
      "Trained batch 421 batch loss 1.6047132 epoch total loss 1.66880608\n",
      "Trained batch 422 batch loss 1.56289792 epoch total loss 1.66855526\n",
      "Trained batch 423 batch loss 1.54185653 epoch total loss 1.66825569\n",
      "Trained batch 424 batch loss 1.59856796 epoch total loss 1.66809142\n",
      "Trained batch 425 batch loss 1.65524602 epoch total loss 1.66806126\n",
      "Trained batch 426 batch loss 1.717803 epoch total loss 1.66817796\n",
      "Trained batch 427 batch loss 1.67280662 epoch total loss 1.66818869\n",
      "Trained batch 428 batch loss 1.60038769 epoch total loss 1.66803038\n",
      "Trained batch 429 batch loss 1.47339356 epoch total loss 1.66757667\n",
      "Trained batch 430 batch loss 1.43840361 epoch total loss 1.66704369\n",
      "Trained batch 431 batch loss 1.57812977 epoch total loss 1.66683745\n",
      "Trained batch 432 batch loss 1.61163068 epoch total loss 1.66670966\n",
      "Trained batch 433 batch loss 1.64799452 epoch total loss 1.66666639\n",
      "Trained batch 434 batch loss 1.65703666 epoch total loss 1.66664422\n",
      "Trained batch 435 batch loss 1.59113526 epoch total loss 1.66647065\n",
      "Trained batch 436 batch loss 1.56984067 epoch total loss 1.66624904\n",
      "Trained batch 437 batch loss 1.55758667 epoch total loss 1.66600025\n",
      "Trained batch 438 batch loss 1.34493721 epoch total loss 1.66526723\n",
      "Trained batch 439 batch loss 1.56056762 epoch total loss 1.66502869\n",
      "Trained batch 440 batch loss 1.53719723 epoch total loss 1.66473806\n",
      "Trained batch 441 batch loss 1.46681428 epoch total loss 1.66428924\n",
      "Trained batch 442 batch loss 1.3470974 epoch total loss 1.6635716\n",
      "Trained batch 443 batch loss 1.34080446 epoch total loss 1.66284299\n",
      "Trained batch 444 batch loss 1.45109928 epoch total loss 1.66236615\n",
      "Trained batch 445 batch loss 1.45124912 epoch total loss 1.6618917\n",
      "Trained batch 446 batch loss 1.35446513 epoch total loss 1.66120243\n",
      "Trained batch 447 batch loss 1.52496612 epoch total loss 1.66089773\n",
      "Trained batch 448 batch loss 1.47647119 epoch total loss 1.6604861\n",
      "Trained batch 449 batch loss 1.50426793 epoch total loss 1.66013813\n",
      "Trained batch 450 batch loss 1.49053073 epoch total loss 1.65976131\n",
      "Trained batch 451 batch loss 1.50853062 epoch total loss 1.65942597\n",
      "Trained batch 452 batch loss 1.48188019 epoch total loss 1.65903318\n",
      "Trained batch 453 batch loss 1.62265527 epoch total loss 1.65895295\n",
      "Trained batch 454 batch loss 1.66353834 epoch total loss 1.65896297\n",
      "Trained batch 455 batch loss 1.6940999 epoch total loss 1.65904021\n",
      "Trained batch 456 batch loss 1.67451787 epoch total loss 1.65907407\n",
      "Trained batch 457 batch loss 1.70304108 epoch total loss 1.65917039\n",
      "Trained batch 458 batch loss 1.65300655 epoch total loss 1.65915692\n",
      "Trained batch 459 batch loss 1.55435157 epoch total loss 1.65892851\n",
      "Trained batch 460 batch loss 1.5821898 epoch total loss 1.65876174\n",
      "Trained batch 461 batch loss 1.52554011 epoch total loss 1.65847266\n",
      "Trained batch 462 batch loss 1.56291115 epoch total loss 1.65826583\n",
      "Trained batch 463 batch loss 1.58873367 epoch total loss 1.65811574\n",
      "Trained batch 464 batch loss 1.56579316 epoch total loss 1.65791678\n",
      "Trained batch 465 batch loss 1.64494383 epoch total loss 1.65788889\n",
      "Trained batch 466 batch loss 1.6029104 epoch total loss 1.65777087\n",
      "Trained batch 467 batch loss 1.49903607 epoch total loss 1.65743101\n",
      "Trained batch 468 batch loss 1.64919198 epoch total loss 1.65741336\n",
      "Trained batch 469 batch loss 1.63969123 epoch total loss 1.65737557\n",
      "Trained batch 470 batch loss 1.56336689 epoch total loss 1.65717554\n",
      "Trained batch 471 batch loss 1.62545645 epoch total loss 1.65710807\n",
      "Trained batch 472 batch loss 1.57234406 epoch total loss 1.65692854\n",
      "Trained batch 473 batch loss 1.52368367 epoch total loss 1.65664685\n",
      "Trained batch 474 batch loss 1.57671 epoch total loss 1.65647817\n",
      "Trained batch 475 batch loss 1.53341186 epoch total loss 1.65621901\n",
      "Trained batch 476 batch loss 1.59135509 epoch total loss 1.65608275\n",
      "Trained batch 477 batch loss 1.66341364 epoch total loss 1.65609813\n",
      "Trained batch 478 batch loss 1.61002111 epoch total loss 1.65600181\n",
      "Trained batch 479 batch loss 1.6252166 epoch total loss 1.65593755\n",
      "Trained batch 480 batch loss 1.4935627 epoch total loss 1.65559936\n",
      "Trained batch 481 batch loss 1.58266091 epoch total loss 1.6554476\n",
      "Trained batch 482 batch loss 1.49765575 epoch total loss 1.65512037\n",
      "Trained batch 483 batch loss 1.42517173 epoch total loss 1.65464425\n",
      "Trained batch 484 batch loss 1.48073244 epoch total loss 1.65428495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 485 batch loss 1.47712564 epoch total loss 1.65391958\n",
      "Trained batch 486 batch loss 1.60836017 epoch total loss 1.65382576\n",
      "Trained batch 487 batch loss 1.49609578 epoch total loss 1.65350187\n",
      "Trained batch 488 batch loss 1.52606416 epoch total loss 1.6532408\n",
      "Trained batch 489 batch loss 1.50624108 epoch total loss 1.65294015\n",
      "Trained batch 490 batch loss 1.5855031 epoch total loss 1.65280247\n",
      "Trained batch 491 batch loss 1.55257297 epoch total loss 1.65259838\n",
      "Trained batch 492 batch loss 1.52263045 epoch total loss 1.65233421\n",
      "Trained batch 493 batch loss 1.45849395 epoch total loss 1.65194106\n",
      "Trained batch 494 batch loss 1.58462143 epoch total loss 1.65180469\n",
      "Trained batch 495 batch loss 1.54995668 epoch total loss 1.65159893\n",
      "Trained batch 496 batch loss 1.52354 epoch total loss 1.65134072\n",
      "Trained batch 497 batch loss 1.63889337 epoch total loss 1.65131569\n",
      "Trained batch 498 batch loss 1.68223667 epoch total loss 1.6513778\n",
      "Trained batch 499 batch loss 1.61124802 epoch total loss 1.65129745\n",
      "Trained batch 500 batch loss 1.58661902 epoch total loss 1.65116811\n",
      "Trained batch 501 batch loss 1.51607394 epoch total loss 1.65089846\n",
      "Trained batch 502 batch loss 1.54026604 epoch total loss 1.65067804\n",
      "Trained batch 503 batch loss 1.57055509 epoch total loss 1.65051877\n",
      "Trained batch 504 batch loss 1.58138847 epoch total loss 1.65038157\n",
      "Trained batch 505 batch loss 1.5464927 epoch total loss 1.65017581\n",
      "Trained batch 506 batch loss 1.53662336 epoch total loss 1.64995146\n",
      "Trained batch 507 batch loss 1.5142622 epoch total loss 1.64968383\n",
      "Trained batch 508 batch loss 1.48886764 epoch total loss 1.64936733\n",
      "Trained batch 509 batch loss 1.57283115 epoch total loss 1.64921689\n",
      "Trained batch 510 batch loss 1.47217989 epoch total loss 1.64886975\n",
      "Trained batch 511 batch loss 1.55777645 epoch total loss 1.64869153\n",
      "Trained batch 512 batch loss 1.44178557 epoch total loss 1.64828742\n",
      "Trained batch 513 batch loss 1.50739419 epoch total loss 1.64801276\n",
      "Trained batch 514 batch loss 1.48660541 epoch total loss 1.64769876\n",
      "Trained batch 515 batch loss 1.60986 epoch total loss 1.64762533\n",
      "Trained batch 516 batch loss 1.60056341 epoch total loss 1.64753413\n",
      "Trained batch 517 batch loss 1.62244725 epoch total loss 1.64748561\n",
      "Trained batch 518 batch loss 1.62842226 epoch total loss 1.64744878\n",
      "Trained batch 519 batch loss 1.46254 epoch total loss 1.64709246\n",
      "Trained batch 520 batch loss 1.48735082 epoch total loss 1.64678538\n",
      "Trained batch 521 batch loss 1.51311183 epoch total loss 1.64652872\n",
      "Trained batch 522 batch loss 1.54011059 epoch total loss 1.64632487\n",
      "Trained batch 523 batch loss 1.60763884 epoch total loss 1.64625096\n",
      "Trained batch 524 batch loss 1.5771383 epoch total loss 1.64611912\n",
      "Trained batch 525 batch loss 1.59916711 epoch total loss 1.64602971\n",
      "Trained batch 526 batch loss 1.5672828 epoch total loss 1.64588\n",
      "Trained batch 527 batch loss 1.58450365 epoch total loss 1.64576352\n",
      "Trained batch 528 batch loss 1.62283635 epoch total loss 1.64572012\n",
      "Trained batch 529 batch loss 1.556288 epoch total loss 1.64555109\n",
      "Trained batch 530 batch loss 1.6077255 epoch total loss 1.64547968\n",
      "Trained batch 531 batch loss 1.60268521 epoch total loss 1.64539909\n",
      "Trained batch 532 batch loss 1.5834465 epoch total loss 1.64528263\n",
      "Trained batch 533 batch loss 1.54174018 epoch total loss 1.64508832\n",
      "Trained batch 534 batch loss 1.5683701 epoch total loss 1.64494467\n",
      "Trained batch 535 batch loss 1.53657138 epoch total loss 1.64474213\n",
      "Trained batch 536 batch loss 1.6184 epoch total loss 1.6446929\n",
      "Trained batch 537 batch loss 1.61829257 epoch total loss 1.64464378\n",
      "Trained batch 538 batch loss 1.50939167 epoch total loss 1.64439237\n",
      "Trained batch 539 batch loss 1.69131505 epoch total loss 1.64447951\n",
      "Trained batch 540 batch loss 1.55231583 epoch total loss 1.64430881\n",
      "Trained batch 541 batch loss 1.58955312 epoch total loss 1.6442076\n",
      "Trained batch 542 batch loss 1.55672729 epoch total loss 1.64404607\n",
      "Trained batch 543 batch loss 1.44412458 epoch total loss 1.64367795\n",
      "Trained batch 544 batch loss 1.41469944 epoch total loss 1.64325702\n",
      "Trained batch 545 batch loss 1.52936721 epoch total loss 1.64304805\n",
      "Trained batch 546 batch loss 1.59350717 epoch total loss 1.64295733\n",
      "Trained batch 547 batch loss 1.5258342 epoch total loss 1.64274311\n",
      "Trained batch 548 batch loss 1.5939045 epoch total loss 1.64265406\n",
      "Trained batch 549 batch loss 1.63334024 epoch total loss 1.64263713\n",
      "Trained batch 550 batch loss 1.54224968 epoch total loss 1.64245462\n",
      "Trained batch 551 batch loss 1.63956141 epoch total loss 1.64244938\n",
      "Trained batch 552 batch loss 1.63080513 epoch total loss 1.64242828\n",
      "Trained batch 553 batch loss 1.61072969 epoch total loss 1.64237094\n",
      "Trained batch 554 batch loss 1.52926123 epoch total loss 1.64216673\n",
      "Trained batch 555 batch loss 1.56161284 epoch total loss 1.64202154\n",
      "Trained batch 556 batch loss 1.7030524 epoch total loss 1.64213133\n",
      "Trained batch 557 batch loss 1.58213711 epoch total loss 1.64202368\n",
      "Trained batch 558 batch loss 1.52124572 epoch total loss 1.6418072\n",
      "Trained batch 559 batch loss 1.51215744 epoch total loss 1.64157522\n",
      "Trained batch 560 batch loss 1.51389444 epoch total loss 1.64134729\n",
      "Trained batch 561 batch loss 1.56476212 epoch total loss 1.64121079\n",
      "Trained batch 562 batch loss 1.5626893 epoch total loss 1.64107108\n",
      "Trained batch 563 batch loss 1.55650389 epoch total loss 1.64092088\n",
      "Trained batch 564 batch loss 1.43489957 epoch total loss 1.6405555\n",
      "Trained batch 565 batch loss 1.51578796 epoch total loss 1.64033473\n",
      "Trained batch 566 batch loss 1.66343713 epoch total loss 1.64037561\n",
      "Trained batch 567 batch loss 1.56700921 epoch total loss 1.64024615\n",
      "Trained batch 568 batch loss 1.60573149 epoch total loss 1.64018536\n",
      "Trained batch 569 batch loss 1.60312915 epoch total loss 1.64012027\n",
      "Trained batch 570 batch loss 1.54906106 epoch total loss 1.63996053\n",
      "Trained batch 571 batch loss 1.54781508 epoch total loss 1.63979912\n",
      "Trained batch 572 batch loss 1.56509805 epoch total loss 1.63966858\n",
      "Trained batch 573 batch loss 1.5717423 epoch total loss 1.63955\n",
      "Trained batch 574 batch loss 1.5015645 epoch total loss 1.63930964\n",
      "Trained batch 575 batch loss 1.54453909 epoch total loss 1.6391449\n",
      "Trained batch 576 batch loss 1.56913531 epoch total loss 1.6390233\n",
      "Trained batch 577 batch loss 1.51116693 epoch total loss 1.63880181\n",
      "Trained batch 578 batch loss 1.4073323 epoch total loss 1.63840139\n",
      "Trained batch 579 batch loss 1.39871693 epoch total loss 1.63798738\n",
      "Trained batch 580 batch loss 1.36631763 epoch total loss 1.637519\n",
      "Trained batch 581 batch loss 1.37051487 epoch total loss 1.63705957\n",
      "Trained batch 582 batch loss 1.50224614 epoch total loss 1.63682795\n",
      "Trained batch 583 batch loss 1.60679626 epoch total loss 1.63677645\n",
      "Trained batch 584 batch loss 1.68814635 epoch total loss 1.63686442\n",
      "Trained batch 585 batch loss 1.58856177 epoch total loss 1.63678193\n",
      "Trained batch 586 batch loss 1.6104908 epoch total loss 1.63673699\n",
      "Trained batch 587 batch loss 1.59954977 epoch total loss 1.63667357\n",
      "Trained batch 588 batch loss 1.45100331 epoch total loss 1.63635778\n",
      "Trained batch 589 batch loss 1.55845475 epoch total loss 1.63622558\n",
      "Trained batch 590 batch loss 1.53267169 epoch total loss 1.63605011\n",
      "Trained batch 591 batch loss 1.37146807 epoch total loss 1.63560236\n",
      "Trained batch 592 batch loss 1.40640628 epoch total loss 1.63521528\n",
      "Trained batch 593 batch loss 1.42498744 epoch total loss 1.63486075\n",
      "Trained batch 594 batch loss 1.46842933 epoch total loss 1.63458061\n",
      "Trained batch 595 batch loss 1.50333071 epoch total loss 1.63436007\n",
      "Trained batch 596 batch loss 1.58758795 epoch total loss 1.63428152\n",
      "Trained batch 597 batch loss 1.54361284 epoch total loss 1.63412976\n",
      "Trained batch 598 batch loss 1.46450365 epoch total loss 1.63384604\n",
      "Trained batch 599 batch loss 1.42391968 epoch total loss 1.63349557\n",
      "Trained batch 600 batch loss 1.53889918 epoch total loss 1.63333786\n",
      "Trained batch 601 batch loss 1.55642605 epoch total loss 1.63321\n",
      "Trained batch 602 batch loss 1.51372337 epoch total loss 1.63301146\n",
      "Trained batch 603 batch loss 1.57879651 epoch total loss 1.63292146\n",
      "Trained batch 604 batch loss 1.50102937 epoch total loss 1.63270319\n",
      "Trained batch 605 batch loss 1.45926809 epoch total loss 1.63241649\n",
      "Trained batch 606 batch loss 1.47922254 epoch total loss 1.63216376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 607 batch loss 1.4507376 epoch total loss 1.63186491\n",
      "Trained batch 608 batch loss 1.50144219 epoch total loss 1.63165045\n",
      "Trained batch 609 batch loss 1.50454319 epoch total loss 1.63144171\n",
      "Trained batch 610 batch loss 1.49025035 epoch total loss 1.63121021\n",
      "Trained batch 611 batch loss 1.50432301 epoch total loss 1.63100255\n",
      "Trained batch 612 batch loss 1.58869886 epoch total loss 1.6309334\n",
      "Trained batch 613 batch loss 1.59522462 epoch total loss 1.63087511\n",
      "Trained batch 614 batch loss 1.54713988 epoch total loss 1.63073874\n",
      "Trained batch 615 batch loss 1.48551416 epoch total loss 1.63050258\n",
      "Trained batch 616 batch loss 1.44846511 epoch total loss 1.63020706\n",
      "Trained batch 617 batch loss 1.46636701 epoch total loss 1.62994158\n",
      "Trained batch 618 batch loss 1.47525096 epoch total loss 1.62969136\n",
      "Trained batch 619 batch loss 1.49884546 epoch total loss 1.62947989\n",
      "Trained batch 620 batch loss 1.63859856 epoch total loss 1.62949467\n",
      "Trained batch 621 batch loss 1.58880806 epoch total loss 1.6294291\n",
      "Trained batch 622 batch loss 1.49958634 epoch total loss 1.62922037\n",
      "Trained batch 623 batch loss 1.55816627 epoch total loss 1.62910628\n",
      "Trained batch 624 batch loss 1.47188735 epoch total loss 1.62885427\n",
      "Trained batch 625 batch loss 1.54757297 epoch total loss 1.62872422\n",
      "Trained batch 626 batch loss 1.54258621 epoch total loss 1.62858665\n",
      "Trained batch 627 batch loss 1.52313399 epoch total loss 1.62841845\n",
      "Trained batch 628 batch loss 1.55649483 epoch total loss 1.628304\n",
      "Trained batch 629 batch loss 1.68838668 epoch total loss 1.62839949\n",
      "Trained batch 630 batch loss 1.58755898 epoch total loss 1.62833464\n",
      "Trained batch 631 batch loss 1.59777164 epoch total loss 1.62828624\n",
      "Trained batch 632 batch loss 1.57598305 epoch total loss 1.62820339\n",
      "Trained batch 633 batch loss 1.59010768 epoch total loss 1.62814319\n",
      "Trained batch 634 batch loss 1.59981084 epoch total loss 1.62809849\n",
      "Trained batch 635 batch loss 1.57264471 epoch total loss 1.62801123\n",
      "Trained batch 636 batch loss 1.52968991 epoch total loss 1.62785661\n",
      "Trained batch 637 batch loss 1.54967129 epoch total loss 1.62773383\n",
      "Trained batch 638 batch loss 1.56010222 epoch total loss 1.62762773\n",
      "Trained batch 639 batch loss 1.55722535 epoch total loss 1.62751758\n",
      "Trained batch 640 batch loss 1.56528747 epoch total loss 1.62742043\n",
      "Trained batch 641 batch loss 1.48860359 epoch total loss 1.62720394\n",
      "Trained batch 642 batch loss 1.5601989 epoch total loss 1.62709951\n",
      "Trained batch 643 batch loss 1.45203114 epoch total loss 1.62682724\n",
      "Trained batch 644 batch loss 1.48093534 epoch total loss 1.62660074\n",
      "Trained batch 645 batch loss 1.43202496 epoch total loss 1.62629902\n",
      "Trained batch 646 batch loss 1.47238421 epoch total loss 1.62606084\n",
      "Trained batch 647 batch loss 1.42739856 epoch total loss 1.62575376\n",
      "Trained batch 648 batch loss 1.53505564 epoch total loss 1.62561369\n",
      "Trained batch 649 batch loss 1.37134659 epoch total loss 1.62522197\n",
      "Trained batch 650 batch loss 1.40040731 epoch total loss 1.62487602\n",
      "Trained batch 651 batch loss 1.52992117 epoch total loss 1.62473023\n",
      "Trained batch 652 batch loss 1.57113898 epoch total loss 1.62464797\n",
      "Trained batch 653 batch loss 1.56806302 epoch total loss 1.62456143\n",
      "Trained batch 654 batch loss 1.54590368 epoch total loss 1.62444115\n",
      "Trained batch 655 batch loss 1.4376452 epoch total loss 1.624156\n",
      "Trained batch 656 batch loss 1.31182432 epoch total loss 1.62367976\n",
      "Trained batch 657 batch loss 1.4742794 epoch total loss 1.62345231\n",
      "Trained batch 658 batch loss 1.59552515 epoch total loss 1.62341\n",
      "Trained batch 659 batch loss 1.55717707 epoch total loss 1.62330937\n",
      "Trained batch 660 batch loss 1.57668662 epoch total loss 1.62323868\n",
      "Trained batch 661 batch loss 1.63219142 epoch total loss 1.62325227\n",
      "Trained batch 662 batch loss 1.59014118 epoch total loss 1.62320209\n",
      "Trained batch 663 batch loss 1.59738278 epoch total loss 1.62316322\n",
      "Trained batch 664 batch loss 1.59756792 epoch total loss 1.6231246\n",
      "Trained batch 665 batch loss 1.58363223 epoch total loss 1.62306523\n",
      "Trained batch 666 batch loss 1.52522683 epoch total loss 1.62291837\n",
      "Trained batch 667 batch loss 1.62097704 epoch total loss 1.62291551\n",
      "Trained batch 668 batch loss 1.51197457 epoch total loss 1.62274933\n",
      "Trained batch 669 batch loss 1.61791015 epoch total loss 1.62274218\n",
      "Trained batch 670 batch loss 1.58535981 epoch total loss 1.62268627\n",
      "Trained batch 671 batch loss 1.53077245 epoch total loss 1.6225493\n",
      "Trained batch 672 batch loss 1.51716197 epoch total loss 1.62239254\n",
      "Trained batch 673 batch loss 1.54357243 epoch total loss 1.62227547\n",
      "Trained batch 674 batch loss 1.47801661 epoch total loss 1.62206149\n",
      "Trained batch 675 batch loss 1.54280293 epoch total loss 1.62194407\n",
      "Trained batch 676 batch loss 1.48902702 epoch total loss 1.62174749\n",
      "Trained batch 677 batch loss 1.60894406 epoch total loss 1.62172842\n",
      "Trained batch 678 batch loss 1.52833152 epoch total loss 1.62159061\n",
      "Trained batch 679 batch loss 1.49705935 epoch total loss 1.62140727\n",
      "Trained batch 680 batch loss 1.43075776 epoch total loss 1.62112701\n",
      "Trained batch 681 batch loss 1.48905373 epoch total loss 1.62093294\n",
      "Trained batch 682 batch loss 1.4414624 epoch total loss 1.62066972\n",
      "Trained batch 683 batch loss 1.53550076 epoch total loss 1.62054503\n",
      "Trained batch 684 batch loss 1.5427537 epoch total loss 1.6204313\n",
      "Trained batch 685 batch loss 1.59140897 epoch total loss 1.62038898\n",
      "Trained batch 686 batch loss 1.67236054 epoch total loss 1.62046468\n",
      "Trained batch 687 batch loss 1.66552341 epoch total loss 1.62053037\n",
      "Trained batch 688 batch loss 1.62256813 epoch total loss 1.62053323\n",
      "Trained batch 689 batch loss 1.62942696 epoch total loss 1.6205461\n",
      "Trained batch 690 batch loss 1.43836915 epoch total loss 1.62028205\n",
      "Trained batch 691 batch loss 1.47271013 epoch total loss 1.62006843\n",
      "Trained batch 692 batch loss 1.30259979 epoch total loss 1.61960971\n",
      "Trained batch 693 batch loss 1.27217817 epoch total loss 1.61910844\n",
      "Trained batch 694 batch loss 1.42149758 epoch total loss 1.61882365\n",
      "Trained batch 695 batch loss 1.52886987 epoch total loss 1.61869431\n",
      "Trained batch 696 batch loss 1.47604072 epoch total loss 1.61848938\n",
      "Trained batch 697 batch loss 1.35325 epoch total loss 1.61810887\n",
      "Trained batch 698 batch loss 1.3250767 epoch total loss 1.61768901\n",
      "Trained batch 699 batch loss 1.44594252 epoch total loss 1.61744332\n",
      "Trained batch 700 batch loss 1.55757058 epoch total loss 1.61735785\n",
      "Trained batch 701 batch loss 1.59970117 epoch total loss 1.6173327\n",
      "Trained batch 702 batch loss 1.55966806 epoch total loss 1.61725068\n",
      "Trained batch 703 batch loss 1.5125885 epoch total loss 1.61710167\n",
      "Trained batch 704 batch loss 1.48646688 epoch total loss 1.61691618\n",
      "Trained batch 705 batch loss 1.51824021 epoch total loss 1.61677611\n",
      "Trained batch 706 batch loss 1.44195426 epoch total loss 1.61652839\n",
      "Trained batch 707 batch loss 1.39971685 epoch total loss 1.61622167\n",
      "Trained batch 708 batch loss 1.47045648 epoch total loss 1.61601579\n",
      "Trained batch 709 batch loss 1.5239023 epoch total loss 1.61588585\n",
      "Trained batch 710 batch loss 1.52682853 epoch total loss 1.61576045\n",
      "Trained batch 711 batch loss 1.50908744 epoch total loss 1.61561036\n",
      "Trained batch 712 batch loss 1.53422201 epoch total loss 1.61549604\n",
      "Trained batch 713 batch loss 1.52820575 epoch total loss 1.61537361\n",
      "Trained batch 714 batch loss 1.55200362 epoch total loss 1.6152848\n",
      "Trained batch 715 batch loss 1.55327129 epoch total loss 1.61519802\n",
      "Trained batch 716 batch loss 1.58243501 epoch total loss 1.61515224\n",
      "Trained batch 717 batch loss 1.5350647 epoch total loss 1.61504042\n",
      "Trained batch 718 batch loss 1.55515885 epoch total loss 1.61495709\n",
      "Trained batch 719 batch loss 1.60346651 epoch total loss 1.61494112\n",
      "Trained batch 720 batch loss 1.51682353 epoch total loss 1.61480498\n",
      "Trained batch 721 batch loss 1.58382535 epoch total loss 1.61476207\n",
      "Trained batch 722 batch loss 1.52422988 epoch total loss 1.61463654\n",
      "Trained batch 723 batch loss 1.50845349 epoch total loss 1.61448967\n",
      "Trained batch 724 batch loss 1.50807226 epoch total loss 1.61434257\n",
      "Trained batch 725 batch loss 1.51957035 epoch total loss 1.6142118\n",
      "Trained batch 726 batch loss 1.54713476 epoch total loss 1.61411941\n",
      "Trained batch 727 batch loss 1.63306475 epoch total loss 1.61414552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 728 batch loss 1.67686677 epoch total loss 1.61423171\n",
      "Trained batch 729 batch loss 1.57363772 epoch total loss 1.61417592\n",
      "Trained batch 730 batch loss 1.59670281 epoch total loss 1.61415195\n",
      "Trained batch 731 batch loss 1.62685156 epoch total loss 1.61416936\n",
      "Trained batch 732 batch loss 1.62543833 epoch total loss 1.61418474\n",
      "Trained batch 733 batch loss 1.50854242 epoch total loss 1.61404061\n",
      "Trained batch 734 batch loss 1.56988966 epoch total loss 1.61398053\n",
      "Trained batch 735 batch loss 1.58593297 epoch total loss 1.61394238\n",
      "Trained batch 736 batch loss 1.44041991 epoch total loss 1.61370671\n",
      "Trained batch 737 batch loss 1.63965619 epoch total loss 1.61374187\n",
      "Trained batch 738 batch loss 1.50855398 epoch total loss 1.6135993\n",
      "Trained batch 739 batch loss 1.57698321 epoch total loss 1.61354983\n",
      "Trained batch 740 batch loss 1.68276274 epoch total loss 1.61364329\n",
      "Trained batch 741 batch loss 1.58965087 epoch total loss 1.61361086\n",
      "Trained batch 742 batch loss 1.55648744 epoch total loss 1.61353397\n",
      "Trained batch 743 batch loss 1.62153101 epoch total loss 1.61354482\n",
      "Trained batch 744 batch loss 1.54147482 epoch total loss 1.6134479\n",
      "Trained batch 745 batch loss 1.47137797 epoch total loss 1.61325729\n",
      "Trained batch 746 batch loss 1.52767277 epoch total loss 1.61314261\n",
      "Trained batch 747 batch loss 1.5532819 epoch total loss 1.61306238\n",
      "Trained batch 748 batch loss 1.63569736 epoch total loss 1.61309278\n",
      "Trained batch 749 batch loss 1.56825697 epoch total loss 1.61303282\n",
      "Trained batch 750 batch loss 1.53158629 epoch total loss 1.61292434\n",
      "Trained batch 751 batch loss 1.60563111 epoch total loss 1.61291456\n",
      "Trained batch 752 batch loss 1.55350423 epoch total loss 1.61283553\n",
      "Trained batch 753 batch loss 1.57140875 epoch total loss 1.61278045\n",
      "Trained batch 754 batch loss 1.5654037 epoch total loss 1.61271763\n",
      "Trained batch 755 batch loss 1.55230117 epoch total loss 1.61263764\n",
      "Trained batch 756 batch loss 1.62243521 epoch total loss 1.61265051\n",
      "Trained batch 757 batch loss 1.47325706 epoch total loss 1.61246645\n",
      "Trained batch 758 batch loss 1.36448526 epoch total loss 1.61213934\n",
      "Trained batch 759 batch loss 1.40655029 epoch total loss 1.61186838\n",
      "Trained batch 760 batch loss 1.60181117 epoch total loss 1.61185515\n",
      "Trained batch 761 batch loss 1.39357126 epoch total loss 1.61156821\n",
      "Trained batch 762 batch loss 1.43820024 epoch total loss 1.61134076\n",
      "Trained batch 763 batch loss 1.42386854 epoch total loss 1.61109507\n",
      "Trained batch 764 batch loss 1.51655769 epoch total loss 1.61097133\n",
      "Trained batch 765 batch loss 1.58493698 epoch total loss 1.61093736\n",
      "Trained batch 766 batch loss 1.53663445 epoch total loss 1.61084032\n",
      "Trained batch 767 batch loss 1.46234131 epoch total loss 1.61064661\n",
      "Trained batch 768 batch loss 1.45122147 epoch total loss 1.61043894\n",
      "Trained batch 769 batch loss 1.45454741 epoch total loss 1.61023629\n",
      "Trained batch 770 batch loss 1.53312826 epoch total loss 1.61013615\n",
      "Trained batch 771 batch loss 1.4850316 epoch total loss 1.60997379\n",
      "Trained batch 772 batch loss 1.44101727 epoch total loss 1.60975492\n",
      "Trained batch 773 batch loss 1.39920759 epoch total loss 1.60948253\n",
      "Trained batch 774 batch loss 1.60744882 epoch total loss 1.6094799\n",
      "Trained batch 775 batch loss 1.51985991 epoch total loss 1.60936427\n",
      "Trained batch 776 batch loss 1.48609734 epoch total loss 1.60920537\n",
      "Trained batch 777 batch loss 1.44859791 epoch total loss 1.60899878\n",
      "Trained batch 778 batch loss 1.5518117 epoch total loss 1.6089251\n",
      "Trained batch 779 batch loss 1.53123808 epoch total loss 1.60882545\n",
      "Trained batch 780 batch loss 1.4983151 epoch total loss 1.60868371\n",
      "Trained batch 781 batch loss 1.44036102 epoch total loss 1.60846817\n",
      "Trained batch 782 batch loss 1.55046153 epoch total loss 1.60839391\n",
      "Trained batch 783 batch loss 1.49172771 epoch total loss 1.6082449\n",
      "Trained batch 784 batch loss 1.50065732 epoch total loss 1.60810757\n",
      "Trained batch 785 batch loss 1.45410848 epoch total loss 1.60791135\n",
      "Trained batch 786 batch loss 1.52942157 epoch total loss 1.60781157\n",
      "Trained batch 787 batch loss 1.40796471 epoch total loss 1.60755765\n",
      "Trained batch 788 batch loss 1.49359918 epoch total loss 1.60741305\n",
      "Trained batch 789 batch loss 1.51666784 epoch total loss 1.60729814\n",
      "Trained batch 790 batch loss 1.45105469 epoch total loss 1.60710037\n",
      "Trained batch 791 batch loss 1.50469601 epoch total loss 1.60697079\n",
      "Trained batch 792 batch loss 1.5086894 epoch total loss 1.60684669\n",
      "Trained batch 793 batch loss 1.38509572 epoch total loss 1.60656703\n",
      "Trained batch 794 batch loss 1.53940487 epoch total loss 1.60648251\n",
      "Trained batch 795 batch loss 1.63366222 epoch total loss 1.60651672\n",
      "Trained batch 796 batch loss 1.5339458 epoch total loss 1.60642552\n",
      "Trained batch 797 batch loss 1.59039021 epoch total loss 1.60640538\n",
      "Trained batch 798 batch loss 1.73910701 epoch total loss 1.60657167\n",
      "Trained batch 799 batch loss 1.64597917 epoch total loss 1.60662103\n",
      "Trained batch 800 batch loss 1.62639344 epoch total loss 1.6066457\n",
      "Trained batch 801 batch loss 1.64268613 epoch total loss 1.60669065\n",
      "Trained batch 802 batch loss 1.48914504 epoch total loss 1.60654414\n",
      "Trained batch 803 batch loss 1.49790108 epoch total loss 1.60640883\n",
      "Trained batch 804 batch loss 1.51406407 epoch total loss 1.60629392\n",
      "Trained batch 805 batch loss 1.58532667 epoch total loss 1.60626793\n",
      "Trained batch 806 batch loss 1.57379556 epoch total loss 1.60622764\n",
      "Trained batch 807 batch loss 1.58077669 epoch total loss 1.60619617\n",
      "Trained batch 808 batch loss 1.49780881 epoch total loss 1.60606205\n",
      "Trained batch 809 batch loss 1.54962194 epoch total loss 1.60599232\n",
      "Trained batch 810 batch loss 1.59491575 epoch total loss 1.60597873\n",
      "Trained batch 811 batch loss 1.56105161 epoch total loss 1.6059233\n",
      "Trained batch 812 batch loss 1.58596945 epoch total loss 1.60589874\n",
      "Trained batch 813 batch loss 1.56583548 epoch total loss 1.60584939\n",
      "Trained batch 814 batch loss 1.55973482 epoch total loss 1.60579264\n",
      "Trained batch 815 batch loss 1.56178296 epoch total loss 1.60573864\n",
      "Trained batch 816 batch loss 1.53826666 epoch total loss 1.60565591\n",
      "Trained batch 817 batch loss 1.551952 epoch total loss 1.60559022\n",
      "Trained batch 818 batch loss 1.55863321 epoch total loss 1.60553277\n",
      "Trained batch 819 batch loss 1.56513321 epoch total loss 1.60548353\n",
      "Trained batch 820 batch loss 1.4354682 epoch total loss 1.60527611\n",
      "Trained batch 821 batch loss 1.54723644 epoch total loss 1.60520542\n",
      "Trained batch 822 batch loss 1.47534645 epoch total loss 1.60504746\n",
      "Trained batch 823 batch loss 1.43424225 epoch total loss 1.6048398\n",
      "Trained batch 824 batch loss 1.40436769 epoch total loss 1.60459661\n",
      "Trained batch 825 batch loss 1.48475444 epoch total loss 1.6044513\n",
      "Trained batch 826 batch loss 1.43463135 epoch total loss 1.60424566\n",
      "Trained batch 827 batch loss 1.45915473 epoch total loss 1.60407019\n",
      "Trained batch 828 batch loss 1.5596267 epoch total loss 1.60401642\n",
      "Trained batch 829 batch loss 1.4707725 epoch total loss 1.60385573\n",
      "Trained batch 830 batch loss 1.5516336 epoch total loss 1.60379291\n",
      "Trained batch 831 batch loss 1.44322634 epoch total loss 1.60359967\n",
      "Trained batch 832 batch loss 1.51274896 epoch total loss 1.60349035\n",
      "Trained batch 833 batch loss 1.48951626 epoch total loss 1.6033535\n",
      "Trained batch 834 batch loss 1.48665619 epoch total loss 1.60321367\n",
      "Trained batch 835 batch loss 1.48584676 epoch total loss 1.60307312\n",
      "Trained batch 836 batch loss 1.54084349 epoch total loss 1.60299873\n",
      "Trained batch 837 batch loss 1.60626328 epoch total loss 1.60300267\n",
      "Trained batch 838 batch loss 1.66688669 epoch total loss 1.60307896\n",
      "Trained batch 839 batch loss 1.4724772 epoch total loss 1.60292327\n",
      "Trained batch 840 batch loss 1.53119385 epoch total loss 1.60283804\n",
      "Trained batch 841 batch loss 1.52684009 epoch total loss 1.60274768\n",
      "Trained batch 842 batch loss 1.4888804 epoch total loss 1.60261238\n",
      "Trained batch 843 batch loss 1.40548885 epoch total loss 1.60237861\n",
      "Trained batch 844 batch loss 1.43109989 epoch total loss 1.60217571\n",
      "Trained batch 845 batch loss 1.50057435 epoch total loss 1.60205555\n",
      "Trained batch 846 batch loss 1.54302502 epoch total loss 1.60198569\n",
      "Trained batch 847 batch loss 1.48529315 epoch total loss 1.60184801\n",
      "Trained batch 848 batch loss 1.50010169 epoch total loss 1.60172808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 849 batch loss 1.30015671 epoch total loss 1.60137284\n",
      "Trained batch 850 batch loss 1.26087761 epoch total loss 1.60097229\n",
      "Trained batch 851 batch loss 1.3507973 epoch total loss 1.60067832\n",
      "Trained batch 852 batch loss 1.47751164 epoch total loss 1.60053384\n",
      "Trained batch 853 batch loss 1.47682691 epoch total loss 1.60038877\n",
      "Trained batch 854 batch loss 1.59431744 epoch total loss 1.60038173\n",
      "Trained batch 855 batch loss 1.56315041 epoch total loss 1.6003381\n",
      "Trained batch 856 batch loss 1.6254828 epoch total loss 1.60036743\n",
      "Trained batch 857 batch loss 1.58695817 epoch total loss 1.60035181\n",
      "Trained batch 858 batch loss 1.58205938 epoch total loss 1.60033047\n",
      "Trained batch 859 batch loss 1.52068 epoch total loss 1.60023761\n",
      "Trained batch 860 batch loss 1.6076237 epoch total loss 1.60024631\n",
      "Trained batch 861 batch loss 1.55911887 epoch total loss 1.60019851\n",
      "Trained batch 862 batch loss 1.62855101 epoch total loss 1.60023129\n",
      "Trained batch 863 batch loss 1.59034586 epoch total loss 1.60021985\n",
      "Trained batch 864 batch loss 1.61524272 epoch total loss 1.60023725\n",
      "Trained batch 865 batch loss 1.516523 epoch total loss 1.60014045\n",
      "Trained batch 866 batch loss 1.40540695 epoch total loss 1.5999155\n",
      "Trained batch 867 batch loss 1.39054918 epoch total loss 1.59967399\n",
      "Trained batch 868 batch loss 1.40349579 epoch total loss 1.59944797\n",
      "Trained batch 869 batch loss 1.52846551 epoch total loss 1.59936619\n",
      "Trained batch 870 batch loss 1.47041297 epoch total loss 1.59921801\n",
      "Trained batch 871 batch loss 1.52640343 epoch total loss 1.59913445\n",
      "Trained batch 872 batch loss 1.6197505 epoch total loss 1.59915805\n",
      "Trained batch 873 batch loss 1.60026467 epoch total loss 1.59915924\n",
      "Trained batch 874 batch loss 1.69594383 epoch total loss 1.59927\n",
      "Trained batch 875 batch loss 1.50670028 epoch total loss 1.59916425\n",
      "Trained batch 876 batch loss 1.44055 epoch total loss 1.59898317\n",
      "Trained batch 877 batch loss 1.41841853 epoch total loss 1.59877729\n",
      "Trained batch 878 batch loss 1.52172256 epoch total loss 1.59868956\n",
      "Trained batch 879 batch loss 1.43109322 epoch total loss 1.59849894\n",
      "Trained batch 880 batch loss 1.50102389 epoch total loss 1.59838808\n",
      "Trained batch 881 batch loss 1.52635431 epoch total loss 1.59830642\n",
      "Trained batch 882 batch loss 1.55124211 epoch total loss 1.59825301\n",
      "Trained batch 883 batch loss 1.46786761 epoch total loss 1.59810543\n",
      "Trained batch 884 batch loss 1.40334082 epoch total loss 1.59788501\n",
      "Trained batch 885 batch loss 1.44815409 epoch total loss 1.59771585\n",
      "Trained batch 886 batch loss 1.52050233 epoch total loss 1.59762871\n",
      "Trained batch 887 batch loss 1.46943235 epoch total loss 1.59748423\n",
      "Trained batch 888 batch loss 1.48600984 epoch total loss 1.59735858\n",
      "Trained batch 889 batch loss 1.38284695 epoch total loss 1.5971173\n",
      "Trained batch 890 batch loss 1.45647061 epoch total loss 1.59695923\n",
      "Trained batch 891 batch loss 1.50846601 epoch total loss 1.59685981\n",
      "Trained batch 892 batch loss 1.49876726 epoch total loss 1.5967499\n",
      "Trained batch 893 batch loss 1.52461624 epoch total loss 1.5966692\n",
      "Trained batch 894 batch loss 1.47772193 epoch total loss 1.59653604\n",
      "Trained batch 895 batch loss 1.48659909 epoch total loss 1.59641314\n",
      "Trained batch 896 batch loss 1.44558585 epoch total loss 1.59624481\n",
      "Trained batch 897 batch loss 1.46358812 epoch total loss 1.59609699\n",
      "Trained batch 898 batch loss 1.5089612 epoch total loss 1.59599984\n",
      "Trained batch 899 batch loss 1.50301778 epoch total loss 1.59589648\n",
      "Trained batch 900 batch loss 1.51225531 epoch total loss 1.5958035\n",
      "Trained batch 901 batch loss 1.46441793 epoch total loss 1.59565771\n",
      "Trained batch 902 batch loss 1.42034245 epoch total loss 1.59546328\n",
      "Trained batch 903 batch loss 1.52028084 epoch total loss 1.59538007\n",
      "Trained batch 904 batch loss 1.53881 epoch total loss 1.59531748\n",
      "Trained batch 905 batch loss 1.58321881 epoch total loss 1.59530413\n",
      "Trained batch 906 batch loss 1.54805148 epoch total loss 1.59525204\n",
      "Trained batch 907 batch loss 1.57020354 epoch total loss 1.59522438\n",
      "Trained batch 908 batch loss 1.57074511 epoch total loss 1.59519756\n",
      "Trained batch 909 batch loss 1.51444411 epoch total loss 1.59510863\n",
      "Trained batch 910 batch loss 1.54103208 epoch total loss 1.59504914\n",
      "Trained batch 911 batch loss 1.47874594 epoch total loss 1.59492147\n",
      "Trained batch 912 batch loss 1.49866295 epoch total loss 1.59481597\n",
      "Trained batch 913 batch loss 1.47713947 epoch total loss 1.5946871\n",
      "Trained batch 914 batch loss 1.53116059 epoch total loss 1.59461761\n",
      "Trained batch 915 batch loss 1.47935832 epoch total loss 1.5944916\n",
      "Trained batch 916 batch loss 1.56649 epoch total loss 1.59446108\n",
      "Trained batch 917 batch loss 1.49853683 epoch total loss 1.59435654\n",
      "Trained batch 918 batch loss 1.54226398 epoch total loss 1.59429967\n",
      "Trained batch 919 batch loss 1.44089174 epoch total loss 1.59413278\n",
      "Trained batch 920 batch loss 1.43355942 epoch total loss 1.59395826\n",
      "Trained batch 921 batch loss 1.44524574 epoch total loss 1.59379673\n",
      "Trained batch 922 batch loss 1.49477863 epoch total loss 1.59368932\n",
      "Trained batch 923 batch loss 1.54364586 epoch total loss 1.5936352\n",
      "Trained batch 924 batch loss 1.46854305 epoch total loss 1.59349978\n",
      "Trained batch 925 batch loss 1.53089 epoch total loss 1.59343207\n",
      "Trained batch 926 batch loss 1.49052441 epoch total loss 1.59332097\n",
      "Trained batch 927 batch loss 1.52381372 epoch total loss 1.59324586\n",
      "Trained batch 928 batch loss 1.43587744 epoch total loss 1.59307635\n",
      "Trained batch 929 batch loss 1.38436007 epoch total loss 1.59285176\n",
      "Trained batch 930 batch loss 1.47590256 epoch total loss 1.59272611\n",
      "Trained batch 931 batch loss 1.56030393 epoch total loss 1.59269118\n",
      "Trained batch 932 batch loss 1.52210796 epoch total loss 1.59261549\n",
      "Trained batch 933 batch loss 1.46482706 epoch total loss 1.59247851\n",
      "Trained batch 934 batch loss 1.440189 epoch total loss 1.59231544\n",
      "Trained batch 935 batch loss 1.5202 epoch total loss 1.59223831\n",
      "Trained batch 936 batch loss 1.47446537 epoch total loss 1.59211254\n",
      "Trained batch 937 batch loss 1.4199729 epoch total loss 1.59192872\n",
      "Trained batch 938 batch loss 1.39048779 epoch total loss 1.59171402\n",
      "Trained batch 939 batch loss 1.44749308 epoch total loss 1.59156036\n",
      "Trained batch 940 batch loss 1.31765366 epoch total loss 1.59126902\n",
      "Trained batch 941 batch loss 1.41133642 epoch total loss 1.5910778\n",
      "Trained batch 942 batch loss 1.4036113 epoch total loss 1.59087873\n",
      "Trained batch 943 batch loss 1.40264142 epoch total loss 1.59067905\n",
      "Trained batch 944 batch loss 1.26207757 epoch total loss 1.59033096\n",
      "Trained batch 945 batch loss 1.31798756 epoch total loss 1.59004283\n",
      "Trained batch 946 batch loss 1.37129951 epoch total loss 1.58981156\n",
      "Trained batch 947 batch loss 1.42879748 epoch total loss 1.58964169\n",
      "Trained batch 948 batch loss 1.42593598 epoch total loss 1.58946896\n",
      "Trained batch 949 batch loss 1.4513979 epoch total loss 1.5893234\n",
      "Trained batch 950 batch loss 1.50471127 epoch total loss 1.58923447\n",
      "Trained batch 951 batch loss 1.55381298 epoch total loss 1.58919716\n",
      "Trained batch 952 batch loss 1.53359592 epoch total loss 1.58913875\n",
      "Trained batch 953 batch loss 1.55152273 epoch total loss 1.58909929\n",
      "Trained batch 954 batch loss 1.51070201 epoch total loss 1.58901715\n",
      "Trained batch 955 batch loss 1.47824252 epoch total loss 1.58890116\n",
      "Trained batch 956 batch loss 1.45794475 epoch total loss 1.58876419\n",
      "Trained batch 957 batch loss 1.54510331 epoch total loss 1.58871841\n",
      "Trained batch 958 batch loss 1.39687645 epoch total loss 1.58851814\n",
      "Trained batch 959 batch loss 1.54596817 epoch total loss 1.58847392\n",
      "Trained batch 960 batch loss 1.48181081 epoch total loss 1.58836281\n",
      "Trained batch 961 batch loss 1.43128 epoch total loss 1.58819926\n",
      "Trained batch 962 batch loss 1.44057548 epoch total loss 1.58804584\n",
      "Trained batch 963 batch loss 1.43474078 epoch total loss 1.58788657\n",
      "Trained batch 964 batch loss 1.39634943 epoch total loss 1.58768785\n",
      "Trained batch 965 batch loss 1.38394296 epoch total loss 1.58747673\n",
      "Trained batch 966 batch loss 1.38443863 epoch total loss 1.58726645\n",
      "Trained batch 967 batch loss 1.44741845 epoch total loss 1.58712184\n",
      "Trained batch 968 batch loss 1.32580519 epoch total loss 1.58685184\n",
      "Trained batch 969 batch loss 1.40932143 epoch total loss 1.58666861\n",
      "Trained batch 970 batch loss 1.47434497 epoch total loss 1.58655286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 971 batch loss 1.39222479 epoch total loss 1.58635271\n",
      "Trained batch 972 batch loss 1.45998812 epoch total loss 1.58622265\n",
      "Trained batch 973 batch loss 1.49334836 epoch total loss 1.58612728\n",
      "Trained batch 974 batch loss 1.59178543 epoch total loss 1.58613312\n",
      "Trained batch 975 batch loss 1.43828356 epoch total loss 1.58598149\n",
      "Trained batch 976 batch loss 1.51342416 epoch total loss 1.5859071\n",
      "Trained batch 977 batch loss 1.47148919 epoch total loss 1.58578992\n",
      "Trained batch 978 batch loss 1.55852449 epoch total loss 1.58576202\n",
      "Trained batch 979 batch loss 1.43330383 epoch total loss 1.58560634\n",
      "Trained batch 980 batch loss 1.43351889 epoch total loss 1.58545113\n",
      "Trained batch 981 batch loss 1.44488382 epoch total loss 1.58530772\n",
      "Trained batch 982 batch loss 1.46862447 epoch total loss 1.58518887\n",
      "Trained batch 983 batch loss 1.42790377 epoch total loss 1.58502889\n",
      "Trained batch 984 batch loss 1.41455436 epoch total loss 1.58485556\n",
      "Trained batch 985 batch loss 1.50769866 epoch total loss 1.58477724\n",
      "Trained batch 986 batch loss 1.55402946 epoch total loss 1.58474612\n",
      "Trained batch 987 batch loss 1.48296726 epoch total loss 1.58464301\n",
      "Trained batch 988 batch loss 1.56863987 epoch total loss 1.58462667\n",
      "Trained batch 989 batch loss 1.47335505 epoch total loss 1.58451426\n",
      "Trained batch 990 batch loss 1.5436579 epoch total loss 1.58447301\n",
      "Trained batch 991 batch loss 1.54690135 epoch total loss 1.58443511\n",
      "Trained batch 992 batch loss 1.54083085 epoch total loss 1.58439112\n",
      "Trained batch 993 batch loss 1.48170257 epoch total loss 1.58428764\n",
      "Trained batch 994 batch loss 1.48590899 epoch total loss 1.5841887\n",
      "Trained batch 995 batch loss 1.4536643 epoch total loss 1.58405745\n",
      "Trained batch 996 batch loss 1.40558 epoch total loss 1.5838784\n",
      "Trained batch 997 batch loss 1.40777993 epoch total loss 1.58370173\n",
      "Trained batch 998 batch loss 1.43926048 epoch total loss 1.58355701\n",
      "Trained batch 999 batch loss 1.41444457 epoch total loss 1.58338773\n",
      "Trained batch 1000 batch loss 1.38065541 epoch total loss 1.58318496\n",
      "Trained batch 1001 batch loss 1.4251945 epoch total loss 1.58302712\n",
      "Trained batch 1002 batch loss 1.41235268 epoch total loss 1.58285677\n",
      "Trained batch 1003 batch loss 1.55502605 epoch total loss 1.582829\n",
      "Trained batch 1004 batch loss 1.52362955 epoch total loss 1.58277011\n",
      "Trained batch 1005 batch loss 1.47662115 epoch total loss 1.58266449\n",
      "Trained batch 1006 batch loss 1.54059362 epoch total loss 1.58262265\n",
      "Trained batch 1007 batch loss 1.50645328 epoch total loss 1.58254707\n",
      "Trained batch 1008 batch loss 1.5182972 epoch total loss 1.58248329\n",
      "Trained batch 1009 batch loss 1.54641843 epoch total loss 1.58244753\n",
      "Trained batch 1010 batch loss 1.54633594 epoch total loss 1.58241189\n",
      "Trained batch 1011 batch loss 1.56616521 epoch total loss 1.58239579\n",
      "Trained batch 1012 batch loss 1.57110727 epoch total loss 1.58238471\n",
      "Trained batch 1013 batch loss 1.56361127 epoch total loss 1.58236611\n",
      "Trained batch 1014 batch loss 1.46382475 epoch total loss 1.58224928\n",
      "Trained batch 1015 batch loss 1.550475 epoch total loss 1.58221793\n",
      "Trained batch 1016 batch loss 1.61323452 epoch total loss 1.58224845\n",
      "Trained batch 1017 batch loss 1.62390625 epoch total loss 1.58228946\n",
      "Trained batch 1018 batch loss 1.50658631 epoch total loss 1.58221507\n",
      "Trained batch 1019 batch loss 1.50036073 epoch total loss 1.58213472\n",
      "Trained batch 1020 batch loss 1.49334955 epoch total loss 1.58204782\n",
      "Trained batch 1021 batch loss 1.41198039 epoch total loss 1.58188117\n",
      "Trained batch 1022 batch loss 1.48953128 epoch total loss 1.5817908\n",
      "Trained batch 1023 batch loss 1.47261548 epoch total loss 1.58168411\n",
      "Trained batch 1024 batch loss 1.46867085 epoch total loss 1.58157372\n",
      "Trained batch 1025 batch loss 1.48684287 epoch total loss 1.58148134\n",
      "Trained batch 1026 batch loss 1.41377771 epoch total loss 1.5813179\n",
      "Trained batch 1027 batch loss 1.34631503 epoch total loss 1.58108902\n",
      "Trained batch 1028 batch loss 1.27420831 epoch total loss 1.58079052\n",
      "Trained batch 1029 batch loss 1.20769238 epoch total loss 1.58042789\n",
      "Trained batch 1030 batch loss 1.28890967 epoch total loss 1.58014488\n",
      "Trained batch 1031 batch loss 1.50855827 epoch total loss 1.58007538\n",
      "Trained batch 1032 batch loss 1.57574701 epoch total loss 1.58007121\n",
      "Trained batch 1033 batch loss 1.48404551 epoch total loss 1.57997823\n",
      "Trained batch 1034 batch loss 1.4150176 epoch total loss 1.57981873\n",
      "Trained batch 1035 batch loss 1.50628674 epoch total loss 1.5797478\n",
      "Trained batch 1036 batch loss 1.43247724 epoch total loss 1.57960558\n",
      "Trained batch 1037 batch loss 1.35143661 epoch total loss 1.57938564\n",
      "Trained batch 1038 batch loss 1.384408 epoch total loss 1.57919776\n",
      "Trained batch 1039 batch loss 1.44025588 epoch total loss 1.57906413\n",
      "Trained batch 1040 batch loss 1.37808025 epoch total loss 1.57887077\n",
      "Trained batch 1041 batch loss 1.54162574 epoch total loss 1.57883501\n",
      "Trained batch 1042 batch loss 1.51397896 epoch total loss 1.57877278\n",
      "Trained batch 1043 batch loss 1.44517481 epoch total loss 1.57864475\n",
      "Trained batch 1044 batch loss 1.53957248 epoch total loss 1.57860732\n",
      "Trained batch 1045 batch loss 1.39730346 epoch total loss 1.57843387\n",
      "Trained batch 1046 batch loss 1.51582408 epoch total loss 1.57837403\n",
      "Trained batch 1047 batch loss 1.45153093 epoch total loss 1.57825291\n",
      "Trained batch 1048 batch loss 1.49339879 epoch total loss 1.57817197\n",
      "Trained batch 1049 batch loss 1.47926068 epoch total loss 1.57807767\n",
      "Trained batch 1050 batch loss 1.5864687 epoch total loss 1.57808554\n",
      "Trained batch 1051 batch loss 1.53440058 epoch total loss 1.57804406\n",
      "Trained batch 1052 batch loss 1.57615197 epoch total loss 1.57804227\n",
      "Trained batch 1053 batch loss 1.46456742 epoch total loss 1.5779345\n",
      "Trained batch 1054 batch loss 1.483922 epoch total loss 1.57784534\n",
      "Trained batch 1055 batch loss 1.42425609 epoch total loss 1.57769978\n",
      "Trained batch 1056 batch loss 1.47085595 epoch total loss 1.57759857\n",
      "Trained batch 1057 batch loss 1.44026327 epoch total loss 1.57746863\n",
      "Trained batch 1058 batch loss 1.47933006 epoch total loss 1.57737601\n",
      "Trained batch 1059 batch loss 1.42851484 epoch total loss 1.57723534\n",
      "Trained batch 1060 batch loss 1.42341697 epoch total loss 1.57709026\n",
      "Trained batch 1061 batch loss 1.44995677 epoch total loss 1.57697046\n",
      "Trained batch 1062 batch loss 1.43642044 epoch total loss 1.57683814\n",
      "Trained batch 1063 batch loss 1.52389014 epoch total loss 1.57678831\n",
      "Trained batch 1064 batch loss 1.52457786 epoch total loss 1.57673919\n",
      "Trained batch 1065 batch loss 1.46998167 epoch total loss 1.57663894\n",
      "Trained batch 1066 batch loss 1.53010261 epoch total loss 1.57659531\n",
      "Trained batch 1067 batch loss 1.48663223 epoch total loss 1.57651103\n",
      "Trained batch 1068 batch loss 1.49811947 epoch total loss 1.57643759\n",
      "Trained batch 1069 batch loss 1.44586635 epoch total loss 1.57631552\n",
      "Trained batch 1070 batch loss 1.51997638 epoch total loss 1.57626295\n",
      "Trained batch 1071 batch loss 1.5822345 epoch total loss 1.57626855\n",
      "Trained batch 1072 batch loss 1.46964192 epoch total loss 1.57616901\n",
      "Trained batch 1073 batch loss 1.45987582 epoch total loss 1.57606065\n",
      "Trained batch 1074 batch loss 1.47782564 epoch total loss 1.5759691\n",
      "Trained batch 1075 batch loss 1.48547161 epoch total loss 1.57588494\n",
      "Trained batch 1076 batch loss 1.4545902 epoch total loss 1.57577217\n",
      "Trained batch 1077 batch loss 1.47150683 epoch total loss 1.57567549\n",
      "Trained batch 1078 batch loss 1.46331215 epoch total loss 1.57557118\n",
      "Trained batch 1079 batch loss 1.46473551 epoch total loss 1.57546842\n",
      "Trained batch 1080 batch loss 1.48360789 epoch total loss 1.57538342\n",
      "Trained batch 1081 batch loss 1.49536157 epoch total loss 1.5753094\n",
      "Trained batch 1082 batch loss 1.49009156 epoch total loss 1.5752306\n",
      "Trained batch 1083 batch loss 1.46757245 epoch total loss 1.57513118\n",
      "Trained batch 1084 batch loss 1.51374829 epoch total loss 1.57507455\n",
      "Trained batch 1085 batch loss 1.51958668 epoch total loss 1.57502341\n",
      "Trained batch 1086 batch loss 1.56741738 epoch total loss 1.57501638\n",
      "Trained batch 1087 batch loss 1.49192846 epoch total loss 1.57494\n",
      "Trained batch 1088 batch loss 1.46685243 epoch total loss 1.57484055\n",
      "Trained batch 1089 batch loss 1.46290445 epoch total loss 1.57473779\n",
      "Trained batch 1090 batch loss 1.41382658 epoch total loss 1.57459009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1091 batch loss 1.40776432 epoch total loss 1.57443714\n",
      "Trained batch 1092 batch loss 1.3796711 epoch total loss 1.5742588\n",
      "Trained batch 1093 batch loss 1.47490215 epoch total loss 1.57416785\n",
      "Trained batch 1094 batch loss 1.38882732 epoch total loss 1.57399833\n",
      "Trained batch 1095 batch loss 1.4507401 epoch total loss 1.5738858\n",
      "Trained batch 1096 batch loss 1.4366684 epoch total loss 1.57376051\n",
      "Trained batch 1097 batch loss 1.37954259 epoch total loss 1.57358348\n",
      "Trained batch 1098 batch loss 1.45707345 epoch total loss 1.57347727\n",
      "Trained batch 1099 batch loss 1.4801805 epoch total loss 1.57339251\n",
      "Trained batch 1100 batch loss 1.55619168 epoch total loss 1.57337677\n",
      "Trained batch 1101 batch loss 1.59631395 epoch total loss 1.57339764\n",
      "Trained batch 1102 batch loss 1.47221363 epoch total loss 1.57330573\n",
      "Trained batch 1103 batch loss 1.55737901 epoch total loss 1.5732913\n",
      "Trained batch 1104 batch loss 1.48957944 epoch total loss 1.57321548\n",
      "Trained batch 1105 batch loss 1.49966776 epoch total loss 1.57314897\n",
      "Trained batch 1106 batch loss 1.44836044 epoch total loss 1.57303607\n",
      "Trained batch 1107 batch loss 1.43013144 epoch total loss 1.57290709\n",
      "Trained batch 1108 batch loss 1.54707599 epoch total loss 1.57288384\n",
      "Trained batch 1109 batch loss 1.50773597 epoch total loss 1.57282495\n",
      "Trained batch 1110 batch loss 1.81699467 epoch total loss 1.57304502\n",
      "Trained batch 1111 batch loss 1.712708 epoch total loss 1.57317078\n",
      "Trained batch 1112 batch loss 1.60044563 epoch total loss 1.57319534\n",
      "Trained batch 1113 batch loss 1.59805369 epoch total loss 1.57321763\n",
      "Trained batch 1114 batch loss 1.54362774 epoch total loss 1.57319105\n",
      "Trained batch 1115 batch loss 1.41998696 epoch total loss 1.57305372\n",
      "Trained batch 1116 batch loss 1.43956375 epoch total loss 1.57293403\n",
      "Trained batch 1117 batch loss 1.48440158 epoch total loss 1.57285476\n",
      "Trained batch 1118 batch loss 1.48458529 epoch total loss 1.57277584\n",
      "Trained batch 1119 batch loss 1.52941322 epoch total loss 1.5727371\n",
      "Trained batch 1120 batch loss 1.45814252 epoch total loss 1.57263482\n",
      "Trained batch 1121 batch loss 1.3865912 epoch total loss 1.57246888\n",
      "Trained batch 1122 batch loss 1.40440249 epoch total loss 1.57231903\n",
      "Trained batch 1123 batch loss 1.43872547 epoch total loss 1.57220006\n",
      "Trained batch 1124 batch loss 1.4441812 epoch total loss 1.57208622\n",
      "Trained batch 1125 batch loss 1.46369791 epoch total loss 1.57198989\n",
      "Trained batch 1126 batch loss 1.43412971 epoch total loss 1.57186747\n",
      "Trained batch 1127 batch loss 1.41985071 epoch total loss 1.57173252\n",
      "Trained batch 1128 batch loss 1.48743749 epoch total loss 1.57165778\n",
      "Trained batch 1129 batch loss 1.45492947 epoch total loss 1.57155442\n",
      "Trained batch 1130 batch loss 1.52598202 epoch total loss 1.57151413\n",
      "Trained batch 1131 batch loss 1.54002094 epoch total loss 1.57148623\n",
      "Trained batch 1132 batch loss 1.5290308 epoch total loss 1.5714488\n",
      "Trained batch 1133 batch loss 1.43168759 epoch total loss 1.57132542\n",
      "Trained batch 1134 batch loss 1.48123145 epoch total loss 1.57124591\n",
      "Trained batch 1135 batch loss 1.41425788 epoch total loss 1.57110763\n",
      "Trained batch 1136 batch loss 1.50622749 epoch total loss 1.57105052\n",
      "Trained batch 1137 batch loss 1.49217272 epoch total loss 1.57098114\n",
      "Trained batch 1138 batch loss 1.35276341 epoch total loss 1.57078946\n",
      "Trained batch 1139 batch loss 1.52696228 epoch total loss 1.57075095\n",
      "Trained batch 1140 batch loss 1.58146679 epoch total loss 1.57076025\n",
      "Trained batch 1141 batch loss 1.47403347 epoch total loss 1.57067549\n",
      "Trained batch 1142 batch loss 1.50013208 epoch total loss 1.57061374\n",
      "Trained batch 1143 batch loss 1.42765391 epoch total loss 1.57048857\n",
      "Trained batch 1144 batch loss 1.31568706 epoch total loss 1.57026589\n",
      "Trained batch 1145 batch loss 1.46894193 epoch total loss 1.57017744\n",
      "Trained batch 1146 batch loss 1.47132456 epoch total loss 1.57009113\n",
      "Trained batch 1147 batch loss 1.37105799 epoch total loss 1.56991768\n",
      "Trained batch 1148 batch loss 1.43128324 epoch total loss 1.56979692\n",
      "Trained batch 1149 batch loss 1.61732197 epoch total loss 1.56983829\n",
      "Trained batch 1150 batch loss 1.66509795 epoch total loss 1.56992102\n",
      "Trained batch 1151 batch loss 1.66572571 epoch total loss 1.57000434\n",
      "Trained batch 1152 batch loss 1.55676639 epoch total loss 1.56999278\n",
      "Trained batch 1153 batch loss 1.4077388 epoch total loss 1.56985211\n",
      "Trained batch 1154 batch loss 1.42434478 epoch total loss 1.56972599\n",
      "Trained batch 1155 batch loss 1.47721875 epoch total loss 1.56964576\n",
      "Trained batch 1156 batch loss 1.44532204 epoch total loss 1.56953824\n",
      "Trained batch 1157 batch loss 1.37518167 epoch total loss 1.56937027\n",
      "Trained batch 1158 batch loss 1.44585156 epoch total loss 1.56926346\n",
      "Trained batch 1159 batch loss 1.4546361 epoch total loss 1.56916463\n",
      "Trained batch 1160 batch loss 1.5380522 epoch total loss 1.56913781\n",
      "Trained batch 1161 batch loss 1.6040442 epoch total loss 1.56916785\n",
      "Trained batch 1162 batch loss 1.50208771 epoch total loss 1.56911\n",
      "Trained batch 1163 batch loss 1.47625351 epoch total loss 1.56903017\n",
      "Trained batch 1164 batch loss 1.4315356 epoch total loss 1.56891203\n",
      "Trained batch 1165 batch loss 1.52732253 epoch total loss 1.56887639\n",
      "Trained batch 1166 batch loss 1.44207048 epoch total loss 1.56876755\n",
      "Trained batch 1167 batch loss 1.41408539 epoch total loss 1.56863499\n",
      "Trained batch 1168 batch loss 1.35666883 epoch total loss 1.56845355\n",
      "Trained batch 1169 batch loss 1.39230514 epoch total loss 1.56830287\n",
      "Trained batch 1170 batch loss 1.41388226 epoch total loss 1.56817091\n",
      "Trained batch 1171 batch loss 1.48284268 epoch total loss 1.56809807\n",
      "Trained batch 1172 batch loss 1.60907066 epoch total loss 1.568133\n",
      "Trained batch 1173 batch loss 1.61752379 epoch total loss 1.5681752\n",
      "Trained batch 1174 batch loss 1.48810673 epoch total loss 1.56810701\n",
      "Trained batch 1175 batch loss 1.58342648 epoch total loss 1.56812\n",
      "Trained batch 1176 batch loss 1.50366914 epoch total loss 1.56806517\n",
      "Trained batch 1177 batch loss 1.49019647 epoch total loss 1.56799912\n",
      "Trained batch 1178 batch loss 1.44218731 epoch total loss 1.56789219\n",
      "Trained batch 1179 batch loss 1.45712221 epoch total loss 1.56779826\n",
      "Trained batch 1180 batch loss 1.47239089 epoch total loss 1.56771743\n",
      "Trained batch 1181 batch loss 1.44159532 epoch total loss 1.56761074\n",
      "Trained batch 1182 batch loss 1.48737955 epoch total loss 1.56754291\n",
      "Trained batch 1183 batch loss 1.52824616 epoch total loss 1.56750965\n",
      "Trained batch 1184 batch loss 1.53518569 epoch total loss 1.56748235\n",
      "Trained batch 1185 batch loss 1.49048162 epoch total loss 1.56741726\n",
      "Trained batch 1186 batch loss 1.5560286 epoch total loss 1.56740773\n",
      "Trained batch 1187 batch loss 1.52212477 epoch total loss 1.56736958\n",
      "Trained batch 1188 batch loss 1.46221375 epoch total loss 1.56728101\n",
      "Trained batch 1189 batch loss 1.36590075 epoch total loss 1.56711161\n",
      "Trained batch 1190 batch loss 1.44401729 epoch total loss 1.56700814\n",
      "Trained batch 1191 batch loss 1.45619166 epoch total loss 1.56691504\n",
      "Trained batch 1192 batch loss 1.32001674 epoch total loss 1.56670797\n",
      "Trained batch 1193 batch loss 1.40798426 epoch total loss 1.56657493\n",
      "Trained batch 1194 batch loss 1.51526248 epoch total loss 1.5665319\n",
      "Trained batch 1195 batch loss 1.4183284 epoch total loss 1.56640792\n",
      "Trained batch 1196 batch loss 1.44663644 epoch total loss 1.56630778\n",
      "Trained batch 1197 batch loss 1.47655559 epoch total loss 1.5662328\n",
      "Trained batch 1198 batch loss 1.49248385 epoch total loss 1.56617117\n",
      "Trained batch 1199 batch loss 1.51065063 epoch total loss 1.5661248\n",
      "Trained batch 1200 batch loss 1.48847961 epoch total loss 1.56606019\n",
      "Trained batch 1201 batch loss 1.43663907 epoch total loss 1.56595242\n",
      "Trained batch 1202 batch loss 1.49668598 epoch total loss 1.56589484\n",
      "Trained batch 1203 batch loss 1.5687809 epoch total loss 1.56589723\n",
      "Trained batch 1204 batch loss 1.45826817 epoch total loss 1.56580782\n",
      "Trained batch 1205 batch loss 1.48770094 epoch total loss 1.56574297\n",
      "Trained batch 1206 batch loss 1.45549524 epoch total loss 1.56565142\n",
      "Trained batch 1207 batch loss 1.48518085 epoch total loss 1.56558478\n",
      "Trained batch 1208 batch loss 1.33687472 epoch total loss 1.56539547\n",
      "Trained batch 1209 batch loss 1.3732152 epoch total loss 1.56523657\n",
      "Trained batch 1210 batch loss 1.53690338 epoch total loss 1.56521308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1211 batch loss 1.50788713 epoch total loss 1.56516576\n",
      "Trained batch 1212 batch loss 1.44226861 epoch total loss 1.56506443\n",
      "Trained batch 1213 batch loss 1.36261868 epoch total loss 1.56489754\n",
      "Trained batch 1214 batch loss 1.37047267 epoch total loss 1.56473744\n",
      "Trained batch 1215 batch loss 1.43630934 epoch total loss 1.5646317\n",
      "Trained batch 1216 batch loss 1.51367807 epoch total loss 1.56458974\n",
      "Trained batch 1217 batch loss 1.54997563 epoch total loss 1.5645777\n",
      "Trained batch 1218 batch loss 1.43096972 epoch total loss 1.56446803\n",
      "Trained batch 1219 batch loss 1.66516268 epoch total loss 1.56455064\n",
      "Trained batch 1220 batch loss 1.65340734 epoch total loss 1.56462348\n",
      "Trained batch 1221 batch loss 1.54924655 epoch total loss 1.56461084\n",
      "Trained batch 1222 batch loss 1.50921082 epoch total loss 1.56456554\n",
      "Trained batch 1223 batch loss 1.29836512 epoch total loss 1.56434786\n",
      "Trained batch 1224 batch loss 1.25444937 epoch total loss 1.56409454\n",
      "Trained batch 1225 batch loss 1.37337613 epoch total loss 1.56393898\n",
      "Trained batch 1226 batch loss 1.36419713 epoch total loss 1.56377602\n",
      "Trained batch 1227 batch loss 1.23920846 epoch total loss 1.56351161\n",
      "Trained batch 1228 batch loss 1.20379186 epoch total loss 1.56321859\n",
      "Trained batch 1229 batch loss 1.22573018 epoch total loss 1.56294394\n",
      "Trained batch 1230 batch loss 1.27344561 epoch total loss 1.56270862\n",
      "Trained batch 1231 batch loss 1.35361719 epoch total loss 1.56253874\n",
      "Trained batch 1232 batch loss 1.45606089 epoch total loss 1.56245232\n",
      "Trained batch 1233 batch loss 1.4974972 epoch total loss 1.56239963\n",
      "Trained batch 1234 batch loss 1.51128769 epoch total loss 1.56235814\n",
      "Trained batch 1235 batch loss 1.56218803 epoch total loss 1.56235802\n",
      "Trained batch 1236 batch loss 1.52984142 epoch total loss 1.56233156\n",
      "Trained batch 1237 batch loss 1.47193778 epoch total loss 1.56225848\n",
      "Trained batch 1238 batch loss 1.55974424 epoch total loss 1.56225646\n",
      "Trained batch 1239 batch loss 1.70465302 epoch total loss 1.56237137\n",
      "Trained batch 1240 batch loss 1.60066247 epoch total loss 1.56240237\n",
      "Trained batch 1241 batch loss 1.55662858 epoch total loss 1.56239772\n",
      "Trained batch 1242 batch loss 1.52092683 epoch total loss 1.56236422\n",
      "Trained batch 1243 batch loss 1.45530677 epoch total loss 1.56227815\n",
      "Trained batch 1244 batch loss 1.42774689 epoch total loss 1.56217\n",
      "Trained batch 1245 batch loss 1.43791139 epoch total loss 1.56207013\n",
      "Trained batch 1246 batch loss 1.50903547 epoch total loss 1.56202757\n",
      "Trained batch 1247 batch loss 1.52999759 epoch total loss 1.56200194\n",
      "Trained batch 1248 batch loss 1.43527102 epoch total loss 1.56190038\n",
      "Trained batch 1249 batch loss 1.38661516 epoch total loss 1.56176007\n",
      "Trained batch 1250 batch loss 1.41726363 epoch total loss 1.56164443\n",
      "Trained batch 1251 batch loss 1.41500831 epoch total loss 1.56152725\n",
      "Trained batch 1252 batch loss 1.53240037 epoch total loss 1.56150389\n",
      "Trained batch 1253 batch loss 1.66492879 epoch total loss 1.5615865\n",
      "Trained batch 1254 batch loss 1.7185998 epoch total loss 1.56171167\n",
      "Trained batch 1255 batch loss 1.51698446 epoch total loss 1.56167603\n",
      "Trained batch 1256 batch loss 1.52603865 epoch total loss 1.56164765\n",
      "Trained batch 1257 batch loss 1.66576362 epoch total loss 1.5617305\n",
      "Trained batch 1258 batch loss 1.5042125 epoch total loss 1.56168485\n",
      "Trained batch 1259 batch loss 1.41762805 epoch total loss 1.56157041\n",
      "Trained batch 1260 batch loss 1.41850543 epoch total loss 1.5614568\n",
      "Trained batch 1261 batch loss 1.40886414 epoch total loss 1.56133568\n",
      "Trained batch 1262 batch loss 1.44315195 epoch total loss 1.5612421\n",
      "Trained batch 1263 batch loss 1.43648398 epoch total loss 1.56114328\n",
      "Trained batch 1264 batch loss 1.4651022 epoch total loss 1.56106734\n",
      "Trained batch 1265 batch loss 1.47978914 epoch total loss 1.56100297\n",
      "Trained batch 1266 batch loss 1.44033384 epoch total loss 1.56090772\n",
      "Trained batch 1267 batch loss 1.52818918 epoch total loss 1.56088185\n",
      "Trained batch 1268 batch loss 1.47799253 epoch total loss 1.56081653\n",
      "Trained batch 1269 batch loss 1.52164745 epoch total loss 1.56078565\n",
      "Trained batch 1270 batch loss 1.41184497 epoch total loss 1.56066835\n",
      "Trained batch 1271 batch loss 1.36104858 epoch total loss 1.56051135\n",
      "Trained batch 1272 batch loss 1.37477875 epoch total loss 1.56036532\n",
      "Trained batch 1273 batch loss 1.47944975 epoch total loss 1.56030178\n",
      "Trained batch 1274 batch loss 1.37899685 epoch total loss 1.56015944\n",
      "Trained batch 1275 batch loss 1.48936069 epoch total loss 1.56010401\n",
      "Trained batch 1276 batch loss 1.49874854 epoch total loss 1.56005597\n",
      "Trained batch 1277 batch loss 1.44085622 epoch total loss 1.55996251\n",
      "Trained batch 1278 batch loss 1.41950369 epoch total loss 1.55985272\n",
      "Trained batch 1279 batch loss 1.39960861 epoch total loss 1.55972743\n",
      "Trained batch 1280 batch loss 1.47674155 epoch total loss 1.55966258\n",
      "Trained batch 1281 batch loss 1.43953943 epoch total loss 1.55956876\n",
      "Trained batch 1282 batch loss 1.45668745 epoch total loss 1.55948853\n",
      "Trained batch 1283 batch loss 1.50960541 epoch total loss 1.55944967\n",
      "Trained batch 1284 batch loss 1.29438591 epoch total loss 1.55924332\n",
      "Trained batch 1285 batch loss 1.41178906 epoch total loss 1.55912852\n",
      "Trained batch 1286 batch loss 1.39776707 epoch total loss 1.55900311\n",
      "Trained batch 1287 batch loss 1.46659732 epoch total loss 1.55893123\n",
      "Trained batch 1288 batch loss 1.43974221 epoch total loss 1.55883861\n",
      "Trained batch 1289 batch loss 1.39833927 epoch total loss 1.55871415\n",
      "Trained batch 1290 batch loss 1.37456441 epoch total loss 1.55857134\n",
      "Trained batch 1291 batch loss 1.35019946 epoch total loss 1.55840993\n",
      "Trained batch 1292 batch loss 1.47450948 epoch total loss 1.55834496\n",
      "Trained batch 1293 batch loss 1.36431217 epoch total loss 1.55819488\n",
      "Trained batch 1294 batch loss 1.35325456 epoch total loss 1.55803657\n",
      "Trained batch 1295 batch loss 1.46547663 epoch total loss 1.55796504\n",
      "Trained batch 1296 batch loss 1.4283421 epoch total loss 1.55786502\n",
      "Trained batch 1297 batch loss 1.46583712 epoch total loss 1.55779397\n",
      "Trained batch 1298 batch loss 1.63892841 epoch total loss 1.55785656\n",
      "Trained batch 1299 batch loss 1.4907949 epoch total loss 1.55780494\n",
      "Trained batch 1300 batch loss 1.51288939 epoch total loss 1.55777049\n",
      "Trained batch 1301 batch loss 1.39761508 epoch total loss 1.55764735\n",
      "Trained batch 1302 batch loss 1.52124047 epoch total loss 1.55761933\n",
      "Trained batch 1303 batch loss 1.38769233 epoch total loss 1.55748892\n",
      "Trained batch 1304 batch loss 1.49162722 epoch total loss 1.55743837\n",
      "Trained batch 1305 batch loss 1.41124856 epoch total loss 1.55732632\n",
      "Trained batch 1306 batch loss 1.31714416 epoch total loss 1.5571425\n",
      "Trained batch 1307 batch loss 1.42618978 epoch total loss 1.55704224\n",
      "Trained batch 1308 batch loss 1.31382298 epoch total loss 1.55685627\n",
      "Trained batch 1309 batch loss 1.53384 epoch total loss 1.55683875\n",
      "Trained batch 1310 batch loss 1.45339727 epoch total loss 1.55675972\n",
      "Trained batch 1311 batch loss 1.51156151 epoch total loss 1.55672526\n",
      "Trained batch 1312 batch loss 1.44395876 epoch total loss 1.55663931\n",
      "Trained batch 1313 batch loss 1.38635445 epoch total loss 1.55650961\n",
      "Trained batch 1314 batch loss 1.37018013 epoch total loss 1.55636787\n",
      "Trained batch 1315 batch loss 1.44324589 epoch total loss 1.55628181\n",
      "Trained batch 1316 batch loss 1.30517972 epoch total loss 1.55609107\n",
      "Trained batch 1317 batch loss 1.37296844 epoch total loss 1.55595195\n",
      "Trained batch 1318 batch loss 1.40603244 epoch total loss 1.55583823\n",
      "Trained batch 1319 batch loss 1.44201732 epoch total loss 1.55575204\n",
      "Trained batch 1320 batch loss 1.48721361 epoch total loss 1.55570018\n",
      "Trained batch 1321 batch loss 1.40898609 epoch total loss 1.55558908\n",
      "Trained batch 1322 batch loss 1.4424907 epoch total loss 1.55550337\n",
      "Trained batch 1323 batch loss 1.39126587 epoch total loss 1.55537927\n",
      "Trained batch 1324 batch loss 1.46449924 epoch total loss 1.55531073\n",
      "Trained batch 1325 batch loss 1.4064579 epoch total loss 1.55519843\n",
      "Trained batch 1326 batch loss 1.3636353 epoch total loss 1.55505395\n",
      "Trained batch 1327 batch loss 1.51820421 epoch total loss 1.55502617\n",
      "Trained batch 1328 batch loss 1.26850474 epoch total loss 1.55481052\n",
      "Trained batch 1329 batch loss 1.36060202 epoch total loss 1.55466437\n",
      "Trained batch 1330 batch loss 1.5148344 epoch total loss 1.55463445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1331 batch loss 1.49362445 epoch total loss 1.55458868\n",
      "Trained batch 1332 batch loss 1.49063408 epoch total loss 1.55454063\n",
      "Trained batch 1333 batch loss 1.34290564 epoch total loss 1.55438197\n",
      "Trained batch 1334 batch loss 1.37356472 epoch total loss 1.55424643\n",
      "Trained batch 1335 batch loss 1.35938358 epoch total loss 1.55410051\n",
      "Trained batch 1336 batch loss 1.28366888 epoch total loss 1.5538981\n",
      "Trained batch 1337 batch loss 1.2711165 epoch total loss 1.5536865\n",
      "Trained batch 1338 batch loss 1.35280824 epoch total loss 1.5535363\n",
      "Trained batch 1339 batch loss 1.37430966 epoch total loss 1.55340242\n",
      "Trained batch 1340 batch loss 1.37719274 epoch total loss 1.55327094\n",
      "Trained batch 1341 batch loss 1.45365453 epoch total loss 1.55319667\n",
      "Trained batch 1342 batch loss 1.53955388 epoch total loss 1.55318642\n",
      "Trained batch 1343 batch loss 1.46454191 epoch total loss 1.55312049\n",
      "Trained batch 1344 batch loss 1.43945324 epoch total loss 1.55303597\n",
      "Trained batch 1345 batch loss 1.19737411 epoch total loss 1.55277145\n",
      "Trained batch 1346 batch loss 1.27691 epoch total loss 1.55256641\n",
      "Trained batch 1347 batch loss 1.40086401 epoch total loss 1.55245376\n",
      "Trained batch 1348 batch loss 1.32665515 epoch total loss 1.55228627\n",
      "Trained batch 1349 batch loss 1.36483204 epoch total loss 1.55214727\n",
      "Trained batch 1350 batch loss 1.39778757 epoch total loss 1.55203283\n",
      "Trained batch 1351 batch loss 1.40315938 epoch total loss 1.55192268\n",
      "Trained batch 1352 batch loss 1.45772052 epoch total loss 1.55185294\n",
      "Trained batch 1353 batch loss 1.4633168 epoch total loss 1.55178761\n",
      "Trained batch 1354 batch loss 1.41888475 epoch total loss 1.55168951\n",
      "Trained batch 1355 batch loss 1.49426866 epoch total loss 1.55164719\n",
      "Trained batch 1356 batch loss 1.37201476 epoch total loss 1.55151474\n",
      "Trained batch 1357 batch loss 1.38644087 epoch total loss 1.55139315\n",
      "Trained batch 1358 batch loss 1.37821484 epoch total loss 1.5512656\n",
      "Trained batch 1359 batch loss 1.40090096 epoch total loss 1.55115497\n",
      "Trained batch 1360 batch loss 1.49031723 epoch total loss 1.55111015\n",
      "Trained batch 1361 batch loss 1.56753027 epoch total loss 1.55112231\n",
      "Trained batch 1362 batch loss 1.57264113 epoch total loss 1.55113816\n",
      "Trained batch 1363 batch loss 1.45716202 epoch total loss 1.55106926\n",
      "Trained batch 1364 batch loss 1.35156107 epoch total loss 1.55092299\n",
      "Trained batch 1365 batch loss 1.49659741 epoch total loss 1.55088317\n",
      "Trained batch 1366 batch loss 1.36859727 epoch total loss 1.55074978\n",
      "Trained batch 1367 batch loss 1.32947469 epoch total loss 1.55058801\n",
      "Trained batch 1368 batch loss 1.40482497 epoch total loss 1.55048144\n",
      "Trained batch 1369 batch loss 1.28572381 epoch total loss 1.55028796\n",
      "Trained batch 1370 batch loss 1.21689057 epoch total loss 1.55004454\n",
      "Trained batch 1371 batch loss 1.35510373 epoch total loss 1.54990244\n",
      "Trained batch 1372 batch loss 1.31029427 epoch total loss 1.5497278\n",
      "Trained batch 1373 batch loss 1.38478315 epoch total loss 1.54960763\n",
      "Trained batch 1374 batch loss 1.38827884 epoch total loss 1.54949021\n",
      "Trained batch 1375 batch loss 1.482651 epoch total loss 1.54944158\n",
      "Trained batch 1376 batch loss 1.4398303 epoch total loss 1.54936206\n",
      "Trained batch 1377 batch loss 1.43591332 epoch total loss 1.54927969\n",
      "Trained batch 1378 batch loss 1.4456495 epoch total loss 1.54920447\n",
      "Trained batch 1379 batch loss 1.35088956 epoch total loss 1.54906058\n",
      "Trained batch 1380 batch loss 1.47407281 epoch total loss 1.54900622\n",
      "Trained batch 1381 batch loss 1.31505156 epoch total loss 1.54883683\n",
      "Trained batch 1382 batch loss 1.10617566 epoch total loss 1.54851651\n",
      "Trained batch 1383 batch loss 1.18669593 epoch total loss 1.54825497\n",
      "Trained batch 1384 batch loss 1.37600815 epoch total loss 1.54813051\n",
      "Trained batch 1385 batch loss 1.544173 epoch total loss 1.54812765\n",
      "Trained batch 1386 batch loss 1.58035934 epoch total loss 1.54815078\n",
      "Trained batch 1387 batch loss 1.60597 epoch total loss 1.5481925\n",
      "Trained batch 1388 batch loss 1.56460392 epoch total loss 1.54820442\n",
      "Epoch 1 train loss 1.5482044219970703\n",
      "Validated batch 1 batch loss 1.4154923\n",
      "Validated batch 2 batch loss 1.36816728\n",
      "Validated batch 3 batch loss 1.39945567\n",
      "Validated batch 4 batch loss 1.42261052\n",
      "Validated batch 5 batch loss 1.43006849\n",
      "Validated batch 6 batch loss 1.45679665\n",
      "Validated batch 7 batch loss 1.43715107\n",
      "Validated batch 8 batch loss 1.44722462\n",
      "Validated batch 9 batch loss 1.45200372\n",
      "Validated batch 10 batch loss 1.48456252\n",
      "Validated batch 11 batch loss 1.50715148\n",
      "Validated batch 12 batch loss 1.44186151\n",
      "Validated batch 13 batch loss 1.43456292\n",
      "Validated batch 14 batch loss 1.38773251\n",
      "Validated batch 15 batch loss 1.46634936\n",
      "Validated batch 16 batch loss 1.46838856\n",
      "Validated batch 17 batch loss 1.51302338\n",
      "Validated batch 18 batch loss 1.44166338\n",
      "Validated batch 19 batch loss 1.39918649\n",
      "Validated batch 20 batch loss 1.49622393\n",
      "Validated batch 21 batch loss 1.41325676\n",
      "Validated batch 22 batch loss 1.43235159\n",
      "Validated batch 23 batch loss 1.4771874\n",
      "Validated batch 24 batch loss 1.41770947\n",
      "Validated batch 25 batch loss 1.47764659\n",
      "Validated batch 26 batch loss 1.33172035\n",
      "Validated batch 27 batch loss 1.48386145\n",
      "Validated batch 28 batch loss 1.44574714\n",
      "Validated batch 29 batch loss 1.43319321\n",
      "Validated batch 30 batch loss 1.53573406\n",
      "Validated batch 31 batch loss 1.4947716\n",
      "Validated batch 32 batch loss 1.49282622\n",
      "Validated batch 33 batch loss 1.50901592\n",
      "Validated batch 34 batch loss 1.55625355\n",
      "Validated batch 35 batch loss 1.48868573\n",
      "Validated batch 36 batch loss 1.42402434\n",
      "Validated batch 37 batch loss 1.5416981\n",
      "Validated batch 38 batch loss 1.50264311\n",
      "Validated batch 39 batch loss 1.43404901\n",
      "Validated batch 40 batch loss 1.54678321\n",
      "Validated batch 41 batch loss 1.26218843\n",
      "Validated batch 42 batch loss 1.44955897\n",
      "Validated batch 43 batch loss 1.3051\n",
      "Validated batch 44 batch loss 1.46496391\n",
      "Validated batch 45 batch loss 1.54875028\n",
      "Validated batch 46 batch loss 1.36948681\n",
      "Validated batch 47 batch loss 1.56395888\n",
      "Validated batch 48 batch loss 1.43573046\n",
      "Validated batch 49 batch loss 1.43456376\n",
      "Validated batch 50 batch loss 1.40364349\n",
      "Validated batch 51 batch loss 1.43251395\n",
      "Validated batch 52 batch loss 1.445575\n",
      "Validated batch 53 batch loss 1.46498287\n",
      "Validated batch 54 batch loss 1.35977626\n",
      "Validated batch 55 batch loss 1.42628777\n",
      "Validated batch 56 batch loss 1.40837932\n",
      "Validated batch 57 batch loss 1.45522118\n",
      "Validated batch 58 batch loss 1.43739748\n",
      "Validated batch 59 batch loss 1.43985069\n",
      "Validated batch 60 batch loss 1.38441706\n",
      "Validated batch 61 batch loss 1.40228367\n",
      "Validated batch 62 batch loss 1.45139658\n",
      "Validated batch 63 batch loss 1.46778297\n",
      "Validated batch 64 batch loss 1.4663589\n",
      "Validated batch 65 batch loss 1.56261635\n",
      "Validated batch 66 batch loss 1.63925445\n",
      "Validated batch 67 batch loss 1.54068851\n",
      "Validated batch 68 batch loss 1.46823502\n",
      "Validated batch 69 batch loss 1.32991159\n",
      "Validated batch 70 batch loss 1.46789718\n",
      "Validated batch 71 batch loss 1.41321445\n",
      "Validated batch 72 batch loss 1.42617846\n",
      "Validated batch 73 batch loss 1.35247946\n",
      "Validated batch 74 batch loss 1.41969621\n",
      "Validated batch 75 batch loss 1.54622197\n",
      "Validated batch 76 batch loss 1.36237013\n",
      "Validated batch 77 batch loss 1.47254133\n",
      "Validated batch 78 batch loss 1.44442713\n",
      "Validated batch 79 batch loss 1.43611336\n",
      "Validated batch 80 batch loss 1.44424736\n",
      "Validated batch 81 batch loss 1.32087493\n",
      "Validated batch 82 batch loss 1.52587199\n",
      "Validated batch 83 batch loss 1.41507411\n",
      "Validated batch 84 batch loss 1.44690049\n",
      "Validated batch 85 batch loss 1.41231132\n",
      "Validated batch 86 batch loss 1.42825365\n",
      "Validated batch 87 batch loss 1.34858751\n",
      "Validated batch 88 batch loss 1.41045356\n",
      "Validated batch 89 batch loss 1.40063453\n",
      "Validated batch 90 batch loss 1.37445045\n",
      "Validated batch 91 batch loss 1.42913806\n",
      "Validated batch 92 batch loss 1.43688142\n",
      "Validated batch 93 batch loss 1.40592098\n",
      "Validated batch 94 batch loss 1.51637614\n",
      "Validated batch 95 batch loss 1.53085494\n",
      "Validated batch 96 batch loss 1.41381633\n",
      "Validated batch 97 batch loss 1.48553681\n",
      "Validated batch 98 batch loss 1.54840124\n",
      "Validated batch 99 batch loss 1.32132912\n",
      "Validated batch 100 batch loss 1.45722973\n",
      "Validated batch 101 batch loss 1.43697917\n",
      "Validated batch 102 batch loss 1.47901416\n",
      "Validated batch 103 batch loss 1.49974489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 104 batch loss 1.38179207\n",
      "Validated batch 105 batch loss 1.25204921\n",
      "Validated batch 106 batch loss 1.45615935\n",
      "Validated batch 107 batch loss 1.43008041\n",
      "Validated batch 108 batch loss 1.45442152\n",
      "Validated batch 109 batch loss 1.45813191\n",
      "Validated batch 110 batch loss 1.43685913\n",
      "Validated batch 111 batch loss 1.4839474\n",
      "Validated batch 112 batch loss 1.51958799\n",
      "Validated batch 113 batch loss 1.4358654\n",
      "Validated batch 114 batch loss 1.47252214\n",
      "Validated batch 115 batch loss 1.37993121\n",
      "Validated batch 116 batch loss 1.37858725\n",
      "Validated batch 117 batch loss 1.38659132\n",
      "Validated batch 118 batch loss 1.41703629\n",
      "Validated batch 119 batch loss 1.36780751\n",
      "Validated batch 120 batch loss 1.37571645\n",
      "Validated batch 121 batch loss 1.5293684\n",
      "Validated batch 122 batch loss 1.44445121\n",
      "Validated batch 123 batch loss 1.47117531\n",
      "Validated batch 124 batch loss 1.52784538\n",
      "Validated batch 125 batch loss 1.50164616\n",
      "Validated batch 126 batch loss 1.45393205\n",
      "Validated batch 127 batch loss 1.52104867\n",
      "Validated batch 128 batch loss 1.45635748\n",
      "Validated batch 129 batch loss 1.51287365\n",
      "Validated batch 130 batch loss 1.52771246\n",
      "Validated batch 131 batch loss 1.53339946\n",
      "Validated batch 132 batch loss 1.48045564\n",
      "Validated batch 133 batch loss 1.35117948\n",
      "Validated batch 134 batch loss 1.44341898\n",
      "Validated batch 135 batch loss 1.46437\n",
      "Validated batch 136 batch loss 1.5087378\n",
      "Validated batch 137 batch loss 1.44552922\n",
      "Validated batch 138 batch loss 1.41507936\n",
      "Validated batch 139 batch loss 1.41954982\n",
      "Validated batch 140 batch loss 1.43871808\n",
      "Validated batch 141 batch loss 1.42631423\n",
      "Validated batch 142 batch loss 1.24532187\n",
      "Validated batch 143 batch loss 1.35004187\n",
      "Validated batch 144 batch loss 1.48249936\n",
      "Validated batch 145 batch loss 1.38131666\n",
      "Validated batch 146 batch loss 1.40269732\n",
      "Validated batch 147 batch loss 1.392735\n",
      "Validated batch 148 batch loss 1.46944427\n",
      "Validated batch 149 batch loss 1.38070536\n",
      "Validated batch 150 batch loss 1.45567179\n",
      "Validated batch 151 batch loss 1.37254953\n",
      "Validated batch 152 batch loss 1.46478188\n",
      "Validated batch 153 batch loss 1.49801981\n",
      "Validated batch 154 batch loss 1.52243781\n",
      "Validated batch 155 batch loss 1.40968895\n",
      "Validated batch 156 batch loss 1.52175987\n",
      "Validated batch 157 batch loss 1.366503\n",
      "Validated batch 158 batch loss 1.30398154\n",
      "Validated batch 159 batch loss 1.4591651\n",
      "Validated batch 160 batch loss 1.42958832\n",
      "Validated batch 161 batch loss 1.53188384\n",
      "Validated batch 162 batch loss 1.52200377\n",
      "Validated batch 163 batch loss 1.50011337\n",
      "Validated batch 164 batch loss 1.39832664\n",
      "Validated batch 165 batch loss 1.36092353\n",
      "Validated batch 166 batch loss 1.46784163\n",
      "Validated batch 167 batch loss 1.41446066\n",
      "Validated batch 168 batch loss 1.40829623\n",
      "Validated batch 169 batch loss 1.40191841\n",
      "Validated batch 170 batch loss 1.34789491\n",
      "Validated batch 171 batch loss 1.5132674\n",
      "Validated batch 172 batch loss 1.41497326\n",
      "Validated batch 173 batch loss 1.32750988\n",
      "Validated batch 174 batch loss 1.36972237\n",
      "Validated batch 175 batch loss 1.50952053\n",
      "Validated batch 176 batch loss 1.47132945\n",
      "Validated batch 177 batch loss 1.51317477\n",
      "Validated batch 178 batch loss 1.37364781\n",
      "Validated batch 179 batch loss 1.52265739\n",
      "Validated batch 180 batch loss 1.40433562\n",
      "Validated batch 181 batch loss 1.47870398\n",
      "Validated batch 182 batch loss 1.51073873\n",
      "Validated batch 183 batch loss 1.26305687\n",
      "Validated batch 184 batch loss 1.40914273\n",
      "Validated batch 185 batch loss 1.57586396\n",
      "Epoch 1 val loss 1.4423792362213135\n",
      "Model /aiffel/aiffel/model_weight/GD08/model-epoch-1-loss-1.4424.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.46217728 epoch total loss 1.46217728\n",
      "Trained batch 2 batch loss 1.55930436 epoch total loss 1.51074076\n",
      "Trained batch 3 batch loss 1.46480727 epoch total loss 1.49542964\n",
      "Trained batch 4 batch loss 1.52394891 epoch total loss 1.50255942\n",
      "Trained batch 5 batch loss 1.48142 epoch total loss 1.49833155\n",
      "Trained batch 6 batch loss 1.53193247 epoch total loss 1.50393164\n",
      "Trained batch 7 batch loss 1.45302248 epoch total loss 1.49665892\n",
      "Trained batch 8 batch loss 1.40500832 epoch total loss 1.48520255\n",
      "Trained batch 9 batch loss 1.49167347 epoch total loss 1.4859215\n",
      "Trained batch 10 batch loss 1.53183913 epoch total loss 1.49051332\n",
      "Trained batch 11 batch loss 1.5544014 epoch total loss 1.4963212\n",
      "Trained batch 12 batch loss 1.51557529 epoch total loss 1.49792576\n",
      "Trained batch 13 batch loss 1.46069443 epoch total loss 1.49506187\n",
      "Trained batch 14 batch loss 1.4773922 epoch total loss 1.49379981\n",
      "Trained batch 15 batch loss 1.43775046 epoch total loss 1.49006307\n",
      "Trained batch 16 batch loss 1.50351906 epoch total loss 1.49090409\n",
      "Trained batch 17 batch loss 1.48505259 epoch total loss 1.49055982\n",
      "Trained batch 18 batch loss 1.5246526 epoch total loss 1.49245393\n",
      "Trained batch 19 batch loss 1.44085598 epoch total loss 1.48973823\n",
      "Trained batch 20 batch loss 1.40280604 epoch total loss 1.48539162\n",
      "Trained batch 21 batch loss 1.31546307 epoch total loss 1.47729981\n",
      "Trained batch 22 batch loss 1.28522754 epoch total loss 1.46856928\n",
      "Trained batch 23 batch loss 1.33262193 epoch total loss 1.46265864\n",
      "Trained batch 24 batch loss 1.4113884 epoch total loss 1.46052229\n",
      "Trained batch 25 batch loss 1.37590134 epoch total loss 1.45713747\n",
      "Trained batch 26 batch loss 1.53773165 epoch total loss 1.46023726\n",
      "Trained batch 27 batch loss 1.5976243 epoch total loss 1.46532571\n",
      "Trained batch 28 batch loss 1.49477577 epoch total loss 1.4663775\n",
      "Trained batch 29 batch loss 1.44463015 epoch total loss 1.46562755\n",
      "Trained batch 30 batch loss 1.39757144 epoch total loss 1.46335912\n",
      "Trained batch 31 batch loss 1.44682336 epoch total loss 1.46282566\n",
      "Trained batch 32 batch loss 1.46397805 epoch total loss 1.46286166\n",
      "Trained batch 33 batch loss 1.53392684 epoch total loss 1.46501517\n",
      "Trained batch 34 batch loss 1.58665311 epoch total loss 1.46859276\n",
      "Trained batch 35 batch loss 1.62381315 epoch total loss 1.47302771\n",
      "Trained batch 36 batch loss 1.59342241 epoch total loss 1.476372\n",
      "Trained batch 37 batch loss 1.52067971 epoch total loss 1.47756946\n",
      "Trained batch 38 batch loss 1.53168392 epoch total loss 1.47899354\n",
      "Trained batch 39 batch loss 1.60857558 epoch total loss 1.48231614\n",
      "Trained batch 40 batch loss 1.5581336 epoch total loss 1.48421156\n",
      "Trained batch 41 batch loss 1.589136 epoch total loss 1.48677063\n",
      "Trained batch 42 batch loss 1.42849851 epoch total loss 1.48538315\n",
      "Trained batch 43 batch loss 1.35812449 epoch total loss 1.48242366\n",
      "Trained batch 44 batch loss 1.39329064 epoch total loss 1.48039782\n",
      "Trained batch 45 batch loss 1.40426493 epoch total loss 1.478706\n",
      "Trained batch 46 batch loss 1.40264273 epoch total loss 1.47705245\n",
      "Trained batch 47 batch loss 1.40518713 epoch total loss 1.47552347\n",
      "Trained batch 48 batch loss 1.36896634 epoch total loss 1.47330344\n",
      "Trained batch 49 batch loss 1.39802551 epoch total loss 1.47176719\n",
      "Trained batch 50 batch loss 1.47755539 epoch total loss 1.47188294\n",
      "Trained batch 51 batch loss 1.49632072 epoch total loss 1.47236216\n",
      "Trained batch 52 batch loss 1.49872816 epoch total loss 1.47286916\n",
      "Trained batch 53 batch loss 1.45588458 epoch total loss 1.47254872\n",
      "Trained batch 54 batch loss 1.47416532 epoch total loss 1.47257864\n",
      "Trained batch 55 batch loss 1.44258189 epoch total loss 1.47203326\n",
      "Trained batch 56 batch loss 1.28108764 epoch total loss 1.46862352\n",
      "Trained batch 57 batch loss 1.34281051 epoch total loss 1.46641636\n",
      "Trained batch 58 batch loss 1.45892179 epoch total loss 1.46628714\n",
      "Trained batch 59 batch loss 1.41558313 epoch total loss 1.46542776\n",
      "Trained batch 60 batch loss 1.49874449 epoch total loss 1.46598291\n",
      "Trained batch 61 batch loss 1.28992367 epoch total loss 1.46309674\n",
      "Trained batch 62 batch loss 1.38769507 epoch total loss 1.46188056\n",
      "Trained batch 63 batch loss 1.4375205 epoch total loss 1.46149397\n",
      "Trained batch 64 batch loss 1.33612704 epoch total loss 1.45953512\n",
      "Trained batch 65 batch loss 1.24493968 epoch total loss 1.45623374\n",
      "Trained batch 66 batch loss 1.33385563 epoch total loss 1.45437944\n",
      "Trained batch 67 batch loss 1.50570726 epoch total loss 1.45514548\n",
      "Trained batch 68 batch loss 1.47683489 epoch total loss 1.45546448\n",
      "Trained batch 69 batch loss 1.50296891 epoch total loss 1.45615304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 70 batch loss 1.27202392 epoch total loss 1.45352256\n",
      "Trained batch 71 batch loss 1.27889526 epoch total loss 1.45106304\n",
      "Trained batch 72 batch loss 1.28818178 epoch total loss 1.4488008\n",
      "Trained batch 73 batch loss 1.47301483 epoch total loss 1.44913256\n",
      "Trained batch 74 batch loss 1.55378175 epoch total loss 1.45054662\n",
      "Trained batch 75 batch loss 1.56103814 epoch total loss 1.45201981\n",
      "Trained batch 76 batch loss 1.54936874 epoch total loss 1.45330083\n",
      "Trained batch 77 batch loss 1.52116394 epoch total loss 1.45418215\n",
      "Trained batch 78 batch loss 1.48564672 epoch total loss 1.45458555\n",
      "Trained batch 79 batch loss 1.49414706 epoch total loss 1.45508635\n",
      "Trained batch 80 batch loss 1.45825148 epoch total loss 1.45512593\n",
      "Trained batch 81 batch loss 1.4306711 epoch total loss 1.45482397\n",
      "Trained batch 82 batch loss 1.47961569 epoch total loss 1.45512629\n",
      "Trained batch 83 batch loss 1.46657693 epoch total loss 1.45526421\n",
      "Trained batch 84 batch loss 1.53247011 epoch total loss 1.45618343\n",
      "Trained batch 85 batch loss 1.46339202 epoch total loss 1.45626819\n",
      "Trained batch 86 batch loss 1.41808832 epoch total loss 1.45582426\n",
      "Trained batch 87 batch loss 1.34306741 epoch total loss 1.45452833\n",
      "Trained batch 88 batch loss 1.34735906 epoch total loss 1.45331049\n",
      "Trained batch 89 batch loss 1.32970405 epoch total loss 1.45192158\n",
      "Trained batch 90 batch loss 1.46857595 epoch total loss 1.45210671\n",
      "Trained batch 91 batch loss 1.49250865 epoch total loss 1.45255065\n",
      "Trained batch 92 batch loss 1.46137571 epoch total loss 1.45264661\n",
      "Trained batch 93 batch loss 1.2964673 epoch total loss 1.45096731\n",
      "Trained batch 94 batch loss 1.33105826 epoch total loss 1.44969165\n",
      "Trained batch 95 batch loss 1.34101915 epoch total loss 1.44854772\n",
      "Trained batch 96 batch loss 1.47271776 epoch total loss 1.44879949\n",
      "Trained batch 97 batch loss 1.46097171 epoch total loss 1.4489249\n",
      "Trained batch 98 batch loss 1.35527241 epoch total loss 1.4479692\n",
      "Trained batch 99 batch loss 1.26465344 epoch total loss 1.44611752\n",
      "Trained batch 100 batch loss 1.38127506 epoch total loss 1.44546902\n",
      "Trained batch 101 batch loss 1.45038164 epoch total loss 1.44551766\n",
      "Trained batch 102 batch loss 1.48442578 epoch total loss 1.44589901\n",
      "Trained batch 103 batch loss 1.51523936 epoch total loss 1.4465723\n",
      "Trained batch 104 batch loss 1.39457202 epoch total loss 1.44607234\n",
      "Trained batch 105 batch loss 1.33077884 epoch total loss 1.4449743\n",
      "Trained batch 106 batch loss 1.27821076 epoch total loss 1.4434011\n",
      "Trained batch 107 batch loss 1.31687856 epoch total loss 1.44221866\n",
      "Trained batch 108 batch loss 1.3483665 epoch total loss 1.44134974\n",
      "Trained batch 109 batch loss 1.31060719 epoch total loss 1.44015026\n",
      "Trained batch 110 batch loss 1.48001337 epoch total loss 1.44051266\n",
      "Trained batch 111 batch loss 1.46114707 epoch total loss 1.44069862\n",
      "Trained batch 112 batch loss 1.6529938 epoch total loss 1.44259417\n",
      "Trained batch 113 batch loss 1.70298576 epoch total loss 1.44489849\n",
      "Trained batch 114 batch loss 1.61735582 epoch total loss 1.44641125\n",
      "Trained batch 115 batch loss 1.48899567 epoch total loss 1.44678164\n",
      "Trained batch 116 batch loss 1.53757751 epoch total loss 1.44756436\n",
      "Trained batch 117 batch loss 1.40333188 epoch total loss 1.44718635\n",
      "Trained batch 118 batch loss 1.33771968 epoch total loss 1.44625866\n",
      "Trained batch 119 batch loss 1.33082747 epoch total loss 1.44528866\n",
      "Trained batch 120 batch loss 1.47938263 epoch total loss 1.44557273\n",
      "Trained batch 121 batch loss 1.48730087 epoch total loss 1.44591773\n",
      "Trained batch 122 batch loss 1.40089238 epoch total loss 1.44554865\n",
      "Trained batch 123 batch loss 1.40036488 epoch total loss 1.44518125\n",
      "Trained batch 124 batch loss 1.38667011 epoch total loss 1.44470942\n",
      "Trained batch 125 batch loss 1.26180542 epoch total loss 1.44324625\n",
      "Trained batch 126 batch loss 1.36378431 epoch total loss 1.44261563\n",
      "Trained batch 127 batch loss 1.39725804 epoch total loss 1.44225848\n",
      "Trained batch 128 batch loss 1.35247517 epoch total loss 1.44155705\n",
      "Trained batch 129 batch loss 1.32327127 epoch total loss 1.44064009\n",
      "Trained batch 130 batch loss 1.45036674 epoch total loss 1.44071496\n",
      "Trained batch 131 batch loss 1.333359 epoch total loss 1.43989539\n",
      "Trained batch 132 batch loss 1.43202865 epoch total loss 1.43983579\n",
      "Trained batch 133 batch loss 1.53364742 epoch total loss 1.44054103\n",
      "Trained batch 134 batch loss 1.48313677 epoch total loss 1.44085896\n",
      "Trained batch 135 batch loss 1.46034598 epoch total loss 1.44100332\n",
      "Trained batch 136 batch loss 1.27489531 epoch total loss 1.43978202\n",
      "Trained batch 137 batch loss 1.44646323 epoch total loss 1.43983066\n",
      "Trained batch 138 batch loss 1.44811559 epoch total loss 1.43989074\n",
      "Trained batch 139 batch loss 1.36453676 epoch total loss 1.43934858\n",
      "Trained batch 140 batch loss 1.25762153 epoch total loss 1.43805051\n",
      "Trained batch 141 batch loss 1.317379 epoch total loss 1.43719471\n",
      "Trained batch 142 batch loss 1.28548133 epoch total loss 1.43612623\n",
      "Trained batch 143 batch loss 1.33091283 epoch total loss 1.43539059\n",
      "Trained batch 144 batch loss 1.30318105 epoch total loss 1.43447244\n",
      "Trained batch 145 batch loss 1.4000361 epoch total loss 1.43423498\n",
      "Trained batch 146 batch loss 1.45469809 epoch total loss 1.43437505\n",
      "Trained batch 147 batch loss 1.46420419 epoch total loss 1.43457794\n",
      "Trained batch 148 batch loss 1.49541652 epoch total loss 1.43498909\n",
      "Trained batch 149 batch loss 1.38518965 epoch total loss 1.43465495\n",
      "Trained batch 150 batch loss 1.68084812 epoch total loss 1.43629622\n",
      "Trained batch 151 batch loss 1.53134418 epoch total loss 1.43692565\n",
      "Trained batch 152 batch loss 1.49355 epoch total loss 1.43729818\n",
      "Trained batch 153 batch loss 1.44321179 epoch total loss 1.43733668\n",
      "Trained batch 154 batch loss 1.47898006 epoch total loss 1.43760705\n",
      "Trained batch 155 batch loss 1.50002337 epoch total loss 1.43800986\n",
      "Trained batch 156 batch loss 1.49214292 epoch total loss 1.43835688\n",
      "Trained batch 157 batch loss 1.4415437 epoch total loss 1.43837714\n",
      "Trained batch 158 batch loss 1.44665098 epoch total loss 1.43842947\n",
      "Trained batch 159 batch loss 1.37122476 epoch total loss 1.43800688\n",
      "Trained batch 160 batch loss 1.38273454 epoch total loss 1.43766141\n",
      "Trained batch 161 batch loss 1.49541092 epoch total loss 1.43802011\n",
      "Trained batch 162 batch loss 1.42115331 epoch total loss 1.43791604\n",
      "Trained batch 163 batch loss 1.41021967 epoch total loss 1.43774605\n",
      "Trained batch 164 batch loss 1.41591775 epoch total loss 1.43761301\n",
      "Trained batch 165 batch loss 1.34905577 epoch total loss 1.43707633\n",
      "Trained batch 166 batch loss 1.37429607 epoch total loss 1.4366982\n",
      "Trained batch 167 batch loss 1.22381938 epoch total loss 1.43542349\n",
      "Trained batch 168 batch loss 1.21669459 epoch total loss 1.43412149\n",
      "Trained batch 169 batch loss 1.25010836 epoch total loss 1.43303263\n",
      "Trained batch 170 batch loss 1.47197294 epoch total loss 1.43326163\n",
      "Trained batch 171 batch loss 1.45283592 epoch total loss 1.43337607\n",
      "Trained batch 172 batch loss 1.54506767 epoch total loss 1.43402553\n",
      "Trained batch 173 batch loss 1.62941647 epoch total loss 1.43515491\n",
      "Trained batch 174 batch loss 1.44319868 epoch total loss 1.43520105\n",
      "Trained batch 175 batch loss 1.40596128 epoch total loss 1.43503404\n",
      "Trained batch 176 batch loss 1.36363375 epoch total loss 1.43462837\n",
      "Trained batch 177 batch loss 1.43827891 epoch total loss 1.43464899\n",
      "Trained batch 178 batch loss 1.48404944 epoch total loss 1.43492651\n",
      "Trained batch 179 batch loss 1.38318133 epoch total loss 1.43463743\n",
      "Trained batch 180 batch loss 1.37767494 epoch total loss 1.43432105\n",
      "Trained batch 181 batch loss 1.35815716 epoch total loss 1.43390024\n",
      "Trained batch 182 batch loss 1.39023185 epoch total loss 1.43366027\n",
      "Trained batch 183 batch loss 1.29967761 epoch total loss 1.4329282\n",
      "Trained batch 184 batch loss 1.32843232 epoch total loss 1.43236029\n",
      "Trained batch 185 batch loss 1.3327477 epoch total loss 1.4318217\n",
      "Trained batch 186 batch loss 1.37692463 epoch total loss 1.43152654\n",
      "Trained batch 187 batch loss 1.27822781 epoch total loss 1.43070686\n",
      "Trained batch 188 batch loss 1.32971931 epoch total loss 1.43016958\n",
      "Trained batch 189 batch loss 1.39696896 epoch total loss 1.42999399\n",
      "Trained batch 190 batch loss 1.35639167 epoch total loss 1.42960656\n",
      "Trained batch 191 batch loss 1.38438964 epoch total loss 1.42936981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 192 batch loss 1.4146632 epoch total loss 1.42929327\n",
      "Trained batch 193 batch loss 1.42677987 epoch total loss 1.42928028\n",
      "Trained batch 194 batch loss 1.46629655 epoch total loss 1.42947125\n",
      "Trained batch 195 batch loss 1.36480081 epoch total loss 1.42913961\n",
      "Trained batch 196 batch loss 1.43621492 epoch total loss 1.42917573\n",
      "Trained batch 197 batch loss 1.47966015 epoch total loss 1.42943203\n",
      "Trained batch 198 batch loss 1.50689185 epoch total loss 1.42982328\n",
      "Trained batch 199 batch loss 1.36926222 epoch total loss 1.42951894\n",
      "Trained batch 200 batch loss 1.48028052 epoch total loss 1.42977285\n",
      "Trained batch 201 batch loss 1.4775027 epoch total loss 1.43001032\n",
      "Trained batch 202 batch loss 1.41973901 epoch total loss 1.42995942\n",
      "Trained batch 203 batch loss 1.45199537 epoch total loss 1.43006802\n",
      "Trained batch 204 batch loss 1.5088551 epoch total loss 1.43045413\n",
      "Trained batch 205 batch loss 1.45182097 epoch total loss 1.43055832\n",
      "Trained batch 206 batch loss 1.36024714 epoch total loss 1.43021715\n",
      "Trained batch 207 batch loss 1.4315865 epoch total loss 1.4302237\n",
      "Trained batch 208 batch loss 1.40891433 epoch total loss 1.43012118\n",
      "Trained batch 209 batch loss 1.47937644 epoch total loss 1.43035686\n",
      "Trained batch 210 batch loss 1.43296719 epoch total loss 1.43036926\n",
      "Trained batch 211 batch loss 1.42468202 epoch total loss 1.4303422\n",
      "Trained batch 212 batch loss 1.43867588 epoch total loss 1.43038166\n",
      "Trained batch 213 batch loss 1.43913734 epoch total loss 1.43042278\n",
      "Trained batch 214 batch loss 1.52155161 epoch total loss 1.4308486\n",
      "Trained batch 215 batch loss 1.44127047 epoch total loss 1.43089712\n",
      "Trained batch 216 batch loss 1.41060746 epoch total loss 1.43080318\n",
      "Trained batch 217 batch loss 1.42839324 epoch total loss 1.43079221\n",
      "Trained batch 218 batch loss 1.50288129 epoch total loss 1.43112278\n",
      "Trained batch 219 batch loss 1.38464522 epoch total loss 1.43091059\n",
      "Trained batch 220 batch loss 1.42787695 epoch total loss 1.43089688\n",
      "Trained batch 221 batch loss 1.39969015 epoch total loss 1.43075562\n",
      "Trained batch 222 batch loss 1.33353829 epoch total loss 1.43031764\n",
      "Trained batch 223 batch loss 1.45115316 epoch total loss 1.43041098\n",
      "Trained batch 224 batch loss 1.4187088 epoch total loss 1.43035877\n",
      "Trained batch 225 batch loss 1.43188977 epoch total loss 1.43036556\n",
      "Trained batch 226 batch loss 1.3643291 epoch total loss 1.43007326\n",
      "Trained batch 227 batch loss 1.50789356 epoch total loss 1.43041611\n",
      "Trained batch 228 batch loss 1.44327569 epoch total loss 1.43047249\n",
      "Trained batch 229 batch loss 1.41461468 epoch total loss 1.43040323\n",
      "Trained batch 230 batch loss 1.46718431 epoch total loss 1.43056321\n",
      "Trained batch 231 batch loss 1.51588464 epoch total loss 1.43093264\n",
      "Trained batch 232 batch loss 1.47397196 epoch total loss 1.43111813\n",
      "Trained batch 233 batch loss 1.38333774 epoch total loss 1.43091309\n",
      "Trained batch 234 batch loss 1.37884033 epoch total loss 1.43069053\n",
      "Trained batch 235 batch loss 1.39621413 epoch total loss 1.43054378\n",
      "Trained batch 236 batch loss 1.27902639 epoch total loss 1.42990172\n",
      "Trained batch 237 batch loss 1.26100874 epoch total loss 1.42918921\n",
      "Trained batch 238 batch loss 1.43061614 epoch total loss 1.42919517\n",
      "Trained batch 239 batch loss 1.52149916 epoch total loss 1.42958128\n",
      "Trained batch 240 batch loss 1.36629677 epoch total loss 1.42931759\n",
      "Trained batch 241 batch loss 1.39115596 epoch total loss 1.42915916\n",
      "Trained batch 242 batch loss 1.46767688 epoch total loss 1.42931843\n",
      "Trained batch 243 batch loss 1.42960227 epoch total loss 1.4293195\n",
      "Trained batch 244 batch loss 1.36164904 epoch total loss 1.42904222\n",
      "Trained batch 245 batch loss 1.3067404 epoch total loss 1.42854297\n",
      "Trained batch 246 batch loss 1.39669013 epoch total loss 1.42841363\n",
      "Trained batch 247 batch loss 1.4474622 epoch total loss 1.42849064\n",
      "Trained batch 248 batch loss 1.43476081 epoch total loss 1.42851591\n",
      "Trained batch 249 batch loss 1.41799235 epoch total loss 1.42847371\n",
      "Trained batch 250 batch loss 1.39027917 epoch total loss 1.42832088\n",
      "Trained batch 251 batch loss 1.33276916 epoch total loss 1.42794025\n",
      "Trained batch 252 batch loss 1.34428775 epoch total loss 1.42760825\n",
      "Trained batch 253 batch loss 1.38237715 epoch total loss 1.42742956\n",
      "Trained batch 254 batch loss 1.50652218 epoch total loss 1.42774093\n",
      "Trained batch 255 batch loss 1.44892943 epoch total loss 1.42782414\n",
      "Trained batch 256 batch loss 1.43299556 epoch total loss 1.42784429\n",
      "Trained batch 257 batch loss 1.39656079 epoch total loss 1.42772257\n",
      "Trained batch 258 batch loss 1.28426874 epoch total loss 1.42716658\n",
      "Trained batch 259 batch loss 1.38624084 epoch total loss 1.42700851\n",
      "Trained batch 260 batch loss 1.45558584 epoch total loss 1.42711854\n",
      "Trained batch 261 batch loss 1.51876938 epoch total loss 1.42746961\n",
      "Trained batch 262 batch loss 1.41190279 epoch total loss 1.42741024\n",
      "Trained batch 263 batch loss 1.39875126 epoch total loss 1.42730117\n",
      "Trained batch 264 batch loss 1.38693678 epoch total loss 1.42714834\n",
      "Trained batch 265 batch loss 1.39951956 epoch total loss 1.42704403\n",
      "Trained batch 266 batch loss 1.41766047 epoch total loss 1.42700875\n",
      "Trained batch 267 batch loss 1.42675209 epoch total loss 1.42700779\n",
      "Trained batch 268 batch loss 1.40712833 epoch total loss 1.42693365\n",
      "Trained batch 269 batch loss 1.49450958 epoch total loss 1.42718482\n",
      "Trained batch 270 batch loss 1.44215012 epoch total loss 1.42724025\n",
      "Trained batch 271 batch loss 1.47276378 epoch total loss 1.42740822\n",
      "Trained batch 272 batch loss 1.43793464 epoch total loss 1.42744696\n",
      "Trained batch 273 batch loss 1.46547866 epoch total loss 1.42758632\n",
      "Trained batch 274 batch loss 1.39396167 epoch total loss 1.42746353\n",
      "Trained batch 275 batch loss 1.35126352 epoch total loss 1.42718637\n",
      "Trained batch 276 batch loss 1.39315271 epoch total loss 1.42706311\n",
      "Trained batch 277 batch loss 1.3951838 epoch total loss 1.42694795\n",
      "Trained batch 278 batch loss 1.31404924 epoch total loss 1.42654192\n",
      "Trained batch 279 batch loss 1.45561326 epoch total loss 1.42664611\n",
      "Trained batch 280 batch loss 1.39570594 epoch total loss 1.42653561\n",
      "Trained batch 281 batch loss 1.42069626 epoch total loss 1.42651474\n",
      "Trained batch 282 batch loss 1.36547542 epoch total loss 1.42629826\n",
      "Trained batch 283 batch loss 1.38812411 epoch total loss 1.42616343\n",
      "Trained batch 284 batch loss 1.4369123 epoch total loss 1.42620134\n",
      "Trained batch 285 batch loss 1.44092822 epoch total loss 1.42625296\n",
      "Trained batch 286 batch loss 1.41258407 epoch total loss 1.42620516\n",
      "Trained batch 287 batch loss 1.42376947 epoch total loss 1.42619669\n",
      "Trained batch 288 batch loss 1.35742366 epoch total loss 1.42595792\n",
      "Trained batch 289 batch loss 1.44159245 epoch total loss 1.42601204\n",
      "Trained batch 290 batch loss 1.39842689 epoch total loss 1.42591691\n",
      "Trained batch 291 batch loss 1.42446983 epoch total loss 1.4259119\n",
      "Trained batch 292 batch loss 1.40589345 epoch total loss 1.42584336\n",
      "Trained batch 293 batch loss 1.50479162 epoch total loss 1.42611277\n",
      "Trained batch 294 batch loss 1.30456281 epoch total loss 1.42569935\n",
      "Trained batch 295 batch loss 1.42072964 epoch total loss 1.42568243\n",
      "Trained batch 296 batch loss 1.43817031 epoch total loss 1.42572463\n",
      "Trained batch 297 batch loss 1.38903499 epoch total loss 1.42560112\n",
      "Trained batch 298 batch loss 1.5106616 epoch total loss 1.42588651\n",
      "Trained batch 299 batch loss 1.51737356 epoch total loss 1.42619252\n",
      "Trained batch 300 batch loss 1.45946074 epoch total loss 1.42630339\n",
      "Trained batch 301 batch loss 1.38582397 epoch total loss 1.42616892\n",
      "Trained batch 302 batch loss 1.35575485 epoch total loss 1.42593575\n",
      "Trained batch 303 batch loss 1.4341681 epoch total loss 1.42596292\n",
      "Trained batch 304 batch loss 1.53325856 epoch total loss 1.4263159\n",
      "Trained batch 305 batch loss 1.47425938 epoch total loss 1.42647314\n",
      "Trained batch 306 batch loss 1.46778047 epoch total loss 1.42660809\n",
      "Trained batch 307 batch loss 1.35494494 epoch total loss 1.42637467\n",
      "Trained batch 308 batch loss 1.34819913 epoch total loss 1.42612088\n",
      "Trained batch 309 batch loss 1.29052925 epoch total loss 1.42568207\n",
      "Trained batch 310 batch loss 1.34460688 epoch total loss 1.42542052\n",
      "Trained batch 311 batch loss 1.47316623 epoch total loss 1.42557406\n",
      "Trained batch 312 batch loss 1.55387342 epoch total loss 1.42598534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 313 batch loss 1.62024832 epoch total loss 1.42660594\n",
      "Trained batch 314 batch loss 1.46995187 epoch total loss 1.42674387\n",
      "Trained batch 315 batch loss 1.53827524 epoch total loss 1.42709792\n",
      "Trained batch 316 batch loss 1.49525142 epoch total loss 1.42731357\n",
      "Trained batch 317 batch loss 1.35661042 epoch total loss 1.42709053\n",
      "Trained batch 318 batch loss 1.38271236 epoch total loss 1.42695105\n",
      "Trained batch 319 batch loss 1.28288293 epoch total loss 1.42649937\n",
      "Trained batch 320 batch loss 1.51015925 epoch total loss 1.42676091\n",
      "Trained batch 321 batch loss 1.396698 epoch total loss 1.42666721\n",
      "Trained batch 322 batch loss 1.46014762 epoch total loss 1.42677116\n",
      "Trained batch 323 batch loss 1.46174371 epoch total loss 1.42687941\n",
      "Trained batch 324 batch loss 1.36476147 epoch total loss 1.42668772\n",
      "Trained batch 325 batch loss 1.37925684 epoch total loss 1.42654181\n",
      "Trained batch 326 batch loss 1.21979403 epoch total loss 1.42590749\n",
      "Trained batch 327 batch loss 1.28133035 epoch total loss 1.42546546\n",
      "Trained batch 328 batch loss 1.32751298 epoch total loss 1.42516685\n",
      "Trained batch 329 batch loss 1.22839904 epoch total loss 1.42456865\n",
      "Trained batch 330 batch loss 1.37958217 epoch total loss 1.4244324\n",
      "Trained batch 331 batch loss 1.36288905 epoch total loss 1.42424643\n",
      "Trained batch 332 batch loss 1.29358029 epoch total loss 1.4238528\n",
      "Trained batch 333 batch loss 1.42002487 epoch total loss 1.42384136\n",
      "Trained batch 334 batch loss 1.3993535 epoch total loss 1.42376804\n",
      "Trained batch 335 batch loss 1.31946266 epoch total loss 1.42345667\n",
      "Trained batch 336 batch loss 1.33180952 epoch total loss 1.42318392\n",
      "Trained batch 337 batch loss 1.3095057 epoch total loss 1.42284656\n",
      "Trained batch 338 batch loss 1.27184701 epoch total loss 1.42239988\n",
      "Trained batch 339 batch loss 1.40772009 epoch total loss 1.42235649\n",
      "Trained batch 340 batch loss 1.31610227 epoch total loss 1.42204404\n",
      "Trained batch 341 batch loss 1.4661088 epoch total loss 1.42217314\n",
      "Trained batch 342 batch loss 1.54808879 epoch total loss 1.42254138\n",
      "Trained batch 343 batch loss 1.57647574 epoch total loss 1.4229902\n",
      "Trained batch 344 batch loss 1.41035748 epoch total loss 1.42295349\n",
      "Trained batch 345 batch loss 1.33776569 epoch total loss 1.4227066\n",
      "Trained batch 346 batch loss 1.28685093 epoch total loss 1.42231393\n",
      "Trained batch 347 batch loss 1.40724409 epoch total loss 1.42227054\n",
      "Trained batch 348 batch loss 1.38966179 epoch total loss 1.42217684\n",
      "Trained batch 349 batch loss 1.39807141 epoch total loss 1.42210782\n",
      "Trained batch 350 batch loss 1.32276607 epoch total loss 1.42182386\n",
      "Trained batch 351 batch loss 1.38048136 epoch total loss 1.4217062\n",
      "Trained batch 352 batch loss 1.39006829 epoch total loss 1.42161632\n",
      "Trained batch 353 batch loss 1.41508222 epoch total loss 1.42159772\n",
      "Trained batch 354 batch loss 1.51787758 epoch total loss 1.42186975\n",
      "Trained batch 355 batch loss 1.32205081 epoch total loss 1.42158854\n",
      "Trained batch 356 batch loss 1.40236187 epoch total loss 1.42153454\n",
      "Trained batch 357 batch loss 1.39159656 epoch total loss 1.42145073\n",
      "Trained batch 358 batch loss 1.38040924 epoch total loss 1.42133605\n",
      "Trained batch 359 batch loss 1.37263489 epoch total loss 1.42120039\n",
      "Trained batch 360 batch loss 1.32515502 epoch total loss 1.4209336\n",
      "Trained batch 361 batch loss 1.34808433 epoch total loss 1.4207319\n",
      "Trained batch 362 batch loss 1.4311471 epoch total loss 1.42076063\n",
      "Trained batch 363 batch loss 1.4120307 epoch total loss 1.42073667\n",
      "Trained batch 364 batch loss 1.39175129 epoch total loss 1.42065704\n",
      "Trained batch 365 batch loss 1.48689723 epoch total loss 1.42083836\n",
      "Trained batch 366 batch loss 1.40398562 epoch total loss 1.42079234\n",
      "Trained batch 367 batch loss 1.49888623 epoch total loss 1.42100525\n",
      "Trained batch 368 batch loss 1.26367092 epoch total loss 1.42057765\n",
      "Trained batch 369 batch loss 1.35356903 epoch total loss 1.42039609\n",
      "Trained batch 370 batch loss 1.36250627 epoch total loss 1.42023957\n",
      "Trained batch 371 batch loss 1.39527714 epoch total loss 1.42017221\n",
      "Trained batch 372 batch loss 1.40793896 epoch total loss 1.42013943\n",
      "Trained batch 373 batch loss 1.38310909 epoch total loss 1.42004013\n",
      "Trained batch 374 batch loss 1.42049265 epoch total loss 1.42004132\n",
      "Trained batch 375 batch loss 1.37551916 epoch total loss 1.41992271\n",
      "Trained batch 376 batch loss 1.37428808 epoch total loss 1.41980124\n",
      "Trained batch 377 batch loss 1.42147148 epoch total loss 1.41980565\n",
      "Trained batch 378 batch loss 1.42341638 epoch total loss 1.41981518\n",
      "Trained batch 379 batch loss 1.46195138 epoch total loss 1.4199264\n",
      "Trained batch 380 batch loss 1.32419193 epoch total loss 1.41967452\n",
      "Trained batch 381 batch loss 1.31977224 epoch total loss 1.41941226\n",
      "Trained batch 382 batch loss 1.32460368 epoch total loss 1.41916406\n",
      "Trained batch 383 batch loss 1.45578563 epoch total loss 1.41925979\n",
      "Trained batch 384 batch loss 1.43369603 epoch total loss 1.41929734\n",
      "Trained batch 385 batch loss 1.46716881 epoch total loss 1.41942167\n",
      "Trained batch 386 batch loss 1.51941085 epoch total loss 1.41968071\n",
      "Trained batch 387 batch loss 1.44493818 epoch total loss 1.41974604\n",
      "Trained batch 388 batch loss 1.44128323 epoch total loss 1.41980159\n",
      "Trained batch 389 batch loss 1.46438217 epoch total loss 1.41991603\n",
      "Trained batch 390 batch loss 1.37589383 epoch total loss 1.41980326\n",
      "Trained batch 391 batch loss 1.33247268 epoch total loss 1.41957986\n",
      "Trained batch 392 batch loss 1.24752831 epoch total loss 1.41914105\n",
      "Trained batch 393 batch loss 1.32644892 epoch total loss 1.41890526\n",
      "Trained batch 394 batch loss 1.22313249 epoch total loss 1.41840839\n",
      "Trained batch 395 batch loss 1.39065766 epoch total loss 1.41833818\n",
      "Trained batch 396 batch loss 1.38917279 epoch total loss 1.41826451\n",
      "Trained batch 397 batch loss 1.45467365 epoch total loss 1.41835618\n",
      "Trained batch 398 batch loss 1.33971119 epoch total loss 1.41815865\n",
      "Trained batch 399 batch loss 1.41547954 epoch total loss 1.41815186\n",
      "Trained batch 400 batch loss 1.44571376 epoch total loss 1.41822088\n",
      "Trained batch 401 batch loss 1.44521606 epoch total loss 1.41828811\n",
      "Trained batch 402 batch loss 1.38176453 epoch total loss 1.41819727\n",
      "Trained batch 403 batch loss 1.24932718 epoch total loss 1.41777825\n",
      "Trained batch 404 batch loss 1.33032835 epoch total loss 1.41756177\n",
      "Trained batch 405 batch loss 1.328758 epoch total loss 1.41734242\n",
      "Trained batch 406 batch loss 1.43018484 epoch total loss 1.41737401\n",
      "Trained batch 407 batch loss 1.50222027 epoch total loss 1.41758239\n",
      "Trained batch 408 batch loss 1.44535637 epoch total loss 1.41765058\n",
      "Trained batch 409 batch loss 1.38778114 epoch total loss 1.41757751\n",
      "Trained batch 410 batch loss 1.3606267 epoch total loss 1.41743863\n",
      "Trained batch 411 batch loss 1.33728349 epoch total loss 1.4172436\n",
      "Trained batch 412 batch loss 1.3315146 epoch total loss 1.41703558\n",
      "Trained batch 413 batch loss 1.15308774 epoch total loss 1.4163965\n",
      "Trained batch 414 batch loss 1.24746776 epoch total loss 1.41598845\n",
      "Trained batch 415 batch loss 1.35762095 epoch total loss 1.41584778\n",
      "Trained batch 416 batch loss 1.53323829 epoch total loss 1.41613007\n",
      "Trained batch 417 batch loss 1.49241948 epoch total loss 1.41631305\n",
      "Trained batch 418 batch loss 1.47153258 epoch total loss 1.41644526\n",
      "Trained batch 419 batch loss 1.50232637 epoch total loss 1.41665018\n",
      "Trained batch 420 batch loss 1.459975 epoch total loss 1.41675329\n",
      "Trained batch 421 batch loss 1.43338799 epoch total loss 1.41679287\n",
      "Trained batch 422 batch loss 1.48595536 epoch total loss 1.41695678\n",
      "Trained batch 423 batch loss 1.49339843 epoch total loss 1.4171375\n",
      "Trained batch 424 batch loss 1.38818359 epoch total loss 1.4170692\n",
      "Trained batch 425 batch loss 1.333318 epoch total loss 1.41687214\n",
      "Trained batch 426 batch loss 1.3163029 epoch total loss 1.41663599\n",
      "Trained batch 427 batch loss 1.30582666 epoch total loss 1.41637659\n",
      "Trained batch 428 batch loss 1.27637458 epoch total loss 1.41604948\n",
      "Trained batch 429 batch loss 1.41475284 epoch total loss 1.41604638\n",
      "Trained batch 430 batch loss 1.43628 epoch total loss 1.41609347\n",
      "Trained batch 431 batch loss 1.45885265 epoch total loss 1.41619265\n",
      "Trained batch 432 batch loss 1.55236399 epoch total loss 1.41650784\n",
      "Trained batch 433 batch loss 1.42028117 epoch total loss 1.41651654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 434 batch loss 1.4176712 epoch total loss 1.41651928\n",
      "Trained batch 435 batch loss 1.39492941 epoch total loss 1.41646969\n",
      "Trained batch 436 batch loss 1.35342038 epoch total loss 1.41632497\n",
      "Trained batch 437 batch loss 1.36662364 epoch total loss 1.41621125\n",
      "Trained batch 438 batch loss 1.39049697 epoch total loss 1.4161526\n",
      "Trained batch 439 batch loss 1.41069818 epoch total loss 1.4161402\n",
      "Trained batch 440 batch loss 1.35870159 epoch total loss 1.41600966\n",
      "Trained batch 441 batch loss 1.2934742 epoch total loss 1.41573179\n",
      "Trained batch 442 batch loss 1.35700941 epoch total loss 1.41559887\n",
      "Trained batch 443 batch loss 1.54708695 epoch total loss 1.41589558\n",
      "Trained batch 444 batch loss 1.39525557 epoch total loss 1.41584921\n",
      "Trained batch 445 batch loss 1.46016598 epoch total loss 1.41594875\n",
      "Trained batch 446 batch loss 1.46850169 epoch total loss 1.41606653\n",
      "Trained batch 447 batch loss 1.44910526 epoch total loss 1.41614044\n",
      "Trained batch 448 batch loss 1.40621459 epoch total loss 1.41611826\n",
      "Trained batch 449 batch loss 1.41424036 epoch total loss 1.41611409\n",
      "Trained batch 450 batch loss 1.46975648 epoch total loss 1.41623318\n",
      "Trained batch 451 batch loss 1.4606086 epoch total loss 1.41633165\n",
      "Trained batch 452 batch loss 1.36853409 epoch total loss 1.41622591\n",
      "Trained batch 453 batch loss 1.33209801 epoch total loss 1.41604018\n",
      "Trained batch 454 batch loss 1.27025676 epoch total loss 1.41571903\n",
      "Trained batch 455 batch loss 1.3638829 epoch total loss 1.41560519\n",
      "Trained batch 456 batch loss 1.30108643 epoch total loss 1.41535401\n",
      "Trained batch 457 batch loss 1.41605854 epoch total loss 1.41535556\n",
      "Trained batch 458 batch loss 1.42883766 epoch total loss 1.41538501\n",
      "Trained batch 459 batch loss 1.37177563 epoch total loss 1.41529\n",
      "Trained batch 460 batch loss 1.5205245 epoch total loss 1.41551876\n",
      "Trained batch 461 batch loss 1.52575 epoch total loss 1.41575789\n",
      "Trained batch 462 batch loss 1.5552702 epoch total loss 1.41605985\n",
      "Trained batch 463 batch loss 1.43360221 epoch total loss 1.41609776\n",
      "Trained batch 464 batch loss 1.3819524 epoch total loss 1.41602421\n",
      "Trained batch 465 batch loss 1.33147883 epoch total loss 1.41584241\n",
      "Trained batch 466 batch loss 1.35766399 epoch total loss 1.41571748\n",
      "Trained batch 467 batch loss 1.29737735 epoch total loss 1.41546404\n",
      "Trained batch 468 batch loss 1.31593645 epoch total loss 1.41525137\n",
      "Trained batch 469 batch loss 1.37066162 epoch total loss 1.41515636\n",
      "Trained batch 470 batch loss 1.32096159 epoch total loss 1.41495597\n",
      "Trained batch 471 batch loss 1.30973911 epoch total loss 1.41473258\n",
      "Trained batch 472 batch loss 1.28507972 epoch total loss 1.41445792\n",
      "Trained batch 473 batch loss 1.34375644 epoch total loss 1.41430843\n",
      "Trained batch 474 batch loss 1.3993752 epoch total loss 1.41427684\n",
      "Trained batch 475 batch loss 1.33507371 epoch total loss 1.41411018\n",
      "Trained batch 476 batch loss 1.2856704 epoch total loss 1.41384029\n",
      "Trained batch 477 batch loss 1.26472783 epoch total loss 1.41352761\n",
      "Trained batch 478 batch loss 1.33202839 epoch total loss 1.41335714\n",
      "Trained batch 479 batch loss 1.30200183 epoch total loss 1.41312468\n",
      "Trained batch 480 batch loss 1.20657766 epoch total loss 1.41269445\n",
      "Trained batch 481 batch loss 1.24768245 epoch total loss 1.41235137\n",
      "Trained batch 482 batch loss 1.30183101 epoch total loss 1.41212201\n",
      "Trained batch 483 batch loss 1.36875272 epoch total loss 1.41203225\n",
      "Trained batch 484 batch loss 1.33817196 epoch total loss 1.41187978\n",
      "Trained batch 485 batch loss 1.25957942 epoch total loss 1.41156578\n",
      "Trained batch 486 batch loss 1.55919611 epoch total loss 1.41186953\n",
      "Trained batch 487 batch loss 1.37177289 epoch total loss 1.41178715\n",
      "Trained batch 488 batch loss 1.3555727 epoch total loss 1.411672\n",
      "Trained batch 489 batch loss 1.35868359 epoch total loss 1.41156363\n",
      "Trained batch 490 batch loss 1.35209286 epoch total loss 1.4114424\n",
      "Trained batch 491 batch loss 1.42125678 epoch total loss 1.41146231\n",
      "Trained batch 492 batch loss 1.24067831 epoch total loss 1.41111517\n",
      "Trained batch 493 batch loss 1.02464092 epoch total loss 1.41033125\n",
      "Trained batch 494 batch loss 1.13945389 epoch total loss 1.40978301\n",
      "Trained batch 495 batch loss 1.37473869 epoch total loss 1.4097122\n",
      "Trained batch 496 batch loss 1.51057196 epoch total loss 1.40991557\n",
      "Trained batch 497 batch loss 1.56005502 epoch total loss 1.41021764\n",
      "Trained batch 498 batch loss 1.5422399 epoch total loss 1.41048276\n",
      "Trained batch 499 batch loss 1.47159839 epoch total loss 1.41060531\n",
      "Trained batch 500 batch loss 1.38276911 epoch total loss 1.41054952\n",
      "Trained batch 501 batch loss 1.4137553 epoch total loss 1.41055596\n",
      "Trained batch 502 batch loss 1.40064406 epoch total loss 1.41053617\n",
      "Trained batch 503 batch loss 1.35595536 epoch total loss 1.41042769\n",
      "Trained batch 504 batch loss 1.32991874 epoch total loss 1.41026795\n",
      "Trained batch 505 batch loss 1.23419034 epoch total loss 1.40991926\n",
      "Trained batch 506 batch loss 1.28954148 epoch total loss 1.40968132\n",
      "Trained batch 507 batch loss 1.24527919 epoch total loss 1.40935719\n",
      "Trained batch 508 batch loss 1.32811725 epoch total loss 1.40919721\n",
      "Trained batch 509 batch loss 1.32370925 epoch total loss 1.40902936\n",
      "Trained batch 510 batch loss 1.39631236 epoch total loss 1.40900433\n",
      "Trained batch 511 batch loss 1.34560728 epoch total loss 1.40888023\n",
      "Trained batch 512 batch loss 1.31096959 epoch total loss 1.40868902\n",
      "Trained batch 513 batch loss 1.50614643 epoch total loss 1.40887904\n",
      "Trained batch 514 batch loss 1.32449973 epoch total loss 1.40871489\n",
      "Trained batch 515 batch loss 1.24358749 epoch total loss 1.40839434\n",
      "Trained batch 516 batch loss 1.44448233 epoch total loss 1.40846419\n",
      "Trained batch 517 batch loss 1.37361574 epoch total loss 1.40839672\n",
      "Trained batch 518 batch loss 1.33407557 epoch total loss 1.40825319\n",
      "Trained batch 519 batch loss 1.4675293 epoch total loss 1.4083674\n",
      "Trained batch 520 batch loss 1.48814023 epoch total loss 1.40852082\n",
      "Trained batch 521 batch loss 1.54218972 epoch total loss 1.40877736\n",
      "Trained batch 522 batch loss 1.38890707 epoch total loss 1.40873933\n",
      "Trained batch 523 batch loss 1.34379494 epoch total loss 1.40861523\n",
      "Trained batch 524 batch loss 1.24018681 epoch total loss 1.40829372\n",
      "Trained batch 525 batch loss 1.2761538 epoch total loss 1.40804207\n",
      "Trained batch 526 batch loss 1.35106587 epoch total loss 1.40793383\n",
      "Trained batch 527 batch loss 1.36491227 epoch total loss 1.40785217\n",
      "Trained batch 528 batch loss 1.36528361 epoch total loss 1.40777159\n",
      "Trained batch 529 batch loss 1.20489621 epoch total loss 1.40738809\n",
      "Trained batch 530 batch loss 1.31111515 epoch total loss 1.40720642\n",
      "Trained batch 531 batch loss 1.39301574 epoch total loss 1.40717971\n",
      "Trained batch 532 batch loss 1.44003272 epoch total loss 1.40724134\n",
      "Trained batch 533 batch loss 1.46873808 epoch total loss 1.40735674\n",
      "Trained batch 534 batch loss 1.23263526 epoch total loss 1.40702951\n",
      "Trained batch 535 batch loss 1.33358479 epoch total loss 1.40689218\n",
      "Trained batch 536 batch loss 1.28893673 epoch total loss 1.40667212\n",
      "Trained batch 537 batch loss 1.44237816 epoch total loss 1.40673864\n",
      "Trained batch 538 batch loss 1.45540869 epoch total loss 1.406829\n",
      "Trained batch 539 batch loss 1.50934386 epoch total loss 1.40701926\n",
      "Trained batch 540 batch loss 1.36022103 epoch total loss 1.40693259\n",
      "Trained batch 541 batch loss 1.37476599 epoch total loss 1.40687311\n",
      "Trained batch 542 batch loss 1.28106856 epoch total loss 1.40664101\n",
      "Trained batch 543 batch loss 1.45323551 epoch total loss 1.40672684\n",
      "Trained batch 544 batch loss 1.55208409 epoch total loss 1.40699399\n",
      "Trained batch 545 batch loss 1.47760499 epoch total loss 1.40712357\n",
      "Trained batch 546 batch loss 1.40923107 epoch total loss 1.40712738\n",
      "Trained batch 547 batch loss 1.35185182 epoch total loss 1.40702641\n",
      "Trained batch 548 batch loss 1.40703607 epoch total loss 1.40702641\n",
      "Trained batch 549 batch loss 1.38160467 epoch total loss 1.40698\n",
      "Trained batch 550 batch loss 1.36372161 epoch total loss 1.40690136\n",
      "Trained batch 551 batch loss 1.34452963 epoch total loss 1.40678823\n",
      "Trained batch 552 batch loss 1.43788505 epoch total loss 1.4068445\n",
      "Trained batch 553 batch loss 1.38412476 epoch total loss 1.40680337\n",
      "Trained batch 554 batch loss 1.39669371 epoch total loss 1.40678513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 555 batch loss 1.34554422 epoch total loss 1.40667474\n",
      "Trained batch 556 batch loss 1.3104521 epoch total loss 1.40650165\n",
      "Trained batch 557 batch loss 1.3578732 epoch total loss 1.40641427\n",
      "Trained batch 558 batch loss 1.36624587 epoch total loss 1.40634227\n",
      "Trained batch 559 batch loss 1.34830332 epoch total loss 1.40623856\n",
      "Trained batch 560 batch loss 1.30962217 epoch total loss 1.40606606\n",
      "Trained batch 561 batch loss 1.27923942 epoch total loss 1.40583992\n",
      "Trained batch 562 batch loss 1.36604404 epoch total loss 1.40576911\n",
      "Trained batch 563 batch loss 1.37388921 epoch total loss 1.40571249\n",
      "Trained batch 564 batch loss 1.42617273 epoch total loss 1.40574872\n",
      "Trained batch 565 batch loss 1.38624382 epoch total loss 1.40571415\n",
      "Trained batch 566 batch loss 1.49331021 epoch total loss 1.40586889\n",
      "Trained batch 567 batch loss 1.54623103 epoch total loss 1.40611637\n",
      "Trained batch 568 batch loss 1.45957732 epoch total loss 1.40621054\n",
      "Trained batch 569 batch loss 1.4775871 epoch total loss 1.40633607\n",
      "Trained batch 570 batch loss 1.40061474 epoch total loss 1.40632606\n",
      "Trained batch 571 batch loss 1.39148557 epoch total loss 1.40630007\n",
      "Trained batch 572 batch loss 1.27359188 epoch total loss 1.40606809\n",
      "Trained batch 573 batch loss 1.45986533 epoch total loss 1.4061619\n",
      "Trained batch 574 batch loss 1.40926242 epoch total loss 1.40616727\n",
      "Trained batch 575 batch loss 1.38665867 epoch total loss 1.40613329\n",
      "Trained batch 576 batch loss 1.31910288 epoch total loss 1.40598226\n",
      "Trained batch 577 batch loss 1.27559316 epoch total loss 1.40575624\n",
      "Trained batch 578 batch loss 1.25392151 epoch total loss 1.4054935\n",
      "Trained batch 579 batch loss 1.36443329 epoch total loss 1.40542257\n",
      "Trained batch 580 batch loss 1.55115604 epoch total loss 1.40567386\n",
      "Trained batch 581 batch loss 1.63120174 epoch total loss 1.40606201\n",
      "Trained batch 582 batch loss 1.61837769 epoch total loss 1.40642679\n",
      "Trained batch 583 batch loss 1.47530055 epoch total loss 1.40654492\n",
      "Trained batch 584 batch loss 1.50727248 epoch total loss 1.40671742\n",
      "Trained batch 585 batch loss 1.55589926 epoch total loss 1.40697241\n",
      "Trained batch 586 batch loss 1.47361779 epoch total loss 1.40708613\n",
      "Trained batch 587 batch loss 1.40868235 epoch total loss 1.40708888\n",
      "Trained batch 588 batch loss 1.35975444 epoch total loss 1.40700841\n",
      "Trained batch 589 batch loss 1.34705436 epoch total loss 1.4069066\n",
      "Trained batch 590 batch loss 1.40499926 epoch total loss 1.40690339\n",
      "Trained batch 591 batch loss 1.54559433 epoch total loss 1.40713811\n",
      "Trained batch 592 batch loss 1.5710032 epoch total loss 1.40741479\n",
      "Trained batch 593 batch loss 1.44780254 epoch total loss 1.40748298\n",
      "Trained batch 594 batch loss 1.50700569 epoch total loss 1.40765047\n",
      "Trained batch 595 batch loss 1.51246965 epoch total loss 1.40782666\n",
      "Trained batch 596 batch loss 1.47741151 epoch total loss 1.40794337\n",
      "Trained batch 597 batch loss 1.45374465 epoch total loss 1.40802014\n",
      "Trained batch 598 batch loss 1.42637277 epoch total loss 1.40805089\n",
      "Trained batch 599 batch loss 1.45746434 epoch total loss 1.40813327\n",
      "Trained batch 600 batch loss 1.34697437 epoch total loss 1.40803146\n",
      "Trained batch 601 batch loss 1.39203215 epoch total loss 1.40800476\n",
      "Trained batch 602 batch loss 1.40527463 epoch total loss 1.40800023\n",
      "Trained batch 603 batch loss 1.44416618 epoch total loss 1.40806019\n",
      "Trained batch 604 batch loss 1.41029215 epoch total loss 1.40806389\n",
      "Trained batch 605 batch loss 1.36728418 epoch total loss 1.40799654\n",
      "Trained batch 606 batch loss 1.36111975 epoch total loss 1.40791917\n",
      "Trained batch 607 batch loss 1.32391989 epoch total loss 1.40778077\n",
      "Trained batch 608 batch loss 1.31275749 epoch total loss 1.40762448\n",
      "Trained batch 609 batch loss 1.37405896 epoch total loss 1.40756941\n",
      "Trained batch 610 batch loss 1.39586663 epoch total loss 1.40755022\n",
      "Trained batch 611 batch loss 1.45210242 epoch total loss 1.40762317\n",
      "Trained batch 612 batch loss 1.33833539 epoch total loss 1.40750992\n",
      "Trained batch 613 batch loss 1.34988952 epoch total loss 1.40741599\n",
      "Trained batch 614 batch loss 1.3026787 epoch total loss 1.4072454\n",
      "Trained batch 615 batch loss 1.31340802 epoch total loss 1.40709281\n",
      "Trained batch 616 batch loss 1.32549429 epoch total loss 1.40696037\n",
      "Trained batch 617 batch loss 1.22684515 epoch total loss 1.40666842\n",
      "Trained batch 618 batch loss 1.3022244 epoch total loss 1.40649951\n",
      "Trained batch 619 batch loss 1.25702012 epoch total loss 1.40625799\n",
      "Trained batch 620 batch loss 1.33477414 epoch total loss 1.40614271\n",
      "Trained batch 621 batch loss 1.28741884 epoch total loss 1.4059515\n",
      "Trained batch 622 batch loss 1.29982805 epoch total loss 1.40578091\n",
      "Trained batch 623 batch loss 1.32814431 epoch total loss 1.40565622\n",
      "Trained batch 624 batch loss 1.44591093 epoch total loss 1.40572071\n",
      "Trained batch 625 batch loss 1.45035195 epoch total loss 1.40579224\n",
      "Trained batch 626 batch loss 1.35469055 epoch total loss 1.40571058\n",
      "Trained batch 627 batch loss 1.49920583 epoch total loss 1.40585959\n",
      "Trained batch 628 batch loss 1.37332392 epoch total loss 1.40580785\n",
      "Trained batch 629 batch loss 1.34066379 epoch total loss 1.40570426\n",
      "Trained batch 630 batch loss 1.40484738 epoch total loss 1.40570295\n",
      "Trained batch 631 batch loss 1.37661862 epoch total loss 1.40565681\n",
      "Trained batch 632 batch loss 1.36391115 epoch total loss 1.40559077\n",
      "Trained batch 633 batch loss 1.43454468 epoch total loss 1.40563655\n",
      "Trained batch 634 batch loss 1.45397842 epoch total loss 1.40571284\n",
      "Trained batch 635 batch loss 1.29466057 epoch total loss 1.40553796\n",
      "Trained batch 636 batch loss 1.37493861 epoch total loss 1.4054898\n",
      "Trained batch 637 batch loss 1.36045051 epoch total loss 1.40541923\n",
      "Trained batch 638 batch loss 1.27336228 epoch total loss 1.40521216\n",
      "Trained batch 639 batch loss 1.33196819 epoch total loss 1.4050976\n",
      "Trained batch 640 batch loss 1.35389185 epoch total loss 1.40501761\n",
      "Trained batch 641 batch loss 1.32728624 epoch total loss 1.40489626\n",
      "Trained batch 642 batch loss 1.48988056 epoch total loss 1.40502858\n",
      "Trained batch 643 batch loss 1.61172593 epoch total loss 1.40535009\n",
      "Trained batch 644 batch loss 1.55164814 epoch total loss 1.4055773\n",
      "Trained batch 645 batch loss 1.4963814 epoch total loss 1.40571809\n",
      "Trained batch 646 batch loss 1.39407849 epoch total loss 1.40570009\n",
      "Trained batch 647 batch loss 1.36634731 epoch total loss 1.40563929\n",
      "Trained batch 648 batch loss 1.20672917 epoch total loss 1.40533233\n",
      "Trained batch 649 batch loss 1.14517391 epoch total loss 1.40493155\n",
      "Trained batch 650 batch loss 1.17082834 epoch total loss 1.40457129\n",
      "Trained batch 651 batch loss 1.14851427 epoch total loss 1.40417802\n",
      "Trained batch 652 batch loss 1.27256203 epoch total loss 1.4039762\n",
      "Trained batch 653 batch loss 1.29463434 epoch total loss 1.40380871\n",
      "Trained batch 654 batch loss 1.49563265 epoch total loss 1.40394902\n",
      "Trained batch 655 batch loss 1.47489607 epoch total loss 1.40405738\n",
      "Trained batch 656 batch loss 1.49394572 epoch total loss 1.40419447\n",
      "Trained batch 657 batch loss 1.42135632 epoch total loss 1.40422058\n",
      "Trained batch 658 batch loss 1.34117162 epoch total loss 1.40412486\n",
      "Trained batch 659 batch loss 1.2629025 epoch total loss 1.40391052\n",
      "Trained batch 660 batch loss 1.32824087 epoch total loss 1.40379584\n",
      "Trained batch 661 batch loss 1.36351812 epoch total loss 1.40373492\n",
      "Trained batch 662 batch loss 1.42448008 epoch total loss 1.40376627\n",
      "Trained batch 663 batch loss 1.36677051 epoch total loss 1.40371048\n",
      "Trained batch 664 batch loss 1.44533527 epoch total loss 1.40377307\n",
      "Trained batch 665 batch loss 1.41744876 epoch total loss 1.40379369\n",
      "Trained batch 666 batch loss 1.49891567 epoch total loss 1.40393639\n",
      "Trained batch 667 batch loss 1.4693408 epoch total loss 1.4040345\n",
      "Trained batch 668 batch loss 1.47744548 epoch total loss 1.40414441\n",
      "Trained batch 669 batch loss 1.4752264 epoch total loss 1.40425062\n",
      "Trained batch 670 batch loss 1.42383754 epoch total loss 1.40427983\n",
      "Trained batch 671 batch loss 1.48887074 epoch total loss 1.40440595\n",
      "Trained batch 672 batch loss 1.47121119 epoch total loss 1.40450537\n",
      "Trained batch 673 batch loss 1.50397778 epoch total loss 1.40465307\n",
      "Trained batch 674 batch loss 1.3877728 epoch total loss 1.40462804\n",
      "Trained batch 675 batch loss 1.40477657 epoch total loss 1.40462828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 676 batch loss 1.2738868 epoch total loss 1.4044348\n",
      "Trained batch 677 batch loss 1.2776494 epoch total loss 1.40424752\n",
      "Trained batch 678 batch loss 1.33926845 epoch total loss 1.4041518\n",
      "Trained batch 679 batch loss 1.364501 epoch total loss 1.40409338\n",
      "Trained batch 680 batch loss 1.20725179 epoch total loss 1.40380394\n",
      "Trained batch 681 batch loss 1.20227122 epoch total loss 1.40350795\n",
      "Trained batch 682 batch loss 1.2703948 epoch total loss 1.4033128\n",
      "Trained batch 683 batch loss 1.23832774 epoch total loss 1.40307128\n",
      "Trained batch 684 batch loss 1.23987293 epoch total loss 1.40283263\n",
      "Trained batch 685 batch loss 1.18690884 epoch total loss 1.40251744\n",
      "Trained batch 686 batch loss 1.31553435 epoch total loss 1.4023906\n",
      "Trained batch 687 batch loss 1.40935111 epoch total loss 1.40240073\n",
      "Trained batch 688 batch loss 1.36521173 epoch total loss 1.40234673\n",
      "Trained batch 689 batch loss 1.27605498 epoch total loss 1.40216351\n",
      "Trained batch 690 batch loss 1.23307133 epoch total loss 1.40191841\n",
      "Trained batch 691 batch loss 1.34759974 epoch total loss 1.40183985\n",
      "Trained batch 692 batch loss 1.44992852 epoch total loss 1.40190935\n",
      "Trained batch 693 batch loss 1.42755902 epoch total loss 1.40194631\n",
      "Trained batch 694 batch loss 1.39271176 epoch total loss 1.40193307\n",
      "Trained batch 695 batch loss 1.35141945 epoch total loss 1.40186036\n",
      "Trained batch 696 batch loss 1.38096035 epoch total loss 1.40183043\n",
      "Trained batch 697 batch loss 1.29731834 epoch total loss 1.40168047\n",
      "Trained batch 698 batch loss 1.24593019 epoch total loss 1.40145731\n",
      "Trained batch 699 batch loss 1.24890792 epoch total loss 1.40123904\n",
      "Trained batch 700 batch loss 1.31679416 epoch total loss 1.4011184\n",
      "Trained batch 701 batch loss 1.34989631 epoch total loss 1.40104532\n",
      "Trained batch 702 batch loss 1.33907413 epoch total loss 1.40095699\n",
      "Trained batch 703 batch loss 1.35549819 epoch total loss 1.40089226\n",
      "Trained batch 704 batch loss 1.33504939 epoch total loss 1.40079868\n",
      "Trained batch 705 batch loss 1.39716458 epoch total loss 1.40079355\n",
      "Trained batch 706 batch loss 1.41151106 epoch total loss 1.40080869\n",
      "Trained batch 707 batch loss 1.4530828 epoch total loss 1.4008826\n",
      "Trained batch 708 batch loss 1.35749745 epoch total loss 1.40082133\n",
      "Trained batch 709 batch loss 1.33810937 epoch total loss 1.40073287\n",
      "Trained batch 710 batch loss 1.28639913 epoch total loss 1.40057182\n",
      "Trained batch 711 batch loss 1.33265591 epoch total loss 1.40047634\n",
      "Trained batch 712 batch loss 1.27137268 epoch total loss 1.4002949\n",
      "Trained batch 713 batch loss 1.36882043 epoch total loss 1.40025079\n",
      "Trained batch 714 batch loss 1.29466093 epoch total loss 1.40010297\n",
      "Trained batch 715 batch loss 1.32878625 epoch total loss 1.40000319\n",
      "Trained batch 716 batch loss 1.43969607 epoch total loss 1.40005863\n",
      "Trained batch 717 batch loss 1.42908287 epoch total loss 1.40009916\n",
      "Trained batch 718 batch loss 1.47613907 epoch total loss 1.40020502\n",
      "Trained batch 719 batch loss 1.33532989 epoch total loss 1.40011477\n",
      "Trained batch 720 batch loss 1.42524147 epoch total loss 1.4001497\n",
      "Trained batch 721 batch loss 1.47801745 epoch total loss 1.40025771\n",
      "Trained batch 722 batch loss 1.35249376 epoch total loss 1.40019155\n",
      "Trained batch 723 batch loss 1.29542375 epoch total loss 1.40004659\n",
      "Trained batch 724 batch loss 1.28990877 epoch total loss 1.39989448\n",
      "Trained batch 725 batch loss 1.23321104 epoch total loss 1.39966464\n",
      "Trained batch 726 batch loss 1.15745175 epoch total loss 1.39933097\n",
      "Trained batch 727 batch loss 1.28172076 epoch total loss 1.39916921\n",
      "Trained batch 728 batch loss 1.20557511 epoch total loss 1.39890337\n",
      "Trained batch 729 batch loss 1.16697073 epoch total loss 1.3985852\n",
      "Trained batch 730 batch loss 1.07080734 epoch total loss 1.39813614\n",
      "Trained batch 731 batch loss 1.05651629 epoch total loss 1.39766884\n",
      "Trained batch 732 batch loss 1.22749043 epoch total loss 1.39743638\n",
      "Trained batch 733 batch loss 1.28254175 epoch total loss 1.39727962\n",
      "Trained batch 734 batch loss 1.34699643 epoch total loss 1.39721119\n",
      "Trained batch 735 batch loss 1.3212285 epoch total loss 1.39710784\n",
      "Trained batch 736 batch loss 1.35278785 epoch total loss 1.39704764\n",
      "Trained batch 737 batch loss 1.25489569 epoch total loss 1.39685476\n",
      "Trained batch 738 batch loss 1.40619779 epoch total loss 1.39686751\n",
      "Trained batch 739 batch loss 1.30839956 epoch total loss 1.39674771\n",
      "Trained batch 740 batch loss 1.2809391 epoch total loss 1.39659107\n",
      "Trained batch 741 batch loss 1.31762505 epoch total loss 1.39648449\n",
      "Trained batch 742 batch loss 1.40326881 epoch total loss 1.39649379\n",
      "Trained batch 743 batch loss 1.44089687 epoch total loss 1.39655352\n",
      "Trained batch 744 batch loss 1.41153467 epoch total loss 1.39657366\n",
      "Trained batch 745 batch loss 1.30048907 epoch total loss 1.39644468\n",
      "Trained batch 746 batch loss 1.30909169 epoch total loss 1.39632761\n",
      "Trained batch 747 batch loss 1.25213838 epoch total loss 1.39613461\n",
      "Trained batch 748 batch loss 1.30777252 epoch total loss 1.39601648\n",
      "Trained batch 749 batch loss 1.34263682 epoch total loss 1.39594519\n",
      "Trained batch 750 batch loss 1.41699564 epoch total loss 1.39597332\n",
      "Trained batch 751 batch loss 1.3374964 epoch total loss 1.39589548\n",
      "Trained batch 752 batch loss 1.38846815 epoch total loss 1.39588559\n",
      "Trained batch 753 batch loss 1.4385643 epoch total loss 1.39594233\n",
      "Trained batch 754 batch loss 1.37223625 epoch total loss 1.39591074\n",
      "Trained batch 755 batch loss 1.31512964 epoch total loss 1.39580381\n",
      "Trained batch 756 batch loss 1.34131908 epoch total loss 1.39573181\n",
      "Trained batch 757 batch loss 1.32894588 epoch total loss 1.39564359\n",
      "Trained batch 758 batch loss 1.37339759 epoch total loss 1.39561427\n",
      "Trained batch 759 batch loss 1.58971763 epoch total loss 1.39587\n",
      "Trained batch 760 batch loss 1.43684196 epoch total loss 1.39592397\n",
      "Trained batch 761 batch loss 1.41846502 epoch total loss 1.39595354\n",
      "Trained batch 762 batch loss 1.38747442 epoch total loss 1.39594245\n",
      "Trained batch 763 batch loss 1.43578696 epoch total loss 1.39599466\n",
      "Trained batch 764 batch loss 1.421767 epoch total loss 1.3960284\n",
      "Trained batch 765 batch loss 1.4316833 epoch total loss 1.39607489\n",
      "Trained batch 766 batch loss 1.35417759 epoch total loss 1.39602017\n",
      "Trained batch 767 batch loss 1.40206647 epoch total loss 1.39602804\n",
      "Trained batch 768 batch loss 1.36512148 epoch total loss 1.39598787\n",
      "Trained batch 769 batch loss 1.35821688 epoch total loss 1.39593875\n",
      "Trained batch 770 batch loss 1.36452448 epoch total loss 1.39589798\n",
      "Trained batch 771 batch loss 1.46675205 epoch total loss 1.39598989\n",
      "Trained batch 772 batch loss 1.38090694 epoch total loss 1.39597034\n",
      "Trained batch 773 batch loss 1.33401048 epoch total loss 1.39589012\n",
      "Trained batch 774 batch loss 1.21348286 epoch total loss 1.39565444\n",
      "Trained batch 775 batch loss 1.2684139 epoch total loss 1.39549029\n",
      "Trained batch 776 batch loss 1.18487144 epoch total loss 1.39521885\n",
      "Trained batch 777 batch loss 1.26449049 epoch total loss 1.39505064\n",
      "Trained batch 778 batch loss 1.32914901 epoch total loss 1.39496589\n",
      "Trained batch 779 batch loss 1.19985676 epoch total loss 1.39471543\n",
      "Trained batch 780 batch loss 1.23383951 epoch total loss 1.3945092\n",
      "Trained batch 781 batch loss 1.36934352 epoch total loss 1.39447701\n",
      "Trained batch 782 batch loss 1.32531285 epoch total loss 1.39438856\n",
      "Trained batch 783 batch loss 1.4206562 epoch total loss 1.39442217\n",
      "Trained batch 784 batch loss 1.49117076 epoch total loss 1.39454556\n",
      "Trained batch 785 batch loss 1.40244174 epoch total loss 1.39455569\n",
      "Trained batch 786 batch loss 1.32170749 epoch total loss 1.39446294\n",
      "Trained batch 787 batch loss 1.46168494 epoch total loss 1.3945483\n",
      "Trained batch 788 batch loss 1.39760387 epoch total loss 1.39455211\n",
      "Trained batch 789 batch loss 1.3743881 epoch total loss 1.3945266\n",
      "Trained batch 790 batch loss 1.42499912 epoch total loss 1.39456522\n",
      "Trained batch 791 batch loss 1.41635299 epoch total loss 1.39459288\n",
      "Trained batch 792 batch loss 1.39653659 epoch total loss 1.39459527\n",
      "Trained batch 793 batch loss 1.43484485 epoch total loss 1.39464593\n",
      "Trained batch 794 batch loss 1.47138524 epoch total loss 1.39474261\n",
      "Trained batch 795 batch loss 1.37543988 epoch total loss 1.39471841\n",
      "Trained batch 796 batch loss 1.29435563 epoch total loss 1.39459229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 797 batch loss 1.33824968 epoch total loss 1.39452159\n",
      "Trained batch 798 batch loss 1.44347167 epoch total loss 1.39458299\n",
      "Trained batch 799 batch loss 1.43773806 epoch total loss 1.39463699\n",
      "Trained batch 800 batch loss 1.27617645 epoch total loss 1.39448881\n",
      "Trained batch 801 batch loss 1.26422298 epoch total loss 1.39432633\n",
      "Trained batch 802 batch loss 1.24976277 epoch total loss 1.39414597\n",
      "Trained batch 803 batch loss 1.28268397 epoch total loss 1.39400721\n",
      "Trained batch 804 batch loss 1.36390078 epoch total loss 1.39396977\n",
      "Trained batch 805 batch loss 1.21513605 epoch total loss 1.39374757\n",
      "Trained batch 806 batch loss 1.27239871 epoch total loss 1.39359689\n",
      "Trained batch 807 batch loss 1.15541744 epoch total loss 1.39330173\n",
      "Trained batch 808 batch loss 1.29508281 epoch total loss 1.39318013\n",
      "Trained batch 809 batch loss 1.5979948 epoch total loss 1.39343333\n",
      "Trained batch 810 batch loss 1.53495979 epoch total loss 1.39360809\n",
      "Trained batch 811 batch loss 1.33105159 epoch total loss 1.39353096\n",
      "Trained batch 812 batch loss 1.35033274 epoch total loss 1.39347768\n",
      "Trained batch 813 batch loss 1.43899024 epoch total loss 1.39353371\n",
      "Trained batch 814 batch loss 1.25807619 epoch total loss 1.39336729\n",
      "Trained batch 815 batch loss 1.36055958 epoch total loss 1.393327\n",
      "Trained batch 816 batch loss 1.29212785 epoch total loss 1.39320302\n",
      "Trained batch 817 batch loss 1.35538137 epoch total loss 1.39315665\n",
      "Trained batch 818 batch loss 1.34880912 epoch total loss 1.39310241\n",
      "Trained batch 819 batch loss 1.40393615 epoch total loss 1.39311564\n",
      "Trained batch 820 batch loss 1.37114275 epoch total loss 1.3930887\n",
      "Trained batch 821 batch loss 1.33371687 epoch total loss 1.39301646\n",
      "Trained batch 822 batch loss 1.33360088 epoch total loss 1.39294422\n",
      "Trained batch 823 batch loss 1.31546938 epoch total loss 1.39285\n",
      "Trained batch 824 batch loss 1.34175563 epoch total loss 1.39278805\n",
      "Trained batch 825 batch loss 1.33418369 epoch total loss 1.392717\n",
      "Trained batch 826 batch loss 1.34749448 epoch total loss 1.39266241\n",
      "Trained batch 827 batch loss 1.40101397 epoch total loss 1.39267242\n",
      "Trained batch 828 batch loss 1.34425306 epoch total loss 1.39261401\n",
      "Trained batch 829 batch loss 1.41580606 epoch total loss 1.3926419\n",
      "Trained batch 830 batch loss 1.40306711 epoch total loss 1.39265442\n",
      "Trained batch 831 batch loss 1.38446629 epoch total loss 1.39264464\n",
      "Trained batch 832 batch loss 1.29155385 epoch total loss 1.39252305\n",
      "Trained batch 833 batch loss 1.30994534 epoch total loss 1.39242399\n",
      "Trained batch 834 batch loss 1.31710267 epoch total loss 1.39233375\n",
      "Trained batch 835 batch loss 1.36960089 epoch total loss 1.39230645\n",
      "Trained batch 836 batch loss 1.48058021 epoch total loss 1.39241207\n",
      "Trained batch 837 batch loss 1.35960269 epoch total loss 1.39237297\n",
      "Trained batch 838 batch loss 1.36042762 epoch total loss 1.39233482\n",
      "Trained batch 839 batch loss 1.43819451 epoch total loss 1.39238954\n",
      "Trained batch 840 batch loss 1.3619132 epoch total loss 1.3923533\n",
      "Trained batch 841 batch loss 1.24568915 epoch total loss 1.39217901\n",
      "Trained batch 842 batch loss 1.33245087 epoch total loss 1.39210796\n",
      "Trained batch 843 batch loss 1.32127225 epoch total loss 1.39202392\n",
      "Trained batch 844 batch loss 1.42577147 epoch total loss 1.39206398\n",
      "Trained batch 845 batch loss 1.36518764 epoch total loss 1.39203215\n",
      "Trained batch 846 batch loss 1.30366135 epoch total loss 1.39192784\n",
      "Trained batch 847 batch loss 1.27055335 epoch total loss 1.39178443\n",
      "Trained batch 848 batch loss 1.2064445 epoch total loss 1.39156592\n",
      "Trained batch 849 batch loss 1.32135105 epoch total loss 1.39148319\n",
      "Trained batch 850 batch loss 1.39507926 epoch total loss 1.39148736\n",
      "Trained batch 851 batch loss 1.25368404 epoch total loss 1.39132547\n",
      "Trained batch 852 batch loss 1.39295244 epoch total loss 1.39132738\n",
      "Trained batch 853 batch loss 1.32798207 epoch total loss 1.39125311\n",
      "Trained batch 854 batch loss 1.44325113 epoch total loss 1.39131391\n",
      "Trained batch 855 batch loss 1.38010216 epoch total loss 1.39130092\n",
      "Trained batch 856 batch loss 1.43381476 epoch total loss 1.39135063\n",
      "Trained batch 857 batch loss 1.38489556 epoch total loss 1.391343\n",
      "Trained batch 858 batch loss 1.44048679 epoch total loss 1.39140022\n",
      "Trained batch 859 batch loss 1.40756047 epoch total loss 1.39141905\n",
      "Trained batch 860 batch loss 1.43292749 epoch total loss 1.39146745\n",
      "Trained batch 861 batch loss 1.39289141 epoch total loss 1.39146912\n",
      "Trained batch 862 batch loss 1.39552164 epoch total loss 1.39147377\n",
      "Trained batch 863 batch loss 1.3835206 epoch total loss 1.39146459\n",
      "Trained batch 864 batch loss 1.55903912 epoch total loss 1.39165866\n",
      "Trained batch 865 batch loss 1.52737033 epoch total loss 1.39181554\n",
      "Trained batch 866 batch loss 1.39410651 epoch total loss 1.39181817\n",
      "Trained batch 867 batch loss 1.32121074 epoch total loss 1.39173675\n",
      "Trained batch 868 batch loss 1.60211611 epoch total loss 1.39197922\n",
      "Trained batch 869 batch loss 1.50322366 epoch total loss 1.39210713\n",
      "Trained batch 870 batch loss 1.46819139 epoch total loss 1.39219451\n",
      "Trained batch 871 batch loss 1.25967658 epoch total loss 1.39204228\n",
      "Trained batch 872 batch loss 1.33766007 epoch total loss 1.39197993\n",
      "Trained batch 873 batch loss 1.30972862 epoch total loss 1.39188564\n",
      "Trained batch 874 batch loss 1.30166662 epoch total loss 1.3917824\n",
      "Trained batch 875 batch loss 1.4088912 epoch total loss 1.39180207\n",
      "Trained batch 876 batch loss 1.34848142 epoch total loss 1.3917526\n",
      "Trained batch 877 batch loss 1.43830538 epoch total loss 1.39180577\n",
      "Trained batch 878 batch loss 1.46429646 epoch total loss 1.39188838\n",
      "Trained batch 879 batch loss 1.46999049 epoch total loss 1.39197719\n",
      "Trained batch 880 batch loss 1.40884745 epoch total loss 1.39199638\n",
      "Trained batch 881 batch loss 1.38460636 epoch total loss 1.39198804\n",
      "Trained batch 882 batch loss 1.31580091 epoch total loss 1.39190161\n",
      "Trained batch 883 batch loss 1.34398246 epoch total loss 1.39184737\n",
      "Trained batch 884 batch loss 1.37896085 epoch total loss 1.39183271\n",
      "Trained batch 885 batch loss 1.3456682 epoch total loss 1.39178061\n",
      "Trained batch 886 batch loss 1.29460764 epoch total loss 1.39167082\n",
      "Trained batch 887 batch loss 1.26226485 epoch total loss 1.39152491\n",
      "Trained batch 888 batch loss 1.28261626 epoch total loss 1.39140224\n",
      "Trained batch 889 batch loss 1.3436296 epoch total loss 1.39134848\n",
      "Trained batch 890 batch loss 1.23513305 epoch total loss 1.39117289\n",
      "Trained batch 891 batch loss 1.37230635 epoch total loss 1.39115179\n",
      "Trained batch 892 batch loss 1.35684395 epoch total loss 1.39111328\n",
      "Trained batch 893 batch loss 1.43132424 epoch total loss 1.39115822\n",
      "Trained batch 894 batch loss 1.46876574 epoch total loss 1.39124501\n",
      "Trained batch 895 batch loss 1.53072286 epoch total loss 1.39140093\n",
      "Trained batch 896 batch loss 1.49307895 epoch total loss 1.39151442\n",
      "Trained batch 897 batch loss 1.33069372 epoch total loss 1.39144659\n",
      "Trained batch 898 batch loss 1.31087112 epoch total loss 1.39135683\n",
      "Trained batch 899 batch loss 1.2430476 epoch total loss 1.39119196\n",
      "Trained batch 900 batch loss 1.39177227 epoch total loss 1.39119244\n",
      "Trained batch 901 batch loss 1.37076712 epoch total loss 1.39116979\n",
      "Trained batch 902 batch loss 1.47717023 epoch total loss 1.39126515\n",
      "Trained batch 903 batch loss 1.49233353 epoch total loss 1.39137697\n",
      "Trained batch 904 batch loss 1.46406245 epoch total loss 1.39145744\n",
      "Trained batch 905 batch loss 1.5883112 epoch total loss 1.391675\n",
      "Trained batch 906 batch loss 1.49787664 epoch total loss 1.39179218\n",
      "Trained batch 907 batch loss 1.46792328 epoch total loss 1.3918761\n",
      "Trained batch 908 batch loss 1.4756254 epoch total loss 1.39196837\n",
      "Trained batch 909 batch loss 1.44335508 epoch total loss 1.39202487\n",
      "Trained batch 910 batch loss 1.41909909 epoch total loss 1.39205456\n",
      "Trained batch 911 batch loss 1.40770209 epoch total loss 1.39207172\n",
      "Trained batch 912 batch loss 1.36195564 epoch total loss 1.3920387\n",
      "Trained batch 913 batch loss 1.44296229 epoch total loss 1.39209449\n",
      "Trained batch 914 batch loss 1.40785575 epoch total loss 1.39211178\n",
      "Trained batch 915 batch loss 1.46433723 epoch total loss 1.39219069\n",
      "Trained batch 916 batch loss 1.60537469 epoch total loss 1.39242339\n",
      "Trained batch 917 batch loss 1.54017282 epoch total loss 1.39258456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 918 batch loss 1.39009666 epoch total loss 1.39258182\n",
      "Trained batch 919 batch loss 1.29973471 epoch total loss 1.39248073\n",
      "Trained batch 920 batch loss 1.2020365 epoch total loss 1.39227378\n",
      "Trained batch 921 batch loss 1.20991826 epoch total loss 1.39207578\n",
      "Trained batch 922 batch loss 1.32228231 epoch total loss 1.39200008\n",
      "Trained batch 923 batch loss 1.18614483 epoch total loss 1.39177704\n",
      "Trained batch 924 batch loss 1.06874514 epoch total loss 1.3914274\n",
      "Trained batch 925 batch loss 1.11469626 epoch total loss 1.3911283\n",
      "Trained batch 926 batch loss 1.1845448 epoch total loss 1.39090526\n",
      "Trained batch 927 batch loss 1.2106005 epoch total loss 1.39071071\n",
      "Trained batch 928 batch loss 1.32816625 epoch total loss 1.39064324\n",
      "Trained batch 929 batch loss 1.34092069 epoch total loss 1.39058983\n",
      "Trained batch 930 batch loss 1.43366849 epoch total loss 1.39063621\n",
      "Trained batch 931 batch loss 1.445086 epoch total loss 1.39069462\n",
      "Trained batch 932 batch loss 1.43312681 epoch total loss 1.39074016\n",
      "Trained batch 933 batch loss 1.48518741 epoch total loss 1.39084136\n",
      "Trained batch 934 batch loss 1.3906405 epoch total loss 1.39084113\n",
      "Trained batch 935 batch loss 1.41424608 epoch total loss 1.39086628\n",
      "Trained batch 936 batch loss 1.3103801 epoch total loss 1.39078033\n",
      "Trained batch 937 batch loss 1.23767519 epoch total loss 1.39061689\n",
      "Trained batch 938 batch loss 1.3077383 epoch total loss 1.39052856\n",
      "Trained batch 939 batch loss 1.32966137 epoch total loss 1.39046383\n",
      "Trained batch 940 batch loss 1.27961636 epoch total loss 1.39034593\n",
      "Trained batch 941 batch loss 1.37923288 epoch total loss 1.39033413\n",
      "Trained batch 942 batch loss 1.35133612 epoch total loss 1.39029276\n",
      "Trained batch 943 batch loss 1.3619287 epoch total loss 1.39026272\n",
      "Trained batch 944 batch loss 1.38292742 epoch total loss 1.39025486\n",
      "Trained batch 945 batch loss 1.29191828 epoch total loss 1.39015079\n",
      "Trained batch 946 batch loss 1.34772635 epoch total loss 1.39010596\n",
      "Trained batch 947 batch loss 1.25859785 epoch total loss 1.38996708\n",
      "Trained batch 948 batch loss 1.26886332 epoch total loss 1.38983941\n",
      "Trained batch 949 batch loss 1.3444 epoch total loss 1.38979149\n",
      "Trained batch 950 batch loss 1.38865936 epoch total loss 1.3897903\n",
      "Trained batch 951 batch loss 1.30366325 epoch total loss 1.38969982\n",
      "Trained batch 952 batch loss 1.31881166 epoch total loss 1.38962531\n",
      "Trained batch 953 batch loss 1.25945377 epoch total loss 1.3894887\n",
      "Trained batch 954 batch loss 1.17766714 epoch total loss 1.38926661\n",
      "Trained batch 955 batch loss 1.26578772 epoch total loss 1.38913727\n",
      "Trained batch 956 batch loss 1.42665803 epoch total loss 1.38917649\n",
      "Trained batch 957 batch loss 1.49073 epoch total loss 1.38928258\n",
      "Trained batch 958 batch loss 1.30453897 epoch total loss 1.38919413\n",
      "Trained batch 959 batch loss 1.26458919 epoch total loss 1.38906431\n",
      "Trained batch 960 batch loss 1.23388386 epoch total loss 1.38890266\n",
      "Trained batch 961 batch loss 1.29442096 epoch total loss 1.38880432\n",
      "Trained batch 962 batch loss 1.35930967 epoch total loss 1.38877368\n",
      "Trained batch 963 batch loss 1.47703218 epoch total loss 1.38886535\n",
      "Trained batch 964 batch loss 1.32388675 epoch total loss 1.38879788\n",
      "Trained batch 965 batch loss 1.35527575 epoch total loss 1.38876307\n",
      "Trained batch 966 batch loss 1.33665776 epoch total loss 1.38870919\n",
      "Trained batch 967 batch loss 1.27910531 epoch total loss 1.3885957\n",
      "Trained batch 968 batch loss 1.36416793 epoch total loss 1.38857043\n",
      "Trained batch 969 batch loss 1.3931216 epoch total loss 1.38857508\n",
      "Trained batch 970 batch loss 1.45351791 epoch total loss 1.38864207\n",
      "Trained batch 971 batch loss 1.46973431 epoch total loss 1.38872552\n",
      "Trained batch 972 batch loss 1.39361727 epoch total loss 1.38873065\n",
      "Trained batch 973 batch loss 1.32207406 epoch total loss 1.3886621\n",
      "Trained batch 974 batch loss 1.31604099 epoch total loss 1.38858747\n",
      "Trained batch 975 batch loss 1.28989315 epoch total loss 1.38848639\n",
      "Trained batch 976 batch loss 1.39624596 epoch total loss 1.38849425\n",
      "Trained batch 977 batch loss 1.36614227 epoch total loss 1.38847136\n",
      "Trained batch 978 batch loss 1.27100563 epoch total loss 1.3883512\n",
      "Trained batch 979 batch loss 1.35555017 epoch total loss 1.3883177\n",
      "Trained batch 980 batch loss 1.32804585 epoch total loss 1.38825619\n",
      "Trained batch 981 batch loss 1.30123067 epoch total loss 1.3881675\n",
      "Trained batch 982 batch loss 1.28220332 epoch total loss 1.38805962\n",
      "Trained batch 983 batch loss 1.23855841 epoch total loss 1.38790751\n",
      "Trained batch 984 batch loss 1.3488338 epoch total loss 1.38786793\n",
      "Trained batch 985 batch loss 1.27771497 epoch total loss 1.38775599\n",
      "Trained batch 986 batch loss 1.36337698 epoch total loss 1.38773131\n",
      "Trained batch 987 batch loss 1.44312572 epoch total loss 1.38778746\n",
      "Trained batch 988 batch loss 1.21205735 epoch total loss 1.3876096\n",
      "Trained batch 989 batch loss 1.32472527 epoch total loss 1.38754594\n",
      "Trained batch 990 batch loss 1.41350913 epoch total loss 1.38757217\n",
      "Trained batch 991 batch loss 1.3722446 epoch total loss 1.38755667\n",
      "Trained batch 992 batch loss 1.39699841 epoch total loss 1.38756609\n",
      "Trained batch 993 batch loss 1.31692553 epoch total loss 1.38749492\n",
      "Trained batch 994 batch loss 1.2839725 epoch total loss 1.38739073\n",
      "Trained batch 995 batch loss 1.19498801 epoch total loss 1.38719738\n",
      "Trained batch 996 batch loss 1.28278422 epoch total loss 1.38709259\n",
      "Trained batch 997 batch loss 1.33065629 epoch total loss 1.38703597\n",
      "Trained batch 998 batch loss 1.27532113 epoch total loss 1.38692403\n",
      "Trained batch 999 batch loss 1.27780092 epoch total loss 1.38681483\n",
      "Trained batch 1000 batch loss 1.33553743 epoch total loss 1.38676357\n",
      "Trained batch 1001 batch loss 1.25970507 epoch total loss 1.38663673\n",
      "Trained batch 1002 batch loss 1.20887589 epoch total loss 1.38645923\n",
      "Trained batch 1003 batch loss 1.26041627 epoch total loss 1.38633358\n",
      "Trained batch 1004 batch loss 1.34313273 epoch total loss 1.38629055\n",
      "Trained batch 1005 batch loss 1.23339391 epoch total loss 1.38613844\n",
      "Trained batch 1006 batch loss 1.27053308 epoch total loss 1.3860234\n",
      "Trained batch 1007 batch loss 1.23621762 epoch total loss 1.38587463\n",
      "Trained batch 1008 batch loss 1.2699666 epoch total loss 1.38575971\n",
      "Trained batch 1009 batch loss 1.18531954 epoch total loss 1.38556111\n",
      "Trained batch 1010 batch loss 1.14302707 epoch total loss 1.38532102\n",
      "Trained batch 1011 batch loss 1.15936196 epoch total loss 1.38509738\n",
      "Trained batch 1012 batch loss 1.32819927 epoch total loss 1.38504124\n",
      "Trained batch 1013 batch loss 1.35272944 epoch total loss 1.38500941\n",
      "Trained batch 1014 batch loss 1.28251696 epoch total loss 1.38490832\n",
      "Trained batch 1015 batch loss 1.38285875 epoch total loss 1.38490617\n",
      "Trained batch 1016 batch loss 1.43638206 epoch total loss 1.38495696\n",
      "Trained batch 1017 batch loss 1.45884836 epoch total loss 1.38502955\n",
      "Trained batch 1018 batch loss 1.37566268 epoch total loss 1.38502038\n",
      "Trained batch 1019 batch loss 1.38188624 epoch total loss 1.38501716\n",
      "Trained batch 1020 batch loss 1.33884549 epoch total loss 1.38497198\n",
      "Trained batch 1021 batch loss 1.36869621 epoch total loss 1.384956\n",
      "Trained batch 1022 batch loss 1.28522456 epoch total loss 1.38485849\n",
      "Trained batch 1023 batch loss 1.3327626 epoch total loss 1.38480747\n",
      "Trained batch 1024 batch loss 1.27372491 epoch total loss 1.38469899\n",
      "Trained batch 1025 batch loss 1.2060082 epoch total loss 1.3845247\n",
      "Trained batch 1026 batch loss 1.23139215 epoch total loss 1.38437545\n",
      "Trained batch 1027 batch loss 1.19580138 epoch total loss 1.38419187\n",
      "Trained batch 1028 batch loss 1.30431294 epoch total loss 1.38411415\n",
      "Trained batch 1029 batch loss 1.17275953 epoch total loss 1.38390875\n",
      "Trained batch 1030 batch loss 1.40679038 epoch total loss 1.38393092\n",
      "Trained batch 1031 batch loss 1.20181179 epoch total loss 1.38375425\n",
      "Trained batch 1032 batch loss 1.32468104 epoch total loss 1.38369703\n",
      "Trained batch 1033 batch loss 1.36002862 epoch total loss 1.38367403\n",
      "Trained batch 1034 batch loss 1.3529439 epoch total loss 1.38364434\n",
      "Trained batch 1035 batch loss 1.48163331 epoch total loss 1.38373911\n",
      "Trained batch 1036 batch loss 1.45988226 epoch total loss 1.38381255\n",
      "Trained batch 1037 batch loss 1.33409047 epoch total loss 1.38376462\n",
      "Trained batch 1038 batch loss 1.33072376 epoch total loss 1.38371348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1039 batch loss 1.39088583 epoch total loss 1.38372028\n",
      "Trained batch 1040 batch loss 1.338341 epoch total loss 1.38367677\n",
      "Trained batch 1041 batch loss 1.27416587 epoch total loss 1.38357151\n",
      "Trained batch 1042 batch loss 1.33980107 epoch total loss 1.38352954\n",
      "Trained batch 1043 batch loss 1.302948 epoch total loss 1.3834523\n",
      "Trained batch 1044 batch loss 1.21000862 epoch total loss 1.38328612\n",
      "Trained batch 1045 batch loss 1.14692962 epoch total loss 1.38306\n",
      "Trained batch 1046 batch loss 1.11884594 epoch total loss 1.38280749\n",
      "Trained batch 1047 batch loss 1.3534168 epoch total loss 1.38277936\n",
      "Trained batch 1048 batch loss 1.4166137 epoch total loss 1.38281167\n",
      "Trained batch 1049 batch loss 1.3958149 epoch total loss 1.38282418\n",
      "Trained batch 1050 batch loss 1.49798679 epoch total loss 1.38293386\n",
      "Trained batch 1051 batch loss 1.32544744 epoch total loss 1.38287914\n",
      "Trained batch 1052 batch loss 1.26538062 epoch total loss 1.38276744\n",
      "Trained batch 1053 batch loss 1.4502641 epoch total loss 1.38283157\n",
      "Trained batch 1054 batch loss 1.38348591 epoch total loss 1.38283229\n",
      "Trained batch 1055 batch loss 1.39287364 epoch total loss 1.38284183\n",
      "Trained batch 1056 batch loss 1.44218922 epoch total loss 1.38289797\n",
      "Trained batch 1057 batch loss 1.41282034 epoch total loss 1.38292623\n",
      "Trained batch 1058 batch loss 1.57859957 epoch total loss 1.38311124\n",
      "Trained batch 1059 batch loss 1.35186553 epoch total loss 1.38308167\n",
      "Trained batch 1060 batch loss 1.38527942 epoch total loss 1.3830837\n",
      "Trained batch 1061 batch loss 1.30731106 epoch total loss 1.38301218\n",
      "Trained batch 1062 batch loss 1.22447205 epoch total loss 1.38286293\n",
      "Trained batch 1063 batch loss 1.46324611 epoch total loss 1.38293862\n",
      "Trained batch 1064 batch loss 1.46753991 epoch total loss 1.38301814\n",
      "Trained batch 1065 batch loss 1.46675754 epoch total loss 1.38309681\n",
      "Trained batch 1066 batch loss 1.38602114 epoch total loss 1.38309944\n",
      "Trained batch 1067 batch loss 1.45093548 epoch total loss 1.38316309\n",
      "Trained batch 1068 batch loss 1.51833081 epoch total loss 1.38328958\n",
      "Trained batch 1069 batch loss 1.33462155 epoch total loss 1.38324404\n",
      "Trained batch 1070 batch loss 1.32512152 epoch total loss 1.38318968\n",
      "Trained batch 1071 batch loss 1.34296799 epoch total loss 1.38315213\n",
      "Trained batch 1072 batch loss 1.37459445 epoch total loss 1.38314426\n",
      "Trained batch 1073 batch loss 1.3093338 epoch total loss 1.38307536\n",
      "Trained batch 1074 batch loss 1.36624444 epoch total loss 1.38305974\n",
      "Trained batch 1075 batch loss 1.44456553 epoch total loss 1.38311696\n",
      "Trained batch 1076 batch loss 1.45117736 epoch total loss 1.38318014\n",
      "Trained batch 1077 batch loss 1.37691712 epoch total loss 1.38317442\n",
      "Trained batch 1078 batch loss 1.44644511 epoch total loss 1.38323307\n",
      "Trained batch 1079 batch loss 1.31418467 epoch total loss 1.38316905\n",
      "Trained batch 1080 batch loss 1.34202421 epoch total loss 1.38313103\n",
      "Trained batch 1081 batch loss 1.24037671 epoch total loss 1.38299894\n",
      "Trained batch 1082 batch loss 1.33413 epoch total loss 1.38295376\n",
      "Trained batch 1083 batch loss 1.36746895 epoch total loss 1.38293946\n",
      "Trained batch 1084 batch loss 1.267905 epoch total loss 1.38283336\n",
      "Trained batch 1085 batch loss 1.34703946 epoch total loss 1.38280034\n",
      "Trained batch 1086 batch loss 1.26113284 epoch total loss 1.38268828\n",
      "Trained batch 1087 batch loss 1.20352018 epoch total loss 1.38252342\n",
      "Trained batch 1088 batch loss 1.29176474 epoch total loss 1.38244\n",
      "Trained batch 1089 batch loss 1.3395977 epoch total loss 1.38240063\n",
      "Trained batch 1090 batch loss 1.12950826 epoch total loss 1.38216865\n",
      "Trained batch 1091 batch loss 1.25080693 epoch total loss 1.38204825\n",
      "Trained batch 1092 batch loss 1.26856816 epoch total loss 1.38194442\n",
      "Trained batch 1093 batch loss 1.19760907 epoch total loss 1.38177574\n",
      "Trained batch 1094 batch loss 1.28141928 epoch total loss 1.38168395\n",
      "Trained batch 1095 batch loss 1.22131801 epoch total loss 1.38153756\n",
      "Trained batch 1096 batch loss 1.23208475 epoch total loss 1.38140106\n",
      "Trained batch 1097 batch loss 1.17632508 epoch total loss 1.38121414\n",
      "Trained batch 1098 batch loss 1.48453879 epoch total loss 1.3813082\n",
      "Trained batch 1099 batch loss 1.54375315 epoch total loss 1.3814559\n",
      "Trained batch 1100 batch loss 1.35267758 epoch total loss 1.38142979\n",
      "Trained batch 1101 batch loss 1.4483428 epoch total loss 1.38149059\n",
      "Trained batch 1102 batch loss 1.49315357 epoch total loss 1.38159192\n",
      "Trained batch 1103 batch loss 1.50061274 epoch total loss 1.3816998\n",
      "Trained batch 1104 batch loss 1.46251929 epoch total loss 1.38177299\n",
      "Trained batch 1105 batch loss 1.30832815 epoch total loss 1.3817066\n",
      "Trained batch 1106 batch loss 1.28306603 epoch total loss 1.38161743\n",
      "Trained batch 1107 batch loss 1.42309928 epoch total loss 1.38165486\n",
      "Trained batch 1108 batch loss 1.34216368 epoch total loss 1.38161922\n",
      "Trained batch 1109 batch loss 1.39885736 epoch total loss 1.38163471\n",
      "Trained batch 1110 batch loss 1.3164885 epoch total loss 1.38157606\n",
      "Trained batch 1111 batch loss 1.46258318 epoch total loss 1.3816489\n",
      "Trained batch 1112 batch loss 1.46333241 epoch total loss 1.38172245\n",
      "Trained batch 1113 batch loss 1.36275291 epoch total loss 1.3817054\n",
      "Trained batch 1114 batch loss 1.42170918 epoch total loss 1.3817414\n",
      "Trained batch 1115 batch loss 1.47231436 epoch total loss 1.38182259\n",
      "Trained batch 1116 batch loss 1.34689415 epoch total loss 1.38179135\n",
      "Trained batch 1117 batch loss 1.38681018 epoch total loss 1.38179588\n",
      "Trained batch 1118 batch loss 1.30527937 epoch total loss 1.38172746\n",
      "Trained batch 1119 batch loss 1.26640868 epoch total loss 1.38162434\n",
      "Trained batch 1120 batch loss 1.34764564 epoch total loss 1.38159394\n",
      "Trained batch 1121 batch loss 1.41162658 epoch total loss 1.38162076\n",
      "Trained batch 1122 batch loss 1.3595506 epoch total loss 1.3816011\n",
      "Trained batch 1123 batch loss 1.26161695 epoch total loss 1.38149416\n",
      "Trained batch 1124 batch loss 1.29347479 epoch total loss 1.38141584\n",
      "Trained batch 1125 batch loss 1.34455729 epoch total loss 1.38138318\n",
      "Trained batch 1126 batch loss 1.30351532 epoch total loss 1.38131392\n",
      "Trained batch 1127 batch loss 1.19882607 epoch total loss 1.38115203\n",
      "Trained batch 1128 batch loss 1.39844656 epoch total loss 1.38116741\n",
      "Trained batch 1129 batch loss 1.37781954 epoch total loss 1.38116443\n",
      "Trained batch 1130 batch loss 1.38189828 epoch total loss 1.38116515\n",
      "Trained batch 1131 batch loss 1.41503179 epoch total loss 1.38119507\n",
      "Trained batch 1132 batch loss 1.43154466 epoch total loss 1.38123953\n",
      "Trained batch 1133 batch loss 1.38997769 epoch total loss 1.38124728\n",
      "Trained batch 1134 batch loss 1.39557111 epoch total loss 1.38125992\n",
      "Trained batch 1135 batch loss 1.33573961 epoch total loss 1.38121974\n",
      "Trained batch 1136 batch loss 1.36202788 epoch total loss 1.38120294\n",
      "Trained batch 1137 batch loss 1.37508523 epoch total loss 1.38119757\n",
      "Trained batch 1138 batch loss 1.28040886 epoch total loss 1.381109\n",
      "Trained batch 1139 batch loss 1.39426064 epoch total loss 1.38112056\n",
      "Trained batch 1140 batch loss 1.34658492 epoch total loss 1.38109028\n",
      "Trained batch 1141 batch loss 1.31743526 epoch total loss 1.38103437\n",
      "Trained batch 1142 batch loss 1.22217798 epoch total loss 1.38089526\n",
      "Trained batch 1143 batch loss 1.20467746 epoch total loss 1.38074112\n",
      "Trained batch 1144 batch loss 1.24563265 epoch total loss 1.38062298\n",
      "Trained batch 1145 batch loss 1.3612417 epoch total loss 1.38060606\n",
      "Trained batch 1146 batch loss 1.37478054 epoch total loss 1.38060093\n",
      "Trained batch 1147 batch loss 1.33436453 epoch total loss 1.38056064\n",
      "Trained batch 1148 batch loss 1.44831705 epoch total loss 1.38061965\n",
      "Trained batch 1149 batch loss 1.34600067 epoch total loss 1.38058949\n",
      "Trained batch 1150 batch loss 1.2440716 epoch total loss 1.38047075\n",
      "Trained batch 1151 batch loss 1.24212146 epoch total loss 1.38035047\n",
      "Trained batch 1152 batch loss 1.28337204 epoch total loss 1.38026631\n",
      "Trained batch 1153 batch loss 1.36907542 epoch total loss 1.38025653\n",
      "Trained batch 1154 batch loss 1.31481779 epoch total loss 1.38019979\n",
      "Trained batch 1155 batch loss 1.37745762 epoch total loss 1.38019741\n",
      "Trained batch 1156 batch loss 1.41211677 epoch total loss 1.38022506\n",
      "Trained batch 1157 batch loss 1.44360924 epoch total loss 1.38027978\n",
      "Trained batch 1158 batch loss 1.43015695 epoch total loss 1.38032293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1159 batch loss 1.49023652 epoch total loss 1.3804177\n",
      "Trained batch 1160 batch loss 1.41289639 epoch total loss 1.38044572\n",
      "Trained batch 1161 batch loss 1.23979855 epoch total loss 1.38032448\n",
      "Trained batch 1162 batch loss 1.38960278 epoch total loss 1.38033259\n",
      "Trained batch 1163 batch loss 1.33931768 epoch total loss 1.3802973\n",
      "Trained batch 1164 batch loss 1.35852182 epoch total loss 1.38027859\n",
      "Trained batch 1165 batch loss 1.40025 epoch total loss 1.38029575\n",
      "Trained batch 1166 batch loss 1.26640129 epoch total loss 1.380198\n",
      "Trained batch 1167 batch loss 1.28057218 epoch total loss 1.38011265\n",
      "Trained batch 1168 batch loss 1.24602747 epoch total loss 1.37999773\n",
      "Trained batch 1169 batch loss 1.29546022 epoch total loss 1.37992537\n",
      "Trained batch 1170 batch loss 1.25969934 epoch total loss 1.37982261\n",
      "Trained batch 1171 batch loss 1.33455157 epoch total loss 1.37978399\n",
      "Trained batch 1172 batch loss 1.19637203 epoch total loss 1.37962759\n",
      "Trained batch 1173 batch loss 1.12334585 epoch total loss 1.37940896\n",
      "Trained batch 1174 batch loss 1.16976166 epoch total loss 1.3792305\n",
      "Trained batch 1175 batch loss 1.50531268 epoch total loss 1.37933779\n",
      "Trained batch 1176 batch loss 1.55760217 epoch total loss 1.37948942\n",
      "Trained batch 1177 batch loss 1.59289145 epoch total loss 1.37967074\n",
      "Trained batch 1178 batch loss 1.36822224 epoch total loss 1.37966096\n",
      "Trained batch 1179 batch loss 1.33257318 epoch total loss 1.37962091\n",
      "Trained batch 1180 batch loss 1.36340046 epoch total loss 1.3796072\n",
      "Trained batch 1181 batch loss 1.43135202 epoch total loss 1.37965107\n",
      "Trained batch 1182 batch loss 1.42934418 epoch total loss 1.37969315\n",
      "Trained batch 1183 batch loss 1.32262945 epoch total loss 1.37964487\n",
      "Trained batch 1184 batch loss 1.43551052 epoch total loss 1.37969208\n",
      "Trained batch 1185 batch loss 1.45868826 epoch total loss 1.37975883\n",
      "Trained batch 1186 batch loss 1.34130287 epoch total loss 1.37972641\n",
      "Trained batch 1187 batch loss 1.34775305 epoch total loss 1.37969947\n",
      "Trained batch 1188 batch loss 1.23290253 epoch total loss 1.37957585\n",
      "Trained batch 1189 batch loss 1.29229975 epoch total loss 1.37950253\n",
      "Trained batch 1190 batch loss 1.3390578 epoch total loss 1.37946856\n",
      "Trained batch 1191 batch loss 1.35239744 epoch total loss 1.37944591\n",
      "Trained batch 1192 batch loss 1.1934973 epoch total loss 1.37928987\n",
      "Trained batch 1193 batch loss 1.23390341 epoch total loss 1.37916803\n",
      "Trained batch 1194 batch loss 1.34517622 epoch total loss 1.37913954\n",
      "Trained batch 1195 batch loss 1.38198853 epoch total loss 1.37914193\n",
      "Trained batch 1196 batch loss 1.42110181 epoch total loss 1.37917697\n",
      "Trained batch 1197 batch loss 1.28950691 epoch total loss 1.37910211\n",
      "Trained batch 1198 batch loss 1.39327431 epoch total loss 1.37911403\n",
      "Trained batch 1199 batch loss 1.32724476 epoch total loss 1.37907076\n",
      "Trained batch 1200 batch loss 1.35394692 epoch total loss 1.3790499\n",
      "Trained batch 1201 batch loss 1.29189408 epoch total loss 1.3789773\n",
      "Trained batch 1202 batch loss 1.38051629 epoch total loss 1.37897861\n",
      "Trained batch 1203 batch loss 1.30075884 epoch total loss 1.37891352\n",
      "Trained batch 1204 batch loss 1.34027982 epoch total loss 1.37888145\n",
      "Trained batch 1205 batch loss 1.389732 epoch total loss 1.37889051\n",
      "Trained batch 1206 batch loss 1.42314029 epoch total loss 1.37892723\n",
      "Trained batch 1207 batch loss 1.35566437 epoch total loss 1.37890792\n",
      "Trained batch 1208 batch loss 1.29342651 epoch total loss 1.37883723\n",
      "Trained batch 1209 batch loss 1.21824813 epoch total loss 1.37870443\n",
      "Trained batch 1210 batch loss 1.21492326 epoch total loss 1.37856913\n",
      "Trained batch 1211 batch loss 1.27599537 epoch total loss 1.37848437\n",
      "Trained batch 1212 batch loss 1.33094382 epoch total loss 1.37844515\n",
      "Trained batch 1213 batch loss 1.40638232 epoch total loss 1.37846816\n",
      "Trained batch 1214 batch loss 1.40259671 epoch total loss 1.37848806\n",
      "Trained batch 1215 batch loss 1.44645751 epoch total loss 1.37854397\n",
      "Trained batch 1216 batch loss 1.43803024 epoch total loss 1.37859285\n",
      "Trained batch 1217 batch loss 1.28423834 epoch total loss 1.37851524\n",
      "Trained batch 1218 batch loss 1.16058373 epoch total loss 1.37833643\n",
      "Trained batch 1219 batch loss 1.12483549 epoch total loss 1.37812841\n",
      "Trained batch 1220 batch loss 1.49899518 epoch total loss 1.37822759\n",
      "Trained batch 1221 batch loss 1.45076323 epoch total loss 1.37828696\n",
      "Trained batch 1222 batch loss 1.38984942 epoch total loss 1.37829649\n",
      "Trained batch 1223 batch loss 1.27111077 epoch total loss 1.37820888\n",
      "Trained batch 1224 batch loss 1.41950619 epoch total loss 1.37824261\n",
      "Trained batch 1225 batch loss 1.36579442 epoch total loss 1.37823248\n",
      "Trained batch 1226 batch loss 1.39174199 epoch total loss 1.37824357\n",
      "Trained batch 1227 batch loss 1.24379277 epoch total loss 1.37813389\n",
      "Trained batch 1228 batch loss 1.31956458 epoch total loss 1.37808621\n",
      "Trained batch 1229 batch loss 1.35883474 epoch total loss 1.37807059\n",
      "Trained batch 1230 batch loss 1.26907015 epoch total loss 1.37798202\n",
      "Trained batch 1231 batch loss 1.34876847 epoch total loss 1.3779583\n",
      "Trained batch 1232 batch loss 1.31338465 epoch total loss 1.37790585\n",
      "Trained batch 1233 batch loss 1.3022114 epoch total loss 1.37784445\n",
      "Trained batch 1234 batch loss 1.32864714 epoch total loss 1.37780452\n",
      "Trained batch 1235 batch loss 1.40332627 epoch total loss 1.37782526\n",
      "Trained batch 1236 batch loss 1.33527875 epoch total loss 1.37779081\n",
      "Trained batch 1237 batch loss 1.33714747 epoch total loss 1.37775803\n",
      "Trained batch 1238 batch loss 1.2505796 epoch total loss 1.37765527\n",
      "Trained batch 1239 batch loss 1.23172796 epoch total loss 1.37753749\n",
      "Trained batch 1240 batch loss 1.35905719 epoch total loss 1.37752247\n",
      "Trained batch 1241 batch loss 1.40789104 epoch total loss 1.37754691\n",
      "Trained batch 1242 batch loss 1.42808938 epoch total loss 1.37758768\n",
      "Trained batch 1243 batch loss 1.35063314 epoch total loss 1.37756598\n",
      "Trained batch 1244 batch loss 1.35430729 epoch total loss 1.37754714\n",
      "Trained batch 1245 batch loss 1.38156092 epoch total loss 1.37755048\n",
      "Trained batch 1246 batch loss 1.390378 epoch total loss 1.37756073\n",
      "Trained batch 1247 batch loss 1.45829678 epoch total loss 1.37762547\n",
      "Trained batch 1248 batch loss 1.29851341 epoch total loss 1.37756205\n",
      "Trained batch 1249 batch loss 1.28791511 epoch total loss 1.37749028\n",
      "Trained batch 1250 batch loss 1.25500631 epoch total loss 1.37739229\n",
      "Trained batch 1251 batch loss 1.21373856 epoch total loss 1.37726152\n",
      "Trained batch 1252 batch loss 1.34710324 epoch total loss 1.37723732\n",
      "Trained batch 1253 batch loss 1.37683249 epoch total loss 1.37723696\n",
      "Trained batch 1254 batch loss 1.29333258 epoch total loss 1.37717009\n",
      "Trained batch 1255 batch loss 1.33734369 epoch total loss 1.37713838\n",
      "Trained batch 1256 batch loss 1.25966597 epoch total loss 1.37704492\n",
      "Trained batch 1257 batch loss 1.24545562 epoch total loss 1.37694025\n",
      "Trained batch 1258 batch loss 1.39010644 epoch total loss 1.37695074\n",
      "Trained batch 1259 batch loss 1.42843413 epoch total loss 1.37699163\n",
      "Trained batch 1260 batch loss 1.3994633 epoch total loss 1.37700939\n",
      "Trained batch 1261 batch loss 1.20908773 epoch total loss 1.37687624\n",
      "Trained batch 1262 batch loss 1.18952036 epoch total loss 1.37672782\n",
      "Trained batch 1263 batch loss 1.31679547 epoch total loss 1.37668037\n",
      "Trained batch 1264 batch loss 1.25183284 epoch total loss 1.37658155\n",
      "Trained batch 1265 batch loss 1.20248222 epoch total loss 1.37644398\n",
      "Trained batch 1266 batch loss 1.43428373 epoch total loss 1.37648976\n",
      "Trained batch 1267 batch loss 1.25216365 epoch total loss 1.37639165\n",
      "Trained batch 1268 batch loss 1.32294571 epoch total loss 1.37634957\n",
      "Trained batch 1269 batch loss 1.36120903 epoch total loss 1.37633753\n",
      "Trained batch 1270 batch loss 1.36313748 epoch total loss 1.37632716\n",
      "Trained batch 1271 batch loss 1.37741196 epoch total loss 1.37632811\n",
      "Trained batch 1272 batch loss 1.64090347 epoch total loss 1.37653601\n",
      "Trained batch 1273 batch loss 1.56176448 epoch total loss 1.37668157\n",
      "Trained batch 1274 batch loss 1.39195323 epoch total loss 1.37669361\n",
      "Trained batch 1275 batch loss 1.20827913 epoch total loss 1.3765614\n",
      "Trained batch 1276 batch loss 1.38652384 epoch total loss 1.37656927\n",
      "Trained batch 1277 batch loss 1.42197645 epoch total loss 1.3766048\n",
      "Trained batch 1278 batch loss 1.56688988 epoch total loss 1.37675369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1279 batch loss 1.46827817 epoch total loss 1.37682521\n",
      "Trained batch 1280 batch loss 1.34257793 epoch total loss 1.37679839\n",
      "Trained batch 1281 batch loss 1.36818719 epoch total loss 1.37679172\n",
      "Trained batch 1282 batch loss 1.23137951 epoch total loss 1.37667823\n",
      "Trained batch 1283 batch loss 1.3404665 epoch total loss 1.37665\n",
      "Trained batch 1284 batch loss 1.31460238 epoch total loss 1.3766017\n",
      "Trained batch 1285 batch loss 1.16811156 epoch total loss 1.37643933\n",
      "Trained batch 1286 batch loss 1.33714283 epoch total loss 1.37640882\n",
      "Trained batch 1287 batch loss 1.45476794 epoch total loss 1.37646973\n",
      "Trained batch 1288 batch loss 1.38195348 epoch total loss 1.3764739\n",
      "Trained batch 1289 batch loss 1.23711061 epoch total loss 1.37636578\n",
      "Trained batch 1290 batch loss 1.24341357 epoch total loss 1.37626266\n",
      "Trained batch 1291 batch loss 1.20245314 epoch total loss 1.37612808\n",
      "Trained batch 1292 batch loss 1.26838815 epoch total loss 1.37604463\n",
      "Trained batch 1293 batch loss 1.10105097 epoch total loss 1.37583208\n",
      "Trained batch 1294 batch loss 1.19557285 epoch total loss 1.37569273\n",
      "Trained batch 1295 batch loss 1.23090482 epoch total loss 1.37558091\n",
      "Trained batch 1296 batch loss 1.26271343 epoch total loss 1.37549388\n",
      "Trained batch 1297 batch loss 1.24802148 epoch total loss 1.37539554\n",
      "Trained batch 1298 batch loss 1.24308205 epoch total loss 1.37529361\n",
      "Trained batch 1299 batch loss 1.43023241 epoch total loss 1.37533581\n",
      "Trained batch 1300 batch loss 1.40104246 epoch total loss 1.3753556\n",
      "Trained batch 1301 batch loss 1.39053082 epoch total loss 1.37536728\n",
      "Trained batch 1302 batch loss 1.37671208 epoch total loss 1.37536824\n",
      "Trained batch 1303 batch loss 1.32454062 epoch total loss 1.37532926\n",
      "Trained batch 1304 batch loss 1.35653901 epoch total loss 1.37531495\n",
      "Trained batch 1305 batch loss 1.35884309 epoch total loss 1.37530231\n",
      "Trained batch 1306 batch loss 1.37131619 epoch total loss 1.37529933\n",
      "Trained batch 1307 batch loss 1.51179647 epoch total loss 1.37540376\n",
      "Trained batch 1308 batch loss 1.15098977 epoch total loss 1.37523222\n",
      "Trained batch 1309 batch loss 1.31618166 epoch total loss 1.37518704\n",
      "Trained batch 1310 batch loss 1.24384928 epoch total loss 1.3750869\n",
      "Trained batch 1311 batch loss 1.29548073 epoch total loss 1.37502611\n",
      "Trained batch 1312 batch loss 1.37585473 epoch total loss 1.37502682\n",
      "Trained batch 1313 batch loss 1.48995876 epoch total loss 1.37511432\n",
      "Trained batch 1314 batch loss 1.3776505 epoch total loss 1.37511635\n",
      "Trained batch 1315 batch loss 1.32768178 epoch total loss 1.37508023\n",
      "Trained batch 1316 batch loss 1.43064189 epoch total loss 1.37512243\n",
      "Trained batch 1317 batch loss 1.44887161 epoch total loss 1.37517846\n",
      "Trained batch 1318 batch loss 1.31263709 epoch total loss 1.37513101\n",
      "Trained batch 1319 batch loss 1.34622121 epoch total loss 1.37510908\n",
      "Trained batch 1320 batch loss 1.34303391 epoch total loss 1.37508476\n",
      "Trained batch 1321 batch loss 1.31232786 epoch total loss 1.37503719\n",
      "Trained batch 1322 batch loss 1.27275217 epoch total loss 1.37495983\n",
      "Trained batch 1323 batch loss 1.38310075 epoch total loss 1.37496591\n",
      "Trained batch 1324 batch loss 1.3020277 epoch total loss 1.37491083\n",
      "Trained batch 1325 batch loss 1.4248234 epoch total loss 1.3749485\n",
      "Trained batch 1326 batch loss 1.37739503 epoch total loss 1.37495041\n",
      "Trained batch 1327 batch loss 1.2948482 epoch total loss 1.37489\n",
      "Trained batch 1328 batch loss 1.23247 epoch total loss 1.37478268\n",
      "Trained batch 1329 batch loss 1.3422358 epoch total loss 1.37475824\n",
      "Trained batch 1330 batch loss 1.24086952 epoch total loss 1.37465751\n",
      "Trained batch 1331 batch loss 1.25395799 epoch total loss 1.37456679\n",
      "Trained batch 1332 batch loss 1.33642077 epoch total loss 1.37453818\n",
      "Trained batch 1333 batch loss 1.28563786 epoch total loss 1.37447155\n",
      "Trained batch 1334 batch loss 1.37657022 epoch total loss 1.37447309\n",
      "Trained batch 1335 batch loss 1.25194812 epoch total loss 1.3743813\n",
      "Trained batch 1336 batch loss 1.31381011 epoch total loss 1.374336\n",
      "Trained batch 1337 batch loss 1.24991751 epoch total loss 1.3742429\n",
      "Trained batch 1338 batch loss 1.26484537 epoch total loss 1.37416124\n",
      "Trained batch 1339 batch loss 1.2923131 epoch total loss 1.37410009\n",
      "Trained batch 1340 batch loss 1.23559213 epoch total loss 1.37399673\n",
      "Trained batch 1341 batch loss 1.29697013 epoch total loss 1.37393928\n",
      "Trained batch 1342 batch loss 1.25664616 epoch total loss 1.3738519\n",
      "Trained batch 1343 batch loss 1.44432557 epoch total loss 1.37390435\n",
      "Trained batch 1344 batch loss 1.30657649 epoch total loss 1.37385428\n",
      "Trained batch 1345 batch loss 1.28799951 epoch total loss 1.37379038\n",
      "Trained batch 1346 batch loss 1.25340629 epoch total loss 1.37370098\n",
      "Trained batch 1347 batch loss 1.36227536 epoch total loss 1.37369251\n",
      "Trained batch 1348 batch loss 1.34999728 epoch total loss 1.37367487\n",
      "Trained batch 1349 batch loss 1.29345667 epoch total loss 1.37361538\n",
      "Trained batch 1350 batch loss 1.30938196 epoch total loss 1.37356782\n",
      "Trained batch 1351 batch loss 1.23963594 epoch total loss 1.37346864\n",
      "Trained batch 1352 batch loss 1.18956232 epoch total loss 1.37333262\n",
      "Trained batch 1353 batch loss 1.2443639 epoch total loss 1.37323737\n",
      "Trained batch 1354 batch loss 1.2784729 epoch total loss 1.37316728\n",
      "Trained batch 1355 batch loss 1.2973237 epoch total loss 1.37311137\n",
      "Trained batch 1356 batch loss 1.24209094 epoch total loss 1.37301469\n",
      "Trained batch 1357 batch loss 1.2719388 epoch total loss 1.3729403\n",
      "Trained batch 1358 batch loss 1.21292973 epoch total loss 1.3728224\n",
      "Trained batch 1359 batch loss 1.28979373 epoch total loss 1.37276137\n",
      "Trained batch 1360 batch loss 1.40708208 epoch total loss 1.37278652\n",
      "Trained batch 1361 batch loss 1.38628125 epoch total loss 1.37279642\n",
      "Trained batch 1362 batch loss 1.37588549 epoch total loss 1.37279868\n",
      "Trained batch 1363 batch loss 1.26820767 epoch total loss 1.37272191\n",
      "Trained batch 1364 batch loss 1.09225571 epoch total loss 1.37251639\n",
      "Trained batch 1365 batch loss 1.04854453 epoch total loss 1.37227905\n",
      "Trained batch 1366 batch loss 1.14877391 epoch total loss 1.37211549\n",
      "Trained batch 1367 batch loss 1.18446696 epoch total loss 1.37197816\n",
      "Trained batch 1368 batch loss 1.24251282 epoch total loss 1.37188351\n",
      "Trained batch 1369 batch loss 1.24707019 epoch total loss 1.37179244\n",
      "Trained batch 1370 batch loss 1.26370823 epoch total loss 1.3717134\n",
      "Trained batch 1371 batch loss 1.19938099 epoch total loss 1.37158775\n",
      "Trained batch 1372 batch loss 1.24042594 epoch total loss 1.37149215\n",
      "Trained batch 1373 batch loss 1.31870067 epoch total loss 1.37145376\n",
      "Trained batch 1374 batch loss 1.19592345 epoch total loss 1.37132597\n",
      "Trained batch 1375 batch loss 1.25040364 epoch total loss 1.37123799\n",
      "Trained batch 1376 batch loss 1.05724692 epoch total loss 1.37100983\n",
      "Trained batch 1377 batch loss 1.10944557 epoch total loss 1.37081993\n",
      "Trained batch 1378 batch loss 1.35355926 epoch total loss 1.37080741\n",
      "Trained batch 1379 batch loss 1.35447359 epoch total loss 1.37079549\n",
      "Trained batch 1380 batch loss 1.3971138 epoch total loss 1.37081456\n",
      "Trained batch 1381 batch loss 1.31207526 epoch total loss 1.37077212\n",
      "Trained batch 1382 batch loss 1.38405955 epoch total loss 1.37078166\n",
      "Trained batch 1383 batch loss 1.25634515 epoch total loss 1.37069893\n",
      "Trained batch 1384 batch loss 1.20031905 epoch total loss 1.37057579\n",
      "Trained batch 1385 batch loss 1.25699508 epoch total loss 1.37049377\n",
      "Trained batch 1386 batch loss 1.2937572 epoch total loss 1.37043834\n",
      "Trained batch 1387 batch loss 1.29171622 epoch total loss 1.37038171\n",
      "Trained batch 1388 batch loss 1.27170229 epoch total loss 1.37031054\n",
      "Epoch 2 train loss 1.3703105449676514\n",
      "Validated batch 1 batch loss 1.4063102\n",
      "Validated batch 2 batch loss 1.30856109\n",
      "Validated batch 3 batch loss 1.26252747\n",
      "Validated batch 4 batch loss 1.26383483\n",
      "Validated batch 5 batch loss 1.29106331\n",
      "Validated batch 6 batch loss 1.37248385\n",
      "Validated batch 7 batch loss 1.29802227\n",
      "Validated batch 8 batch loss 1.26279759\n",
      "Validated batch 9 batch loss 1.3812741\n",
      "Validated batch 10 batch loss 1.32362783\n",
      "Validated batch 11 batch loss 1.24908233\n",
      "Validated batch 12 batch loss 1.19372094\n",
      "Validated batch 13 batch loss 1.3623172\n",
      "Validated batch 14 batch loss 1.38341808\n",
      "Validated batch 15 batch loss 1.42554665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 16 batch loss 1.40662122\n",
      "Validated batch 17 batch loss 1.302163\n",
      "Validated batch 18 batch loss 1.45260501\n",
      "Validated batch 19 batch loss 1.32962871\n",
      "Validated batch 20 batch loss 1.31351018\n",
      "Validated batch 21 batch loss 1.34186852\n",
      "Validated batch 22 batch loss 1.10756409\n",
      "Validated batch 23 batch loss 1.42195487\n",
      "Validated batch 24 batch loss 1.32683408\n",
      "Validated batch 25 batch loss 1.36225009\n",
      "Validated batch 26 batch loss 1.2416997\n",
      "Validated batch 27 batch loss 1.35699964\n",
      "Validated batch 28 batch loss 1.35319149\n",
      "Validated batch 29 batch loss 1.37378705\n",
      "Validated batch 30 batch loss 1.43477583\n",
      "Validated batch 31 batch loss 1.38679218\n",
      "Validated batch 32 batch loss 1.38443518\n",
      "Validated batch 33 batch loss 1.33705544\n",
      "Validated batch 34 batch loss 1.41086841\n",
      "Validated batch 35 batch loss 1.33227897\n",
      "Validated batch 36 batch loss 1.31222427\n",
      "Validated batch 37 batch loss 1.41106939\n",
      "Validated batch 38 batch loss 1.40475023\n",
      "Validated batch 39 batch loss 1.3124733\n",
      "Validated batch 40 batch loss 1.48121548\n",
      "Validated batch 41 batch loss 1.15159476\n",
      "Validated batch 42 batch loss 1.39177299\n",
      "Validated batch 43 batch loss 1.12660873\n",
      "Validated batch 44 batch loss 1.33292091\n",
      "Validated batch 45 batch loss 1.42911494\n",
      "Validated batch 46 batch loss 1.24689257\n",
      "Validated batch 47 batch loss 1.29025674\n",
      "Validated batch 48 batch loss 1.24993157\n",
      "Validated batch 49 batch loss 1.30762935\n",
      "Validated batch 50 batch loss 1.20717943\n",
      "Validated batch 51 batch loss 1.30757463\n",
      "Validated batch 52 batch loss 1.35526681\n",
      "Validated batch 53 batch loss 1.34710228\n",
      "Validated batch 54 batch loss 1.42886746\n",
      "Validated batch 55 batch loss 1.38376069\n",
      "Validated batch 56 batch loss 1.36107659\n",
      "Validated batch 57 batch loss 1.31193233\n",
      "Validated batch 58 batch loss 1.42159832\n",
      "Validated batch 59 batch loss 1.38093734\n",
      "Validated batch 60 batch loss 1.39731324\n",
      "Validated batch 61 batch loss 1.39982295\n",
      "Validated batch 62 batch loss 1.40195119\n",
      "Validated batch 63 batch loss 1.42194366\n",
      "Validated batch 64 batch loss 1.20478272\n",
      "Validated batch 65 batch loss 1.31109357\n",
      "Validated batch 66 batch loss 1.37896466\n",
      "Validated batch 67 batch loss 1.36155021\n",
      "Validated batch 68 batch loss 1.3278954\n",
      "Validated batch 69 batch loss 1.30838847\n",
      "Validated batch 70 batch loss 1.40993083\n",
      "Validated batch 71 batch loss 1.29950428\n",
      "Validated batch 72 batch loss 1.31844544\n",
      "Validated batch 73 batch loss 1.30384958\n",
      "Validated batch 74 batch loss 1.25385129\n",
      "Validated batch 75 batch loss 1.39049053\n",
      "Validated batch 76 batch loss 1.33974433\n",
      "Validated batch 77 batch loss 1.26322246\n",
      "Validated batch 78 batch loss 1.2956754\n",
      "Validated batch 79 batch loss 1.31952381\n",
      "Validated batch 80 batch loss 1.37805319\n",
      "Validated batch 81 batch loss 1.35274971\n",
      "Validated batch 82 batch loss 1.32625806\n",
      "Validated batch 83 batch loss 1.2206552\n",
      "Validated batch 84 batch loss 1.27228212\n",
      "Validated batch 85 batch loss 1.34325576\n",
      "Validated batch 86 batch loss 1.317819\n",
      "Validated batch 87 batch loss 1.36803126\n",
      "Validated batch 88 batch loss 1.4190675\n",
      "Validated batch 89 batch loss 1.59608\n",
      "Validated batch 90 batch loss 1.42052722\n",
      "Validated batch 91 batch loss 1.34273446\n",
      "Validated batch 92 batch loss 1.21947861\n",
      "Validated batch 93 batch loss 1.14229858\n",
      "Validated batch 94 batch loss 1.34478\n",
      "Validated batch 95 batch loss 1.38577592\n",
      "Validated batch 96 batch loss 1.26801908\n",
      "Validated batch 97 batch loss 1.38202047\n",
      "Validated batch 98 batch loss 1.44782209\n",
      "Validated batch 99 batch loss 1.19845617\n",
      "Validated batch 100 batch loss 1.32663763\n",
      "Validated batch 101 batch loss 1.29116189\n",
      "Validated batch 102 batch loss 1.39416146\n",
      "Validated batch 103 batch loss 1.36835\n",
      "Validated batch 104 batch loss 1.23745918\n",
      "Validated batch 105 batch loss 1.15262628\n",
      "Validated batch 106 batch loss 1.31130219\n",
      "Validated batch 107 batch loss 1.2936554\n",
      "Validated batch 108 batch loss 1.28702462\n",
      "Validated batch 109 batch loss 1.30155635\n",
      "Validated batch 110 batch loss 1.23833621\n",
      "Validated batch 111 batch loss 1.33867109\n",
      "Validated batch 112 batch loss 1.38204074\n",
      "Validated batch 113 batch loss 1.31895435\n",
      "Validated batch 114 batch loss 1.32233453\n",
      "Validated batch 115 batch loss 1.1973846\n",
      "Validated batch 116 batch loss 1.28009844\n",
      "Validated batch 117 batch loss 1.34901214\n",
      "Validated batch 118 batch loss 1.26811671\n",
      "Validated batch 119 batch loss 1.1912601\n",
      "Validated batch 120 batch loss 1.22226334\n",
      "Validated batch 121 batch loss 1.42208564\n",
      "Validated batch 122 batch loss 1.21686208\n",
      "Validated batch 123 batch loss 1.18797112\n",
      "Validated batch 124 batch loss 1.31578708\n",
      "Validated batch 125 batch loss 1.31197333\n",
      "Validated batch 126 batch loss 1.21513593\n",
      "Validated batch 127 batch loss 1.32712007\n",
      "Validated batch 128 batch loss 1.24078262\n",
      "Validated batch 129 batch loss 1.24328852\n",
      "Validated batch 130 batch loss 1.37427795\n",
      "Validated batch 131 batch loss 1.42701924\n",
      "Validated batch 132 batch loss 1.24691856\n",
      "Validated batch 133 batch loss 1.43483925\n",
      "Validated batch 134 batch loss 1.13245511\n",
      "Validated batch 135 batch loss 1.2106545\n",
      "Validated batch 136 batch loss 1.26114595\n",
      "Validated batch 137 batch loss 1.29761696\n",
      "Validated batch 138 batch loss 1.42287505\n",
      "Validated batch 139 batch loss 1.39708257\n",
      "Validated batch 140 batch loss 1.3281225\n",
      "Validated batch 141 batch loss 1.33482742\n",
      "Validated batch 142 batch loss 1.28077042\n",
      "Validated batch 143 batch loss 1.33697391\n",
      "Validated batch 144 batch loss 1.43626022\n",
      "Validated batch 145 batch loss 1.25366116\n",
      "Validated batch 146 batch loss 1.35505724\n",
      "Validated batch 147 batch loss 1.26595426\n",
      "Validated batch 148 batch loss 1.39535832\n",
      "Validated batch 149 batch loss 1.32306623\n",
      "Validated batch 150 batch loss 1.27911556\n",
      "Validated batch 151 batch loss 1.36742687\n",
      "Validated batch 152 batch loss 1.36786842\n",
      "Validated batch 153 batch loss 1.35802698\n",
      "Validated batch 154 batch loss 1.31403136\n",
      "Validated batch 155 batch loss 1.34725404\n",
      "Validated batch 156 batch loss 1.3049953\n",
      "Validated batch 157 batch loss 1.24704635\n",
      "Validated batch 158 batch loss 1.31650782\n",
      "Validated batch 159 batch loss 1.27119493\n",
      "Validated batch 160 batch loss 1.29648626\n",
      "Validated batch 161 batch loss 1.34922874\n",
      "Validated batch 162 batch loss 1.36956179\n",
      "Validated batch 163 batch loss 1.23393118\n",
      "Validated batch 164 batch loss 1.28259897\n",
      "Validated batch 165 batch loss 1.25307274\n",
      "Validated batch 166 batch loss 1.24933207\n",
      "Validated batch 167 batch loss 1.3282522\n",
      "Validated batch 168 batch loss 1.27092803\n",
      "Validated batch 169 batch loss 1.32933\n",
      "Validated batch 170 batch loss 1.39643061\n",
      "Validated batch 171 batch loss 1.20291901\n",
      "Validated batch 172 batch loss 1.42702913\n",
      "Validated batch 173 batch loss 1.36644268\n",
      "Validated batch 174 batch loss 1.20221698\n",
      "Validated batch 175 batch loss 1.3144114\n",
      "Validated batch 176 batch loss 1.34355783\n",
      "Validated batch 177 batch loss 1.30221105\n",
      "Validated batch 178 batch loss 1.41349971\n",
      "Validated batch 179 batch loss 1.33836257\n",
      "Validated batch 180 batch loss 1.37220073\n",
      "Validated batch 181 batch loss 1.26923943\n",
      "Validated batch 182 batch loss 1.37732983\n",
      "Validated batch 183 batch loss 1.32774377\n",
      "Validated batch 184 batch loss 1.26089048\n",
      "Validated batch 185 batch loss 1.39106667\n",
      "Epoch 2 val loss 1.3225915431976318\n",
      "Model /aiffel/aiffel/model_weight/GD08/model-epoch-2-loss-1.3226.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 1.31439126 epoch total loss 1.31439126\n",
      "Trained batch 2 batch loss 1.31752968 epoch total loss 1.31596041\n",
      "Trained batch 3 batch loss 1.26690781 epoch total loss 1.29960954\n",
      "Trained batch 4 batch loss 1.24949145 epoch total loss 1.28708\n",
      "Trained batch 5 batch loss 1.2440654 epoch total loss 1.27847707\n",
      "Trained batch 6 batch loss 1.17622828 epoch total loss 1.26143563\n",
      "Trained batch 7 batch loss 1.21071863 epoch total loss 1.25419044\n",
      "Trained batch 8 batch loss 1.20967185 epoch total loss 1.24862564\n",
      "Trained batch 9 batch loss 1.26647627 epoch total loss 1.25060904\n",
      "Trained batch 10 batch loss 1.22574651 epoch total loss 1.24812281\n",
      "Trained batch 11 batch loss 1.27735031 epoch total loss 1.25077987\n",
      "Trained batch 12 batch loss 1.20372474 epoch total loss 1.2468586\n",
      "Trained batch 13 batch loss 1.21567285 epoch total loss 1.24445975\n",
      "Trained batch 14 batch loss 1.27851343 epoch total loss 1.24689209\n",
      "Trained batch 15 batch loss 1.39658642 epoch total loss 1.25687182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 16 batch loss 1.43101215 epoch total loss 1.26775551\n",
      "Trained batch 17 batch loss 1.40263045 epoch total loss 1.27568924\n",
      "Trained batch 18 batch loss 1.30760467 epoch total loss 1.27746236\n",
      "Trained batch 19 batch loss 1.27594006 epoch total loss 1.27738225\n",
      "Trained batch 20 batch loss 1.35649657 epoch total loss 1.28133798\n",
      "Trained batch 21 batch loss 1.20474362 epoch total loss 1.27769065\n",
      "Trained batch 22 batch loss 1.27382433 epoch total loss 1.27751493\n",
      "Trained batch 23 batch loss 1.1203382 epoch total loss 1.27068114\n",
      "Trained batch 24 batch loss 1.21304917 epoch total loss 1.26827979\n",
      "Trained batch 25 batch loss 1.36879635 epoch total loss 1.27230036\n",
      "Trained batch 26 batch loss 1.22058463 epoch total loss 1.27031136\n",
      "Trained batch 27 batch loss 1.31093645 epoch total loss 1.27181602\n",
      "Trained batch 28 batch loss 1.36028588 epoch total loss 1.27497566\n",
      "Trained batch 29 batch loss 1.37445831 epoch total loss 1.27840602\n",
      "Trained batch 30 batch loss 1.47490513 epoch total loss 1.2849561\n",
      "Trained batch 31 batch loss 1.34914291 epoch total loss 1.28702664\n",
      "Trained batch 32 batch loss 1.43022382 epoch total loss 1.29150164\n",
      "Trained batch 33 batch loss 1.22766471 epoch total loss 1.28956723\n",
      "Trained batch 34 batch loss 1.315 epoch total loss 1.29031515\n",
      "Trained batch 35 batch loss 1.35512793 epoch total loss 1.29216707\n",
      "Trained batch 36 batch loss 1.35082746 epoch total loss 1.29379642\n",
      "Trained batch 37 batch loss 1.25973415 epoch total loss 1.29287589\n",
      "Trained batch 38 batch loss 1.1901443 epoch total loss 1.29017234\n",
      "Trained batch 39 batch loss 1.21636605 epoch total loss 1.28827989\n",
      "Trained batch 40 batch loss 1.19082916 epoch total loss 1.28584361\n",
      "Trained batch 41 batch loss 1.23081279 epoch total loss 1.28450143\n",
      "Trained batch 42 batch loss 1.23650169 epoch total loss 1.28335857\n",
      "Trained batch 43 batch loss 1.24844766 epoch total loss 1.28254676\n",
      "Trained batch 44 batch loss 1.20781291 epoch total loss 1.28084826\n",
      "Trained batch 45 batch loss 1.45664883 epoch total loss 1.28475499\n",
      "Trained batch 46 batch loss 1.44416904 epoch total loss 1.28822041\n",
      "Trained batch 47 batch loss 1.46654701 epoch total loss 1.2920146\n",
      "Trained batch 48 batch loss 1.48408484 epoch total loss 1.2960161\n",
      "Trained batch 49 batch loss 1.38132906 epoch total loss 1.29775715\n",
      "Trained batch 50 batch loss 1.37023616 epoch total loss 1.29920673\n",
      "Trained batch 51 batch loss 1.30915129 epoch total loss 1.29940164\n",
      "Trained batch 52 batch loss 1.27382469 epoch total loss 1.2989099\n",
      "Trained batch 53 batch loss 1.35826957 epoch total loss 1.30002987\n",
      "Trained batch 54 batch loss 1.28123903 epoch total loss 1.2996819\n",
      "Trained batch 55 batch loss 1.32723212 epoch total loss 1.30018282\n",
      "Trained batch 56 batch loss 1.3098073 epoch total loss 1.30035472\n",
      "Trained batch 57 batch loss 1.20851552 epoch total loss 1.29874349\n",
      "Trained batch 58 batch loss 1.25784874 epoch total loss 1.29803848\n",
      "Trained batch 59 batch loss 1.20761287 epoch total loss 1.29650581\n",
      "Trained batch 60 batch loss 1.10034347 epoch total loss 1.29323637\n",
      "Trained batch 61 batch loss 1.23457789 epoch total loss 1.29227483\n",
      "Trained batch 62 batch loss 1.44759405 epoch total loss 1.29478\n",
      "Trained batch 63 batch loss 1.41144872 epoch total loss 1.29663181\n",
      "Trained batch 64 batch loss 1.46220744 epoch total loss 1.29921889\n",
      "Trained batch 65 batch loss 1.35777664 epoch total loss 1.30011976\n",
      "Trained batch 66 batch loss 1.13570225 epoch total loss 1.29762852\n",
      "Trained batch 67 batch loss 1.28901315 epoch total loss 1.2975\n",
      "Trained batch 68 batch loss 1.42622137 epoch total loss 1.29939306\n",
      "Trained batch 69 batch loss 1.31994021 epoch total loss 1.29969084\n",
      "Trained batch 70 batch loss 1.45482659 epoch total loss 1.30190706\n",
      "Trained batch 71 batch loss 1.40063834 epoch total loss 1.30329752\n",
      "Trained batch 72 batch loss 1.43831527 epoch total loss 1.3051728\n",
      "Trained batch 73 batch loss 1.329741 epoch total loss 1.30550933\n",
      "Trained batch 74 batch loss 1.34601688 epoch total loss 1.30605674\n",
      "Trained batch 75 batch loss 1.45770264 epoch total loss 1.30807877\n",
      "Trained batch 76 batch loss 1.43376219 epoch total loss 1.30973244\n",
      "Trained batch 77 batch loss 1.3008064 epoch total loss 1.30961645\n",
      "Trained batch 78 batch loss 1.33197665 epoch total loss 1.30990314\n",
      "Trained batch 79 batch loss 1.24654675 epoch total loss 1.3091011\n",
      "Trained batch 80 batch loss 1.13915193 epoch total loss 1.3069768\n",
      "Trained batch 81 batch loss 1.27461278 epoch total loss 1.30657721\n",
      "Trained batch 82 batch loss 1.36740637 epoch total loss 1.30731905\n",
      "Trained batch 83 batch loss 1.29729402 epoch total loss 1.30719829\n",
      "Trained batch 84 batch loss 1.31828642 epoch total loss 1.30733025\n",
      "Trained batch 85 batch loss 1.4496938 epoch total loss 1.30900514\n",
      "Trained batch 86 batch loss 1.37619901 epoch total loss 1.30978644\n",
      "Trained batch 87 batch loss 1.30479884 epoch total loss 1.3097291\n",
      "Trained batch 88 batch loss 1.22423744 epoch total loss 1.30875766\n",
      "Trained batch 89 batch loss 1.23585176 epoch total loss 1.30793846\n",
      "Trained batch 90 batch loss 1.21149302 epoch total loss 1.30686688\n",
      "Trained batch 91 batch loss 1.39610076 epoch total loss 1.3078475\n",
      "Trained batch 92 batch loss 1.22417259 epoch total loss 1.30693805\n",
      "Trained batch 93 batch loss 1.30008101 epoch total loss 1.30686426\n",
      "Trained batch 94 batch loss 1.36981905 epoch total loss 1.30753398\n",
      "Trained batch 95 batch loss 1.39428282 epoch total loss 1.30844712\n",
      "Trained batch 96 batch loss 1.36374021 epoch total loss 1.30902302\n",
      "Trained batch 97 batch loss 1.39489639 epoch total loss 1.30990839\n",
      "Trained batch 98 batch loss 1.29019976 epoch total loss 1.30970728\n",
      "Trained batch 99 batch loss 1.28885329 epoch total loss 1.30949664\n",
      "Trained batch 100 batch loss 1.3666985 epoch total loss 1.31006861\n",
      "Trained batch 101 batch loss 1.37768245 epoch total loss 1.31073809\n",
      "Trained batch 102 batch loss 1.31106257 epoch total loss 1.31074131\n",
      "Trained batch 103 batch loss 1.36061502 epoch total loss 1.31122553\n",
      "Trained batch 104 batch loss 1.38217092 epoch total loss 1.31190765\n",
      "Trained batch 105 batch loss 1.42921042 epoch total loss 1.31302488\n",
      "Trained batch 106 batch loss 1.51190281 epoch total loss 1.31490111\n",
      "Trained batch 107 batch loss 1.24557602 epoch total loss 1.31425321\n",
      "Trained batch 108 batch loss 1.37172973 epoch total loss 1.31478548\n",
      "Trained batch 109 batch loss 1.32438564 epoch total loss 1.31487346\n",
      "Trained batch 110 batch loss 1.35632253 epoch total loss 1.31525028\n",
      "Trained batch 111 batch loss 1.28916955 epoch total loss 1.31501532\n",
      "Trained batch 112 batch loss 1.2522558 epoch total loss 1.31445503\n",
      "Trained batch 113 batch loss 1.2298944 epoch total loss 1.31370664\n",
      "Trained batch 114 batch loss 1.07710493 epoch total loss 1.3116312\n",
      "Trained batch 115 batch loss 1.28384483 epoch total loss 1.31138957\n",
      "Trained batch 116 batch loss 1.30794406 epoch total loss 1.31135976\n",
      "Trained batch 117 batch loss 1.45724654 epoch total loss 1.31260669\n",
      "Trained batch 118 batch loss 1.50008237 epoch total loss 1.31419539\n",
      "Trained batch 119 batch loss 1.50916696 epoch total loss 1.31583381\n",
      "Trained batch 120 batch loss 1.39681768 epoch total loss 1.31650877\n",
      "Trained batch 121 batch loss 1.36280704 epoch total loss 1.31689143\n",
      "Trained batch 122 batch loss 1.17084026 epoch total loss 1.31569421\n",
      "Trained batch 123 batch loss 1.2805047 epoch total loss 1.31540811\n",
      "Trained batch 124 batch loss 1.31396151 epoch total loss 1.31539643\n",
      "Trained batch 125 batch loss 1.31354749 epoch total loss 1.31538177\n",
      "Trained batch 126 batch loss 1.32297802 epoch total loss 1.31544209\n",
      "Trained batch 127 batch loss 1.33667099 epoch total loss 1.31560922\n",
      "Trained batch 128 batch loss 1.32954168 epoch total loss 1.31571805\n",
      "Trained batch 129 batch loss 1.27279699 epoch total loss 1.31538534\n",
      "Trained batch 130 batch loss 1.22566068 epoch total loss 1.31469512\n",
      "Trained batch 131 batch loss 1.23229063 epoch total loss 1.31406605\n",
      "Trained batch 132 batch loss 1.0252 epoch total loss 1.31187773\n",
      "Trained batch 133 batch loss 1.11532271 epoch total loss 1.31039989\n",
      "Trained batch 134 batch loss 1.38778079 epoch total loss 1.31097746\n",
      "Trained batch 135 batch loss 1.59373271 epoch total loss 1.31307197\n",
      "Trained batch 136 batch loss 1.58358943 epoch total loss 1.31506097\n",
      "Trained batch 137 batch loss 1.29730463 epoch total loss 1.31493139\n",
      "Trained batch 138 batch loss 1.34645116 epoch total loss 1.3151598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 139 batch loss 1.26282954 epoch total loss 1.31478333\n",
      "Trained batch 140 batch loss 1.40363503 epoch total loss 1.315418\n",
      "Trained batch 141 batch loss 1.35094345 epoch total loss 1.31566989\n",
      "Trained batch 142 batch loss 1.27495885 epoch total loss 1.31538332\n",
      "Trained batch 143 batch loss 1.32313132 epoch total loss 1.31543744\n",
      "Trained batch 144 batch loss 1.38200724 epoch total loss 1.31589973\n",
      "Trained batch 145 batch loss 1.32943809 epoch total loss 1.31599307\n",
      "Trained batch 146 batch loss 1.32756221 epoch total loss 1.31607234\n",
      "Trained batch 147 batch loss 1.18742859 epoch total loss 1.31519723\n",
      "Trained batch 148 batch loss 1.20190883 epoch total loss 1.31443167\n",
      "Trained batch 149 batch loss 1.31437659 epoch total loss 1.31443131\n",
      "Trained batch 150 batch loss 1.30375028 epoch total loss 1.31436014\n",
      "Trained batch 151 batch loss 1.17275643 epoch total loss 1.31342244\n",
      "Trained batch 152 batch loss 1.1770637 epoch total loss 1.31252527\n",
      "Trained batch 153 batch loss 1.29926109 epoch total loss 1.31243861\n",
      "Trained batch 154 batch loss 1.32098377 epoch total loss 1.31249404\n",
      "Trained batch 155 batch loss 1.20824671 epoch total loss 1.31182158\n",
      "Trained batch 156 batch loss 1.3082608 epoch total loss 1.31179869\n",
      "Trained batch 157 batch loss 1.25091577 epoch total loss 1.3114109\n",
      "Trained batch 158 batch loss 1.3390764 epoch total loss 1.31158602\n",
      "Trained batch 159 batch loss 1.3139565 epoch total loss 1.31160092\n",
      "Trained batch 160 batch loss 1.29065454 epoch total loss 1.31146991\n",
      "Trained batch 161 batch loss 1.38592386 epoch total loss 1.31193244\n",
      "Trained batch 162 batch loss 1.33972633 epoch total loss 1.31210399\n",
      "Trained batch 163 batch loss 1.37236547 epoch total loss 1.31247365\n",
      "Trained batch 164 batch loss 1.35784888 epoch total loss 1.31275034\n",
      "Trained batch 165 batch loss 1.35650587 epoch total loss 1.31301546\n",
      "Trained batch 166 batch loss 1.5322473 epoch total loss 1.31433606\n",
      "Trained batch 167 batch loss 1.30221093 epoch total loss 1.31426358\n",
      "Trained batch 168 batch loss 1.16277933 epoch total loss 1.31336188\n",
      "Trained batch 169 batch loss 1.386742 epoch total loss 1.31379604\n",
      "Trained batch 170 batch loss 1.20238638 epoch total loss 1.31314075\n",
      "Trained batch 171 batch loss 1.18776608 epoch total loss 1.31240761\n",
      "Trained batch 172 batch loss 1.19547 epoch total loss 1.31172764\n",
      "Trained batch 173 batch loss 1.27739716 epoch total loss 1.31152928\n",
      "Trained batch 174 batch loss 1.30432081 epoch total loss 1.31148779\n",
      "Trained batch 175 batch loss 1.27010024 epoch total loss 1.31125128\n",
      "Trained batch 176 batch loss 1.14120233 epoch total loss 1.31028509\n",
      "Trained batch 177 batch loss 1.08950257 epoch total loss 1.3090378\n",
      "Trained batch 178 batch loss 1.20465183 epoch total loss 1.30845141\n",
      "Trained batch 179 batch loss 1.31428301 epoch total loss 1.30848396\n",
      "Trained batch 180 batch loss 1.28101707 epoch total loss 1.30833137\n",
      "Trained batch 181 batch loss 1.39303529 epoch total loss 1.30879939\n",
      "Trained batch 182 batch loss 1.26706338 epoch total loss 1.30857\n",
      "Trained batch 183 batch loss 1.28232515 epoch total loss 1.30842662\n",
      "Trained batch 184 batch loss 1.29742539 epoch total loss 1.30836678\n",
      "Trained batch 185 batch loss 1.24190378 epoch total loss 1.30800748\n",
      "Trained batch 186 batch loss 1.23729205 epoch total loss 1.30762732\n",
      "Trained batch 187 batch loss 1.32659531 epoch total loss 1.30772877\n",
      "Trained batch 188 batch loss 1.19642103 epoch total loss 1.30713665\n",
      "Trained batch 189 batch loss 1.0154959 epoch total loss 1.30559361\n",
      "Trained batch 190 batch loss 1.02650118 epoch total loss 1.30412471\n",
      "Trained batch 191 batch loss 1.18979096 epoch total loss 1.30352616\n",
      "Trained batch 192 batch loss 1.46000326 epoch total loss 1.3043412\n",
      "Trained batch 193 batch loss 1.486601 epoch total loss 1.30528557\n",
      "Trained batch 194 batch loss 1.48710918 epoch total loss 1.3062228\n",
      "Trained batch 195 batch loss 1.29517651 epoch total loss 1.30616617\n",
      "Trained batch 196 batch loss 1.31655443 epoch total loss 1.3062191\n",
      "Trained batch 197 batch loss 1.42811072 epoch total loss 1.3068378\n",
      "Trained batch 198 batch loss 1.47718453 epoch total loss 1.30769813\n",
      "Trained batch 199 batch loss 1.25477266 epoch total loss 1.30743206\n",
      "Trained batch 200 batch loss 1.42972302 epoch total loss 1.30804348\n",
      "Trained batch 201 batch loss 1.38575172 epoch total loss 1.30843008\n",
      "Trained batch 202 batch loss 1.40017676 epoch total loss 1.30888426\n",
      "Trained batch 203 batch loss 1.27757633 epoch total loss 1.30873013\n",
      "Trained batch 204 batch loss 1.28996134 epoch total loss 1.30863798\n",
      "Trained batch 205 batch loss 1.4028821 epoch total loss 1.30909777\n",
      "Trained batch 206 batch loss 1.53029513 epoch total loss 1.3101716\n",
      "Trained batch 207 batch loss 1.4104079 epoch total loss 1.31065583\n",
      "Trained batch 208 batch loss 1.27604127 epoch total loss 1.31048942\n",
      "Trained batch 209 batch loss 1.295174 epoch total loss 1.3104161\n",
      "Trained batch 210 batch loss 1.28752899 epoch total loss 1.31030715\n",
      "Trained batch 211 batch loss 1.3451879 epoch total loss 1.31047237\n",
      "Trained batch 212 batch loss 1.424474 epoch total loss 1.31101012\n",
      "Trained batch 213 batch loss 1.51618934 epoch total loss 1.31197333\n",
      "Trained batch 214 batch loss 1.39271951 epoch total loss 1.31235075\n",
      "Trained batch 215 batch loss 1.27622843 epoch total loss 1.31218266\n",
      "Trained batch 216 batch loss 1.15707362 epoch total loss 1.31146455\n",
      "Trained batch 217 batch loss 1.19645131 epoch total loss 1.31093442\n",
      "Trained batch 218 batch loss 1.44935286 epoch total loss 1.31156933\n",
      "Trained batch 219 batch loss 1.20536518 epoch total loss 1.31108439\n",
      "Trained batch 220 batch loss 1.14852297 epoch total loss 1.31034541\n",
      "Trained batch 221 batch loss 1.21521449 epoch total loss 1.30991495\n",
      "Trained batch 222 batch loss 1.19230139 epoch total loss 1.30938518\n",
      "Trained batch 223 batch loss 1.30358744 epoch total loss 1.30935919\n",
      "Trained batch 224 batch loss 1.16093481 epoch total loss 1.30869663\n",
      "Trained batch 225 batch loss 1.26299536 epoch total loss 1.30849349\n",
      "Trained batch 226 batch loss 1.28394377 epoch total loss 1.3083849\n",
      "Trained batch 227 batch loss 1.26040971 epoch total loss 1.30817354\n",
      "Trained batch 228 batch loss 1.34440935 epoch total loss 1.30833244\n",
      "Trained batch 229 batch loss 1.44265795 epoch total loss 1.30891907\n",
      "Trained batch 230 batch loss 1.34589577 epoch total loss 1.30907977\n",
      "Trained batch 231 batch loss 1.17433381 epoch total loss 1.30849648\n",
      "Trained batch 232 batch loss 1.21597159 epoch total loss 1.30809772\n",
      "Trained batch 233 batch loss 1.30843258 epoch total loss 1.30809915\n",
      "Trained batch 234 batch loss 1.38804221 epoch total loss 1.3084408\n",
      "Trained batch 235 batch loss 1.43772459 epoch total loss 1.30899084\n",
      "Trained batch 236 batch loss 1.32099855 epoch total loss 1.30904174\n",
      "Trained batch 237 batch loss 1.31549668 epoch total loss 1.30906892\n",
      "Trained batch 238 batch loss 1.220783 epoch total loss 1.30869794\n",
      "Trained batch 239 batch loss 1.27876806 epoch total loss 1.30857277\n",
      "Trained batch 240 batch loss 1.35494637 epoch total loss 1.30876601\n",
      "Trained batch 241 batch loss 1.44241893 epoch total loss 1.30932057\n",
      "Trained batch 242 batch loss 1.31064248 epoch total loss 1.30932605\n",
      "Trained batch 243 batch loss 1.26956534 epoch total loss 1.30916238\n",
      "Trained batch 244 batch loss 1.34832466 epoch total loss 1.30932295\n",
      "Trained batch 245 batch loss 1.39256692 epoch total loss 1.3096627\n",
      "Trained batch 246 batch loss 1.35676742 epoch total loss 1.30985427\n",
      "Trained batch 247 batch loss 1.37607419 epoch total loss 1.31012237\n",
      "Trained batch 248 batch loss 1.40349364 epoch total loss 1.31049883\n",
      "Trained batch 249 batch loss 1.25798202 epoch total loss 1.31028807\n",
      "Trained batch 250 batch loss 1.3618238 epoch total loss 1.31049418\n",
      "Trained batch 251 batch loss 1.36227095 epoch total loss 1.31070042\n",
      "Trained batch 252 batch loss 1.29910707 epoch total loss 1.3106544\n",
      "Trained batch 253 batch loss 1.37663639 epoch total loss 1.31091523\n",
      "Trained batch 254 batch loss 1.33766842 epoch total loss 1.31102061\n",
      "Trained batch 255 batch loss 1.3412894 epoch total loss 1.31113923\n",
      "Trained batch 256 batch loss 1.36354661 epoch total loss 1.31134403\n",
      "Trained batch 257 batch loss 1.25628507 epoch total loss 1.31112981\n",
      "Trained batch 258 batch loss 1.29808497 epoch total loss 1.31107926\n",
      "Trained batch 259 batch loss 1.37737918 epoch total loss 1.31133521\n",
      "Trained batch 260 batch loss 1.18558466 epoch total loss 1.31085157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 261 batch loss 1.35347283 epoch total loss 1.31101489\n",
      "Trained batch 262 batch loss 1.30117011 epoch total loss 1.31097734\n",
      "Trained batch 263 batch loss 1.36238205 epoch total loss 1.31117284\n",
      "Trained batch 264 batch loss 1.25441778 epoch total loss 1.31095791\n",
      "Trained batch 265 batch loss 1.20176911 epoch total loss 1.31054592\n",
      "Trained batch 266 batch loss 1.3151238 epoch total loss 1.31056321\n",
      "Trained batch 267 batch loss 1.2009871 epoch total loss 1.31015277\n",
      "Trained batch 268 batch loss 1.28832388 epoch total loss 1.31007135\n",
      "Trained batch 269 batch loss 1.26642537 epoch total loss 1.30990911\n",
      "Trained batch 270 batch loss 1.26952457 epoch total loss 1.3097595\n",
      "Trained batch 271 batch loss 1.33731079 epoch total loss 1.30986118\n",
      "Trained batch 272 batch loss 1.21739316 epoch total loss 1.30952132\n",
      "Trained batch 273 batch loss 1.20240843 epoch total loss 1.309129\n",
      "Trained batch 274 batch loss 1.23816812 epoch total loss 1.30887\n",
      "Trained batch 275 batch loss 1.24466 epoch total loss 1.30863643\n",
      "Trained batch 276 batch loss 1.26372623 epoch total loss 1.30847383\n",
      "Trained batch 277 batch loss 1.22060502 epoch total loss 1.30815661\n",
      "Trained batch 278 batch loss 1.27639711 epoch total loss 1.30804241\n",
      "Trained batch 279 batch loss 1.24605858 epoch total loss 1.3078202\n",
      "Trained batch 280 batch loss 1.24710858 epoch total loss 1.30760336\n",
      "Trained batch 281 batch loss 1.26338434 epoch total loss 1.307446\n",
      "Trained batch 282 batch loss 1.36825991 epoch total loss 1.30766165\n",
      "Trained batch 283 batch loss 1.25490069 epoch total loss 1.30747533\n",
      "Trained batch 284 batch loss 1.34186053 epoch total loss 1.30759633\n",
      "Trained batch 285 batch loss 1.38353014 epoch total loss 1.30786288\n",
      "Trained batch 286 batch loss 1.32104135 epoch total loss 1.30790889\n",
      "Trained batch 287 batch loss 1.52284956 epoch total loss 1.30865788\n",
      "Trained batch 288 batch loss 1.38121 epoch total loss 1.30890977\n",
      "Trained batch 289 batch loss 1.27840853 epoch total loss 1.30880415\n",
      "Trained batch 290 batch loss 1.39819908 epoch total loss 1.30911243\n",
      "Trained batch 291 batch loss 1.42496109 epoch total loss 1.30951059\n",
      "Trained batch 292 batch loss 1.46285343 epoch total loss 1.31003571\n",
      "Trained batch 293 batch loss 1.30388665 epoch total loss 1.31001472\n",
      "Trained batch 294 batch loss 1.18250883 epoch total loss 1.30958104\n",
      "Trained batch 295 batch loss 1.23506522 epoch total loss 1.30932844\n",
      "Trained batch 296 batch loss 1.33031225 epoch total loss 1.30939937\n",
      "Trained batch 297 batch loss 1.35843158 epoch total loss 1.30956447\n",
      "Trained batch 298 batch loss 1.2948606 epoch total loss 1.30951512\n",
      "Trained batch 299 batch loss 1.27224708 epoch total loss 1.30939043\n",
      "Trained batch 300 batch loss 1.23896813 epoch total loss 1.30915582\n",
      "Trained batch 301 batch loss 1.26370454 epoch total loss 1.30900478\n",
      "Trained batch 302 batch loss 1.20432127 epoch total loss 1.30865812\n",
      "Trained batch 303 batch loss 1.22493947 epoch total loss 1.3083818\n",
      "Trained batch 304 batch loss 1.26096368 epoch total loss 1.30822587\n",
      "Trained batch 305 batch loss 1.26312852 epoch total loss 1.30807793\n",
      "Trained batch 306 batch loss 1.2627728 epoch total loss 1.30793\n",
      "Trained batch 307 batch loss 1.30095279 epoch total loss 1.30790722\n",
      "Trained batch 308 batch loss 1.25323892 epoch total loss 1.30772972\n",
      "Trained batch 309 batch loss 1.19201648 epoch total loss 1.30735528\n",
      "Trained batch 310 batch loss 1.28299594 epoch total loss 1.30727673\n",
      "Trained batch 311 batch loss 1.19611907 epoch total loss 1.30691922\n",
      "Trained batch 312 batch loss 1.25927854 epoch total loss 1.30676651\n",
      "Trained batch 313 batch loss 1.20327115 epoch total loss 1.30643582\n",
      "Trained batch 314 batch loss 1.19872797 epoch total loss 1.30609286\n",
      "Trained batch 315 batch loss 1.12428749 epoch total loss 1.30551577\n",
      "Trained batch 316 batch loss 1.12685657 epoch total loss 1.30495036\n",
      "Trained batch 317 batch loss 1.17664373 epoch total loss 1.30454564\n",
      "Trained batch 318 batch loss 1.32037985 epoch total loss 1.30459535\n",
      "Trained batch 319 batch loss 1.24502969 epoch total loss 1.30440867\n",
      "Trained batch 320 batch loss 1.31405139 epoch total loss 1.30443883\n",
      "Trained batch 321 batch loss 1.32492971 epoch total loss 1.30450261\n",
      "Trained batch 322 batch loss 1.34387612 epoch total loss 1.3046248\n",
      "Trained batch 323 batch loss 1.3867662 epoch total loss 1.30487919\n",
      "Trained batch 324 batch loss 1.38432884 epoch total loss 1.3051244\n",
      "Trained batch 325 batch loss 1.33514035 epoch total loss 1.30521679\n",
      "Trained batch 326 batch loss 1.26180494 epoch total loss 1.30508363\n",
      "Trained batch 327 batch loss 1.33368933 epoch total loss 1.30517113\n",
      "Trained batch 328 batch loss 1.31039965 epoch total loss 1.30518699\n",
      "Trained batch 329 batch loss 1.35734534 epoch total loss 1.30534554\n",
      "Trained batch 330 batch loss 1.39746499 epoch total loss 1.30562472\n",
      "Trained batch 331 batch loss 1.50752234 epoch total loss 1.3062346\n",
      "Trained batch 332 batch loss 1.48852956 epoch total loss 1.30678368\n",
      "Trained batch 333 batch loss 1.40458632 epoch total loss 1.30707729\n",
      "Trained batch 334 batch loss 1.28894627 epoch total loss 1.30702305\n",
      "Trained batch 335 batch loss 1.22355688 epoch total loss 1.3067739\n",
      "Trained batch 336 batch loss 1.26430964 epoch total loss 1.30664754\n",
      "Trained batch 337 batch loss 1.15546775 epoch total loss 1.30619884\n",
      "Trained batch 338 batch loss 1.22383738 epoch total loss 1.30595529\n",
      "Trained batch 339 batch loss 1.19519496 epoch total loss 1.30562854\n",
      "Trained batch 340 batch loss 1.23928738 epoch total loss 1.30543339\n",
      "Trained batch 341 batch loss 1.18626511 epoch total loss 1.30508399\n",
      "Trained batch 342 batch loss 1.18795407 epoch total loss 1.3047415\n",
      "Trained batch 343 batch loss 1.17624021 epoch total loss 1.30436683\n",
      "Trained batch 344 batch loss 1.32671213 epoch total loss 1.3044318\n",
      "Trained batch 345 batch loss 1.13444853 epoch total loss 1.3039391\n",
      "Trained batch 346 batch loss 1.2019906 epoch total loss 1.30364454\n",
      "Trained batch 347 batch loss 1.15141344 epoch total loss 1.30320585\n",
      "Trained batch 348 batch loss 1.22318196 epoch total loss 1.30297589\n",
      "Trained batch 349 batch loss 1.21659505 epoch total loss 1.3027283\n",
      "Trained batch 350 batch loss 1.33537281 epoch total loss 1.30282152\n",
      "Trained batch 351 batch loss 1.21143448 epoch total loss 1.30256116\n",
      "Trained batch 352 batch loss 1.39762568 epoch total loss 1.30283117\n",
      "Trained batch 353 batch loss 1.26972556 epoch total loss 1.30273736\n",
      "Trained batch 354 batch loss 1.26942515 epoch total loss 1.3026433\n",
      "Trained batch 355 batch loss 1.32076 epoch total loss 1.30269444\n",
      "Trained batch 356 batch loss 1.33358693 epoch total loss 1.30278122\n",
      "Trained batch 357 batch loss 1.32294869 epoch total loss 1.30283761\n",
      "Trained batch 358 batch loss 1.44498372 epoch total loss 1.3032347\n",
      "Trained batch 359 batch loss 1.30344081 epoch total loss 1.30323529\n",
      "Trained batch 360 batch loss 1.33694732 epoch total loss 1.30332887\n",
      "Trained batch 361 batch loss 1.22043264 epoch total loss 1.30309927\n",
      "Trained batch 362 batch loss 1.24371743 epoch total loss 1.30293524\n",
      "Trained batch 363 batch loss 1.38508451 epoch total loss 1.3031615\n",
      "Trained batch 364 batch loss 1.39335322 epoch total loss 1.30340922\n",
      "Trained batch 365 batch loss 1.41614914 epoch total loss 1.30371809\n",
      "Trained batch 366 batch loss 1.28058743 epoch total loss 1.30365479\n",
      "Trained batch 367 batch loss 1.20437717 epoch total loss 1.3033843\n",
      "Trained batch 368 batch loss 1.31874764 epoch total loss 1.30342603\n",
      "Trained batch 369 batch loss 1.3481822 epoch total loss 1.30354738\n",
      "Trained batch 370 batch loss 1.35639429 epoch total loss 1.3036902\n",
      "Trained batch 371 batch loss 1.27984309 epoch total loss 1.30362594\n",
      "Trained batch 372 batch loss 1.32883906 epoch total loss 1.30369365\n",
      "Trained batch 373 batch loss 1.4076134 epoch total loss 1.30397224\n",
      "Trained batch 374 batch loss 1.28680587 epoch total loss 1.30392635\n",
      "Trained batch 375 batch loss 1.31995285 epoch total loss 1.30396903\n",
      "Trained batch 376 batch loss 1.38380742 epoch total loss 1.30418146\n",
      "Trained batch 377 batch loss 1.3345089 epoch total loss 1.30426192\n",
      "Trained batch 378 batch loss 1.33352673 epoch total loss 1.30433929\n",
      "Trained batch 379 batch loss 1.43138635 epoch total loss 1.30467451\n",
      "Trained batch 380 batch loss 1.37980545 epoch total loss 1.30487216\n",
      "Trained batch 381 batch loss 1.34260774 epoch total loss 1.30497134\n",
      "Trained batch 382 batch loss 1.43275976 epoch total loss 1.30530584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 383 batch loss 1.31918681 epoch total loss 1.30534208\n",
      "Trained batch 384 batch loss 1.25787783 epoch total loss 1.30521846\n",
      "Trained batch 385 batch loss 1.30834913 epoch total loss 1.30522656\n",
      "Trained batch 386 batch loss 1.39305818 epoch total loss 1.30545413\n",
      "Trained batch 387 batch loss 1.31011319 epoch total loss 1.30546618\n",
      "Trained batch 388 batch loss 1.26064646 epoch total loss 1.30535066\n",
      "Trained batch 389 batch loss 1.1761713 epoch total loss 1.30501866\n",
      "Trained batch 390 batch loss 1.19294393 epoch total loss 1.30473125\n",
      "Trained batch 391 batch loss 1.27144897 epoch total loss 1.30464613\n",
      "Trained batch 392 batch loss 1.1599052 epoch total loss 1.30427694\n",
      "Trained batch 393 batch loss 1.35141408 epoch total loss 1.30439687\n",
      "Trained batch 394 batch loss 1.29824471 epoch total loss 1.30438125\n",
      "Trained batch 395 batch loss 1.27814579 epoch total loss 1.30431485\n",
      "Trained batch 396 batch loss 1.23846674 epoch total loss 1.30414855\n",
      "Trained batch 397 batch loss 1.20798314 epoch total loss 1.30390632\n",
      "Trained batch 398 batch loss 1.20917273 epoch total loss 1.30366826\n",
      "Trained batch 399 batch loss 1.26526189 epoch total loss 1.30357206\n",
      "Trained batch 400 batch loss 1.43887198 epoch total loss 1.30391026\n",
      "Trained batch 401 batch loss 1.39992142 epoch total loss 1.30414963\n",
      "Trained batch 402 batch loss 1.43925464 epoch total loss 1.30448568\n",
      "Trained batch 403 batch loss 1.26922297 epoch total loss 1.30439818\n",
      "Trained batch 404 batch loss 1.42841673 epoch total loss 1.30470514\n",
      "Trained batch 405 batch loss 1.36332345 epoch total loss 1.30485\n",
      "Trained batch 406 batch loss 1.3653518 epoch total loss 1.30499899\n",
      "Trained batch 407 batch loss 1.31594622 epoch total loss 1.30502582\n",
      "Trained batch 408 batch loss 1.27597296 epoch total loss 1.30495465\n",
      "Trained batch 409 batch loss 1.25347114 epoch total loss 1.30482876\n",
      "Trained batch 410 batch loss 1.22650671 epoch total loss 1.30463779\n",
      "Trained batch 411 batch loss 1.20206165 epoch total loss 1.30438828\n",
      "Trained batch 412 batch loss 1.30822992 epoch total loss 1.30439758\n",
      "Trained batch 413 batch loss 1.18762255 epoch total loss 1.30411482\n",
      "Trained batch 414 batch loss 1.22917092 epoch total loss 1.30393386\n",
      "Trained batch 415 batch loss 1.28683877 epoch total loss 1.30389273\n",
      "Trained batch 416 batch loss 1.34334946 epoch total loss 1.3039875\n",
      "Trained batch 417 batch loss 1.30374086 epoch total loss 1.30398679\n",
      "Trained batch 418 batch loss 1.3188138 epoch total loss 1.30402219\n",
      "Trained batch 419 batch loss 1.14570451 epoch total loss 1.30364442\n",
      "Trained batch 420 batch loss 1.32933342 epoch total loss 1.30370557\n",
      "Trained batch 421 batch loss 1.15679169 epoch total loss 1.30335665\n",
      "Trained batch 422 batch loss 1.18969202 epoch total loss 1.30308723\n",
      "Trained batch 423 batch loss 1.30039716 epoch total loss 1.30308092\n",
      "Trained batch 424 batch loss 1.19508946 epoch total loss 1.30282617\n",
      "Trained batch 425 batch loss 1.23243105 epoch total loss 1.30266058\n",
      "Trained batch 426 batch loss 1.1252892 epoch total loss 1.30224419\n",
      "Trained batch 427 batch loss 1.25870395 epoch total loss 1.30214226\n",
      "Trained batch 428 batch loss 1.21897519 epoch total loss 1.30194807\n",
      "Trained batch 429 batch loss 1.27356243 epoch total loss 1.30188191\n",
      "Trained batch 430 batch loss 1.35264409 epoch total loss 1.30199993\n",
      "Trained batch 431 batch loss 1.29819322 epoch total loss 1.30199122\n",
      "Trained batch 432 batch loss 1.29447019 epoch total loss 1.30197382\n",
      "Trained batch 433 batch loss 1.27483487 epoch total loss 1.30191123\n",
      "Trained batch 434 batch loss 1.31456637 epoch total loss 1.30194032\n",
      "Trained batch 435 batch loss 1.32760727 epoch total loss 1.30199945\n",
      "Trained batch 436 batch loss 1.28098857 epoch total loss 1.30195129\n",
      "Trained batch 437 batch loss 1.27231908 epoch total loss 1.30188346\n",
      "Trained batch 438 batch loss 1.11221123 epoch total loss 1.30145037\n",
      "Trained batch 439 batch loss 1.26997972 epoch total loss 1.30137873\n",
      "Trained batch 440 batch loss 1.32201552 epoch total loss 1.30142558\n",
      "Trained batch 441 batch loss 1.29996192 epoch total loss 1.30142236\n",
      "Trained batch 442 batch loss 1.33773804 epoch total loss 1.30150461\n",
      "Trained batch 443 batch loss 1.31414771 epoch total loss 1.3015331\n",
      "Trained batch 444 batch loss 1.16470945 epoch total loss 1.30122495\n",
      "Trained batch 445 batch loss 1.33475018 epoch total loss 1.30130041\n",
      "Trained batch 446 batch loss 1.31825554 epoch total loss 1.30133832\n",
      "Trained batch 447 batch loss 1.32129836 epoch total loss 1.30138302\n",
      "Trained batch 448 batch loss 1.24426675 epoch total loss 1.30125546\n",
      "Trained batch 449 batch loss 1.37336349 epoch total loss 1.30141604\n",
      "Trained batch 450 batch loss 1.29152703 epoch total loss 1.30139399\n",
      "Trained batch 451 batch loss 1.15663517 epoch total loss 1.30107307\n",
      "Trained batch 452 batch loss 1.20837498 epoch total loss 1.30086792\n",
      "Trained batch 453 batch loss 1.29732 epoch total loss 1.30086\n",
      "Trained batch 454 batch loss 1.26741087 epoch total loss 1.30078638\n",
      "Trained batch 455 batch loss 1.26275265 epoch total loss 1.30070281\n",
      "Trained batch 456 batch loss 1.28173673 epoch total loss 1.30066121\n",
      "Trained batch 457 batch loss 1.15901482 epoch total loss 1.30035114\n",
      "Trained batch 458 batch loss 1.21712291 epoch total loss 1.30016947\n",
      "Trained batch 459 batch loss 1.26109779 epoch total loss 1.30008435\n",
      "Trained batch 460 batch loss 1.19406438 epoch total loss 1.29985392\n",
      "Trained batch 461 batch loss 1.24067092 epoch total loss 1.29972553\n",
      "Trained batch 462 batch loss 1.317904 epoch total loss 1.29976487\n",
      "Trained batch 463 batch loss 1.40784514 epoch total loss 1.29999828\n",
      "Trained batch 464 batch loss 1.39330924 epoch total loss 1.30019939\n",
      "Trained batch 465 batch loss 1.27040911 epoch total loss 1.30013537\n",
      "Trained batch 466 batch loss 1.29319334 epoch total loss 1.30012047\n",
      "Trained batch 467 batch loss 1.27747917 epoch total loss 1.30007195\n",
      "Trained batch 468 batch loss 1.13148296 epoch total loss 1.2997117\n",
      "Trained batch 469 batch loss 1.29219043 epoch total loss 1.29969561\n",
      "Trained batch 470 batch loss 1.20266533 epoch total loss 1.29948914\n",
      "Trained batch 471 batch loss 1.37207794 epoch total loss 1.29964328\n",
      "Trained batch 472 batch loss 1.34731567 epoch total loss 1.29974413\n",
      "Trained batch 473 batch loss 1.23556042 epoch total loss 1.29960847\n",
      "Trained batch 474 batch loss 1.26348495 epoch total loss 1.29953218\n",
      "Trained batch 475 batch loss 1.30547655 epoch total loss 1.29954469\n",
      "Trained batch 476 batch loss 1.17806244 epoch total loss 1.29928946\n",
      "Trained batch 477 batch loss 1.16376519 epoch total loss 1.29900539\n",
      "Trained batch 478 batch loss 1.30541301 epoch total loss 1.29901874\n",
      "Trained batch 479 batch loss 1.28986418 epoch total loss 1.29899967\n",
      "Trained batch 480 batch loss 1.03249931 epoch total loss 1.29844439\n",
      "Trained batch 481 batch loss 1.0930233 epoch total loss 1.29801726\n",
      "Trained batch 482 batch loss 1.10930204 epoch total loss 1.29762578\n",
      "Trained batch 483 batch loss 1.34564757 epoch total loss 1.2977252\n",
      "Trained batch 484 batch loss 1.26023936 epoch total loss 1.29764783\n",
      "Trained batch 485 batch loss 1.48910964 epoch total loss 1.29804265\n",
      "Trained batch 486 batch loss 1.34410429 epoch total loss 1.29813743\n",
      "Trained batch 487 batch loss 1.23138499 epoch total loss 1.29800034\n",
      "Trained batch 488 batch loss 1.18566847 epoch total loss 1.29777014\n",
      "Trained batch 489 batch loss 1.47909606 epoch total loss 1.298141\n",
      "Trained batch 490 batch loss 1.31031656 epoch total loss 1.2981658\n",
      "Trained batch 491 batch loss 1.36280358 epoch total loss 1.29829741\n",
      "Trained batch 492 batch loss 1.30840659 epoch total loss 1.29831803\n",
      "Trained batch 493 batch loss 1.3913691 epoch total loss 1.29850674\n",
      "Trained batch 494 batch loss 1.43793344 epoch total loss 1.29878891\n",
      "Trained batch 495 batch loss 1.31324685 epoch total loss 1.29881811\n",
      "Trained batch 496 batch loss 1.39448762 epoch total loss 1.29901099\n",
      "Trained batch 497 batch loss 1.18331337 epoch total loss 1.29877818\n",
      "Trained batch 498 batch loss 1.29025674 epoch total loss 1.29876113\n",
      "Trained batch 499 batch loss 1.27892637 epoch total loss 1.29872131\n",
      "Trained batch 500 batch loss 1.23591495 epoch total loss 1.29859567\n",
      "Trained batch 501 batch loss 1.41194963 epoch total loss 1.29882193\n",
      "Trained batch 502 batch loss 1.44896805 epoch total loss 1.29912102\n",
      "Trained batch 503 batch loss 1.43750143 epoch total loss 1.29939616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 504 batch loss 1.36700475 epoch total loss 1.29953027\n",
      "Trained batch 505 batch loss 1.23031759 epoch total loss 1.2993933\n",
      "Trained batch 506 batch loss 1.31129575 epoch total loss 1.29941678\n",
      "Trained batch 507 batch loss 1.22353148 epoch total loss 1.29926705\n",
      "Trained batch 508 batch loss 1.36290145 epoch total loss 1.29939234\n",
      "Trained batch 509 batch loss 1.31058192 epoch total loss 1.2994144\n",
      "Trained batch 510 batch loss 1.24227762 epoch total loss 1.29930234\n",
      "Trained batch 511 batch loss 1.15586877 epoch total loss 1.2990216\n",
      "Trained batch 512 batch loss 1.17713177 epoch total loss 1.29878354\n",
      "Trained batch 513 batch loss 1.13383234 epoch total loss 1.29846203\n",
      "Trained batch 514 batch loss 1.2735796 epoch total loss 1.29841363\n",
      "Trained batch 515 batch loss 1.41984248 epoch total loss 1.29864943\n",
      "Trained batch 516 batch loss 1.71918786 epoch total loss 1.29946434\n",
      "Trained batch 517 batch loss 1.56697559 epoch total loss 1.29998171\n",
      "Trained batch 518 batch loss 1.34498942 epoch total loss 1.30006862\n",
      "Trained batch 519 batch loss 1.45788813 epoch total loss 1.30037272\n",
      "Trained batch 520 batch loss 1.43886673 epoch total loss 1.30063903\n",
      "Trained batch 521 batch loss 1.42623949 epoch total loss 1.30088007\n",
      "Trained batch 522 batch loss 1.30142105 epoch total loss 1.30088115\n",
      "Trained batch 523 batch loss 1.19646335 epoch total loss 1.30068147\n",
      "Trained batch 524 batch loss 1.33112824 epoch total loss 1.30073953\n",
      "Trained batch 525 batch loss 1.30891705 epoch total loss 1.30075514\n",
      "Trained batch 526 batch loss 1.28356719 epoch total loss 1.30072236\n",
      "Trained batch 527 batch loss 1.15310276 epoch total loss 1.30044222\n",
      "Trained batch 528 batch loss 1.2704643 epoch total loss 1.30038548\n",
      "Trained batch 529 batch loss 1.2802999 epoch total loss 1.30034745\n",
      "Trained batch 530 batch loss 1.26927197 epoch total loss 1.3002888\n",
      "Trained batch 531 batch loss 1.28828359 epoch total loss 1.30026615\n",
      "Trained batch 532 batch loss 1.22543836 epoch total loss 1.3001256\n",
      "Trained batch 533 batch loss 1.2292968 epoch total loss 1.29999268\n",
      "Trained batch 534 batch loss 1.25596666 epoch total loss 1.29991031\n",
      "Trained batch 535 batch loss 1.3281275 epoch total loss 1.299963\n",
      "Trained batch 536 batch loss 1.35084617 epoch total loss 1.30005789\n",
      "Trained batch 537 batch loss 1.33063006 epoch total loss 1.30011487\n",
      "Trained batch 538 batch loss 1.25224793 epoch total loss 1.30002594\n",
      "Trained batch 539 batch loss 1.15237689 epoch total loss 1.299752\n",
      "Trained batch 540 batch loss 1.13292146 epoch total loss 1.29944313\n",
      "Trained batch 541 batch loss 1.22130311 epoch total loss 1.29929864\n",
      "Trained batch 542 batch loss 1.25737 epoch total loss 1.2992214\n",
      "Trained batch 543 batch loss 1.16591239 epoch total loss 1.29897583\n",
      "Trained batch 544 batch loss 1.37268329 epoch total loss 1.29911137\n",
      "Trained batch 545 batch loss 1.29148328 epoch total loss 1.2990973\n",
      "Trained batch 546 batch loss 1.34778523 epoch total loss 1.29918647\n",
      "Trained batch 547 batch loss 1.21277893 epoch total loss 1.29902852\n",
      "Trained batch 548 batch loss 1.26786828 epoch total loss 1.29897165\n",
      "Trained batch 549 batch loss 1.33471417 epoch total loss 1.29903674\n",
      "Trained batch 550 batch loss 1.39301598 epoch total loss 1.29920769\n",
      "Trained batch 551 batch loss 1.26578033 epoch total loss 1.29914701\n",
      "Trained batch 552 batch loss 1.10987711 epoch total loss 1.29880416\n",
      "Trained batch 553 batch loss 1.02029037 epoch total loss 1.29830039\n",
      "Trained batch 554 batch loss 1.25704813 epoch total loss 1.29822588\n",
      "Trained batch 555 batch loss 1.19402719 epoch total loss 1.29803824\n",
      "Trained batch 556 batch loss 1.19152057 epoch total loss 1.29784667\n",
      "Trained batch 557 batch loss 1.27042699 epoch total loss 1.29779744\n",
      "Trained batch 558 batch loss 1.26172757 epoch total loss 1.29773283\n",
      "Trained batch 559 batch loss 1.35065842 epoch total loss 1.29782748\n",
      "Trained batch 560 batch loss 1.37457418 epoch total loss 1.29796445\n",
      "Trained batch 561 batch loss 1.27924562 epoch total loss 1.29793108\n",
      "Trained batch 562 batch loss 1.46489191 epoch total loss 1.29822814\n",
      "Trained batch 563 batch loss 1.53332782 epoch total loss 1.29864573\n",
      "Trained batch 564 batch loss 1.39324164 epoch total loss 1.29881346\n",
      "Trained batch 565 batch loss 1.37054896 epoch total loss 1.29894042\n",
      "Trained batch 566 batch loss 1.22373712 epoch total loss 1.29880762\n",
      "Trained batch 567 batch loss 1.23315644 epoch total loss 1.29869187\n",
      "Trained batch 568 batch loss 1.26210976 epoch total loss 1.29862738\n",
      "Trained batch 569 batch loss 1.20355678 epoch total loss 1.29846036\n",
      "Trained batch 570 batch loss 1.26215839 epoch total loss 1.29839659\n",
      "Trained batch 571 batch loss 1.23912454 epoch total loss 1.29829276\n",
      "Trained batch 572 batch loss 1.1960144 epoch total loss 1.29811406\n",
      "Trained batch 573 batch loss 1.11650205 epoch total loss 1.29779708\n",
      "Trained batch 574 batch loss 0.969801545 epoch total loss 1.29722571\n",
      "Trained batch 575 batch loss 1.20185375 epoch total loss 1.29705977\n",
      "Trained batch 576 batch loss 1.24479151 epoch total loss 1.29696906\n",
      "Trained batch 577 batch loss 1.38653111 epoch total loss 1.29712427\n",
      "Trained batch 578 batch loss 1.27826822 epoch total loss 1.29709172\n",
      "Trained batch 579 batch loss 1.27204061 epoch total loss 1.29704845\n",
      "Trained batch 580 batch loss 1.21924543 epoch total loss 1.29691422\n",
      "Trained batch 581 batch loss 1.1868906 epoch total loss 1.29672492\n",
      "Trained batch 582 batch loss 1.12426257 epoch total loss 1.29642856\n",
      "Trained batch 583 batch loss 1.27065682 epoch total loss 1.29638433\n",
      "Trained batch 584 batch loss 1.21481681 epoch total loss 1.29624474\n",
      "Trained batch 585 batch loss 1.35111904 epoch total loss 1.29633856\n",
      "Trained batch 586 batch loss 1.42261028 epoch total loss 1.29655397\n",
      "Trained batch 587 batch loss 1.30978286 epoch total loss 1.2965765\n",
      "Trained batch 588 batch loss 1.2454145 epoch total loss 1.29648948\n",
      "Trained batch 589 batch loss 1.22837007 epoch total loss 1.29637384\n",
      "Trained batch 590 batch loss 1.31429613 epoch total loss 1.29640424\n",
      "Trained batch 591 batch loss 1.27625656 epoch total loss 1.29637015\n",
      "Trained batch 592 batch loss 1.37083936 epoch total loss 1.29649591\n",
      "Trained batch 593 batch loss 1.30506563 epoch total loss 1.29651034\n",
      "Trained batch 594 batch loss 1.43422711 epoch total loss 1.29674208\n",
      "Trained batch 595 batch loss 1.36530972 epoch total loss 1.29685736\n",
      "Trained batch 596 batch loss 1.3989439 epoch total loss 1.29702866\n",
      "Trained batch 597 batch loss 1.30851865 epoch total loss 1.29704785\n",
      "Trained batch 598 batch loss 1.38652599 epoch total loss 1.29719758\n",
      "Trained batch 599 batch loss 1.26424706 epoch total loss 1.29714251\n",
      "Trained batch 600 batch loss 1.28551173 epoch total loss 1.29712307\n",
      "Trained batch 601 batch loss 1.28096366 epoch total loss 1.29709613\n",
      "Trained batch 602 batch loss 1.33404517 epoch total loss 1.29715753\n",
      "Trained batch 603 batch loss 1.26309896 epoch total loss 1.29710114\n",
      "Trained batch 604 batch loss 1.33279479 epoch total loss 1.29716027\n",
      "Trained batch 605 batch loss 1.37055707 epoch total loss 1.29728162\n",
      "Trained batch 606 batch loss 1.20981228 epoch total loss 1.29713726\n",
      "Trained batch 607 batch loss 1.32306659 epoch total loss 1.29717994\n",
      "Trained batch 608 batch loss 1.30995882 epoch total loss 1.29720092\n",
      "Trained batch 609 batch loss 1.33886015 epoch total loss 1.29726934\n",
      "Trained batch 610 batch loss 1.3261224 epoch total loss 1.29731667\n",
      "Trained batch 611 batch loss 1.37464809 epoch total loss 1.29744315\n",
      "Trained batch 612 batch loss 1.30176508 epoch total loss 1.2974503\n",
      "Trained batch 613 batch loss 1.2575171 epoch total loss 1.2973851\n",
      "Trained batch 614 batch loss 1.26717794 epoch total loss 1.29733586\n",
      "Trained batch 615 batch loss 1.30923367 epoch total loss 1.29735518\n",
      "Trained batch 616 batch loss 1.20468783 epoch total loss 1.29720473\n",
      "Trained batch 617 batch loss 1.19320059 epoch total loss 1.29703617\n",
      "Trained batch 618 batch loss 1.19126534 epoch total loss 1.29686499\n",
      "Trained batch 619 batch loss 1.25559092 epoch total loss 1.29679835\n",
      "Trained batch 620 batch loss 1.39908946 epoch total loss 1.29696345\n",
      "Trained batch 621 batch loss 1.50425506 epoch total loss 1.29729724\n",
      "Trained batch 622 batch loss 1.53333914 epoch total loss 1.29767668\n",
      "Trained batch 623 batch loss 1.43549609 epoch total loss 1.29789793\n",
      "Trained batch 624 batch loss 1.37835503 epoch total loss 1.2980268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 625 batch loss 1.22360754 epoch total loss 1.29790783\n",
      "Trained batch 626 batch loss 1.19096172 epoch total loss 1.297737\n",
      "Trained batch 627 batch loss 1.01479149 epoch total loss 1.29728568\n",
      "Trained batch 628 batch loss 1.05422056 epoch total loss 1.2968986\n",
      "Trained batch 629 batch loss 1.08806801 epoch total loss 1.29656661\n",
      "Trained batch 630 batch loss 1.26085854 epoch total loss 1.29651\n",
      "Trained batch 631 batch loss 1.18135965 epoch total loss 1.29632747\n",
      "Trained batch 632 batch loss 1.19662416 epoch total loss 1.29616964\n",
      "Trained batch 633 batch loss 1.2007612 epoch total loss 1.29601884\n",
      "Trained batch 634 batch loss 1.19803381 epoch total loss 1.29586434\n",
      "Trained batch 635 batch loss 1.08563411 epoch total loss 1.2955333\n",
      "Trained batch 636 batch loss 1.1975522 epoch total loss 1.29537928\n",
      "Trained batch 637 batch loss 1.23092067 epoch total loss 1.29527807\n",
      "Trained batch 638 batch loss 1.11383009 epoch total loss 1.29499364\n",
      "Trained batch 639 batch loss 1.189991 epoch total loss 1.29482937\n",
      "Trained batch 640 batch loss 1.29574525 epoch total loss 1.29483068\n",
      "Trained batch 641 batch loss 1.1345917 epoch total loss 1.2945807\n",
      "Trained batch 642 batch loss 1.33599591 epoch total loss 1.29464519\n",
      "Trained batch 643 batch loss 1.50691116 epoch total loss 1.29497528\n",
      "Trained batch 644 batch loss 1.23629284 epoch total loss 1.2948842\n",
      "Trained batch 645 batch loss 1.39282 epoch total loss 1.29503596\n",
      "Trained batch 646 batch loss 1.33522916 epoch total loss 1.29509819\n",
      "Trained batch 647 batch loss 1.38360631 epoch total loss 1.29523492\n",
      "Trained batch 648 batch loss 1.3759768 epoch total loss 1.29535961\n",
      "Trained batch 649 batch loss 1.33677137 epoch total loss 1.29542339\n",
      "Trained batch 650 batch loss 1.19573879 epoch total loss 1.29527009\n",
      "Trained batch 651 batch loss 1.20068622 epoch total loss 1.29512477\n",
      "Trained batch 652 batch loss 1.25263536 epoch total loss 1.29505956\n",
      "Trained batch 653 batch loss 1.37754083 epoch total loss 1.29518592\n",
      "Trained batch 654 batch loss 1.44646454 epoch total loss 1.29541719\n",
      "Trained batch 655 batch loss 1.28163111 epoch total loss 1.29539621\n",
      "Trained batch 656 batch loss 1.14394712 epoch total loss 1.2951653\n",
      "Trained batch 657 batch loss 1.12478638 epoch total loss 1.2949059\n",
      "Trained batch 658 batch loss 1.28884602 epoch total loss 1.2948966\n",
      "Trained batch 659 batch loss 1.3647784 epoch total loss 1.2950027\n",
      "Trained batch 660 batch loss 1.34509754 epoch total loss 1.29507864\n",
      "Trained batch 661 batch loss 1.30419278 epoch total loss 1.29509246\n",
      "Trained batch 662 batch loss 1.24115145 epoch total loss 1.29501092\n",
      "Trained batch 663 batch loss 1.30439198 epoch total loss 1.29502511\n",
      "Trained batch 664 batch loss 1.21131361 epoch total loss 1.29489899\n",
      "Trained batch 665 batch loss 1.25187469 epoch total loss 1.29483426\n",
      "Trained batch 666 batch loss 1.24542689 epoch total loss 1.29476011\n",
      "Trained batch 667 batch loss 1.44563615 epoch total loss 1.29498625\n",
      "Trained batch 668 batch loss 1.45329487 epoch total loss 1.29522336\n",
      "Trained batch 669 batch loss 1.11014247 epoch total loss 1.29494667\n",
      "Trained batch 670 batch loss 1.29067373 epoch total loss 1.29494023\n",
      "Trained batch 671 batch loss 1.23236656 epoch total loss 1.29484701\n",
      "Trained batch 672 batch loss 1.28581357 epoch total loss 1.29483354\n",
      "Trained batch 673 batch loss 1.06350708 epoch total loss 1.29448986\n",
      "Trained batch 674 batch loss 1.13812256 epoch total loss 1.29425788\n",
      "Trained batch 675 batch loss 1.22107935 epoch total loss 1.2941494\n",
      "Trained batch 676 batch loss 1.20879078 epoch total loss 1.29402316\n",
      "Trained batch 677 batch loss 1.27538705 epoch total loss 1.29399562\n",
      "Trained batch 678 batch loss 1.24747396 epoch total loss 1.29392707\n",
      "Trained batch 679 batch loss 1.18104339 epoch total loss 1.29376078\n",
      "Trained batch 680 batch loss 1.19411314 epoch total loss 1.29361415\n",
      "Trained batch 681 batch loss 1.28553343 epoch total loss 1.29360235\n",
      "Trained batch 682 batch loss 1.23211873 epoch total loss 1.29351211\n",
      "Trained batch 683 batch loss 1.32086635 epoch total loss 1.29355216\n",
      "Trained batch 684 batch loss 1.08051145 epoch total loss 1.29324079\n",
      "Trained batch 685 batch loss 1.16384614 epoch total loss 1.29305184\n",
      "Trained batch 686 batch loss 1.2783798 epoch total loss 1.29303038\n",
      "Trained batch 687 batch loss 1.25326931 epoch total loss 1.29297256\n",
      "Trained batch 688 batch loss 1.33328927 epoch total loss 1.29303122\n",
      "Trained batch 689 batch loss 1.49413133 epoch total loss 1.29332304\n",
      "Trained batch 690 batch loss 1.51210308 epoch total loss 1.29364014\n",
      "Trained batch 691 batch loss 1.47437358 epoch total loss 1.29390168\n",
      "Trained batch 692 batch loss 1.16107249 epoch total loss 1.29370975\n",
      "Trained batch 693 batch loss 1.36544633 epoch total loss 1.29381323\n",
      "Trained batch 694 batch loss 1.13971615 epoch total loss 1.29359114\n",
      "Trained batch 695 batch loss 1.28685975 epoch total loss 1.29358149\n",
      "Trained batch 696 batch loss 1.2673924 epoch total loss 1.29354382\n",
      "Trained batch 697 batch loss 1.25990808 epoch total loss 1.29349554\n",
      "Trained batch 698 batch loss 1.3281523 epoch total loss 1.29354513\n",
      "Trained batch 699 batch loss 1.26231122 epoch total loss 1.29350054\n",
      "Trained batch 700 batch loss 1.26090562 epoch total loss 1.29345393\n",
      "Trained batch 701 batch loss 1.23909521 epoch total loss 1.29337645\n",
      "Trained batch 702 batch loss 1.28902364 epoch total loss 1.29337013\n",
      "Trained batch 703 batch loss 1.23441124 epoch total loss 1.29328632\n",
      "Trained batch 704 batch loss 1.29786491 epoch total loss 1.29329288\n",
      "Trained batch 705 batch loss 1.38521457 epoch total loss 1.29342318\n",
      "Trained batch 706 batch loss 1.28223562 epoch total loss 1.29340732\n",
      "Trained batch 707 batch loss 1.31475842 epoch total loss 1.29343748\n",
      "Trained batch 708 batch loss 1.23883462 epoch total loss 1.29336035\n",
      "Trained batch 709 batch loss 1.34495258 epoch total loss 1.29343319\n",
      "Trained batch 710 batch loss 1.36077583 epoch total loss 1.29352808\n",
      "Trained batch 711 batch loss 1.27193248 epoch total loss 1.29349768\n",
      "Trained batch 712 batch loss 1.26772285 epoch total loss 1.29346144\n",
      "Trained batch 713 batch loss 1.3623538 epoch total loss 1.293558\n",
      "Trained batch 714 batch loss 1.1752634 epoch total loss 1.29339242\n",
      "Trained batch 715 batch loss 1.30516887 epoch total loss 1.29340887\n",
      "Trained batch 716 batch loss 1.38203669 epoch total loss 1.29353261\n",
      "Trained batch 717 batch loss 1.41159463 epoch total loss 1.29369736\n",
      "Trained batch 718 batch loss 1.29829168 epoch total loss 1.29370368\n",
      "Trained batch 719 batch loss 1.33565927 epoch total loss 1.29376209\n",
      "Trained batch 720 batch loss 1.41651487 epoch total loss 1.29393256\n",
      "Trained batch 721 batch loss 1.2875185 epoch total loss 1.29392362\n",
      "Trained batch 722 batch loss 1.34950531 epoch total loss 1.29400063\n",
      "Trained batch 723 batch loss 1.39389277 epoch total loss 1.29413879\n",
      "Trained batch 724 batch loss 1.29887247 epoch total loss 1.29414535\n",
      "Trained batch 725 batch loss 1.21162796 epoch total loss 1.2940315\n",
      "Trained batch 726 batch loss 1.28652525 epoch total loss 1.29402113\n",
      "Trained batch 727 batch loss 1.22147977 epoch total loss 1.29392135\n",
      "Trained batch 728 batch loss 1.29125893 epoch total loss 1.29391778\n",
      "Trained batch 729 batch loss 1.25604653 epoch total loss 1.2938658\n",
      "Trained batch 730 batch loss 1.23426819 epoch total loss 1.29378414\n",
      "Trained batch 731 batch loss 1.12661445 epoch total loss 1.29355538\n",
      "Trained batch 732 batch loss 1.14836454 epoch total loss 1.29335701\n",
      "Trained batch 733 batch loss 1.23961794 epoch total loss 1.2932837\n",
      "Trained batch 734 batch loss 1.27244091 epoch total loss 1.29325533\n",
      "Trained batch 735 batch loss 1.31951952 epoch total loss 1.29329109\n",
      "Trained batch 736 batch loss 1.32037008 epoch total loss 1.29332793\n",
      "Trained batch 737 batch loss 1.23582411 epoch total loss 1.29325\n",
      "Trained batch 738 batch loss 1.41675234 epoch total loss 1.29341722\n",
      "Trained batch 739 batch loss 1.29029763 epoch total loss 1.29341304\n",
      "Trained batch 740 batch loss 1.36675811 epoch total loss 1.29351211\n",
      "Trained batch 741 batch loss 1.36822772 epoch total loss 1.29361296\n",
      "Trained batch 742 batch loss 1.17535734 epoch total loss 1.29345357\n",
      "Trained batch 743 batch loss 1.21670032 epoch total loss 1.29335022\n",
      "Trained batch 744 batch loss 1.05527472 epoch total loss 1.29303026\n",
      "Trained batch 745 batch loss 1.31504369 epoch total loss 1.29305983\n",
      "Trained batch 746 batch loss 1.34040332 epoch total loss 1.29312325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 747 batch loss 1.37051785 epoch total loss 1.29322696\n",
      "Trained batch 748 batch loss 1.38558781 epoch total loss 1.29335034\n",
      "Trained batch 749 batch loss 1.51966941 epoch total loss 1.29365253\n",
      "Trained batch 750 batch loss 1.3246479 epoch total loss 1.2936939\n",
      "Trained batch 751 batch loss 1.34975 epoch total loss 1.29376841\n",
      "Trained batch 752 batch loss 1.38099241 epoch total loss 1.2938844\n",
      "Trained batch 753 batch loss 1.34294462 epoch total loss 1.2939496\n",
      "Trained batch 754 batch loss 1.3175565 epoch total loss 1.29398096\n",
      "Trained batch 755 batch loss 1.31307769 epoch total loss 1.29400623\n",
      "Trained batch 756 batch loss 1.44532752 epoch total loss 1.29420638\n",
      "Trained batch 757 batch loss 1.33214808 epoch total loss 1.29425645\n",
      "Trained batch 758 batch loss 1.32896924 epoch total loss 1.29430223\n",
      "Trained batch 759 batch loss 1.36873388 epoch total loss 1.29440033\n",
      "Trained batch 760 batch loss 1.25730169 epoch total loss 1.29435146\n",
      "Trained batch 761 batch loss 1.19473886 epoch total loss 1.29422069\n",
      "Trained batch 762 batch loss 1.15464067 epoch total loss 1.29403746\n",
      "Trained batch 763 batch loss 1.15068519 epoch total loss 1.29384959\n",
      "Trained batch 764 batch loss 1.07086492 epoch total loss 1.29355776\n",
      "Trained batch 765 batch loss 1.38700223 epoch total loss 1.29368\n",
      "Trained batch 766 batch loss 1.45062804 epoch total loss 1.29388487\n",
      "Trained batch 767 batch loss 1.36064398 epoch total loss 1.2939719\n",
      "Trained batch 768 batch loss 1.18112361 epoch total loss 1.29382503\n",
      "Trained batch 769 batch loss 1.24876976 epoch total loss 1.29376638\n",
      "Trained batch 770 batch loss 1.27018666 epoch total loss 1.29373586\n",
      "Trained batch 771 batch loss 1.24956942 epoch total loss 1.29367852\n",
      "Trained batch 772 batch loss 1.20898521 epoch total loss 1.29356885\n",
      "Trained batch 773 batch loss 1.27209735 epoch total loss 1.29354107\n",
      "Trained batch 774 batch loss 1.21323037 epoch total loss 1.29343736\n",
      "Trained batch 775 batch loss 1.27794111 epoch total loss 1.29341733\n",
      "Trained batch 776 batch loss 1.34800196 epoch total loss 1.29348767\n",
      "Trained batch 777 batch loss 1.26248276 epoch total loss 1.29344785\n",
      "Trained batch 778 batch loss 1.22994661 epoch total loss 1.29336619\n",
      "Trained batch 779 batch loss 1.20290351 epoch total loss 1.29325008\n",
      "Trained batch 780 batch loss 1.28221023 epoch total loss 1.2932359\n",
      "Trained batch 781 batch loss 1.26454842 epoch total loss 1.29319918\n",
      "Trained batch 782 batch loss 1.23170817 epoch total loss 1.2931205\n",
      "Trained batch 783 batch loss 1.20314455 epoch total loss 1.29300559\n",
      "Trained batch 784 batch loss 1.39593613 epoch total loss 1.29313684\n",
      "Trained batch 785 batch loss 1.33353448 epoch total loss 1.29318833\n",
      "Trained batch 786 batch loss 1.33918011 epoch total loss 1.29324687\n",
      "Trained batch 787 batch loss 1.18831837 epoch total loss 1.29311347\n",
      "Trained batch 788 batch loss 1.12919235 epoch total loss 1.29290545\n",
      "Trained batch 789 batch loss 1.14324379 epoch total loss 1.29271579\n",
      "Trained batch 790 batch loss 1.18222356 epoch total loss 1.29257596\n",
      "Trained batch 791 batch loss 1.30965328 epoch total loss 1.29259753\n",
      "Trained batch 792 batch loss 1.3142854 epoch total loss 1.29262483\n",
      "Trained batch 793 batch loss 1.17906606 epoch total loss 1.29248166\n",
      "Trained batch 794 batch loss 1.22073126 epoch total loss 1.2923913\n",
      "Trained batch 795 batch loss 1.26418805 epoch total loss 1.29235578\n",
      "Trained batch 796 batch loss 1.42229235 epoch total loss 1.29251897\n",
      "Trained batch 797 batch loss 1.29590297 epoch total loss 1.29252315\n",
      "Trained batch 798 batch loss 1.31518793 epoch total loss 1.29255164\n",
      "Trained batch 799 batch loss 1.15503097 epoch total loss 1.2923795\n",
      "Trained batch 800 batch loss 1.06612 epoch total loss 1.29209673\n",
      "Trained batch 801 batch loss 1.01435 epoch total loss 1.29175007\n",
      "Trained batch 802 batch loss 1.036062 epoch total loss 1.29143119\n",
      "Trained batch 803 batch loss 1.31981599 epoch total loss 1.29146647\n",
      "Trained batch 804 batch loss 1.27137744 epoch total loss 1.29144156\n",
      "Trained batch 805 batch loss 1.28511178 epoch total loss 1.29143369\n",
      "Trained batch 806 batch loss 1.34074593 epoch total loss 1.29149485\n",
      "Trained batch 807 batch loss 1.260988 epoch total loss 1.29145706\n",
      "Trained batch 808 batch loss 1.3703388 epoch total loss 1.29155469\n",
      "Trained batch 809 batch loss 1.2497139 epoch total loss 1.29150295\n",
      "Trained batch 810 batch loss 1.2818352 epoch total loss 1.29149115\n",
      "Trained batch 811 batch loss 1.18782163 epoch total loss 1.29136336\n",
      "Trained batch 812 batch loss 1.21243477 epoch total loss 1.29126608\n",
      "Trained batch 813 batch loss 1.38403368 epoch total loss 1.29138017\n",
      "Trained batch 814 batch loss 1.38194907 epoch total loss 1.29149151\n",
      "Trained batch 815 batch loss 1.27753329 epoch total loss 1.29147434\n",
      "Trained batch 816 batch loss 1.27472138 epoch total loss 1.29145396\n",
      "Trained batch 817 batch loss 1.36745262 epoch total loss 1.29154694\n",
      "Trained batch 818 batch loss 1.31251895 epoch total loss 1.29157257\n",
      "Trained batch 819 batch loss 1.27335942 epoch total loss 1.29155028\n",
      "Trained batch 820 batch loss 1.14407611 epoch total loss 1.29137039\n",
      "Trained batch 821 batch loss 1.24848151 epoch total loss 1.29131818\n",
      "Trained batch 822 batch loss 1.31636047 epoch total loss 1.2913487\n",
      "Trained batch 823 batch loss 1.35492945 epoch total loss 1.29142606\n",
      "Trained batch 824 batch loss 1.25726581 epoch total loss 1.2913847\n",
      "Trained batch 825 batch loss 1.25072563 epoch total loss 1.29133534\n",
      "Trained batch 826 batch loss 1.2949636 epoch total loss 1.29133976\n",
      "Trained batch 827 batch loss 1.43011069 epoch total loss 1.29150748\n",
      "Trained batch 828 batch loss 1.61642683 epoch total loss 1.29189992\n",
      "Trained batch 829 batch loss 1.54221475 epoch total loss 1.29220188\n",
      "Trained batch 830 batch loss 1.48660445 epoch total loss 1.292436\n",
      "Trained batch 831 batch loss 1.31517041 epoch total loss 1.29246342\n",
      "Trained batch 832 batch loss 1.35169518 epoch total loss 1.29253459\n",
      "Trained batch 833 batch loss 1.18584156 epoch total loss 1.29240644\n",
      "Trained batch 834 batch loss 1.19391429 epoch total loss 1.29228842\n",
      "Trained batch 835 batch loss 1.31220233 epoch total loss 1.29231238\n",
      "Trained batch 836 batch loss 1.38943541 epoch total loss 1.29242849\n",
      "Trained batch 837 batch loss 1.3094629 epoch total loss 1.29244888\n",
      "Trained batch 838 batch loss 1.28850055 epoch total loss 1.29244411\n",
      "Trained batch 839 batch loss 1.15679562 epoch total loss 1.29228234\n",
      "Trained batch 840 batch loss 1.22310829 epoch total loss 1.2922\n",
      "Trained batch 841 batch loss 1.17369342 epoch total loss 1.29205906\n",
      "Trained batch 842 batch loss 1.22552621 epoch total loss 1.29198015\n",
      "Trained batch 843 batch loss 1.26276565 epoch total loss 1.29194558\n",
      "Trained batch 844 batch loss 1.15869713 epoch total loss 1.29178762\n",
      "Trained batch 845 batch loss 1.21675873 epoch total loss 1.29169893\n",
      "Trained batch 846 batch loss 1.39095676 epoch total loss 1.29181635\n",
      "Trained batch 847 batch loss 1.34224391 epoch total loss 1.29187584\n",
      "Trained batch 848 batch loss 1.29567635 epoch total loss 1.29188037\n",
      "Trained batch 849 batch loss 1.30381775 epoch total loss 1.29189444\n",
      "Trained batch 850 batch loss 1.34794486 epoch total loss 1.29196036\n",
      "Trained batch 851 batch loss 1.33919394 epoch total loss 1.29201591\n",
      "Trained batch 852 batch loss 1.36489642 epoch total loss 1.29210138\n",
      "Trained batch 853 batch loss 1.34728146 epoch total loss 1.29216611\n",
      "Trained batch 854 batch loss 1.35712707 epoch total loss 1.29224217\n",
      "Trained batch 855 batch loss 1.26058054 epoch total loss 1.29220521\n",
      "Trained batch 856 batch loss 1.31125629 epoch total loss 1.29222751\n",
      "Trained batch 857 batch loss 1.30311513 epoch total loss 1.29224014\n",
      "Trained batch 858 batch loss 1.36515713 epoch total loss 1.29232514\n",
      "Trained batch 859 batch loss 1.34502137 epoch total loss 1.29238641\n",
      "Trained batch 860 batch loss 1.25098991 epoch total loss 1.29233825\n",
      "Trained batch 861 batch loss 1.17354465 epoch total loss 1.29220033\n",
      "Trained batch 862 batch loss 1.07673 epoch total loss 1.29195046\n",
      "Trained batch 863 batch loss 1.22095275 epoch total loss 1.29186809\n",
      "Trained batch 864 batch loss 1.23201013 epoch total loss 1.29179895\n",
      "Trained batch 865 batch loss 1.24648631 epoch total loss 1.2917465\n",
      "Trained batch 866 batch loss 1.07779408 epoch total loss 1.29149938\n",
      "Trained batch 867 batch loss 1.19392395 epoch total loss 1.29138696\n",
      "Trained batch 868 batch loss 1.4129827 epoch total loss 1.29152691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 869 batch loss 1.53718221 epoch total loss 1.29180968\n",
      "Trained batch 870 batch loss 1.37946963 epoch total loss 1.29191053\n",
      "Trained batch 871 batch loss 1.15879512 epoch total loss 1.2917577\n",
      "Trained batch 872 batch loss 1.27437901 epoch total loss 1.29173779\n",
      "Trained batch 873 batch loss 1.24378908 epoch total loss 1.29168284\n",
      "Trained batch 874 batch loss 1.27923775 epoch total loss 1.29166877\n",
      "Trained batch 875 batch loss 1.33784485 epoch total loss 1.29172158\n",
      "Trained batch 876 batch loss 1.2800132 epoch total loss 1.29170823\n",
      "Trained batch 877 batch loss 1.38145566 epoch total loss 1.29181051\n",
      "Trained batch 878 batch loss 1.39284444 epoch total loss 1.29192555\n",
      "Trained batch 879 batch loss 1.40705562 epoch total loss 1.29205668\n",
      "Trained batch 880 batch loss 1.29842436 epoch total loss 1.29206395\n",
      "Trained batch 881 batch loss 1.34682047 epoch total loss 1.29212606\n",
      "Trained batch 882 batch loss 1.22326922 epoch total loss 1.29204798\n",
      "Trained batch 883 batch loss 1.23674154 epoch total loss 1.29198527\n",
      "Trained batch 884 batch loss 1.33077645 epoch total loss 1.29202914\n",
      "Trained batch 885 batch loss 1.32029808 epoch total loss 1.29206121\n",
      "Trained batch 886 batch loss 1.18980074 epoch total loss 1.29194582\n",
      "Trained batch 887 batch loss 1.16787779 epoch total loss 1.29180586\n",
      "Trained batch 888 batch loss 1.18313503 epoch total loss 1.29168344\n",
      "Trained batch 889 batch loss 1.29246974 epoch total loss 1.29168439\n",
      "Trained batch 890 batch loss 1.41268623 epoch total loss 1.29182029\n",
      "Trained batch 891 batch loss 1.29742324 epoch total loss 1.29182661\n",
      "Trained batch 892 batch loss 1.14749122 epoch total loss 1.29166472\n",
      "Trained batch 893 batch loss 1.15450191 epoch total loss 1.29151118\n",
      "Trained batch 894 batch loss 1.26138353 epoch total loss 1.29147744\n",
      "Trained batch 895 batch loss 1.35945475 epoch total loss 1.29155338\n",
      "Trained batch 896 batch loss 1.35015023 epoch total loss 1.2916187\n",
      "Trained batch 897 batch loss 1.32251859 epoch total loss 1.29165316\n",
      "Trained batch 898 batch loss 1.24834538 epoch total loss 1.29160488\n",
      "Trained batch 899 batch loss 1.3180145 epoch total loss 1.2916342\n",
      "Trained batch 900 batch loss 1.27803659 epoch total loss 1.29161918\n",
      "Trained batch 901 batch loss 1.24419832 epoch total loss 1.29156649\n",
      "Trained batch 902 batch loss 1.22564495 epoch total loss 1.2914933\n",
      "Trained batch 903 batch loss 1.35179543 epoch total loss 1.29156017\n",
      "Trained batch 904 batch loss 1.26087928 epoch total loss 1.2915262\n",
      "Trained batch 905 batch loss 1.27912152 epoch total loss 1.29151249\n",
      "Trained batch 906 batch loss 1.23021078 epoch total loss 1.2914449\n",
      "Trained batch 907 batch loss 1.27487421 epoch total loss 1.29142666\n",
      "Trained batch 908 batch loss 1.30969524 epoch total loss 1.29144681\n",
      "Trained batch 909 batch loss 1.29186821 epoch total loss 1.29144728\n",
      "Trained batch 910 batch loss 1.3295927 epoch total loss 1.29148912\n",
      "Trained batch 911 batch loss 1.31003451 epoch total loss 1.29150951\n",
      "Trained batch 912 batch loss 1.24131203 epoch total loss 1.29145455\n",
      "Trained batch 913 batch loss 1.27346051 epoch total loss 1.29143476\n",
      "Trained batch 914 batch loss 1.28156698 epoch total loss 1.29142404\n",
      "Trained batch 915 batch loss 1.26100564 epoch total loss 1.29139078\n",
      "Trained batch 916 batch loss 1.31702912 epoch total loss 1.29141879\n",
      "Trained batch 917 batch loss 1.19537854 epoch total loss 1.29131413\n",
      "Trained batch 918 batch loss 1.17086196 epoch total loss 1.29118288\n",
      "Trained batch 919 batch loss 1.32042301 epoch total loss 1.2912147\n",
      "Trained batch 920 batch loss 1.35218167 epoch total loss 1.29128098\n",
      "Trained batch 921 batch loss 1.32921076 epoch total loss 1.29132223\n",
      "Trained batch 922 batch loss 1.33909941 epoch total loss 1.29137397\n",
      "Trained batch 923 batch loss 1.1784519 epoch total loss 1.29125166\n",
      "Trained batch 924 batch loss 1.17638206 epoch total loss 1.29112744\n",
      "Trained batch 925 batch loss 1.28946352 epoch total loss 1.29112554\n",
      "Trained batch 926 batch loss 1.41310847 epoch total loss 1.29125726\n",
      "Trained batch 927 batch loss 1.28706193 epoch total loss 1.29125273\n",
      "Trained batch 928 batch loss 1.4256 epoch total loss 1.29139757\n",
      "Trained batch 929 batch loss 1.44063962 epoch total loss 1.29155827\n",
      "Trained batch 930 batch loss 1.33224416 epoch total loss 1.29160213\n",
      "Trained batch 931 batch loss 1.29797089 epoch total loss 1.29160893\n",
      "Trained batch 932 batch loss 1.31442368 epoch total loss 1.29163349\n",
      "Trained batch 933 batch loss 1.28803527 epoch total loss 1.29162967\n",
      "Trained batch 934 batch loss 1.35067785 epoch total loss 1.29169285\n",
      "Trained batch 935 batch loss 1.36486614 epoch total loss 1.29177117\n",
      "Trained batch 936 batch loss 1.31593192 epoch total loss 1.29179692\n",
      "Trained batch 937 batch loss 1.32311451 epoch total loss 1.29183042\n",
      "Trained batch 938 batch loss 1.33570838 epoch total loss 1.29187715\n",
      "Trained batch 939 batch loss 1.28875589 epoch total loss 1.29187381\n",
      "Trained batch 940 batch loss 1.29016864 epoch total loss 1.29187191\n",
      "Trained batch 941 batch loss 1.13530254 epoch total loss 1.29170549\n",
      "Trained batch 942 batch loss 1.10205054 epoch total loss 1.29150414\n",
      "Trained batch 943 batch loss 1.09145033 epoch total loss 1.29129195\n",
      "Trained batch 944 batch loss 1.15499854 epoch total loss 1.29114771\n",
      "Trained batch 945 batch loss 1.16175902 epoch total loss 1.29101074\n",
      "Trained batch 946 batch loss 1.06610584 epoch total loss 1.29077303\n",
      "Trained batch 947 batch loss 1.02710724 epoch total loss 1.29049456\n",
      "Trained batch 948 batch loss 1.03254104 epoch total loss 1.29022253\n",
      "Trained batch 949 batch loss 1.06846845 epoch total loss 1.28998888\n",
      "Trained batch 950 batch loss 1.21025288 epoch total loss 1.28990495\n",
      "Trained batch 951 batch loss 1.23009515 epoch total loss 1.28984201\n",
      "Trained batch 952 batch loss 1.28751254 epoch total loss 1.28983951\n",
      "Trained batch 953 batch loss 1.28504395 epoch total loss 1.2898345\n",
      "Trained batch 954 batch loss 1.2278583 epoch total loss 1.28976953\n",
      "Trained batch 955 batch loss 1.41503358 epoch total loss 1.28990078\n",
      "Trained batch 956 batch loss 1.32968211 epoch total loss 1.28994238\n",
      "Trained batch 957 batch loss 1.35059392 epoch total loss 1.2900058\n",
      "Trained batch 958 batch loss 1.23094654 epoch total loss 1.28994417\n",
      "Trained batch 959 batch loss 1.18280602 epoch total loss 1.28983247\n",
      "Trained batch 960 batch loss 1.24783039 epoch total loss 1.28978872\n",
      "Trained batch 961 batch loss 1.16953266 epoch total loss 1.28966355\n",
      "Trained batch 962 batch loss 1.39064801 epoch total loss 1.28976858\n",
      "Trained batch 963 batch loss 1.25396204 epoch total loss 1.28973126\n",
      "Trained batch 964 batch loss 1.19845128 epoch total loss 1.28963661\n",
      "Trained batch 965 batch loss 1.2492348 epoch total loss 1.28959477\n",
      "Trained batch 966 batch loss 1.23967266 epoch total loss 1.28954303\n",
      "Trained batch 967 batch loss 1.15873396 epoch total loss 1.28940773\n",
      "Trained batch 968 batch loss 1.2638092 epoch total loss 1.28938127\n",
      "Trained batch 969 batch loss 1.1884346 epoch total loss 1.2892772\n",
      "Trained batch 970 batch loss 1.15321684 epoch total loss 1.28913689\n",
      "Trained batch 971 batch loss 1.28704619 epoch total loss 1.28913462\n",
      "Trained batch 972 batch loss 1.21482706 epoch total loss 1.28905821\n",
      "Trained batch 973 batch loss 1.28235173 epoch total loss 1.28905129\n",
      "Trained batch 974 batch loss 1.17923427 epoch total loss 1.28893852\n",
      "Trained batch 975 batch loss 1.19240212 epoch total loss 1.28883946\n",
      "Trained batch 976 batch loss 1.16421676 epoch total loss 1.28871179\n",
      "Trained batch 977 batch loss 1.13843846 epoch total loss 1.28855801\n",
      "Trained batch 978 batch loss 1.18382454 epoch total loss 1.28845096\n",
      "Trained batch 979 batch loss 1.11703551 epoch total loss 1.28827584\n",
      "Trained batch 980 batch loss 1.27966738 epoch total loss 1.28826702\n",
      "Trained batch 981 batch loss 1.26675463 epoch total loss 1.28824508\n",
      "Trained batch 982 batch loss 1.38567698 epoch total loss 1.28834426\n",
      "Trained batch 983 batch loss 1.26821184 epoch total loss 1.28832376\n",
      "Trained batch 984 batch loss 1.2506032 epoch total loss 1.28828537\n",
      "Trained batch 985 batch loss 1.16638446 epoch total loss 1.28816164\n",
      "Trained batch 986 batch loss 1.1012758 epoch total loss 1.28797221\n",
      "Trained batch 987 batch loss 1.21514261 epoch total loss 1.2878983\n",
      "Trained batch 988 batch loss 1.29237962 epoch total loss 1.28790283\n",
      "Trained batch 989 batch loss 1.33516943 epoch total loss 1.28795063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 990 batch loss 1.31273699 epoch total loss 1.28797567\n",
      "Trained batch 991 batch loss 1.30587864 epoch total loss 1.28799379\n",
      "Trained batch 992 batch loss 1.26211691 epoch total loss 1.28796768\n",
      "Trained batch 993 batch loss 1.27313232 epoch total loss 1.28795266\n",
      "Trained batch 994 batch loss 1.13463426 epoch total loss 1.2877984\n",
      "Trained batch 995 batch loss 1.35420454 epoch total loss 1.28786528\n",
      "Trained batch 996 batch loss 1.23803139 epoch total loss 1.28781521\n",
      "Trained batch 997 batch loss 1.36397862 epoch total loss 1.28789163\n",
      "Trained batch 998 batch loss 1.34791374 epoch total loss 1.28795171\n",
      "Trained batch 999 batch loss 1.33845496 epoch total loss 1.28800237\n",
      "Trained batch 1000 batch loss 1.23991597 epoch total loss 1.28795421\n",
      "Trained batch 1001 batch loss 1.2545234 epoch total loss 1.28792083\n",
      "Trained batch 1002 batch loss 1.26948142 epoch total loss 1.28790247\n",
      "Trained batch 1003 batch loss 1.12022901 epoch total loss 1.28773534\n",
      "Trained batch 1004 batch loss 1.18214035 epoch total loss 1.28763008\n",
      "Trained batch 1005 batch loss 1.1500814 epoch total loss 1.28749323\n",
      "Trained batch 1006 batch loss 1.16388273 epoch total loss 1.28737032\n",
      "Trained batch 1007 batch loss 1.17977035 epoch total loss 1.28726351\n",
      "Trained batch 1008 batch loss 1.12157118 epoch total loss 1.28709924\n",
      "Trained batch 1009 batch loss 1.16687274 epoch total loss 1.28698\n",
      "Trained batch 1010 batch loss 1.26118493 epoch total loss 1.28695452\n",
      "Trained batch 1011 batch loss 1.17669284 epoch total loss 1.28684545\n",
      "Trained batch 1012 batch loss 1.39332402 epoch total loss 1.28695059\n",
      "Trained batch 1013 batch loss 1.33857441 epoch total loss 1.28700161\n",
      "Trained batch 1014 batch loss 1.2737484 epoch total loss 1.28698862\n",
      "Trained batch 1015 batch loss 1.36245656 epoch total loss 1.287063\n",
      "Trained batch 1016 batch loss 1.16371512 epoch total loss 1.28694153\n",
      "Trained batch 1017 batch loss 1.21160257 epoch total loss 1.28686738\n",
      "Trained batch 1018 batch loss 1.34037352 epoch total loss 1.28692\n",
      "Trained batch 1019 batch loss 1.33935905 epoch total loss 1.28697133\n",
      "Trained batch 1020 batch loss 1.36282706 epoch total loss 1.28704572\n",
      "Trained batch 1021 batch loss 1.30660355 epoch total loss 1.28706491\n",
      "Trained batch 1022 batch loss 1.26659966 epoch total loss 1.28704488\n",
      "Trained batch 1023 batch loss 1.1861515 epoch total loss 1.2869463\n",
      "Trained batch 1024 batch loss 1.26202154 epoch total loss 1.28692186\n",
      "Trained batch 1025 batch loss 1.34556723 epoch total loss 1.28697908\n",
      "Trained batch 1026 batch loss 1.32080209 epoch total loss 1.2870121\n",
      "Trained batch 1027 batch loss 1.38407254 epoch total loss 1.28710651\n",
      "Trained batch 1028 batch loss 1.21312618 epoch total loss 1.28703451\n",
      "Trained batch 1029 batch loss 1.19397581 epoch total loss 1.28694415\n",
      "Trained batch 1030 batch loss 1.33289516 epoch total loss 1.28698874\n",
      "Trained batch 1031 batch loss 1.32147479 epoch total loss 1.28702223\n",
      "Trained batch 1032 batch loss 1.1991502 epoch total loss 1.286937\n",
      "Trained batch 1033 batch loss 1.22254205 epoch total loss 1.28687465\n",
      "Trained batch 1034 batch loss 1.27082038 epoch total loss 1.28685915\n",
      "Trained batch 1035 batch loss 1.3729099 epoch total loss 1.28694236\n",
      "Trained batch 1036 batch loss 1.29983914 epoch total loss 1.28695476\n",
      "Trained batch 1037 batch loss 1.2961396 epoch total loss 1.2869637\n",
      "Trained batch 1038 batch loss 1.4245466 epoch total loss 1.28709626\n",
      "Trained batch 1039 batch loss 1.36096239 epoch total loss 1.28716731\n",
      "Trained batch 1040 batch loss 1.26487076 epoch total loss 1.28714585\n",
      "Trained batch 1041 batch loss 1.22519886 epoch total loss 1.28708637\n",
      "Trained batch 1042 batch loss 1.19969761 epoch total loss 1.28700256\n",
      "Trained batch 1043 batch loss 1.30371201 epoch total loss 1.28701854\n",
      "Trained batch 1044 batch loss 1.32915401 epoch total loss 1.28705883\n",
      "Trained batch 1045 batch loss 1.2875309 epoch total loss 1.28705931\n",
      "Trained batch 1046 batch loss 1.42486644 epoch total loss 1.28719103\n",
      "Trained batch 1047 batch loss 1.56539702 epoch total loss 1.28745687\n",
      "Trained batch 1048 batch loss 1.43213701 epoch total loss 1.28759491\n",
      "Trained batch 1049 batch loss 1.15288246 epoch total loss 1.28746641\n",
      "Trained batch 1050 batch loss 1.07602656 epoch total loss 1.28726506\n",
      "Trained batch 1051 batch loss 1.09921074 epoch total loss 1.28708613\n",
      "Trained batch 1052 batch loss 1.20999622 epoch total loss 1.28701282\n",
      "Trained batch 1053 batch loss 1.19767487 epoch total loss 1.28692794\n",
      "Trained batch 1054 batch loss 1.08942962 epoch total loss 1.28674066\n",
      "Trained batch 1055 batch loss 0.984866798 epoch total loss 1.28645444\n",
      "Trained batch 1056 batch loss 1.04917264 epoch total loss 1.28622985\n",
      "Trained batch 1057 batch loss 1.12743354 epoch total loss 1.28607953\n",
      "Trained batch 1058 batch loss 1.15553641 epoch total loss 1.28595614\n",
      "Trained batch 1059 batch loss 1.36552858 epoch total loss 1.28603125\n",
      "Trained batch 1060 batch loss 1.21952581 epoch total loss 1.28596842\n",
      "Trained batch 1061 batch loss 1.31858647 epoch total loss 1.28599918\n",
      "Trained batch 1062 batch loss 1.28459477 epoch total loss 1.28599787\n",
      "Trained batch 1063 batch loss 1.36013961 epoch total loss 1.28606761\n",
      "Trained batch 1064 batch loss 1.22999477 epoch total loss 1.28601491\n",
      "Trained batch 1065 batch loss 1.2861836 epoch total loss 1.28601503\n",
      "Trained batch 1066 batch loss 1.36244333 epoch total loss 1.28608668\n",
      "Trained batch 1067 batch loss 1.24572 epoch total loss 1.28604889\n",
      "Trained batch 1068 batch loss 1.21544433 epoch total loss 1.28598273\n",
      "Trained batch 1069 batch loss 1.33880138 epoch total loss 1.28603208\n",
      "Trained batch 1070 batch loss 1.2131269 epoch total loss 1.28596401\n",
      "Trained batch 1071 batch loss 1.13930249 epoch total loss 1.28582704\n",
      "Trained batch 1072 batch loss 1.19955587 epoch total loss 1.28574657\n",
      "Trained batch 1073 batch loss 1.30962336 epoch total loss 1.28576875\n",
      "Trained batch 1074 batch loss 1.28624213 epoch total loss 1.28576922\n",
      "Trained batch 1075 batch loss 1.31864536 epoch total loss 1.28579974\n",
      "Trained batch 1076 batch loss 1.29647636 epoch total loss 1.28580976\n",
      "Trained batch 1077 batch loss 1.3740766 epoch total loss 1.28589165\n",
      "Trained batch 1078 batch loss 1.29174709 epoch total loss 1.28589702\n",
      "Trained batch 1079 batch loss 1.30487394 epoch total loss 1.28591466\n",
      "Trained batch 1080 batch loss 1.26819634 epoch total loss 1.28589833\n",
      "Trained batch 1081 batch loss 1.19939625 epoch total loss 1.28581822\n",
      "Trained batch 1082 batch loss 1.27258325 epoch total loss 1.28580594\n",
      "Trained batch 1083 batch loss 1.23039937 epoch total loss 1.2857548\n",
      "Trained batch 1084 batch loss 1.18499184 epoch total loss 1.28566182\n",
      "Trained batch 1085 batch loss 1.27118766 epoch total loss 1.28564847\n",
      "Trained batch 1086 batch loss 1.26365447 epoch total loss 1.2856282\n",
      "Trained batch 1087 batch loss 1.26034677 epoch total loss 1.28560495\n",
      "Trained batch 1088 batch loss 1.28301358 epoch total loss 1.28560257\n",
      "Trained batch 1089 batch loss 1.27325892 epoch total loss 1.28559124\n",
      "Trained batch 1090 batch loss 1.38461685 epoch total loss 1.2856822\n",
      "Trained batch 1091 batch loss 1.50280333 epoch total loss 1.28588116\n",
      "Trained batch 1092 batch loss 1.36234593 epoch total loss 1.28595114\n",
      "Trained batch 1093 batch loss 1.35011 epoch total loss 1.28600991\n",
      "Trained batch 1094 batch loss 1.29718089 epoch total loss 1.28602016\n",
      "Trained batch 1095 batch loss 1.4173286 epoch total loss 1.28614008\n",
      "Trained batch 1096 batch loss 1.34189224 epoch total loss 1.28619099\n",
      "Trained batch 1097 batch loss 1.34824634 epoch total loss 1.28624749\n",
      "Trained batch 1098 batch loss 1.27223611 epoch total loss 1.28623474\n",
      "Trained batch 1099 batch loss 1.25609243 epoch total loss 1.28620732\n",
      "Trained batch 1100 batch loss 1.24097276 epoch total loss 1.28616619\n",
      "Trained batch 1101 batch loss 1.30701327 epoch total loss 1.28618515\n",
      "Trained batch 1102 batch loss 1.37015676 epoch total loss 1.28626132\n",
      "Trained batch 1103 batch loss 1.21511102 epoch total loss 1.28619683\n",
      "Trained batch 1104 batch loss 1.27656388 epoch total loss 1.28618813\n",
      "Trained batch 1105 batch loss 1.23099041 epoch total loss 1.28613806\n",
      "Trained batch 1106 batch loss 1.26352 epoch total loss 1.28611767\n",
      "Trained batch 1107 batch loss 1.31386733 epoch total loss 1.28614271\n",
      "Trained batch 1108 batch loss 1.18032277 epoch total loss 1.28604722\n",
      "Trained batch 1109 batch loss 1.40767694 epoch total loss 1.28615689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1110 batch loss 1.3657465 epoch total loss 1.28622866\n",
      "Trained batch 1111 batch loss 1.20431614 epoch total loss 1.28615487\n",
      "Trained batch 1112 batch loss 1.24707842 epoch total loss 1.2861197\n",
      "Trained batch 1113 batch loss 1.20316637 epoch total loss 1.28604519\n",
      "Trained batch 1114 batch loss 1.31387162 epoch total loss 1.28607011\n",
      "Trained batch 1115 batch loss 1.23850346 epoch total loss 1.28602755\n",
      "Trained batch 1116 batch loss 1.20899081 epoch total loss 1.28595841\n",
      "Trained batch 1117 batch loss 1.33448577 epoch total loss 1.28600192\n",
      "Trained batch 1118 batch loss 1.28785586 epoch total loss 1.28600359\n",
      "Trained batch 1119 batch loss 1.19559777 epoch total loss 1.28592277\n",
      "Trained batch 1120 batch loss 1.28476381 epoch total loss 1.28592169\n",
      "Trained batch 1121 batch loss 1.28232825 epoch total loss 1.28591847\n",
      "Trained batch 1122 batch loss 1.24401164 epoch total loss 1.28588116\n",
      "Trained batch 1123 batch loss 1.29471636 epoch total loss 1.28588903\n",
      "Trained batch 1124 batch loss 1.34018302 epoch total loss 1.28593731\n",
      "Trained batch 1125 batch loss 1.15602612 epoch total loss 1.2858218\n",
      "Trained batch 1126 batch loss 1.20016205 epoch total loss 1.28574574\n",
      "Trained batch 1127 batch loss 1.26818407 epoch total loss 1.28573024\n",
      "Trained batch 1128 batch loss 1.37930834 epoch total loss 1.28581309\n",
      "Trained batch 1129 batch loss 1.37168837 epoch total loss 1.28588927\n",
      "Trained batch 1130 batch loss 1.31715035 epoch total loss 1.28591692\n",
      "Trained batch 1131 batch loss 1.13439572 epoch total loss 1.28578293\n",
      "Trained batch 1132 batch loss 1.12261736 epoch total loss 1.28563869\n",
      "Trained batch 1133 batch loss 1.22731173 epoch total loss 1.28558719\n",
      "Trained batch 1134 batch loss 1.19669938 epoch total loss 1.28550875\n",
      "Trained batch 1135 batch loss 1.13069129 epoch total loss 1.28537238\n",
      "Trained batch 1136 batch loss 1.30689919 epoch total loss 1.28539133\n",
      "Trained batch 1137 batch loss 1.30620933 epoch total loss 1.28540957\n",
      "Trained batch 1138 batch loss 1.15365553 epoch total loss 1.28529382\n",
      "Trained batch 1139 batch loss 1.30450153 epoch total loss 1.28531075\n",
      "Trained batch 1140 batch loss 1.34015703 epoch total loss 1.28535879\n",
      "Trained batch 1141 batch loss 1.42649341 epoch total loss 1.28548253\n",
      "Trained batch 1142 batch loss 1.43236816 epoch total loss 1.28561115\n",
      "Trained batch 1143 batch loss 1.57086718 epoch total loss 1.28586078\n",
      "Trained batch 1144 batch loss 1.37175751 epoch total loss 1.28593588\n",
      "Trained batch 1145 batch loss 1.16083121 epoch total loss 1.28582668\n",
      "Trained batch 1146 batch loss 1.29569888 epoch total loss 1.28583515\n",
      "Trained batch 1147 batch loss 1.46909308 epoch total loss 1.28599501\n",
      "Trained batch 1148 batch loss 1.42462599 epoch total loss 1.28611577\n",
      "Trained batch 1149 batch loss 1.30518103 epoch total loss 1.28613234\n",
      "Trained batch 1150 batch loss 1.22036743 epoch total loss 1.28607512\n",
      "Trained batch 1151 batch loss 1.31629872 epoch total loss 1.28610146\n",
      "Trained batch 1152 batch loss 1.23737073 epoch total loss 1.28605914\n",
      "Trained batch 1153 batch loss 1.28505135 epoch total loss 1.28605831\n",
      "Trained batch 1154 batch loss 1.35796177 epoch total loss 1.28612053\n",
      "Trained batch 1155 batch loss 1.37956357 epoch total loss 1.28620136\n",
      "Trained batch 1156 batch loss 1.38576519 epoch total loss 1.28628755\n",
      "Trained batch 1157 batch loss 1.25682473 epoch total loss 1.28626204\n",
      "Trained batch 1158 batch loss 1.30790257 epoch total loss 1.28628075\n",
      "Trained batch 1159 batch loss 1.25075936 epoch total loss 1.28625\n",
      "Trained batch 1160 batch loss 1.36858439 epoch total loss 1.28632092\n",
      "Trained batch 1161 batch loss 1.30600238 epoch total loss 1.28633797\n",
      "Trained batch 1162 batch loss 1.40372527 epoch total loss 1.28643894\n",
      "Trained batch 1163 batch loss 1.59401727 epoch total loss 1.28670335\n",
      "Trained batch 1164 batch loss 1.31855094 epoch total loss 1.28673077\n",
      "Trained batch 1165 batch loss 1.42018616 epoch total loss 1.28684533\n",
      "Trained batch 1166 batch loss 1.35349882 epoch total loss 1.28690255\n",
      "Trained batch 1167 batch loss 1.37886226 epoch total loss 1.28698134\n",
      "Trained batch 1168 batch loss 1.39252651 epoch total loss 1.2870717\n",
      "Trained batch 1169 batch loss 1.41893935 epoch total loss 1.2871846\n",
      "Trained batch 1170 batch loss 1.1939888 epoch total loss 1.28710485\n",
      "Trained batch 1171 batch loss 1.1792438 epoch total loss 1.2870127\n",
      "Trained batch 1172 batch loss 1.16660452 epoch total loss 1.28691\n",
      "Trained batch 1173 batch loss 1.23303807 epoch total loss 1.28686404\n",
      "Trained batch 1174 batch loss 1.15198755 epoch total loss 1.28674924\n",
      "Trained batch 1175 batch loss 1.27696896 epoch total loss 1.2867409\n",
      "Trained batch 1176 batch loss 1.34049106 epoch total loss 1.28678656\n",
      "Trained batch 1177 batch loss 1.3545711 epoch total loss 1.28684413\n",
      "Trained batch 1178 batch loss 1.32419479 epoch total loss 1.28687596\n",
      "Trained batch 1179 batch loss 1.30940652 epoch total loss 1.28689504\n",
      "Trained batch 1180 batch loss 1.27598417 epoch total loss 1.28688586\n",
      "Trained batch 1181 batch loss 1.33285451 epoch total loss 1.28692472\n",
      "Trained batch 1182 batch loss 1.35632479 epoch total loss 1.28698349\n",
      "Trained batch 1183 batch loss 1.25554907 epoch total loss 1.28695691\n",
      "Trained batch 1184 batch loss 1.27650738 epoch total loss 1.28694797\n",
      "Trained batch 1185 batch loss 1.26017785 epoch total loss 1.28692544\n",
      "Trained batch 1186 batch loss 1.13227272 epoch total loss 1.28679502\n",
      "Trained batch 1187 batch loss 1.21181905 epoch total loss 1.28673184\n",
      "Trained batch 1188 batch loss 1.28496289 epoch total loss 1.28673029\n",
      "Trained batch 1189 batch loss 1.16215301 epoch total loss 1.2866255\n",
      "Trained batch 1190 batch loss 1.18429613 epoch total loss 1.28653955\n",
      "Trained batch 1191 batch loss 1.2183044 epoch total loss 1.28648221\n",
      "Trained batch 1192 batch loss 1.14672863 epoch total loss 1.28636491\n",
      "Trained batch 1193 batch loss 1.19872773 epoch total loss 1.28629148\n",
      "Trained batch 1194 batch loss 1.16147065 epoch total loss 1.28618705\n",
      "Trained batch 1195 batch loss 1.26281846 epoch total loss 1.28616738\n",
      "Trained batch 1196 batch loss 1.25862336 epoch total loss 1.2861445\n",
      "Trained batch 1197 batch loss 1.34973526 epoch total loss 1.28619754\n",
      "Trained batch 1198 batch loss 1.35393238 epoch total loss 1.28625405\n",
      "Trained batch 1199 batch loss 1.2023046 epoch total loss 1.28618407\n",
      "Trained batch 1200 batch loss 1.19771123 epoch total loss 1.28611028\n",
      "Trained batch 1201 batch loss 1.3201263 epoch total loss 1.28613865\n",
      "Trained batch 1202 batch loss 1.33134377 epoch total loss 1.2861762\n",
      "Trained batch 1203 batch loss 1.26931083 epoch total loss 1.28616214\n",
      "Trained batch 1204 batch loss 1.14011979 epoch total loss 1.2860409\n",
      "Trained batch 1205 batch loss 1.1378901 epoch total loss 1.285918\n",
      "Trained batch 1206 batch loss 1.26631188 epoch total loss 1.28590178\n",
      "Trained batch 1207 batch loss 1.21100581 epoch total loss 1.28583968\n",
      "Trained batch 1208 batch loss 1.31915629 epoch total loss 1.28586733\n",
      "Trained batch 1209 batch loss 1.29612756 epoch total loss 1.2858758\n",
      "Trained batch 1210 batch loss 1.26075506 epoch total loss 1.28585505\n",
      "Trained batch 1211 batch loss 1.08116484 epoch total loss 1.28568602\n",
      "Trained batch 1212 batch loss 1.05380154 epoch total loss 1.2854948\n",
      "Trained batch 1213 batch loss 1.10400248 epoch total loss 1.2853452\n",
      "Trained batch 1214 batch loss 1.05724359 epoch total loss 1.2851572\n",
      "Trained batch 1215 batch loss 1.27419078 epoch total loss 1.28514826\n",
      "Trained batch 1216 batch loss 1.2060051 epoch total loss 1.28508317\n",
      "Trained batch 1217 batch loss 1.22808051 epoch total loss 1.28503633\n",
      "Trained batch 1218 batch loss 1.27473116 epoch total loss 1.28502786\n",
      "Trained batch 1219 batch loss 1.27064645 epoch total loss 1.28501606\n",
      "Trained batch 1220 batch loss 1.19071829 epoch total loss 1.28493869\n",
      "Trained batch 1221 batch loss 1.1781199 epoch total loss 1.28485119\n",
      "Trained batch 1222 batch loss 1.4629029 epoch total loss 1.28499687\n",
      "Trained batch 1223 batch loss 1.44937813 epoch total loss 1.28513134\n",
      "Trained batch 1224 batch loss 1.07728028 epoch total loss 1.28496146\n",
      "Trained batch 1225 batch loss 1.09108746 epoch total loss 1.28480315\n",
      "Trained batch 1226 batch loss 1.13197839 epoch total loss 1.28467858\n",
      "Trained batch 1227 batch loss 1.08301425 epoch total loss 1.28451419\n",
      "Trained batch 1228 batch loss 1.00764799 epoch total loss 1.28428876\n",
      "Trained batch 1229 batch loss 1.1367178 epoch total loss 1.28416872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1230 batch loss 1.11617863 epoch total loss 1.28403211\n",
      "Trained batch 1231 batch loss 1.24268973 epoch total loss 1.28399849\n",
      "Trained batch 1232 batch loss 1.13835478 epoch total loss 1.28388023\n",
      "Trained batch 1233 batch loss 1.3190366 epoch total loss 1.28390884\n",
      "Trained batch 1234 batch loss 1.25664639 epoch total loss 1.28388667\n",
      "Trained batch 1235 batch loss 1.32154799 epoch total loss 1.28391719\n",
      "Trained batch 1236 batch loss 1.32117009 epoch total loss 1.28394735\n",
      "Trained batch 1237 batch loss 1.36133909 epoch total loss 1.28400981\n",
      "Trained batch 1238 batch loss 1.4696579 epoch total loss 1.28415978\n",
      "Trained batch 1239 batch loss 1.40296459 epoch total loss 1.28425562\n",
      "Trained batch 1240 batch loss 1.27208078 epoch total loss 1.28424585\n",
      "Trained batch 1241 batch loss 1.30441368 epoch total loss 1.28426206\n",
      "Trained batch 1242 batch loss 1.24093103 epoch total loss 1.28422725\n",
      "Trained batch 1243 batch loss 1.23962724 epoch total loss 1.28419137\n",
      "Trained batch 1244 batch loss 1.30316234 epoch total loss 1.28420663\n",
      "Trained batch 1245 batch loss 1.22662497 epoch total loss 1.28416049\n",
      "Trained batch 1246 batch loss 1.36853158 epoch total loss 1.28422821\n",
      "Trained batch 1247 batch loss 1.29180026 epoch total loss 1.28423417\n",
      "Trained batch 1248 batch loss 1.23879051 epoch total loss 1.28419781\n",
      "Trained batch 1249 batch loss 1.22895718 epoch total loss 1.28415358\n",
      "Trained batch 1250 batch loss 1.23702645 epoch total loss 1.28411591\n",
      "Trained batch 1251 batch loss 1.19024014 epoch total loss 1.28404081\n",
      "Trained batch 1252 batch loss 1.28175187 epoch total loss 1.28403902\n",
      "Trained batch 1253 batch loss 1.23032975 epoch total loss 1.28399611\n",
      "Trained batch 1254 batch loss 1.12584877 epoch total loss 1.28387\n",
      "Trained batch 1255 batch loss 1.15415645 epoch total loss 1.28376675\n",
      "Trained batch 1256 batch loss 1.14329445 epoch total loss 1.28365481\n",
      "Trained batch 1257 batch loss 1.32098603 epoch total loss 1.28368461\n",
      "Trained batch 1258 batch loss 1.42260718 epoch total loss 1.283795\n",
      "Trained batch 1259 batch loss 1.30449533 epoch total loss 1.28381145\n",
      "Trained batch 1260 batch loss 1.35151601 epoch total loss 1.28386521\n",
      "Trained batch 1261 batch loss 1.29762435 epoch total loss 1.28387606\n",
      "Trained batch 1262 batch loss 1.09636259 epoch total loss 1.28372753\n",
      "Trained batch 1263 batch loss 1.24086523 epoch total loss 1.28369355\n",
      "Trained batch 1264 batch loss 1.31929862 epoch total loss 1.28372169\n",
      "Trained batch 1265 batch loss 1.30713236 epoch total loss 1.28374028\n",
      "Trained batch 1266 batch loss 1.08134544 epoch total loss 1.2835803\n",
      "Trained batch 1267 batch loss 1.0732466 epoch total loss 1.28341436\n",
      "Trained batch 1268 batch loss 1.08628917 epoch total loss 1.28325891\n",
      "Trained batch 1269 batch loss 1.22706187 epoch total loss 1.28321457\n",
      "Trained batch 1270 batch loss 1.05532527 epoch total loss 1.28303516\n",
      "Trained batch 1271 batch loss 1.18214941 epoch total loss 1.28295577\n",
      "Trained batch 1272 batch loss 1.2886374 epoch total loss 1.28296018\n",
      "Trained batch 1273 batch loss 1.17444921 epoch total loss 1.28287494\n",
      "Trained batch 1274 batch loss 1.21224451 epoch total loss 1.28281963\n",
      "Trained batch 1275 batch loss 1.13489294 epoch total loss 1.28270352\n",
      "Trained batch 1276 batch loss 1.14738333 epoch total loss 1.28259742\n",
      "Trained batch 1277 batch loss 1.40108228 epoch total loss 1.28269029\n",
      "Trained batch 1278 batch loss 1.27172947 epoch total loss 1.2826817\n",
      "Trained batch 1279 batch loss 1.30909586 epoch total loss 1.28270233\n",
      "Trained batch 1280 batch loss 1.08971417 epoch total loss 1.28255153\n",
      "Trained batch 1281 batch loss 1.09456873 epoch total loss 1.2824049\n",
      "Trained batch 1282 batch loss 1.40674686 epoch total loss 1.28250182\n",
      "Trained batch 1283 batch loss 1.35838377 epoch total loss 1.28256094\n",
      "Trained batch 1284 batch loss 1.29784298 epoch total loss 1.28257287\n",
      "Trained batch 1285 batch loss 1.37181747 epoch total loss 1.28264236\n",
      "Trained batch 1286 batch loss 1.39384091 epoch total loss 1.28272879\n",
      "Trained batch 1287 batch loss 1.21611977 epoch total loss 1.28267705\n",
      "Trained batch 1288 batch loss 1.13881028 epoch total loss 1.28256524\n",
      "Trained batch 1289 batch loss 1.15449321 epoch total loss 1.28246593\n",
      "Trained batch 1290 batch loss 1.28043616 epoch total loss 1.28246439\n",
      "Trained batch 1291 batch loss 1.23582935 epoch total loss 1.28242826\n",
      "Trained batch 1292 batch loss 1.34595013 epoch total loss 1.28247738\n",
      "Trained batch 1293 batch loss 1.30251849 epoch total loss 1.28249288\n",
      "Trained batch 1294 batch loss 1.316221 epoch total loss 1.28251886\n",
      "Trained batch 1295 batch loss 1.4122169 epoch total loss 1.28261912\n",
      "Trained batch 1296 batch loss 1.31314325 epoch total loss 1.2826426\n",
      "Trained batch 1297 batch loss 1.40920925 epoch total loss 1.28274012\n",
      "Trained batch 1298 batch loss 1.38500333 epoch total loss 1.28281891\n",
      "Trained batch 1299 batch loss 1.36573291 epoch total loss 1.28288281\n",
      "Trained batch 1300 batch loss 1.36722541 epoch total loss 1.28294766\n",
      "Trained batch 1301 batch loss 1.30742502 epoch total loss 1.28296638\n",
      "Trained batch 1302 batch loss 1.27956426 epoch total loss 1.28296375\n",
      "Trained batch 1303 batch loss 1.21785736 epoch total loss 1.2829138\n",
      "Trained batch 1304 batch loss 1.39220703 epoch total loss 1.28299761\n",
      "Trained batch 1305 batch loss 1.32052863 epoch total loss 1.28302646\n",
      "Trained batch 1306 batch loss 1.41459167 epoch total loss 1.28312719\n",
      "Trained batch 1307 batch loss 1.4929682 epoch total loss 1.28328764\n",
      "Trained batch 1308 batch loss 1.26266599 epoch total loss 1.28327191\n",
      "Trained batch 1309 batch loss 1.18076277 epoch total loss 1.28319359\n",
      "Trained batch 1310 batch loss 1.31809378 epoch total loss 1.28322029\n",
      "Trained batch 1311 batch loss 1.33083427 epoch total loss 1.28325653\n",
      "Trained batch 1312 batch loss 1.27833807 epoch total loss 1.28325284\n",
      "Trained batch 1313 batch loss 1.3455497 epoch total loss 1.28330028\n",
      "Trained batch 1314 batch loss 1.30195439 epoch total loss 1.28331447\n",
      "Trained batch 1315 batch loss 1.43357718 epoch total loss 1.28342879\n",
      "Trained batch 1316 batch loss 1.43134582 epoch total loss 1.2835412\n",
      "Trained batch 1317 batch loss 1.40627754 epoch total loss 1.28363442\n",
      "Trained batch 1318 batch loss 1.52264905 epoch total loss 1.28381574\n",
      "Trained batch 1319 batch loss 1.42741108 epoch total loss 1.28392458\n",
      "Trained batch 1320 batch loss 1.44766271 epoch total loss 1.28404868\n",
      "Trained batch 1321 batch loss 1.32058334 epoch total loss 1.28407633\n",
      "Trained batch 1322 batch loss 1.36908567 epoch total loss 1.28414059\n",
      "Trained batch 1323 batch loss 1.4398582 epoch total loss 1.28425825\n",
      "Trained batch 1324 batch loss 1.49927258 epoch total loss 1.28442073\n",
      "Trained batch 1325 batch loss 1.44067562 epoch total loss 1.28453863\n",
      "Trained batch 1326 batch loss 1.29334652 epoch total loss 1.2845453\n",
      "Trained batch 1327 batch loss 1.45568621 epoch total loss 1.28467417\n",
      "Trained batch 1328 batch loss 1.39008641 epoch total loss 1.28475368\n",
      "Trained batch 1329 batch loss 1.30463541 epoch total loss 1.28476858\n",
      "Trained batch 1330 batch loss 1.21786261 epoch total loss 1.28471839\n",
      "Trained batch 1331 batch loss 1.22215796 epoch total loss 1.28467131\n",
      "Trained batch 1332 batch loss 1.29697263 epoch total loss 1.2846806\n",
      "Trained batch 1333 batch loss 1.35020423 epoch total loss 1.28472984\n",
      "Trained batch 1334 batch loss 1.24921548 epoch total loss 1.28470325\n",
      "Trained batch 1335 batch loss 1.283916 epoch total loss 1.28470266\n",
      "Trained batch 1336 batch loss 1.36063254 epoch total loss 1.2847594\n",
      "Trained batch 1337 batch loss 1.25726819 epoch total loss 1.2847389\n",
      "Trained batch 1338 batch loss 1.39246845 epoch total loss 1.28481936\n",
      "Trained batch 1339 batch loss 1.21737051 epoch total loss 1.28476906\n",
      "Trained batch 1340 batch loss 1.22476578 epoch total loss 1.28472424\n",
      "Trained batch 1341 batch loss 1.16440773 epoch total loss 1.28463459\n",
      "Trained batch 1342 batch loss 1.27497613 epoch total loss 1.28462744\n",
      "Trained batch 1343 batch loss 1.37160897 epoch total loss 1.28469217\n",
      "Trained batch 1344 batch loss 1.13912117 epoch total loss 1.28458381\n",
      "Trained batch 1345 batch loss 1.19870472 epoch total loss 1.28452\n",
      "Trained batch 1346 batch loss 1.24448311 epoch total loss 1.28449035\n",
      "Trained batch 1347 batch loss 1.3131299 epoch total loss 1.28451157\n",
      "Trained batch 1348 batch loss 1.39815903 epoch total loss 1.28459585\n",
      "Trained batch 1349 batch loss 1.22309053 epoch total loss 1.28455031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained batch 1350 batch loss 1.26366913 epoch total loss 1.28453481\n",
      "Trained batch 1351 batch loss 1.39154148 epoch total loss 1.28461409\n",
      "Trained batch 1352 batch loss 1.28947878 epoch total loss 1.28461766\n",
      "Trained batch 1353 batch loss 1.27930593 epoch total loss 1.28461373\n",
      "Trained batch 1354 batch loss 1.32215238 epoch total loss 1.2846415\n",
      "Trained batch 1355 batch loss 1.26057911 epoch total loss 1.28462374\n",
      "Trained batch 1356 batch loss 1.29021239 epoch total loss 1.2846278\n",
      "Trained batch 1357 batch loss 1.38782406 epoch total loss 1.28470385\n",
      "Trained batch 1358 batch loss 1.38115227 epoch total loss 1.2847749\n",
      "Trained batch 1359 batch loss 1.17325175 epoch total loss 1.28469276\n",
      "Trained batch 1360 batch loss 1.24470198 epoch total loss 1.28466344\n",
      "Trained batch 1361 batch loss 1.12206936 epoch total loss 1.28454387\n",
      "Trained batch 1362 batch loss 1.21075583 epoch total loss 1.28448975\n",
      "Trained batch 1363 batch loss 1.07712078 epoch total loss 1.28433764\n",
      "Trained batch 1364 batch loss 1.28594661 epoch total loss 1.28433883\n",
      "Trained batch 1365 batch loss 1.34892821 epoch total loss 1.28438604\n",
      "Trained batch 1366 batch loss 1.34501839 epoch total loss 1.28443038\n",
      "Trained batch 1367 batch loss 1.36168718 epoch total loss 1.28448701\n",
      "Trained batch 1368 batch loss 1.30181861 epoch total loss 1.28449965\n",
      "Trained batch 1369 batch loss 1.28367412 epoch total loss 1.28449905\n",
      "Trained batch 1370 batch loss 1.28790343 epoch total loss 1.28450155\n",
      "Trained batch 1371 batch loss 1.26208162 epoch total loss 1.28448522\n",
      "Trained batch 1372 batch loss 1.24949324 epoch total loss 1.28445971\n",
      "Trained batch 1373 batch loss 1.24055243 epoch total loss 1.28442776\n",
      "Trained batch 1374 batch loss 1.15314698 epoch total loss 1.28433228\n",
      "Trained batch 1375 batch loss 1.1589992 epoch total loss 1.28424108\n",
      "Trained batch 1376 batch loss 1.10729706 epoch total loss 1.28411257\n",
      "Trained batch 1377 batch loss 1.12818766 epoch total loss 1.28399932\n",
      "Trained batch 1378 batch loss 1.26516318 epoch total loss 1.28398561\n",
      "Trained batch 1379 batch loss 1.17098248 epoch total loss 1.28390372\n",
      "Trained batch 1380 batch loss 1.20661664 epoch total loss 1.28384769\n",
      "Trained batch 1381 batch loss 1.1555897 epoch total loss 1.28375483\n",
      "Trained batch 1382 batch loss 1.19012237 epoch total loss 1.28368711\n",
      "Trained batch 1383 batch loss 1.27712941 epoch total loss 1.28368235\n",
      "Trained batch 1384 batch loss 1.31150687 epoch total loss 1.28370237\n",
      "Trained batch 1385 batch loss 1.31691563 epoch total loss 1.28372645\n",
      "Trained batch 1386 batch loss 1.32307553 epoch total loss 1.28375483\n",
      "Trained batch 1387 batch loss 1.31706119 epoch total loss 1.28377879\n",
      "Trained batch 1388 batch loss 1.29355431 epoch total loss 1.28378582\n",
      "Epoch 3 train loss 1.2837858200073242\n",
      "Validated batch 1 batch loss 1.3273294\n",
      "Validated batch 2 batch loss 1.23800266\n",
      "Validated batch 3 batch loss 1.20200849\n",
      "Validated batch 4 batch loss 1.19935942\n",
      "Validated batch 5 batch loss 1.20069242\n",
      "Validated batch 6 batch loss 1.31679904\n",
      "Validated batch 7 batch loss 1.2622304\n",
      "Validated batch 8 batch loss 1.21093845\n",
      "Validated batch 9 batch loss 1.33434629\n",
      "Validated batch 10 batch loss 1.297207\n",
      "Validated batch 11 batch loss 1.21803403\n",
      "Validated batch 12 batch loss 1.1612643\n",
      "Validated batch 13 batch loss 1.28142965\n",
      "Validated batch 14 batch loss 1.34525752\n",
      "Validated batch 15 batch loss 1.39038658\n",
      "Validated batch 16 batch loss 1.36643791\n",
      "Validated batch 17 batch loss 1.25622022\n",
      "Validated batch 18 batch loss 1.3902452\n",
      "Validated batch 19 batch loss 1.29321635\n",
      "Validated batch 20 batch loss 1.28224289\n",
      "Validated batch 21 batch loss 1.29287767\n",
      "Validated batch 22 batch loss 1.05869532\n",
      "Validated batch 23 batch loss 1.29084241\n",
      "Validated batch 24 batch loss 1.31326413\n",
      "Validated batch 25 batch loss 1.28580856\n",
      "Validated batch 26 batch loss 1.23305523\n",
      "Validated batch 27 batch loss 1.23204982\n",
      "Validated batch 28 batch loss 1.29243731\n",
      "Validated batch 29 batch loss 1.42144263\n",
      "Validated batch 30 batch loss 1.18758726\n",
      "Validated batch 31 batch loss 1.2985419\n",
      "Validated batch 32 batch loss 1.24028683\n",
      "Validated batch 33 batch loss 1.2812767\n",
      "Validated batch 34 batch loss 1.29018068\n",
      "Validated batch 35 batch loss 1.1515348\n",
      "Validated batch 36 batch loss 1.43123651\n",
      "Validated batch 37 batch loss 1.17439485\n",
      "Validated batch 38 batch loss 1.34195721\n",
      "Validated batch 39 batch loss 1.25467801\n",
      "Validated batch 40 batch loss 1.33902073\n",
      "Validated batch 41 batch loss 1.12552953\n",
      "Validated batch 42 batch loss 1.25665331\n",
      "Validated batch 43 batch loss 1.17689037\n",
      "Validated batch 44 batch loss 1.29653049\n",
      "Validated batch 45 batch loss 1.25690687\n",
      "Validated batch 46 batch loss 1.25156617\n",
      "Validated batch 47 batch loss 1.27840662\n",
      "Validated batch 48 batch loss 1.28587031\n",
      "Validated batch 49 batch loss 1.19297433\n",
      "Validated batch 50 batch loss 1.26665473\n",
      "Validated batch 51 batch loss 1.27363598\n",
      "Validated batch 52 batch loss 1.30996156\n",
      "Validated batch 53 batch loss 1.35479033\n",
      "Validated batch 54 batch loss 1.32881021\n",
      "Validated batch 55 batch loss 1.32151055\n",
      "Validated batch 56 batch loss 1.23582041\n",
      "Validated batch 57 batch loss 1.28609705\n",
      "Validated batch 58 batch loss 1.26491451\n",
      "Validated batch 59 batch loss 1.27923095\n",
      "Validated batch 60 batch loss 1.35359597\n",
      "Validated batch 61 batch loss 1.33306122\n",
      "Validated batch 62 batch loss 1.29347324\n",
      "Validated batch 63 batch loss 1.45429993\n",
      "Validated batch 64 batch loss 1.10944796\n",
      "Validated batch 65 batch loss 1.33749795\n",
      "Validated batch 66 batch loss 1.05831099\n",
      "Validated batch 67 batch loss 1.26432896\n",
      "Validated batch 68 batch loss 1.33291316\n",
      "Validated batch 69 batch loss 1.18369007\n",
      "Validated batch 70 batch loss 1.21569657\n",
      "Validated batch 71 batch loss 1.13611364\n",
      "Validated batch 72 batch loss 1.27105308\n",
      "Validated batch 73 batch loss 1.21804857\n",
      "Validated batch 74 batch loss 1.20557475\n",
      "Validated batch 75 batch loss 1.28719044\n",
      "Validated batch 76 batch loss 1.2944206\n",
      "Validated batch 77 batch loss 1.23610389\n",
      "Validated batch 78 batch loss 1.30380809\n",
      "Validated batch 79 batch loss 1.1928854\n",
      "Validated batch 80 batch loss 1.28426921\n",
      "Validated batch 81 batch loss 1.27599359\n",
      "Validated batch 82 batch loss 1.18219018\n",
      "Validated batch 83 batch loss 1.20689392\n",
      "Validated batch 84 batch loss 1.27470088\n",
      "Validated batch 85 batch loss 1.21138752\n",
      "Validated batch 86 batch loss 1.4296031\n",
      "Validated batch 87 batch loss 1.28636944\n",
      "Validated batch 88 batch loss 1.19721842\n",
      "Validated batch 89 batch loss 1.31448364\n",
      "Validated batch 90 batch loss 1.26170993\n",
      "Validated batch 91 batch loss 1.17947674\n",
      "Validated batch 92 batch loss 1.27555251\n",
      "Validated batch 93 batch loss 1.36160445\n",
      "Validated batch 94 batch loss 1.23847127\n",
      "Validated batch 95 batch loss 1.22014594\n",
      "Validated batch 96 batch loss 1.22300828\n",
      "Validated batch 97 batch loss 1.18704379\n",
      "Validated batch 98 batch loss 1.26923\n",
      "Validated batch 99 batch loss 1.27191782\n",
      "Validated batch 100 batch loss 1.20943666\n",
      "Validated batch 101 batch loss 1.17236662\n",
      "Validated batch 102 batch loss 1.25798392\n",
      "Validated batch 103 batch loss 1.27841878\n",
      "Validated batch 104 batch loss 1.35188806\n",
      "Validated batch 105 batch loss 1.25414276\n",
      "Validated batch 106 batch loss 1.1681596\n",
      "Validated batch 107 batch loss 1.19763041\n",
      "Validated batch 108 batch loss 1.2848655\n",
      "Validated batch 109 batch loss 1.23141325\n",
      "Validated batch 110 batch loss 1.30257738\n",
      "Validated batch 111 batch loss 1.30516231\n",
      "Validated batch 112 batch loss 1.46463227\n",
      "Validated batch 113 batch loss 1.39005697\n",
      "Validated batch 114 batch loss 1.25996494\n",
      "Validated batch 115 batch loss 1.16183686\n",
      "Validated batch 116 batch loss 1.15230346\n",
      "Validated batch 117 batch loss 1.28180826\n",
      "Validated batch 118 batch loss 1.22777438\n",
      "Validated batch 119 batch loss 1.14967573\n",
      "Validated batch 120 batch loss 1.19490647\n",
      "Validated batch 121 batch loss 1.3656683\n",
      "Validated batch 122 batch loss 1.18191063\n",
      "Validated batch 123 batch loss 1.11204028\n",
      "Validated batch 124 batch loss 1.23744416\n",
      "Validated batch 125 batch loss 1.22787476\n",
      "Validated batch 126 batch loss 1.14351487\n",
      "Validated batch 127 batch loss 1.26500487\n",
      "Validated batch 128 batch loss 1.21361864\n",
      "Validated batch 129 batch loss 1.17514992\n",
      "Validated batch 130 batch loss 1.30675697\n",
      "Validated batch 131 batch loss 1.36948347\n",
      "Validated batch 132 batch loss 1.17207873\n",
      "Validated batch 133 batch loss 1.38409603\n",
      "Validated batch 134 batch loss 1.09753132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated batch 135 batch loss 1.14571846\n",
      "Validated batch 136 batch loss 1.19008875\n",
      "Validated batch 137 batch loss 1.22097886\n",
      "Validated batch 138 batch loss 1.39260173\n",
      "Validated batch 139 batch loss 1.263533\n",
      "Validated batch 140 batch loss 1.18239141\n",
      "Validated batch 141 batch loss 1.20352948\n",
      "Validated batch 142 batch loss 1.2018857\n",
      "Validated batch 143 batch loss 1.17679381\n",
      "Validated batch 144 batch loss 1.30832398\n",
      "Validated batch 145 batch loss 1.24037\n",
      "Validated batch 146 batch loss 1.36556935\n",
      "Validated batch 147 batch loss 1.32150865\n",
      "Validated batch 148 batch loss 1.3064847\n",
      "Validated batch 149 batch loss 1.25225556\n",
      "Validated batch 150 batch loss 1.38735127\n",
      "Validated batch 151 batch loss 1.29778409\n",
      "Validated batch 152 batch loss 1.33090115\n",
      "Validated batch 153 batch loss 1.32212079\n",
      "Validated batch 154 batch loss 1.4341712\n",
      "Validated batch 155 batch loss 1.33635116\n",
      "Validated batch 156 batch loss 1.18989253\n",
      "Validated batch 157 batch loss 1.24764013\n",
      "Validated batch 158 batch loss 1.33458173\n",
      "Validated batch 159 batch loss 1.35620987\n",
      "Validated batch 160 batch loss 1.20983875\n",
      "Validated batch 161 batch loss 1.27928841\n",
      "Validated batch 162 batch loss 1.19990766\n",
      "Validated batch 163 batch loss 1.17499304\n",
      "Validated batch 164 batch loss 1.29513288\n",
      "Validated batch 165 batch loss 1.24284017\n",
      "Validated batch 166 batch loss 1.22624016\n",
      "Validated batch 167 batch loss 1.43149352\n",
      "Validated batch 168 batch loss 1.107939\n",
      "Validated batch 169 batch loss 1.21993899\n",
      "Validated batch 170 batch loss 1.19149256\n",
      "Validated batch 171 batch loss 1.31043792\n",
      "Validated batch 172 batch loss 1.2798233\n",
      "Validated batch 173 batch loss 1.21409631\n",
      "Validated batch 174 batch loss 0.995042264\n",
      "Validated batch 175 batch loss 1.23177934\n",
      "Validated batch 176 batch loss 1.16879773\n",
      "Validated batch 177 batch loss 1.19024622\n",
      "Validated batch 178 batch loss 1.28410244\n",
      "Validated batch 179 batch loss 1.1058538\n",
      "Validated batch 180 batch loss 1.24639094\n",
      "Validated batch 181 batch loss 1.33080935\n",
      "Validated batch 182 batch loss 1.26297712\n",
      "Validated batch 183 batch loss 1.25267053\n",
      "Validated batch 184 batch loss 1.1372875\n",
      "Validated batch 185 batch loss 1.19854641\n",
      "Epoch 3 val loss 1.257332682609558\n",
      "Model /aiffel/aiffel/model_weight/GD08/model-epoch-3-loss-1.2573.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0d7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
